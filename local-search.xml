<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
  
  <entry>
    <title>强化学习仿真环境搭建</title>
    <link href="/2020/06/DRL-gym/"/>
    <url>/2020/06/DRL-gym/</url>
    
    <content type="html"><![CDATA[<h2 id="环境介绍"><a href="#环境介绍" class="headerlink" title="环境介绍"></a>环境介绍</h2><p>我们使用的仿真环境为OpenAI 的gym(<a href="https://github.com/openai/gym)。" target="_blank" rel="noopener">https://github.com/openai/gym)。</a></p><p>选用gym平台的原因:</p><ul><li>首先gym是OpenAI开发的通用强化学习算法测试平台,背后有大神 Pieter Abbeel、Sergey Levine 等人率领的强大团队的支持。</li><li>其次,学会了gym的基本应用,可以自己学习使用OpenAI的其他开源强化学习软件,如universe、roboschool 和baselines等。</li><li>再次,gym本身集成了很多仿真环境,如经典控制中的⻋摆环境,小⻋爬山环境、雅达利游戏、棋盘环境等。</li></ul><p>利用这些写好的环境,可以学习强化学习算法的基本原理。另外gym是用Python语言写的,可以和深度学习的开源软件如TensorFlow等无缝衔接。</p><h2 id="环境配置脚本"><a href="#环境配置脚本" class="headerlink" title="环境配置脚本"></a>环境配置脚本</h2><p>详见 <a href="https://github.com/sevenold/sevenold.github.io/blob/master/docs/gym_installer.sh" target="_blank" rel="noopener">gym_installer.sh</a>，下载地址：<a href="http://0520.tech/docs/gym_installer.sh">gym_installer.sh</a></p><ul><li><p>记录一个依赖库<code>mujoco-py</code>正常安装失败，非正常安装方式</p><pre><code class="hljs bash">$ git <span class="hljs-built_in">clone</span> -b 1.50.1.0  https://github.com/openai/mujoco-py.git$ <span class="hljs-built_in">cd</span> mujoco-py$ pip install -e .</code></pre></li></ul><h2 id="环境测试"><a href="#环境测试" class="headerlink" title="环境测试"></a>环境测试</h2><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20200611100315.png" srcset="/img/loading.gif" alt="image-20200611100313212"></p><p>如图，看到一个小车的倒立摆系统说明环境配置成功了。</p>]]></content>
    
    
    <categories>
      
      <category>深度强化学习</category>
      
    </categories>
    
    
    <tags>
      
      <tag>DRL</tag>
      
      <tag>强化学习</tag>
      
      <tag>gym，环境搭建</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>自然语言处理之命名实体识别</title>
    <link href="/2020/06/nlp-ner/"/>
    <url>/2020/06/nlp-ner/</url>
    
    <content type="html"><![CDATA[<h3 id="NLP之命名实体识别（NER）"><a href="#NLP之命名实体识别（NER）" class="headerlink" title="NLP之命名实体识别（NER）"></a>NLP之命名实体识别（NER）</h3><h4 id="实体概念"><a href="#实体概念" class="headerlink" title="实体概念"></a>实体概念</h4><blockquote><p><a href="https://link.zhihu.com/?target=https%3A//baike.baidu.com/item/%25E5%25AE%259E%25E4%25BD%2593/422661">百度百科</a>：实体(entity)指客观存在、并可相互区别的事物。实体可以是具体的人、事、物，也可以是概念。</p></blockquote><p><strong>【栗子】</strong></p><p>文本：我爱北京天安门</p><p>实体：北京 天安门</p><h4 id="命名实体"><a href="#命名实体" class="headerlink" title="命名实体"></a>命名实体</h4><blockquote><p>命名实体就是以<code>名称</code>为<code>标识</code>的实体。</p><p>通俗来讲：我们听到一个名字，就能知道这个东西是哪一个具体的事物，那么这个事物就是命名实体。</p><p>从编程语言的角度讲：类的一个实例，就是一个命名实体。</p></blockquote><p>【<strong>常见的命名实体</strong>】</p><blockquote><p>学术上NER所涉及的命名实体一般包括3大类（实体类，时间类，数字类）和7小类（人名、地名、组织机构名、时间、日期、货币、百分比）。</p></blockquote><table><thead><tr><th align="center">实体</th><th align="center">实体类型</th></tr></thead><tbody><tr><td align="center">2019年11月23日</td><td align="center">时间表达式</td></tr><tr><td align="center">成都</td><td align="center">地名</td></tr><tr><td align="center">元旦节</td><td align="center">节日</td></tr><tr><td align="center">张三，李四</td><td align="center">人名</td></tr><tr><td align="center">1,2,3,4,5</td><td align="center">数字</td></tr><tr><td align="center">无糖信息</td><td align="center">组织机构</td></tr></tbody></table><h4 id="命名实体的应用"><a href="#命名实体的应用" class="headerlink" title="命名实体的应用"></a>命名实体的应用</h4><blockquote><p>命名实体是现实世界里的事物，它们和现实世界相互作用、相互影响，因此命名实体在一些场景里特别重要。</p></blockquote><ul><li><p><strong>关系抽取</strong>：我们需要知道事物之间的关系，进而准确地决策</p></li><li><p><strong>摘要生成</strong>：简历往往比较详细，我们只需要通过只提取主要实体（如姓名、教育背景、技能等）来应用它来自动生成简历摘要，来进行简历自动汇总。</p></li><li><p><strong>优化搜索引擎算法</strong>：对文章运行一次NER模型，并永久存储与之相关的实体。然后，可以将搜索查询中的关键标记与与网站文章关联的标记进行比较，以实现快速高效的搜索。</p></li></ul><p>【<strong>总结</strong>】</p><p>NER任务是很多任务的基础，被广泛地应用到了以下的领域中：</p><ul><li><p>信息抽取</p></li><li><p>关系抽取</p></li><li><p>语法分析</p></li><li><p>信息检索</p></li><li><p>问答系统</p></li><li><p>机器翻译</p></li><li><p>…</p></li></ul><h4 id="命名实体标注"><a href="#命名实体标注" class="headerlink" title="命名实体标注"></a>命名实体标注</h4><blockquote><p>命名实体标注任务的流程图。我们将原始文本输入到NER工具里，该工具会输出带有命名实体标记的文本或者命名实体列表。</p></blockquote><p><strong>【标注工具】</strong></p><ul><li><p><a href="https://github.com/SophonPlus/ChineseAnnotator" target="_blank" rel="noopener"><strong>ChineseAnnotator</strong></a></p></li><li><p><strong><a href="https://doccano.herokuapp.com/demo/named-entity-recognition/" target="_blank" rel="noopener">Doccano</a></strong></p></li></ul><p><strong>【标注流程】</strong><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20191123180101.png" srcset="/img/loading.gif" alt="null"></p><p><strong>【标签体系】</strong></p><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20191123180304.png" srcset="/img/loading.gif" alt="null"></p><p><strong>【栗子】</strong></p><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20191123180540.png" srcset="/img/loading.gif" alt="null"></p><h4 id="命名实体识别"><a href="#命名实体识别" class="headerlink" title="命名实体识别"></a>命名实体识别</h4><p><strong>【基于nlpir】</strong></p><p><strong>【栗子】</strong></p><p> import pynlpir<br> pynlpir.open()  </p><p> msg = “张伟是四川成都人，身份证号是：123456700000000000”<br> print(pynlpir.segment(msg, pos_names=’all’))</p><blockquote><p>[(‘张伟’, ‘noun:personal name’), (‘是’, ‘verb:verb 是’), (‘四川’, ‘noun:toponym’), (‘成都’, ‘noun:toponym’), (‘人’, ‘noun’), (‘，’, ‘punctuation mark:comma’), (‘身份证’, ‘noun’), (‘号’, ‘classifier’), (‘是’, ‘verb:verb 是’), (‘：’, ‘punctuation mark:dash’), (‘123456700000000000’, ‘numeral’)]</p></blockquote><blockquote><p>其中：personal name为人名、toponym为地名， numeral为数字。</p></blockquote><p><strong>【基于深度学习算法】</strong></p><p><strong>算法：transformer+blstm+crf</strong></p><blockquote><p>content: 李文兴：1958年8月出生，男，中国国籍，博士研究生学历，本公司独立董事。</p><p>NER识别结果：  NAME: 李文兴  CONT 中国国籍  EDU: 博士研究生学历  ORGANIZATION: 本公司  TITLE: 独立董事</p></blockquote><p><strong>【其他算法】</strong></p><p>参考：<a href="https://github.com/chineseGLUE/chineseGLUE" target="_blank" rel="noopener">https://github.com/chineseGLUE/chineseGLUE</a></p><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20191124152701.png" srcset="/img/loading.gif" alt="null"></p><h4 id="相关工具参考"><a href="#相关工具参考" class="headerlink" title="相关工具参考"></a>相关工具参考</h4><table><thead><tr><th align="left"><strong>工具</strong></th><th align="left"><strong>简介</strong></th><th align="left"><strong>访问地址</strong></th></tr></thead><tbody><tr><td align="left">Stanford NER</td><td align="left">斯坦福大学开发的基于条件随机场的命名实体识别系统，该系统参数是基于CoNLL、MUC-6、MUC-7和ACE命名实体语料训练出来的。</td><td align="left"><a href="https://nlp.stanford.edu/software/CRF-NER.shtml" target="_blank" rel="noopener">官网</a></td></tr><tr><td align="left">MALLET</td><td align="left">麻省大学开发的一个统计自然语言处理的开源包，其序列标注工具的应用中能够实现命名实体识别。</td><td align="left"><a href="http://mallet.cs.umass.edu/" target="_blank" rel="noopener">官网</a></td></tr><tr><td align="left">Hanlp</td><td align="left">HanLP是一系列模型与算法组成的NLP工具包，由大快搜索主导并完全开源，目标是普及自然语言处理在生产环境中的应用。支持命名实体识别。</td><td align="left"><a href="http://hanlp.linrunsoft.com/" target="_blank" rel="noopener">官网</a></td></tr><tr><td align="left"><a href="https://easyai.tech/ai-definition/nltk/" target="_blank" rel="noopener">NLTK</a></td><td align="left">NLTK是一个高效的Python构建的平台,用来处理人类自然语言数据。</td><td align="left"><a href="http://www.nltk.org/" target="_blank" rel="noopener">官网</a></td></tr><tr><td align="left">SpaCy</td><td align="left">工业级的自然语言处理工具，遗憾的是不支持中文。</td><td align="left"><a href="https://spacy.io/" target="_blank" rel="noopener">官网</a></td></tr><tr><td align="left">Crfsuite</td><td align="left">可以载入自己的数据集去训练CRF实体识别模型。</td><td align="left"><a href="https://sklearn-crfsuite.readthedocs.io/en/latest/?badge=latest%0A%0A" target="_blank" rel="noopener">文档</a></td></tr></tbody></table>]]></content>
    
    
    <categories>
      
      <category>自然语言处理</category>
      
    </categories>
    
    
    <tags>
      
      <tag>深度学习</tag>
      
      <tag>自然语言处理</tag>
      
      <tag>命名实体识别</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>自然语言处理之文本数据增强</title>
    <link href="/2020/06/text_EDA/"/>
    <url>/2020/06/text_EDA/</url>
    
    <content type="html"><![CDATA[<h3 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h3><p>数据增强技术已经是图像领域的标配，通过对图像的翻转、旋转、镜像、高斯白噪声等技巧实现数据增强。对图像数据的增强不仅方法多种多样，而且像<code>keras</code>框在做数据预处理的时候已经集成了一些数据增强的方法可以直接调用。</p><p>相较于图像数据增强，文本数据增强，现在还是有很多问题的。往更严格的角度看，文本数据增强更像是同义句生成，但又不完全是，它是一个更大范围的概念。很多时候，需要文本数据增强，一个是常常遇到的数据不足，另一个就是数据不均衡。大方向上看，文本的数据增强有几种方式，<code>EDA</code>、<code>Back Translation</code>、<code>生成对抗网络</code>、<code>语境增强</code>。</p><h3 id="EDA"><a href="#EDA" class="headerlink" title="EDA"></a>EDA</h3><p><strong>ICLR 2019 workshop</strong> 论文<a href="https://arxiv.org/abs/1901.11196" target="_blank" rel="noopener">《EDA: Easy Data Augmentation Techniques for Boosting Performance on Text Classification Tasks》</a>介绍了几种NLP数据增强技术，并推出了<a href="https://github.com/jasonwei20/eda_nlp" target="_blank" rel="noopener">EDA github代码</a>。<code>EDA github repo</code>提出了四种简单的操作来进行数据增强，以防止过拟合，并提高模型的泛化能力。其实就是<code>EDA</code>的四种形式。</p><h4 id="同义词替换（SR-Synonyms-Replace）"><a href="#同义词替换（SR-Synonyms-Replace）" class="headerlink" title="同义词替换（SR: Synonyms Replace）"></a><strong>同义词替换（SR: Synonyms Replace）</strong></h4><blockquote><p>不考虑<code>stopwords</code>，在句子中随机抽取n个词，然后从同义词词典中随机抽取同义词，并进行替换。</p></blockquote><pre><code class="hljs python"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">synonym_replacement</span><span class="hljs-params">(words, n)</span>:</span>    new_words = words.copy()    random_word_list = list(set([word <span class="hljs-keyword">for</span> word <span class="hljs-keyword">in</span> words <span class="hljs-keyword">if</span> word <span class="hljs-keyword">not</span> <span class="hljs-keyword">in</span> stop_words]))    random.shuffle(random_word_list)    num_replaced = <span class="hljs-number">0</span>    <span class="hljs-keyword">for</span> random_word <span class="hljs-keyword">in</span> random_word_list:        synonyms_word = get_synonyms(random_word)        <span class="hljs-keyword">if</span> len(synonyms_word) &gt;= <span class="hljs-number">1</span>:            synonym = random.choice(synonyms_word)            new_words = [synonym <span class="hljs-keyword">if</span> word == random_word <span class="hljs-keyword">else</span> word <span class="hljs-keyword">for</span> word <span class="hljs-keyword">in</span> new_words]            num_replaced += <span class="hljs-number">1</span>        <span class="hljs-keyword">if</span> num_replaced &gt;= n:            <span class="hljs-keyword">break</span>    sentence = <span class="hljs-string">' '</span>.join(new_words)    new_words = sentence.split(<span class="hljs-string">' '</span>)    <span class="hljs-keyword">return</span> new_words<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">get_synonyms</span><span class="hljs-params">(word)</span>:</span>    <span class="hljs-keyword">return</span> synonyms.nearby(word)[<span class="hljs-number">0</span>]</code></pre><pre><code class="hljs python">input: 我要像风一样自由output: [<span class="hljs-string">'我要 像 风 一样 受限制'</span>, <span class="hljs-string">'我要 像 风 一样 公民权利'</span>, <span class="hljs-string">'我要 像 和风 一样 自由'</span>]</code></pre><h4 id="随机插入-RI-Randomly-Insert"><a href="#随机插入-RI-Randomly-Insert" class="headerlink" title="随机插入(RI: Randomly Insert)"></a><strong>随机插入(RI: Randomly Insert)</strong></h4><blockquote><p> 不考虑<code>stopwords</code>，随机抽取一个词，然后在该词的同义词集合中随机选择一个，插入原句子中的随机位置。该过程可以<strong>重复n次</strong>。</p></blockquote><pre><code class="hljs python"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">random_swap</span><span class="hljs-params">(words, n)</span>:</span>    new_words = words.copy()    <span class="hljs-keyword">for</span> _ <span class="hljs-keyword">in</span> range(n):        new_words = swap_word(new_words)    <span class="hljs-keyword">return</span> new_words<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">swap_word</span><span class="hljs-params">(new_words)</span>:</span>    random_idx_1 = random.randint(<span class="hljs-number">0</span>, len(new_words) - <span class="hljs-number">1</span>)    random_idx_2 = random_idx_1    counter = <span class="hljs-number">0</span>    <span class="hljs-keyword">while</span> random_idx_2 == random_idx_1:        random_idx_2 = random.randint(<span class="hljs-number">0</span>, len(new_words) - <span class="hljs-number">1</span>)        counter += <span class="hljs-number">1</span>        <span class="hljs-keyword">if</span> counter &gt; <span class="hljs-number">3</span>:            <span class="hljs-keyword">return</span> new_words    new_words[random_idx_1], new_words[random_idx_2] = new_words[random_idx_2], new_words[random_idx_1]    <span class="hljs-keyword">return</span> new_words</code></pre><pre><code class="hljs python">input: 我要像风一样自由output: [<span class="hljs-string">'我要 像 和风 风 一样 自由'</span>, <span class="hljs-string">'我要 像 风 公民权利 一样 自由'</span>, <span class="hljs-string">'我要 像 风 风 一样 自由'</span>]</code></pre><h4 id="随机交换-RS-Randomly-Swap"><a href="#随机交换-RS-Randomly-Swap" class="headerlink" title="随机交换(RS: Randomly Swap)"></a><strong>随机交换(RS: Randomly Swap)</strong></h4><blockquote><p> 句子中，随机选择两个词，位置交换。该过程可以<strong>重复n次</strong>。</p></blockquote><pre><code class="hljs python"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">random_swap</span><span class="hljs-params">(words, n)</span>:</span>    new_words = words.copy()    <span class="hljs-keyword">for</span> _ <span class="hljs-keyword">in</span> range(n):        new_words = swap_word(new_words)    <span class="hljs-keyword">return</span> new_words<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">swap_word</span><span class="hljs-params">(new_words)</span>:</span>    random_idx_1 = random.randint(<span class="hljs-number">0</span>, len(new_words) - <span class="hljs-number">1</span>)    random_idx_2 = random_idx_1    counter = <span class="hljs-number">0</span>    <span class="hljs-keyword">while</span> random_idx_2 == random_idx_1:        random_idx_2 = random.randint(<span class="hljs-number">0</span>, len(new_words) - <span class="hljs-number">1</span>)        counter += <span class="hljs-number">1</span>        <span class="hljs-keyword">if</span> counter &gt; <span class="hljs-number">3</span>:            <span class="hljs-keyword">return</span> new_words    new_words[random_idx_1], new_words[random_idx_2] = new_words[random_idx_2], new_words[random_idx_1]    <span class="hljs-keyword">return</span> new_words</code></pre><pre><code class="hljs python">input: 我要像风一样自由output: [<span class="hljs-string">'我要 一样 风 像 自由'</span>, <span class="hljs-string">'我要 风 像 一样 自由'</span>, <span class="hljs-string">'我要 像 自由 一样 风'</span>]</code></pre><h4 id="随机删除-RD-Randomly-Delete"><a href="#随机删除-RD-Randomly-Delete" class="headerlink" title="随机删除(RD: Randomly Delete)"></a><strong>随机删除(RD: Randomly Delete)</strong></h4><blockquote><p>句子中的每个词，以<strong>概率p</strong>随机删除。</p></blockquote><pre><code class="hljs python"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">random_deletion</span><span class="hljs-params">(words, p)</span>:</span>    <span class="hljs-keyword">if</span> len(words) == <span class="hljs-number">1</span>:        <span class="hljs-keyword">return</span> words    new_words = []    <span class="hljs-keyword">for</span> word <span class="hljs-keyword">in</span> words:        r = random.uniform(<span class="hljs-number">0</span>, <span class="hljs-number">1</span>)        <span class="hljs-keyword">if</span> r &gt; p:            new_words.append(word)    <span class="hljs-keyword">if</span> len(new_words) == <span class="hljs-number">0</span>:        rand_int = random.randint(<span class="hljs-number">0</span>, len(words) - <span class="hljs-number">1</span>)        <span class="hljs-keyword">return</span> [words[rand_int]]    <span class="hljs-keyword">return</span> new_words</code></pre><pre><code class="hljs python">input: 我要像风一样自由output: [<span class="hljs-string">'我要 像 风 自由'</span>, <span class="hljs-string">'我要 像 风 一样 自由'</span>, <span class="hljs-string">'我要 像 一样 自由'</span>]</code></pre><h3 id="Back-Translation"><a href="#Back-Translation" class="headerlink" title="Back Translation"></a>Back Translation</h3><p><code>回译（Back Translation）</code>是<strong>机器翻译</strong>中非常常用的数据增强的方式，其主要的思想就是<strong>通过翻译工具将一个句子翻译为另一种语言，再把这翻译的另一种语言再翻译为原来的语言，最后得到一个意思相近但表达方式不同的句子。</strong>这种方式也是目前相对靠谱的方式，这种方式不仅有同义词替换，词语增删，还具有对句子结构语序调整的效果，并还能保持与原句子意思相近，是目前一种非常有效的文本数据增强方式。</p><h3 id="生成对抗网络"><a href="#生成对抗网络" class="headerlink" title="生成对抗网络"></a>生成对抗网络</h3><p>随着<strong>深度学习</strong>的发展，这几年<code>生成对抗网络模型（GAN）以及它的各种变形</code>，通过生成器和判别器的相互博弈，以达到一个<strong>**纳</strong>什平衡**，不断迭代增强训练数据达到以假乱真的效果，最后用生成器大量生成同分布的数据，以达到数据增强的效果。但是GAN模型比较难训练，所以需要对GAN模型训练的足够好，才能更加有效的生成高质量的数据。</p>]]></content>
    
    
    <categories>
      
      <category>自然语言处理</category>
      
    </categories>
    
    
    <tags>
      
      <tag>深度学习</tag>
      
      <tag>自然语言处理</tag>
      
      <tag>数据增强</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>机器学习模型部署-simple-tensorflow-serving</title>
    <link href="/2019/06/serving/"/>
    <url>/2019/06/serving/</url>
    
    <content type="html"><![CDATA[<h3 id="simple-tensorflow-serving"><a href="#simple-tensorflow-serving" class="headerlink" title=" simple-tensorflow-serving"></a><a href="https://stfs.readthedocs.io/en/latest/index.html" target="_blank" rel="noopener"> simple-tensorflow-serving</a></h3><blockquote><p>TensorFlow Serving是一种灵活，高性能的机器学习模型服务系统，专为生产环境而设计。TensorFlow服务可以轻松部署新算法和实验，同时保持相同的服务器架构和API。TensorFlow Serving提供与TensorFlow模型的开箱即用集成，但可以轻松扩展以提供其他类型的模型和数据。</p></blockquote><p><code>官方提供的serving只支持TensorFlow相关的模型，但是这里介绍的simple-TensorFlow-serving  Support multiple models of TensorFlow/MXNet/PyTorch/Caffe2/CNTK/ONNX/H2o/Scikit-learn/XGBoost/PMML， 操作流程和官方的serving是一样的，很是简单。</code></p><h4 id="savedmodel格式模型保存"><a href="#savedmodel格式模型保存" class="headerlink" title="savedmodel格式模型保存"></a><strong>savedmodel格式模型保存</strong></h4><h5 id="【直接保存】"><a href="#【直接保存】" class="headerlink" title="【直接保存】"></a>【直接保存】</h5><pre><code class="hljs python">tf.saved_model.simple_save(sess,                                   <span class="hljs-string">"./model/1/"</span>,  <span class="hljs-comment"># 保存路径</span>                                   inputs=&#123;<span class="hljs-string">"myInput"</span>: inputs&#125;, <span class="hljs-comment"># 数据接口</span>                                   outputs=&#123;<span class="hljs-string">"myOutput"</span>: logitic&#125;)  <span class="hljs-comment"># 输出接口</span></code></pre><h5 id="【TensorFlow】"><a href="#【TensorFlow】" class="headerlink" title="【TensorFlow】"></a>【TensorFlow】</h5><h6 id="checkpoints转savedmodel"><a href="#checkpoints转savedmodel" class="headerlink" title="checkpoints转savedmodel"></a><strong>checkpoints转savedmodel</strong></h6><pre><code class="hljs python"><span class="hljs-keyword">import</span> tensorflow <span class="hljs-keyword">as</span> tf<span class="hljs-comment"># 定义输入和标签的placeholder</span>inputs = tf.placeholder(tf.float32, [<span class="hljs-literal">None</span>, <span class="hljs-number">32</span>, <span class="hljs-number">32</span>, <span class="hljs-number">3</span>])  <span class="hljs-comment"># 输入数据</span>logitic = ...  <span class="hljs-comment"># 输出结果</span>print(<span class="hljs-string">'TESTING....'</span>)<span class="hljs-comment"># <span class="hljs-doctag">TODO:</span> 保存模型</span>init = tf.global_variables_initializer()saver = tf.train.Saver()<span class="hljs-keyword">with</span> tf.Session() <span class="hljs-keyword">as</span> sess:    print(<span class="hljs-string">"Evaluate The Model"</span>)    sess.run(init)    <span class="hljs-comment"># <span class="hljs-doctag">TODO:</span> 读取模型</span>    saver.restore(sess, <span class="hljs-string">'./model/cifar.ckpt'</span>)  <span class="hljs-comment"># 加载checkpoint模型</span>    <span class="hljs-comment"># Initialize v1 since the saver will not.</span>    tf.saved_model.simple_save(        sess,        <span class="hljs-string">"./savedmodel/2/"</span>,        inputs=&#123;<span class="hljs-string">"image"</span>: inputs&#125;,        outputs=&#123;<span class="hljs-string">"scores"</span>: logitic&#125;    )</code></pre><h6 id="pb转savedmodel"><a href="#pb转savedmodel" class="headerlink" title=".pb转savedmodel"></a><strong>.pb转savedmodel</strong></h6><pre><code class="hljs python"><span class="hljs-keyword">import</span> tensorflow <span class="hljs-keyword">as</span> tf<span class="hljs-keyword">from</span> tensorflow.python.saved_model <span class="hljs-keyword">import</span> signature_constants<span class="hljs-keyword">from</span> tensorflow.python.saved_model <span class="hljs-keyword">import</span> tag_constantsexport_dir = <span class="hljs-string">'./saved/1'</span>  <span class="hljs-comment"># 保存路径</span>graph_pb = <span class="hljs-string">'freezed.pb'</span>  <span class="hljs-comment"># 导入模型文件</span>builder = tf.saved_model.builder.SavedModelBuilder(export_dir)<span class="hljs-keyword">with</span> tf.gfile.GFile(graph_pb, <span class="hljs-string">"rb"</span>) <span class="hljs-keyword">as</span> f:    graph_def = tf.GraphDef()    graph_def.ParseFromString(f.read())sigs = &#123;&#125;<span class="hljs-keyword">with</span> tf.Session(graph=tf.Graph()) <span class="hljs-keyword">as</span> sess:    <span class="hljs-comment"># name="" is important to ensure we don't get spurious prefixing</span>    tf.import_graph_def(graph_def, name=<span class="hljs-string">""</span>)    g = tf.get_default_graph()    inp = g.get_tensor_by_name(<span class="hljs-string">"input:0"</span>)  <span class="hljs-comment"># 输入数据</span>    out = g.get_tensor_by_name(<span class="hljs-string">"output:0"</span>) <span class="hljs-comment"># 输出结果</span>    sigs[signature_constants.DEFAULT_SERVING_SIGNATURE_DEF_KEY] = \        tf.saved_model.signature_def_utils.predict_signature_def(            &#123;<span class="hljs-string">"in"</span>: inp&#125;, &#123;<span class="hljs-string">"out"</span>: out&#125;)    builder.add_meta_graph_and_variables(sess,                                         [tag_constants.SERVING],                                         signature_def_map=sigs)builder.save()</code></pre><h5 id="【Keras】"><a href="#【Keras】" class="headerlink" title="【Keras】"></a>【Keras】</h5><h6 id="h5转savedmodel"><a href="#h5转savedmodel" class="headerlink" title=".h5转savedmodel"></a><strong>.h5转savedmodel</strong></h6><pre><code class="hljs python"><span class="hljs-keyword">import</span> tensorflow <span class="hljs-keyword">as</span> tf<span class="hljs-keyword">with</span> tf.device(<span class="hljs-string">"/cpu:0"</span>):    model = tf.keras.models.load_model(<span class="hljs-string">'./model.h5'</span>)  <span class="hljs-comment"># 导入模型文件</span>    tf.saved_model.simple_save(      tf.keras.backend.get_session(),      <span class="hljs-string">"h5_savedmodel/1/"</span>,  <span class="hljs-comment"># 保存路径</span>      inputs=&#123;<span class="hljs-string">"image"</span>: model.input&#125;, <span class="hljs-comment"># 输入数据</span>      outputs=&#123;<span class="hljs-string">"scores"</span>: model.output&#125; <span class="hljs-comment"># 输出结果</span>    )print(<span class="hljs-string">'end'</span>)</code></pre><h4 id="模型部署"><a href="#模型部署" class="headerlink" title="模型部署"></a><strong>模型部署</strong></h4><pre><code class="hljs python">simple_tensorflow_serving --model_base_path=<span class="hljs-string">"./models/"</span></code></pre><h4 id="生成客户端代码"><a href="#生成客户端代码" class="headerlink" title="生成客户端代码"></a>生成客户端代码</h4><pre><code class="hljs shell">curl http://localhost:8500/v1/models/default/gen_client?language=python &gt; client.py</code></pre><h3 id="查看模型接口"><a href="#查看模型接口" class="headerlink" title="查看模型接口"></a>查看模型接口</h3><pre><code class="hljs shell">saved_model_cli show --dir saved_model/0 --tag_set serve --signature_def serving_default</code></pre><pre><code class="hljs shell">The given SavedModel SignatureDef contains the following input(s):  inputs['input'] tensor_info:      dtype: DT_FLOAT      shape: (1, -1, 161, 1)      name: Placeholder:0The given SavedModel SignatureDef contains the following output(s):  outputs['output'] tensor_info:      dtype: DT_FLOAT      shape: (-1, -1, 2599)      name: dense/BiasAdd:0Method name is: tensorflow/serving/predict</code></pre>]]></content>
    
    
    <categories>
      
      <category>TensorFlow</category>
      
    </categories>
    
    
    <tags>
      
      <tag>深度学习</tag>
      
      <tag>tensorflow</tag>
      
      <tag>serving</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>【RCNN系列】Faster RCNN</title>
    <link href="/2019/04/object-detection-FasterRCNN/"/>
    <url>/2019/04/object-detection-FasterRCNN/</url>
    
    <content type="html"><![CDATA[<h3 id="论文原文"><a href="#论文原文" class="headerlink" title="论文原文"></a>论文原文</h3><p>链接：<a href="https://arxiv.org/abs/1506.01497" target="_blank" rel="noopener">Faster R-CNN</a></p><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20190415191821.png" srcset="/img/loading.gif" alt=""></p><h3 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h3><p>​    最先进的目标检测网络依靠区域提出算法来假设目标的位置。<code>SPPnet</code>和<code>Fast R-CNN</code>等研究已经减少了这些检测网络的运行时间，使得区域提出计算成为一个瓶颈。在这项工作中，我们引入了一个<code>区域提出网络（RPN）</code>，该网络与检测网络共享全图像的卷积特征，从而使近乎零成本的区域提出成为可能。RPN是一个全卷积网络，可以同时在每个位置预测目标边界和目标分数。RPN经过端到端的训练，可以生成高质量的区域提出，由Fast R-CNN用于检测。我们将RPN和Fast R-CNN通过共享卷积特征进一步合并为一个单一的网络——使用最近流行的具有“注意力”机制的神经网络术语，RPN组件告诉统一网络在哪里寻找。对于非常深的VGG-16模型，我们的检测系统在GPU上的帧率为5fps（包括所有步骤），同时在PASCAL VOC 2007，2012和MS COCO数据集上实现了最新的目标检测精度，每个图像只有300个提出。</p><h3 id="网络结构"><a href="#网络结构" class="headerlink" title="网络结构"></a>网络结构</h3><ul><li>主干网络：13Conv+13relu+4pooling</li><li>RPN：3x3+背景前景区分+初步定位</li><li>ROIPooling</li><li>分类+位置精确定位</li></ul><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20190415195258.png" srcset="/img/loading.gif" alt=""></p><h4 id="【VGG-Model】"><a href="#【VGG-Model】" class="headerlink" title="【VGG-Model】"></a>【VGG-Model】</h4><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20190415202356.png" srcset="/img/loading.gif" alt=""></p><h4 id="【RPN】"><a href="#【RPN】" class="headerlink" title="【RPN】"></a>【RPN】</h4><p>​    经典的检测方法生成检测框都非常耗时, 如RCNN使用SS(Selective Search)方法生成检测框。而Faster RCNN则抛弃了传统的滑动窗口和SS方法，直接使用RPN生成检测框，这也是Faster RCNN的巨大优势，能极大提升检测框的生成速度。</p><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20190415202436.png" srcset="/img/loading.gif" alt=""></p><p>上图展示了RPN网络的具体结构。可以看到RPN网络实际分为2条线</p><ul><li>粗分类：上面一条通过softmax分类anchors获得foreground和background（检测目标是foreground）</li><li>粗定位：下面一条用于计算对于anchors的bounding box regression偏移量，以获得粗略的proposal。</li></ul><p>而最后的Proposal层则负责综合foreground anchors和bounding box regression偏移量获取proposals，同时剔除太小和超出边界的proposals。其实整个网络到了Proposal Layer这里，就完成了相当于目标定位的功能。</p><h4 id="【anchors】"><a href="#【anchors】" class="headerlink" title="【anchors】"></a>【anchors】</h4><p>​    首先使用一个(3, 3)的sliding window在feature map上滑动(就是做卷积，卷积的本质就是卷积核滑动求和)，然后取(3, 3)滑窗的中心点，将这个点对应到原图上，然后取三种不同的尺寸(128, 256, 512)和三种比例(1:1, 2:1, 1:2)在原图上产生9个anchor，所以feature map上的每个点都会在原图上对应点的位置生成9个anchor，而我们从上边的分析已经得知feature map的大小为输入图片的1/16倍，所以最终一共有 $\frac m {16} \times  \frac n {16} \times 9$ 个anchor。</p><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20190612191124.png" srcset="/img/loading.gif" alt=""></p><h3 id="训练流程"><a href="#训练流程" class="headerlink" title="训练流程"></a>训练流程</h3><p>和之前的目标检测网络训练类似，都是在一个预训练网络的基础上进行训练的。一般使用4-step交替训练法来训练Faster R-CNN，主要步骤如下：</p><ol><li>在ImageNet预训练的网络来训练RPN网络。</li><li>使用ImageNet预训练的网络来训练一个单独Fast R-CNN检测网络，网络中的proposal是第一步训练RPN网络中得到的porposal。</li><li>使用上一步训练的detection网络的参数来初始化练RPN网络，并且在训练过程中固定shared conv layers这部分，只更新RPN网络专有的那些层。</li><li>固定shared conv layers这部分，只更新Fast R-CNN专有的那些层，其中proposal也是上一步训练RPN网络中得到的proposal。</li></ol><h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><table><thead><tr><th align="center"><strong>使用方法</strong></th><th align="center"><strong>缺点</strong></th><th><strong>改进</strong></th><th></th></tr></thead><tbody><tr><td align="center"><code>R-CNN</code></td><td align="center">1、SS提取RP；2、CNN提取特征；3、SVM分类；4、BB盒回归。</td><td>1、 训练步骤繁琐（微调网络+训练SVM+训练bbox）；  2、 训练、测试均速度慢 ；3、 训练占空间</td><td>1、 从DPM HSC的34.3%直接提升到了66%（mAP）；2、 引入RP+CNN</td></tr><tr><td align="center"><code>Fast R-CNN</code></td><td align="center">1、SS提取RP；2、CNN提取特征；3、softmax分类；4、多任务损失函数边框回归。</td><td>1、 依旧用SS提取RP(耗时2-3s，特征提取耗时0.32s)；2、 无法满足实时应用，没有真正实现端到端训练测试；3、 利用了GPU，但是区域建议方法是在CPU上实现的。</td><td>1、 由66.9%提升到70%；2、 每张图像耗时约为3s。</td></tr><tr><td align="center"><code>Faster R-CNN</code></td><td align="center">1、RPN提取RP；2、CNN提取特征；3、softmax分类；4、多任务损失函数边框回归。</td><td>1、 还是无法达到实时检测目标；2、 获取region proposal，再对每个proposal分类计算量还是比较大。</td><td>1、 提高了检测精度和速度；2、  真正实现端到端的目标检测框架；3、  生成建议框仅需约10ms。</td></tr></tbody></table>]]></content>
    
    
    <categories>
      
      <category>目标检测</category>
      
    </categories>
    
    
    <tags>
      
      <tag>深度学习</tag>
      
      <tag>Faster RCNN</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>【RCNN系列】Fast RCNN</title>
    <link href="/2019/04/object-detection-FastRCNN/"/>
    <url>/2019/04/object-detection-FastRCNN/</url>
    
    <content type="html"><![CDATA[<h3 id="论文原文"><a href="#论文原文" class="headerlink" title="论文原文"></a>论文原文</h3><p>链接：<a href="https://arxiv.org/pdf/1504.08083.pdf" target="_blank" rel="noopener">Fast R-CNN</a></p><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20190413162602.png" srcset="/img/loading.gif" alt=""></p><blockquote><ul><li><p>RCNN通过卷积神经网络提取图像特征，第一次将目标检测引入了深度学习领域。</p></li><li><p>SPPNet通过空间金字塔池化，避免了对于同一幅图片多次提取特征的时间花费。</p></li><li><p>但是无论是RCNN还是SPPNet，其训练都是多阶段的。</p><ul><li>首先通过ImageNet预训练网络模型，</li><li>然后通过检测数据集微调模型提取每个区域候选的特征，</li><li>之后通过SVM分类每个区域候选的种类，</li><li>最后通过区域回归，精细化每个区域的具体位置。</li></ul></li><li><p>为了避免多阶段训练，同时在单阶段训练中提升识别准确率，Fast RCNN提出了多任务目标函数，将SVM分类以及区域回归的部分纳入了卷积神经网络中。</p></li></ul></blockquote><h3 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h3><p>论文提出了一种快速的基于区域的卷积网络方法（fast R-CNN）用于目标检测。Fast R-CNN建立在以前使用的深卷积网络有效地分类目标的成果上。相比于之前的成果，Fast R-CNN采用了多项创新提高训练和测试速度来提高检测精度。Fast R-CNN训练非常深的VGG16网络比R-CNN快9倍，测试时间快213倍，并在PASCAL VOC上得到更高的精度。与SPPnet相比，fast R-CNN训练VGG16网络比他快3倍，测试速度快10倍，并且更准确。</p><h3 id="Fast-RCNN介绍"><a href="#Fast-RCNN介绍" class="headerlink" title="Fast RCNN介绍"></a>Fast RCNN介绍</h3><ul><li>结合<code>SPP-Net</code>改进RCNN<ul><li><code>ROI Pooling</code>：单层<code>SPP-Net</code></li></ul></li><li>多任务网络同时解决分类和位置回归<ul><li>共享卷积特征</li></ul></li><li>为<code>Faster RCNN</code>的提出打下基础，提供了可能。</li></ul><h3 id="网络结构"><a href="#网络结构" class="headerlink" title="网络结构"></a>网络结构</h3><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20190412205305.png" srcset="/img/loading.gif" alt=""></p><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20190412205853.png" srcset="/img/loading.gif" alt=""></p><h3 id="ROI池化层"><a href="#ROI池化层" class="headerlink" title="ROI池化层"></a>ROI池化层</h3><blockquote><ul><li>池化层的一种</li><li>目的就是为了将proposal抠出来的图像，然后resize到统一的尺寸。</li><li>操作流程如下：<ol><li>根据输入的image，将ROI映射到feature map对应的位置</li><li>将映射后的区域划分为相同大小的sections（sections数量和输出的维度相同）</li><li>对每个sections进行max pooling的操作。</li></ol></li></ul></blockquote><h3 id="工作流程"><a href="#工作流程" class="headerlink" title="工作流程"></a>工作流程</h3><blockquote><ul><li><strong>输入</strong>：输入包括一张图片(224*244)和object proposal的集合(2000个)，应该还是采用selective search来提取的。</li><li><strong>卷积层</strong>：图片经过多个conv layer和max pooling layer得到整张图的feature map。</li><li><strong>RoI pooling layer</strong>：这层是借鉴SPPnet的SPP layer层，输入feature map和object proposal，然后为每个object proposal来提取在feature map上对应的特征，并使得输出都具有相同的size。</li><li><strong>全连接层</strong>：将提取的特征输入两个全连接层(4096)，把前边提取的特征综合起来</li><li><strong>输出</strong>：Fast R-CNN的输出包括两部分：<ul><li>一个是经过FC layer(21个node)+softmax输出的，主要是对object proposal进行分类的，一共包括21类，即20类物体加上1个背景类。这个就取代了之前的SVM分类器。</li><li>另一个是经过FC layer(84个node)+bbox regressor输出的，这个就是为21个类各输出4个值(21*4=84)，而这4个值代表着精修的bounding box position。</li></ul></li></ul></blockquote><h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><h4 id="【优点】"><a href="#【优点】" class="headerlink" title="【优点】"></a>【优点】</h4><blockquote><ol><li>比R-CNN和SPPnet具有更高的目标检测精度（mAP）。</li><li>训练是使用多任务损失的单阶段训练。</li><li>训练可以更新所有网络层参数。</li><li>不需要磁盘空间缓存特征。</li></ol></blockquote><h4 id="【缺点】"><a href="#【缺点】" class="headerlink" title="【缺点】"></a>【缺点】</h4><blockquote><ul><li>虽然速度变得更快的，但是还是存在瓶颈：选择性搜索，在找出所有的候选框十分耗时间。</li></ul></blockquote>]]></content>
    
    
    <categories>
      
      <category>目标检测</category>
      
    </categories>
    
    
    <tags>
      
      <tag>深度学习</tag>
      
      <tag>Fast RCNN</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>【RCNN系列】SPPNet</title>
    <link href="/2019/04/object-detection-SPPNet/"/>
    <url>/2019/04/object-detection-SPPNet/</url>
    
    <content type="html"><![CDATA[<h3 id="论文原文"><a href="#论文原文" class="headerlink" title="论文原文"></a>论文原文</h3><p>链接：<a href="https://arxiv.org/pdf/1406.4729.pdf" target="_blank" rel="noopener">Spatial Pyramid Pooling in Deep Convolutional Networks for Visual Recognition</a></p><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20190413162003.png" srcset="/img/loading.gif" alt=""></p><blockquote><p>这篇paper，是在R-CNN的基础上提出了空间金字塔变换层(Spatial Pyramid Pooling)，SPPNet大幅度提高了R-CNN的训练速度和测试速度，同时算法的精度也上升了.</p></blockquote><h3 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h3><p>沿着上一篇RCNN的思路，我们继续探索目标检测的痛点，其中RCNN使用CNN作为特征提取器，首次使得目标检测跨入深度学习的阶段。但是RCNN对于每一个区域候选都需要首先将图片放缩到固定的尺寸（224*224），然后为每个区域候选提取CNN特征。容易看出这里面存在的一些性能瓶颈：</p><ul><li>速度瓶颈：重复为每个region proposal提取特征是极其费时的，Selective Search对于每幅图片产生2K左右个region proposal，也就是意味着一幅图片需要经过2K次的完整的CNN计算得到最终的结果。</li><li>性能瓶颈：对于所有的region proposal防缩到固定的尺寸会导致我们不期望看到的几何形变，而且由于速度瓶颈的存在，不可能采用多尺度或者是大量的数据增强去训练模型。</li></ul><h3 id="为什么CNN需要固定的输入呢？"><a href="#为什么CNN需要固定的输入呢？" class="headerlink" title="为什么CNN需要固定的输入呢？"></a>为什么CNN需要固定的输入呢？</h3><p>CNN网络可以分解为卷积网络部分以及全连接网络部分。我们知道卷积网络的参数主要是卷积核，完全能够适用任意大小的输入，并且能够产生任意大小的输出。但是全连接层部分不同，全连接层部分的参数是神经元对于所有输入的连接权重，也就是说输入尺寸不固定的话，全连接层参数的个数都不能固定。</p><h3 id="解决方案"><a href="#解决方案" class="headerlink" title="解决方案"></a>解决方案</h3><h4 id="【R-CNN模型】"><a href="#【R-CNN模型】" class="headerlink" title="【R-CNN模型】"></a>【R-CNN模型】</h4><blockquote><p>将侯选区域送到CNN里面提取特征向量时，因为CNN的输入图像需要固定大小，而候选区域的长宽都是不固定的，<strong>故需要对候选区域做填充到固定大小，当我们对侯选区域做cropping或者warping操作，可能会让图片不完整包含物体，一些我们不想见到的几何失真，这都会造成识别精度损失。</strong></p></blockquote><h4 id="【SPPNet】"><a href="#【SPPNet】" class="headerlink" title="【SPPNet】"></a>【SPPNet】</h4><blockquote><p>SPPNet的解决办法是<strong>使用“空间金字塔变换层”将接收任意大小的图像输入，输出固定长度的输出向量</strong>，这样就能让SPPNet可接受任意大小的输入图片，不需要对图像做crop/wrap操作。</p></blockquote><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20190412193027.png" srcset="/img/loading.gif" alt=""></p><blockquote><p>上面这个图可以看出SPPnet和RCNN的区别，首先是输入不需要放缩到指定大小。其次是增加了一个空间金字塔池化层，还有最重要的一点是每幅图片只需要提取一次特征。</p></blockquote><h3 id="R-CNN模型候选区域在CNN重复计算"><a href="#R-CNN模型候选区域在CNN重复计算" class="headerlink" title="R-CNN模型候选区域在CNN重复计算"></a>R-CNN模型候选区域在CNN重复计算</h3><h4 id="【R-CNN模型】-1"><a href="#【R-CNN模型】-1" class="headerlink" title="【R-CNN模型】"></a>【R-CNN模型】</h4><blockquote><p>在R-CNN中，每个候选区域都要塞到CNN内提取特征向量，一张图片有2000个候选区域，<strong>也就是一张图片需要经过2000次CNN的前向传播，这2000重复计算过程会有大量的计算冗余，耗费大量的时间。</strong></p></blockquote><h4 id="【SPPNet】-1"><a href="#【SPPNet】-1" class="headerlink" title="【SPPNet】"></a>【SPPNet】</h4><blockquote><p><strong>SPPNet提出了一种从候选区域到全图的特征映射(feature map)之间的对应关系，通过此种映射关系可以直接获取到候选区域的特征向量</strong>，不需要重复使用CNN提取特征，从而大幅度缩短训练时间。</p></blockquote><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20190412200946.png" srcset="/img/loading.gif" alt=""></p><h3 id="网络结构"><a href="#网络结构" class="headerlink" title="网络结构"></a>网络结构</h3><h4 id="【SPPNet】-2"><a href="#【SPPNet】-2" class="headerlink" title="【SPPNet】"></a>【SPPNet】</h4><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20190412191807.png" srcset="/img/loading.gif" alt=""></p><blockquote><p>完整的网络结构如上图所示，因为卷积层输入的任意大小的图片，所以Conv5计算出的feature map也是任意大小的，现在经过SPP之后，就可以变成固定大小的输出了，以上图为例，一共可以输出（16+4+1）*256的特征，16+4+1表示空间盒的数量（Spatial bins），256则表示卷积核的数量。</p></blockquote><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20190412201553.png" srcset="/img/loading.gif" alt=""></p><h4 id="【空间金字塔池化层-SPP】"><a href="#【空间金字塔池化层-SPP】" class="headerlink" title="【空间金字塔池化层-SPP】"></a>【空间金字塔池化层-SPP】</h4><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20190412200232.png" srcset="/img/loading.gif" alt=""></p><blockquote><p>空间金字塔就是以不同大小的块来对图片提取特征，分别是4*4，2*2，1*1大小的块，将这三张网格放到下面这张特征图上，就可以得到16+4+1=21种不同的切割方式，分别在每一个区域取<code>最大池化</code>，那么就可以得到21组特征。这种以不同的大小格子的组合方式来池化的过程就是空间金字塔池化（SPP）。 </p></blockquote><h3 id="训练流程"><a href="#训练流程" class="headerlink" title="训练流程"></a>训练流程</h3><h4 id="【基本流程】"><a href="#【基本流程】" class="headerlink" title="【基本流程】"></a>【基本流程】</h4><blockquote><ul><li>候选区域生成： 一张图像生成2K个候选区域 （采用Selective Search 方法）</li><li>特征提取：将卷积层的最后一层换成空间金字塔池化（Spatial Pyramid Pooling)<ul><li>把一个图像区域分成了16个块，也就是每个块的大小就是(w/4,h/4)；</li><li>再把这个图像区域划分了4个块，每个块的大小就是(w/2,h/2)；</li><li>再把这个图像区域作为了一个块，也就是块的大小为(w,h)；</li><li>对这些块都采用maxpooling，得到21值，然后再送入全连接层。</li><li>所以不管输入图像的大小是多少，给全连接层的都是21个值。这样解决了不同的size的特征图统一size的问题. </li><li>最后提取出特征</li></ul></li><li>类别判断： 特征送入每一类的SVM 分类器，判别是否属于该类 </li><li>位置修正：回归器校正非极大值抑制后剩下的region proposal </li></ul></blockquote><h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><blockquote><p>【优点】</p><ul><li>SPP-net 采用SPP技术解决了图像固定size的问题,对图片分类的准确度有了一定程度的提高</li><li>采用共享卷积层也一定程度上提高了模型的训练和测试的运行时间</li></ul><p>【不足】</p><ul><li>在Spatial Pyramid Poolin时,由于他分为了不同的块,反向传播梯度的时候,无法将梯度传递到conv层,使得训练时 conv层的参数不能进行更新.训练速度并不是特别的快</li><li>提取region proposal部分依然用的是selective search,分类器也是用了SVM</li></ul></blockquote>]]></content>
    
    
    <categories>
      
      <category>目标检测</category>
      
    </categories>
    
    
    <tags>
      
      <tag>深度学习</tag>
      
      <tag>SPPNet</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>【RCNN系列】R-CNN</title>
    <link href="/2019/04/object-detection-RCNN/"/>
    <url>/2019/04/object-detection-RCNN/</url>
    
    <content type="html"><![CDATA[<h3 id="论文原文"><a href="#论文原文" class="headerlink" title="论文原文"></a>论文原文</h3><p>链接：<a href="https://arxiv.org/abs/1311.2524" target="_blank" rel="noopener">Rich feature hierarchies for accurate object detection and semantic segmentation</a></p><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20190413161846.png" srcset="/img/loading.gif" alt=""></p><blockquote><p>​    RCNN作为第一篇目标检测领域的深度学习文章，大幅提升了目标检测的识别精度，在PASCAL VOC2012数据集上将MAP从35.1%提升至53.7%。使得CNN在目标检测领域成为常态，也使得大家开始探索CNN在其他计算机视觉领域的巨大潜力。这篇文章的创新点有以下几点：将CNN用作目标检测的特征提取器、有监督预训练的方式初始化CNN、在CNN特征上做BoundingBox 回归。</p></blockquote><h3 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h3><p>​    从ILSVRC 2012、2013表现结果来看，CNN在计算机视觉的特征表示能力要远高于传统的HOG、SIFT特征等，而且这种层次的、多阶段、逐步细化的特征计算方式也更加符合人类的认知习惯。但是，如何将在目标检测领域重现这种奇迹呢？目标检测区别于目标识别很重要的一点是其需要目标的具体位置，也就是BoundingBox。而产生BoundingBox最简单的方法就是滑窗，可以在卷积特征上滑窗。</p><p>​    但是我们知道CNN是一个层次的结构，随着网络层数的加深，卷积特征的步伐及感受野也越来越大。例如AlexNet的Pool5层感受野为195*195，步伐为32*32，显然这种感受野是不足以进行目标定位的。使用浅层的神经网络能够保留这种空间结构，但是特征提取的性能就会大打折扣。</p><p>​    RCNN另辟蹊径，既然我们无法使用卷积特征滑窗，那我们通过区域建议方法产生一系列的区域，然后直接使用CNN去分类这些区域是目标还是背景不就可以吗？当然这样做也会面临很多的问题，不过这个思路正是RCNN的核心。因此RCNN全称为Regions with CNN features。</p><h3 id="网络结构"><a href="#网络结构" class="headerlink" title="网络结构"></a>网络结构</h3><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20190412134446.png" srcset="/img/loading.gif" alt=""></p><h3 id="结构流程"><a href="#结构流程" class="headerlink" title="结构流程"></a>结构流程</h3><h4 id="【基本流程】"><a href="#【基本流程】" class="headerlink" title="【基本流程】"></a>【<code>基本流程</code>】</h4><blockquote><p>RCNN算法分为4个步骤 </p><ol><li>候选区域生成： 一张图像生成2K个候选区域 （采用Selective Search 方法）</li><li>统一候选区域尺寸：将每个区域固定成227*227的尺寸送入CNN进行特征提取</li><li>特征提取： 对每个候选区域，使用深度卷积网络提取特征 （CNN） </li><li>类别判断： 特征送入每一类的SVM 分类器，判别是否属于该类 </li><li>位置修正：回归器校正非极大值抑制后剩下的region proposal </li></ol></blockquote><h3 id="【region-proposals】"><a href="#【region-proposals】" class="headerlink" title="【region proposals】"></a>【<strong><code>region proposals</code></strong>】</h3><ul><li><p>输入一张多目标图像，采用<code>selective search</code>算法提取约<code>2000个region proposals</code></p><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20190412150021.png" srcset="/img/loading.gif" alt=""></p></li><li><p><code>selective search</code></p><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20190412135458.png" srcset="/img/loading.gif" alt=""></p></li><li><p>算法流程</p><blockquote><ul><li><p>使用<a href="http://cs.brown.edu/people/pfelzens/segment/" target="_blank" rel="noopener">Efficient GraphBased Image Segmentation算法</a>分割图像得到多个region.</p></li><li><p>计算每两个region的相似度.</p></li><li><p>合并相似度最大的两个region.</p></li><li><p>计算新合并region与其他region的相似度,合并相似度最大的两个region.</p></li><li><p>重复上述过程直到整张图片都聚合成一个大的region.</p></li><li><p>使用一种随机的计分方式给每个region打分，按照分数进行ranking，取出top k的子集，就是selective search的结果(k=2000)</p><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20190412192802.png" srcset="/img/loading.gif" alt=""></p></li></ul></blockquote></li></ul><h3 id="【crop】"><a href="#【crop】" class="headerlink" title="【crop】"></a>【<strong><code>crop</code></strong>】</h3><ul><li><p>在每个region proposal周围加上16个像素值为region proposal像素平均值的边框，再直接变形为227×227的size</p><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20190412135738.png" srcset="/img/loading.gif" alt=""></p><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20190412135755.png" srcset="/img/loading.gif" alt=""></p></li><li><p>作者提出了两种形变方式</p><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20190412145017.png" srcset="/img/loading.gif" alt=""></p><blockquote><ul><li><p>各向异性缩放</p><p>各向异性缩放就是,不管图像是否扭曲,直接将图像缩放到固定大小,如图D所示.</p></li><li><p>各向同性缩放</p><ul><li>将图像在原图中沿着bounding box进行扩展,如果已经延伸到了原始图片的外边界，那么就用bounding box中的颜色均值填充,如图B所示.</li><li>将图像用固定的背景填充,如图C所示.</li></ul></li></ul></blockquote></li></ul><h3 id="【Compute-CNN-Feature】"><a href="#【Compute-CNN-Feature】" class="headerlink" title="【Compute CNN Feature】"></a>【<strong><code>Compute CNN Feature</code></strong>】</h3><h4 id="【预处理操作】"><a href="#【预处理操作】" class="headerlink" title="【预处理操作】"></a>【预处理操作】</h4><ul><li>将所有region proposal像素减去该region proposal像素平均值，</li></ul><h4 id="【AlexNet-CNN】"><a href="#【AlexNet-CNN】" class="headerlink" title="【AlexNet CNN】"></a>【AlexNet CNN】</h4><ul><li>再依次将每个227×227的region proposal输入<code>AlexNet CNN</code>网络获取4096维的特征【比以前的人工经验特征低两个数量级】，2000个建议框的CNN特征组合成<code>2000×4096</code>维矩阵；</li></ul><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20190412141355.png" srcset="/img/loading.gif" alt=""></p><h3 id="【linear-classifiers】"><a href="#【linear-classifiers】" class="headerlink" title="【linear classifiers】"></a>【<code>linear classifiers</code>】</h3><ul><li>将2000×4096维特征与20个SVM组成的权值矩阵4096×20相乘【20种分类，SVM是二分类器，则有20个SVM】，获得2000×20维矩阵表示每个region proposal是某个物体类别的得分</li></ul><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20190412141540.png" srcset="/img/loading.gif" alt=""></p><h4 id="【NMS】"><a href="#【NMS】" class="headerlink" title="【NMS】"></a>【<code>NMS</code>】</h4><ul><li><p>分别对上述2000×20维矩阵中每一列即每一类进行非极大值抑制剔除重叠region proposal，得到该列即该类中得分最高的一些region proposal</p><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20190412142408.png" srcset="/img/loading.gif" alt=""></p></li><li><p>算法流程</p><blockquote><p>假设有有一个候选的boxes的集合, IOU阈值Nt：</p><ol><li>假设有6个候选框，根据分类器类别分类概率做排序，从小到大的概率分别为A、B、C、D、E、F。</li><li>从最大概率矩形框F开始，分别判断A~E与F的重叠度IOU是否大于Nt</li><li>假设B、D与F的重叠度超过阈值，那么就扔掉B、D；并标记第一个矩形框F，是我们保留下来的。</li><li>从剩下的矩形框A、C、E中，选择概率最大的E，然后判断E与A、C的重叠度，重叠度大于一定的阈值，那么就扔掉；并标记E是我们保留下来的第二个矩形框。</li><li>重复1-4，直到找到所有需要被保留下来的矩形框。</li></ol><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20190412143506.png" srcset="/img/loading.gif" alt=""></p></blockquote></li></ul><h3 id="【Bbox-Regression】"><a href="#【Bbox-Regression】" class="headerlink" title="【Bbox Regression】"></a>【<code>Bbox Regression</code>】</h3><ul><li>分别用20个回归器对上述20个类别中剩余的region proposal进行回归操作，最终得到每个类别的修正后的得分最高的bounding box。</li></ul><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20190412141921.png" srcset="/img/loading.gif" alt=""></p><h3 id="网络训练"><a href="#网络训练" class="headerlink" title="网络训练"></a>网络训练</h3><ul><li><p>对CNN的有监督预训练：在ILSVRC样本集（只有分类标签）上对CNN网络进行有监督预训练，此时网络最后的输出是4096维特征-&gt;2000类分类的映射。</p></li><li><p>特定样本下CNN的微调：即domain specific fine-tuning， 在本文中是在PASCAL VOC 2007上进行，学习率是第1步预训练学习率的1/10，将第1步中的1000类分类输出改为21类（20类+背景）,注意此处仍然是softmax，而不是SVM。 </p><ul><li>正样本：Ground Truth+与Ground Truth相交IoU&gt;0.5的Region proposal</li><li>负样本：与Ground Truth相交IoU≤0.5的建议框</li></ul></li><li><p>每一类的SVM的训练：输入正负样本在CNN网络计算下的4096维特征（fc7层） </p><ul><li><p>正样本：Ground Truth</p></li><li><p>负样本：与Ground Truth相交IoU＜0.3的建议框，由于负样本太多，采用hard negative mining的方法在负样本中选取有代表性的负样本</p></li></ul></li><li><p>每一类的Bounding-box regression训练： </p><ul><li>正样本：与Ground Truth的IoU最大，且IoU&gt;0.6的Region Proposal</li></ul></li></ul><h3 id="算法特点"><a href="#算法特点" class="headerlink" title="算法特点"></a>算法特点</h3><blockquote><ol><li>使用Selective Search提取Proposes，然后利用CNN等识别技术进行分类。</li><li>使用识别库进行预训练，而后用检测库调优参数。</li><li>使用SVM代替了CNN网络中最后的Softmax，同时用CNN输出的4096维向量进行Bounding Box回归。</li><li>流程前两个步骤（候选区域提取+特征提取）与待检测类别无关，可以在不同类之间共用；同时检测多类时，需要倍增的只有后两步骤（判别+精修），都是简单的线性运算，速度很快。</li></ol></blockquote><h3 id="存在问题"><a href="#存在问题" class="headerlink" title="存在问题"></a>存在问题</h3><blockquote><ol><li>训练分为多个阶段，步骤繁琐: 微调网络+训练SVM+训练边框回归器。</li><li>训练耗时，占用磁盘空间大：5000张图像产生几百G的特征文件。</li><li>速度慢: 使用GPU, VGG16模型处理一张图像需要47s。</li><li>CNN网络后面接的FC层需要固定的输入大小，限制网络的输入大小</li><li>候选区域会塞给CNN网络用于提取特征向量的，这会有大量的重复计算，造成的计算冗余</li></ol></blockquote>]]></content>
    
    
    <categories>
      
      <category>目标检测</category>
      
    </categories>
    
    
    <tags>
      
      <tag>深度学习</tag>
      
      <tag>R-CNN</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>pytorch实现风格变换</title>
    <link href="/2019/04/pytorch-style/"/>
    <url>/2019/04/pytorch-style/</url>
    
    <content type="html"><![CDATA[<h3 id="pytorch实现风格变换"><a href="#pytorch实现风格变换" class="headerlink" title="pytorch实现风格变换"></a>pytorch实现风格变换</h3><pre><code class="hljs python"><span class="hljs-comment">#!/usr/bin/env python</span><span class="hljs-comment"># -*- coding: utf-8 -*-</span><span class="hljs-comment"># @Time    : 2019/3/6 19:51</span><span class="hljs-comment"># @Author  : Seven</span><span class="hljs-comment"># @File    : StyleTransformation.py</span><span class="hljs-comment"># @Software: PyCharm</span><span class="hljs-comment"># function : pytorch实现风格变换</span><span class="hljs-keyword">from</span> torchvision <span class="hljs-keyword">import</span> models<span class="hljs-keyword">from</span> torchvision <span class="hljs-keyword">import</span> transforms<span class="hljs-keyword">from</span> PIL <span class="hljs-keyword">import</span> Image<span class="hljs-keyword">import</span> argparse<span class="hljs-keyword">import</span> torch<span class="hljs-keyword">import</span> torchvision<span class="hljs-keyword">import</span> torch.nn <span class="hljs-keyword">as</span> nn<span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<span class="hljs-comment"># Device configuration</span>device = torch.device(<span class="hljs-string">'cuda'</span> <span class="hljs-keyword">if</span> torch.cuda.is_available() <span class="hljs-keyword">else</span> <span class="hljs-string">'cpu'</span>)<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">load_image</span><span class="hljs-params">(image_path, transform=None, max_size=None, shape=None)</span>:</span>    <span class="hljs-string">"""</span><span class="hljs-string">    读取一张图片并转换成Tensor变量</span><span class="hljs-string">    :param image_path:</span><span class="hljs-string">    :param transform:</span><span class="hljs-string">    :param max_size:</span><span class="hljs-string">    :param shape:</span><span class="hljs-string">    :return:</span><span class="hljs-string">    """</span>    image = Image.open(image_path)    <span class="hljs-keyword">if</span> max_size:        scale = max_size / max(image.size)        size = np.array(image.size) * scale        image = image.resize(size.astype(int), Image.ANTIALIAS)    <span class="hljs-keyword">if</span> shape:        image = image.resize(shape, Image.LANCZOS)    <span class="hljs-keyword">if</span> transform:        image = transform(image).unsqueeze(<span class="hljs-number">0</span>)    <span class="hljs-keyword">return</span> image.to(device)<span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">VGGNet</span><span class="hljs-params">(nn.Module)</span>:</span>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span><span class="hljs-params">(self)</span>:</span>        <span class="hljs-string">"""</span><span class="hljs-string">        构建五层的神经网络</span><span class="hljs-string">        """</span>        super(VGGNet, self).__init__()        self.select = [<span class="hljs-string">'0'</span>, <span class="hljs-string">'5'</span>, <span class="hljs-string">'10'</span>, <span class="hljs-string">'19'</span>, <span class="hljs-string">'28'</span>]        self.vgg = models.vgg19(pretrained=<span class="hljs-literal">True</span>).features    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">forward</span><span class="hljs-params">(self, x)</span>:</span>        <span class="hljs-string">"""Extract multiple convolutional feature maps."""</span>        features = []        <span class="hljs-keyword">for</span> name, layer <span class="hljs-keyword">in</span> self.vgg._modules.items():            x = layer(x)            <span class="hljs-keyword">if</span> name <span class="hljs-keyword">in</span> self.select:                features.append(x)        <span class="hljs-keyword">return</span> features<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">main</span><span class="hljs-params">(config)</span>:</span>    <span class="hljs-comment"># Image preprocessing</span>    <span class="hljs-comment"># VGGNet was trained on ImageNet where images are normalized by mean=[0.485, 0.456, 0.406]</span>    <span class="hljs-comment"># and std=[0.229, 0.224, 0.225].</span>    <span class="hljs-comment"># We use the same normalization statistics here.</span>    transform = transforms.Compose([        transforms.ToTensor(),        transforms.Normalize(mean=(<span class="hljs-number">0.485</span>, <span class="hljs-number">0.456</span>, <span class="hljs-number">0.406</span>),                             std=(<span class="hljs-number">0.229</span>, <span class="hljs-number">0.224</span>, <span class="hljs-number">0.225</span>))])    <span class="hljs-comment"># Load content and style images</span>    <span class="hljs-comment"># Make the style image same size as the content image</span>    content = load_image(config.content, transform, max_size=config.max_size)    style = load_image(config.style, transform, shape=[content.size(<span class="hljs-number">2</span>), content.size(<span class="hljs-number">3</span>)])    <span class="hljs-comment"># Initialize a target image with the content image</span>    target = content.clone().requires_grad_(<span class="hljs-literal">True</span>)    optimizer = torch.optim.Adam([target], lr=config.lr, betas=[<span class="hljs-number">0.5</span>, <span class="hljs-number">0.999</span>])    vgg = VGGNet().to(device).eval()    <span class="hljs-keyword">for</span> step <span class="hljs-keyword">in</span> range(config.total_step):        <span class="hljs-comment"># Extract multiple(5) conv feature vectors</span>        target_features = vgg(target)        content_features = vgg(content)        style_features = vgg(style)        style_loss = <span class="hljs-number">0</span>        content_loss = <span class="hljs-number">0</span>        <span class="hljs-keyword">for</span> f1, f2, f3 <span class="hljs-keyword">in</span> zip(target_features, content_features, style_features):            <span class="hljs-comment"># Compute content loss with target and content images</span>            content_loss += torch.mean((f1 - f2)**<span class="hljs-number">2</span>)            <span class="hljs-comment"># Reshape convolutional feature maps</span>            _, c, h, w = f1.size()            f1 = f1.view(c, h * w)            f3 = f3.view(c, h * w)            <span class="hljs-comment"># Compute gram matrix</span>            f1 = torch.mm(f1, f1.t())            f3 = torch.mm(f3, f3.t())            <span class="hljs-comment"># Compute style loss with target and style images</span>            style_loss += torch.mean((f1 - f3)**<span class="hljs-number">2</span>) / (c * h * w)        <span class="hljs-comment"># Compute total loss, backprop and optimize</span>        loss = content_loss + config.style_weight * style_loss        optimizer.zero_grad()        loss.backward()        optimizer.step()        <span class="hljs-keyword">if</span> (step+<span class="hljs-number">1</span>) % config.log_step == <span class="hljs-number">0</span>:            print(<span class="hljs-string">'Step [&#123;&#125;/&#123;&#125;], Content Loss: &#123;:.4f&#125;, Style Loss: &#123;:.4f&#125;'</span>                    .format(step+<span class="hljs-number">1</span>, config.total_step, content_loss.item(), style_loss.item()))        <span class="hljs-keyword">if</span> (step+<span class="hljs-number">1</span>) % config.sample_step == <span class="hljs-number">0</span>:            <span class="hljs-comment"># Save the generated image</span>            denorm = transforms.Normalize((<span class="hljs-number">-2.12</span>, <span class="hljs-number">-2.04</span>, <span class="hljs-number">-1.80</span>), (<span class="hljs-number">4.37</span>, <span class="hljs-number">4.46</span>, <span class="hljs-number">4.44</span>))            img = target.clone().squeeze()            img = denorm(img).clamp_(<span class="hljs-number">0</span>, <span class="hljs-number">1</span>)            torchvision.utils.save_image(img, <span class="hljs-string">'output-&#123;&#125;.png'</span>.format(step+<span class="hljs-number">1</span>))<span class="hljs-keyword">if</span> __name__ == <span class="hljs-string">"__main__"</span>:    parser = argparse.ArgumentParser()    parser.add_argument(<span class="hljs-string">'--content'</span>, type=str, default=<span class="hljs-string">'image/test.jpg'</span>)    parser.add_argument(<span class="hljs-string">'--style'</span>, type=str, default=<span class="hljs-string">'image/candy.jpg'</span>)    parser.add_argument(<span class="hljs-string">'--max_size'</span>, type=int, default=<span class="hljs-number">400</span>)    parser.add_argument(<span class="hljs-string">'--total_step'</span>, type=int, default=<span class="hljs-number">2000</span>)    parser.add_argument(<span class="hljs-string">'--log_step'</span>, type=int, default=<span class="hljs-number">10</span>)    parser.add_argument(<span class="hljs-string">'--sample_step'</span>, type=int, default=<span class="hljs-number">500</span>)    parser.add_argument(<span class="hljs-string">'--style_weight'</span>, type=float, default=<span class="hljs-number">100</span>)    parser.add_argument(<span class="hljs-string">'--lr'</span>, type=float, default=<span class="hljs-number">0.003</span>)    config = parser.parse_args()    print(config)    main(config)</code></pre><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20190323163055.png" srcset="/img/loading.gif" alt=""></p>]]></content>
    
    
    <categories>
      
      <category>Pytorch</category>
      
    </categories>
    
    
    <tags>
      
      <tag>深度学习</tag>
      
      <tag>Pytorch</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>pytorch实现对抗生成网络</title>
    <link href="/2019/04/pytorch-GAN/"/>
    <url>/2019/04/pytorch-GAN/</url>
    
    <content type="html"><![CDATA[<h3 id="pytorch实现对抗生成网络"><a href="#pytorch实现对抗生成网络" class="headerlink" title="pytorch实现对抗生成网络"></a>pytorch实现对抗生成网络</h3><pre><code class="hljs python"><span class="hljs-comment">#!/usr/bin/env python</span><span class="hljs-comment"># -*- coding: utf-8 -*-</span><span class="hljs-comment"># @Time    : 2019/3/6 19:48</span><span class="hljs-comment"># @Author  : Seven</span><span class="hljs-comment"># @File    : GAN.py</span><span class="hljs-comment"># @Software: PyCharm</span><span class="hljs-comment"># function : pytorch实现对抗生成网络</span><span class="hljs-keyword">import</span> os<span class="hljs-keyword">import</span> torch<span class="hljs-keyword">import</span> torchvision<span class="hljs-keyword">import</span> torch.nn <span class="hljs-keyword">as</span> nn<span class="hljs-keyword">from</span> torchvision <span class="hljs-keyword">import</span> transforms<span class="hljs-keyword">from</span> torchvision.utils <span class="hljs-keyword">import</span> save_image<span class="hljs-comment"># Device configuration</span>device = torch.device(<span class="hljs-string">'cuda'</span> <span class="hljs-keyword">if</span> torch.cuda.is_available() <span class="hljs-keyword">else</span> <span class="hljs-string">'cpu'</span>)<span class="hljs-comment"># Hyper-parameters</span>latent_size = <span class="hljs-number">64</span>hidden_size = <span class="hljs-number">256</span>image_size = <span class="hljs-number">784</span>num_epochs = <span class="hljs-number">200</span>batch_size = <span class="hljs-number">100</span>sample_dir = <span class="hljs-string">'samples'</span><span class="hljs-comment"># Create a directory if not exists</span><span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> os.path.exists(sample_dir):    os.makedirs(sample_dir)<span class="hljs-comment"># Image processing</span>transform = transforms.Compose([    transforms.ToTensor(),    transforms.Normalize(mean=(<span class="hljs-number">0.5</span>, <span class="hljs-number">0.5</span>, <span class="hljs-number">0.5</span>),  <span class="hljs-comment"># 3 for RGB channels</span>                         std=(<span class="hljs-number">0.5</span>, <span class="hljs-number">0.5</span>, <span class="hljs-number">0.5</span>))])<span class="hljs-comment"># MNIST dataset</span>mnist = torchvision.datasets.MNIST(root=<span class="hljs-string">'data/'</span>,                                   train=<span class="hljs-literal">True</span>,                                   transform=transform,                                   download=<span class="hljs-literal">True</span>)<span class="hljs-comment"># Data loader</span>data_loader = torch.utils.data.DataLoader(dataset=mnist,                                          batch_size=batch_size,                                          shuffle=<span class="hljs-literal">True</span>)<span class="hljs-comment"># Discriminator</span>D = nn.Sequential(    nn.Linear(image_size, hidden_size),    nn.LeakyReLU(<span class="hljs-number">0.2</span>),    nn.Linear(hidden_size, hidden_size),    nn.LeakyReLU(<span class="hljs-number">0.2</span>),    nn.Linear(hidden_size, <span class="hljs-number">1</span>),    nn.Sigmoid())<span class="hljs-comment"># Generator</span>G = nn.Sequential(    nn.Linear(latent_size, hidden_size),    nn.ReLU(),    nn.Linear(hidden_size, hidden_size),    nn.ReLU(),    nn.Linear(hidden_size, image_size),    nn.Tanh())<span class="hljs-comment"># Device setting</span>D = D.to(device)G = G.to(device)<span class="hljs-comment"># Binary cross entropy loss and optimizer</span>criterion = nn.BCELoss()d_optimizer = torch.optim.Adam(D.parameters(), lr=<span class="hljs-number">0.0002</span>)g_optimizer = torch.optim.Adam(G.parameters(), lr=<span class="hljs-number">0.0002</span>)<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">denorm</span><span class="hljs-params">(x)</span>:</span>    out = (x + <span class="hljs-number">1</span>) / <span class="hljs-number">2</span>    <span class="hljs-keyword">return</span> out.clamp(<span class="hljs-number">0</span>, <span class="hljs-number">1</span>)<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">reset_grad</span><span class="hljs-params">()</span>:</span>    d_optimizer.zero_grad()    g_optimizer.zero_grad()<span class="hljs-comment"># Start training</span>total_step = len(data_loader)<span class="hljs-keyword">for</span> epoch <span class="hljs-keyword">in</span> range(num_epochs):    <span class="hljs-keyword">for</span> i, (images, _) <span class="hljs-keyword">in</span> enumerate(data_loader):        images = images.reshape(batch_size, <span class="hljs-number">-1</span>).to(device)        <span class="hljs-comment"># Create the labels which are later used as input for the BCE loss</span>        real_labels = torch.ones(batch_size, <span class="hljs-number">1</span>).to(device)        fake_labels = torch.zeros(batch_size, <span class="hljs-number">1</span>).to(device)        <span class="hljs-comment"># ================================================================== #</span>        <span class="hljs-comment">#                      Train the discriminator                       #</span>        <span class="hljs-comment"># ================================================================== #</span>        <span class="hljs-comment"># Compute BCE_Loss using real images where BCE_Loss(x, y): - y * log(D(x)) - (1-y) * log(1 - D(x))</span>        <span class="hljs-comment"># Second term of the loss is always zero since real_labels == 1</span>        outputs = D(images)        d_loss_real = criterion(outputs, real_labels)        real_score = outputs        <span class="hljs-comment"># Compute BCELoss using fake images</span>        <span class="hljs-comment"># First term of the loss is always zero since fake_labels == 0</span>        z = torch.randn(batch_size, latent_size).to(device)        fake_images = G(z)        outputs = D(fake_images)        d_loss_fake = criterion(outputs, fake_labels)        fake_score = outputs        <span class="hljs-comment"># Backprop and optimize</span>        d_loss = d_loss_real + d_loss_fake        reset_grad()        d_loss.backward()        d_optimizer.step()        <span class="hljs-comment"># ================================================================== #</span>        <span class="hljs-comment">#                        Train the generator                         #</span>        <span class="hljs-comment"># ================================================================== #</span>        <span class="hljs-comment"># Compute loss with fake images</span>        z = torch.randn(batch_size, latent_size).to(device)        fake_images = G(z)        outputs = D(fake_images)        <span class="hljs-comment"># We train G to maximize log(D(G(z)) instead of minimizing log(1-D(G(z)))</span>        <span class="hljs-comment"># For the reason, see the last paragraph of section 3. https://arxiv.org/pdf/1406.2661.pdf</span>        g_loss = criterion(outputs, real_labels)        <span class="hljs-comment"># Backprop and optimize</span>        reset_grad()        g_loss.backward()        g_optimizer.step()        <span class="hljs-keyword">if</span> (i + <span class="hljs-number">1</span>) % <span class="hljs-number">200</span> == <span class="hljs-number">0</span>:            print(<span class="hljs-string">'Epoch [&#123;&#125;/&#123;&#125;], Step [&#123;&#125;/&#123;&#125;], d_loss: &#123;:.4f&#125;, g_loss: &#123;:.4f&#125;, D(x): &#123;:.2f&#125;, D(G(z)): &#123;:.2f&#125;'</span>                  .format(epoch, num_epochs, i + <span class="hljs-number">1</span>, total_step, d_loss.item(), g_loss.item(),                          real_score.mean().item(), fake_score.mean().item()))    <span class="hljs-comment"># Save real images</span>    <span class="hljs-keyword">if</span> (epoch + <span class="hljs-number">1</span>) == <span class="hljs-number">1</span>:        images = images.reshape(images.size(<span class="hljs-number">0</span>), <span class="hljs-number">1</span>, <span class="hljs-number">28</span>, <span class="hljs-number">28</span>)        save_image(denorm(images), os.path.join(sample_dir, <span class="hljs-string">'real_images.png'</span>))    <span class="hljs-comment"># Save sampled images</span>    fake_images = fake_images.reshape(fake_images.size(<span class="hljs-number">0</span>), <span class="hljs-number">1</span>, <span class="hljs-number">28</span>, <span class="hljs-number">28</span>)    save_image(denorm(fake_images), os.path.join(sample_dir, <span class="hljs-string">'fake_images-&#123;&#125;.png'</span>.format(epoch + <span class="hljs-number">1</span>)))<span class="hljs-comment"># Save the model checkpoints</span>torch.save(G.state_dict(), <span class="hljs-string">'G.ckpt'</span>)torch.save(D.state_dict(), <span class="hljs-string">'D.ckpt'</span>)</code></pre>]]></content>
    
    
    <categories>
      
      <category>Pytorch</category>
      
    </categories>
    
    
    <tags>
      
      <tag>深度学习</tag>
      
      <tag>Pytorch</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>pytorch实现LSTM网络</title>
    <link href="/2019/04/pytorch-LSTM/"/>
    <url>/2019/04/pytorch-LSTM/</url>
    
    <content type="html"><![CDATA[<h3 id="pytorch实现LSTM网络"><a href="#pytorch实现LSTM网络" class="headerlink" title="pytorch实现LSTM网络"></a>pytorch实现LSTM网络</h3><pre><code class="hljs python"><span class="hljs-comment">#!/usr/bin/env python</span><span class="hljs-comment"># -*- coding: utf-8 -*-</span><span class="hljs-comment"># @Time    : 2019/3/6 19:45</span><span class="hljs-comment"># @Author  : Seven</span><span class="hljs-comment"># @File    : LSTM.py</span><span class="hljs-comment"># @Software: PyCharm</span><span class="hljs-comment"># function : pytorch实现LSTM网络</span><span class="hljs-keyword">import</span> torch<span class="hljs-keyword">import</span> torch.nn <span class="hljs-keyword">as</span> nn<span class="hljs-keyword">import</span> torchvision<span class="hljs-keyword">import</span> torchvision.transforms <span class="hljs-keyword">as</span> transforms<span class="hljs-comment"># Device configuration</span>device = torch.device(<span class="hljs-string">'cuda'</span> <span class="hljs-keyword">if</span> torch.cuda.is_available() <span class="hljs-keyword">else</span> <span class="hljs-string">'cpu'</span>)<span class="hljs-comment"># Hyper-parameters</span>sequence_length = <span class="hljs-number">28</span>input_size = <span class="hljs-number">28</span>hidden_size = <span class="hljs-number">128</span>num_layers = <span class="hljs-number">2</span>num_classes = <span class="hljs-number">10</span>batch_size = <span class="hljs-number">100</span>num_epochs = <span class="hljs-number">2</span>learning_rate = <span class="hljs-number">0.003</span><span class="hljs-comment"># MNIST dataset</span>train_dataset = torchvision.datasets.MNIST(root=<span class="hljs-string">'data/'</span>,                                           train=<span class="hljs-literal">True</span>,                                           transform=transforms.ToTensor(),                                           download=<span class="hljs-literal">True</span>)test_dataset = torchvision.datasets.MNIST(root=<span class="hljs-string">'data/'</span>,                                          train=<span class="hljs-literal">False</span>,                                          transform=transforms.ToTensor())<span class="hljs-comment"># Data loader</span>train_loader = torch.utils.data.DataLoader(dataset=train_dataset,                                           batch_size=batch_size,                                           shuffle=<span class="hljs-literal">True</span>)test_loader = torch.utils.data.DataLoader(dataset=test_dataset,                                          batch_size=batch_size,                                          shuffle=<span class="hljs-literal">False</span>)<span class="hljs-comment"># Bidirectional recurrent neural network (many-to-one)</span><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">BiRNN</span><span class="hljs-params">(nn.Module)</span>:</span>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span><span class="hljs-params">(self, input_size, hidden_size, num_layers, num_classes)</span>:</span>        super(BiRNN, self).__init__()        self.hidden_size = hidden_size        self.num_layers = num_layers        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=<span class="hljs-literal">True</span>, bidirectional=<span class="hljs-literal">True</span>)        self.fc = nn.Linear(hidden_size * <span class="hljs-number">2</span>, num_classes)  <span class="hljs-comment"># 2 for bidirection</span>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">forward</span><span class="hljs-params">(self, x)</span>:</span>        <span class="hljs-comment"># Set initial states</span>        h0 = torch.zeros(self.num_layers * <span class="hljs-number">2</span>, x.size(<span class="hljs-number">0</span>), self.hidden_size).to(device)  <span class="hljs-comment"># 2 for bidirection</span>        c0 = torch.zeros(self.num_layers * <span class="hljs-number">2</span>, x.size(<span class="hljs-number">0</span>), self.hidden_size).to(device)        <span class="hljs-comment"># Forward propagate LSTM</span>        out, _ = self.lstm(x, (h0, c0))  <span class="hljs-comment"># out: tensor of shape (batch_size, seq_length, hidden_size*2)</span>        <span class="hljs-comment"># Decode the hidden state of the last time step</span>        out = self.fc(out[:, <span class="hljs-number">-1</span>, :])        <span class="hljs-keyword">return</span> outmodel = BiRNN(input_size, hidden_size, num_layers, num_classes).to(device)<span class="hljs-comment"># Loss and optimizer</span>criterion = nn.CrossEntropyLoss()optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)<span class="hljs-comment"># Train the model</span>total_step = len(train_loader)<span class="hljs-keyword">for</span> epoch <span class="hljs-keyword">in</span> range(num_epochs):    <span class="hljs-keyword">for</span> i, (images, labels) <span class="hljs-keyword">in</span> enumerate(train_loader):        images = images.reshape(<span class="hljs-number">-1</span>, sequence_length, input_size).to(device)        labels = labels.to(device)        <span class="hljs-comment"># Forward pass</span>        outputs = model(images)        loss = criterion(outputs, labels)        <span class="hljs-comment"># Backward and optimize</span>        optimizer.zero_grad()        loss.backward()        optimizer.step()        <span class="hljs-keyword">if</span> (i + <span class="hljs-number">1</span>) % <span class="hljs-number">100</span> == <span class="hljs-number">0</span>:            print(<span class="hljs-string">'Epoch [&#123;&#125;/&#123;&#125;], Step [&#123;&#125;/&#123;&#125;], Loss: &#123;:.4f&#125;'</span>                  .format(epoch + <span class="hljs-number">1</span>, num_epochs, i + <span class="hljs-number">1</span>, total_step, loss.item()))<span class="hljs-comment"># Test the model</span><span class="hljs-keyword">with</span> torch.no_grad():    correct = <span class="hljs-number">0</span>    total = <span class="hljs-number">0</span>    <span class="hljs-keyword">for</span> images, labels <span class="hljs-keyword">in</span> test_loader:        images = images.reshape(<span class="hljs-number">-1</span>, sequence_length, input_size).to(device)        labels = labels.to(device)        outputs = model(images)        _, predicted = torch.max(outputs.data, <span class="hljs-number">1</span>)        total += labels.size(<span class="hljs-number">0</span>)        correct += (predicted == labels).sum().item()    print(<span class="hljs-string">'Test Accuracy of the model on the 10000 test images: &#123;&#125; %'</span>.format(<span class="hljs-number">100</span> * correct / total))<span class="hljs-comment"># Save the model checkpoint</span>torch.save(model.state_dict(), <span class="hljs-string">'LSTM-model.ckpt'</span>)</code></pre>]]></content>
    
    
    <categories>
      
      <category>Pytorch</category>
      
    </categories>
    
    
    <tags>
      
      <tag>深度学习</tag>
      
      <tag>Pytorch</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>pytorch 实现卷积神经网络</title>
    <link href="/2019/04/pytorch-CNN/"/>
    <url>/2019/04/pytorch-CNN/</url>
    
    <content type="html"><![CDATA[<h3 id="pytorch-实现卷积神经网络"><a href="#pytorch-实现卷积神经网络" class="headerlink" title="pytorch 实现卷积神经网络"></a>pytorch 实现卷积神经网络</h3><pre><code class="hljs python"><span class="hljs-comment">#!/usr/bin/env python</span><span class="hljs-comment"># -*- coding: utf-8 -*-</span><span class="hljs-comment"># @Time    : 2019/3/6 19:36</span><span class="hljs-comment"># @Author  : Seven</span><span class="hljs-comment"># @File    : CNN.py</span><span class="hljs-comment"># @Software: PyCharm</span><span class="hljs-comment"># function : pytorch 实现卷积神经网络</span><span class="hljs-keyword">import</span> torch<span class="hljs-keyword">import</span> torch.nn <span class="hljs-keyword">as</span> nn<span class="hljs-keyword">import</span> torchvision<span class="hljs-keyword">import</span> torchvision.transforms <span class="hljs-keyword">as</span> transforms<span class="hljs-comment"># Device configuration</span>device = torch.device(<span class="hljs-string">'cuda:0'</span> <span class="hljs-keyword">if</span> torch.cuda.is_available() <span class="hljs-keyword">else</span> <span class="hljs-string">'cpu'</span>)<span class="hljs-comment"># Hyper parameters</span>num_epochs = <span class="hljs-number">5</span>num_classes = <span class="hljs-number">10</span>batch_size = <span class="hljs-number">100</span>learning_rate = <span class="hljs-number">0.001</span><span class="hljs-comment"># MNIST dataset</span>train_dataset = torchvision.datasets.MNIST(root=<span class="hljs-string">'data/'</span>,                                           train=<span class="hljs-literal">True</span>,                                           transform=transforms.ToTensor(),                                           download=<span class="hljs-literal">True</span>)test_dataset = torchvision.datasets.MNIST(root=<span class="hljs-string">'data/'</span>,                                          train=<span class="hljs-literal">False</span>,                                          transform=transforms.ToTensor())<span class="hljs-comment"># Data loader</span>train_loader = torch.utils.data.DataLoader(dataset=train_dataset,                                           batch_size=batch_size,                                           shuffle=<span class="hljs-literal">True</span>)test_loader = torch.utils.data.DataLoader(dataset=test_dataset,                                          batch_size=batch_size,                                          shuffle=<span class="hljs-literal">False</span>)<span class="hljs-comment"># Convolutional neural network (two convolutional layers)</span><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">ConvNet</span><span class="hljs-params">(nn.Module)</span>:</span>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span><span class="hljs-params">(self, num_classes=<span class="hljs-number">10</span>)</span>:</span>        super(ConvNet, self).__init__()        self.layer1 = nn.Sequential(            nn.Conv2d(<span class="hljs-number">1</span>, <span class="hljs-number">16</span>, kernel_size=<span class="hljs-number">5</span>, stride=<span class="hljs-number">1</span>, padding=<span class="hljs-number">2</span>),            nn.BatchNorm2d(<span class="hljs-number">16</span>),            nn.ReLU(),            nn.MaxPool2d(kernel_size=<span class="hljs-number">2</span>, stride=<span class="hljs-number">2</span>))        self.layer2 = nn.Sequential(            nn.Conv2d(<span class="hljs-number">16</span>, <span class="hljs-number">32</span>, kernel_size=<span class="hljs-number">5</span>, stride=<span class="hljs-number">1</span>, padding=<span class="hljs-number">2</span>),            nn.BatchNorm2d(<span class="hljs-number">32</span>),            nn.ReLU(),            nn.MaxPool2d(kernel_size=<span class="hljs-number">2</span>, stride=<span class="hljs-number">2</span>))        self.fc = nn.Linear(<span class="hljs-number">7</span> * <span class="hljs-number">7</span> * <span class="hljs-number">32</span>, num_classes)    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">forward</span><span class="hljs-params">(self, x)</span>:</span>        out = self.layer1(x)        out = self.layer2(out)        out = out.reshape(out.size(<span class="hljs-number">0</span>), <span class="hljs-number">-1</span>)        out = self.fc(out)        <span class="hljs-keyword">return</span> outmodel = ConvNet(num_classes).to(device)<span class="hljs-comment"># Loss and optimizer</span>criterion = nn.CrossEntropyLoss()optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)<span class="hljs-comment"># Train the model</span>total_step = len(train_loader)<span class="hljs-keyword">for</span> epoch <span class="hljs-keyword">in</span> range(num_epochs):    <span class="hljs-keyword">for</span> i, (images, labels) <span class="hljs-keyword">in</span> enumerate(train_loader):        images = images.to(device)        labels = labels.to(device)        <span class="hljs-comment"># Forward pass</span>        outputs = model(images)        loss = criterion(outputs, labels)        <span class="hljs-comment"># Backward and optimize</span>        optimizer.zero_grad()        loss.backward()        optimizer.step()        <span class="hljs-keyword">if</span> (i + <span class="hljs-number">1</span>) % <span class="hljs-number">100</span> == <span class="hljs-number">0</span>:            print(<span class="hljs-string">'Epoch [&#123;&#125;/&#123;&#125;], Step [&#123;&#125;/&#123;&#125;], Loss: &#123;:.4f&#125;'</span>                  .format(epoch + <span class="hljs-number">1</span>, num_epochs, i + <span class="hljs-number">1</span>, total_step, loss.item()))<span class="hljs-comment"># Test the model</span>model.eval()  <span class="hljs-comment"># eval mode (batchnorm uses moving mean/variance instead of mini-batch mean/variance)</span><span class="hljs-keyword">with</span> torch.no_grad():    correct = <span class="hljs-number">0</span>    total = <span class="hljs-number">0</span>    <span class="hljs-keyword">for</span> images, labels <span class="hljs-keyword">in</span> test_loader:        images = images.to(device)        labels = labels.to(device)        outputs = model(images)        _, predicted = torch.max(outputs.data, <span class="hljs-number">1</span>)        total += labels.size(<span class="hljs-number">0</span>)        correct += (predicted == labels).sum().item()    print(<span class="hljs-string">'Test Accuracy of the model on the 10000 test images: &#123;&#125; %'</span>.format(<span class="hljs-number">100</span> * correct / total))<span class="hljs-comment"># Save the model checkpoint</span>torch.save(model.state_dict(), <span class="hljs-string">'CNN-model.ckpt'</span>)</code></pre>]]></content>
    
    
    <categories>
      
      <category>Pytorch</category>
      
    </categories>
    
    
    <tags>
      
      <tag>深度学习</tag>
      
      <tag>Pytorch</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>PyTorch实现全连接神经网络</title>
    <link href="/2019/04/pytorch-DNN/"/>
    <url>/2019/04/pytorch-DNN/</url>
    
    <content type="html"><![CDATA[<h3 id="PyTorch实现全连接神经网络"><a href="#PyTorch实现全连接神经网络" class="headerlink" title="PyTorch实现全连接神经网络"></a>PyTorch实现全连接神经网络</h3><pre><code class="hljs python"><span class="hljs-comment">#!/usr/bin/env python</span><span class="hljs-comment"># -*- coding: utf-8 -*-</span><span class="hljs-comment"># @Time    : 2019/3/6 19:26</span><span class="hljs-comment"># @Author  : Seven</span><span class="hljs-comment"># @File    : DNN.py</span><span class="hljs-comment"># @Software: PyCharm</span><span class="hljs-comment"># function : PyTorch实现全连接神经网络</span><span class="hljs-keyword">import</span> torch<span class="hljs-keyword">import</span> torch.nn <span class="hljs-keyword">as</span> nn<span class="hljs-keyword">import</span> torchvision<span class="hljs-keyword">import</span> torchvision.transforms <span class="hljs-keyword">as</span> transforms<span class="hljs-comment"># Device configuration</span>device = torch.device(<span class="hljs-string">'cuda'</span> <span class="hljs-keyword">if</span> torch.cuda.is_available() <span class="hljs-keyword">else</span> <span class="hljs-string">'cpu'</span>)<span class="hljs-comment"># Hyper-parameters</span>input_size = <span class="hljs-number">784</span>hidden_size = <span class="hljs-number">500</span>num_classes = <span class="hljs-number">10</span>num_epochs = <span class="hljs-number">5</span>batch_size = <span class="hljs-number">100</span>learning_rate = <span class="hljs-number">0.001</span><span class="hljs-comment"># MNIST dataset</span>train_dataset = torchvision.datasets.MNIST(root=<span class="hljs-string">'data'</span>,                                           train=<span class="hljs-literal">True</span>,                                           transform=transforms.ToTensor(),                                           download=<span class="hljs-literal">True</span>)test_dataset = torchvision.datasets.MNIST(root=<span class="hljs-string">'data'</span>,                                          train=<span class="hljs-literal">False</span>,                                          transform=transforms.ToTensor())<span class="hljs-comment"># Data loader</span>train_loader = torch.utils.data.DataLoader(dataset=train_dataset,                                           batch_size=batch_size,                                           shuffle=<span class="hljs-literal">True</span>)test_loader = torch.utils.data.DataLoader(dataset=test_dataset,                                          batch_size=batch_size,                                          shuffle=<span class="hljs-literal">False</span>)<span class="hljs-comment"># Fully connected neural network with one hidden layer</span><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">NeuralNet</span><span class="hljs-params">(nn.Module)</span>:</span>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span><span class="hljs-params">(self, input_size, hidden_size, num_classes)</span>:</span>        super(NeuralNet, self).__init__()        self.fc1 = nn.Linear(input_size, hidden_size)        self.relu = nn.ReLU()        self.fc2 = nn.Linear(hidden_size, num_classes)    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">forward</span><span class="hljs-params">(self, x)</span>:</span>        out = self.fc1(x)        out = self.relu(out)        out = self.fc2(out)        <span class="hljs-keyword">return</span> outmodel = NeuralNet(input_size, hidden_size, num_classes).to(device)<span class="hljs-comment"># Loss and optimizer</span>criterion = nn.CrossEntropyLoss()optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)<span class="hljs-comment"># Train the model</span>total_step = len(train_loader)<span class="hljs-keyword">for</span> epoch <span class="hljs-keyword">in</span> range(num_epochs):    <span class="hljs-keyword">for</span> i, (images, labels) <span class="hljs-keyword">in</span> enumerate(train_loader):        <span class="hljs-comment"># Move tensors to the configured device</span>        images = images.reshape(<span class="hljs-number">-1</span>, <span class="hljs-number">28</span> * <span class="hljs-number">28</span>).to(device)        labels = labels.to(device)        <span class="hljs-comment"># Forward pass</span>        outputs = model(images)        loss = criterion(outputs, labels)        <span class="hljs-comment"># Backward and optimize</span>        optimizer.zero_grad()        loss.backward()        optimizer.step()        <span class="hljs-keyword">if</span> (i + <span class="hljs-number">1</span>) % <span class="hljs-number">100</span> == <span class="hljs-number">0</span>:            print(<span class="hljs-string">'Epoch [&#123;&#125;/&#123;&#125;], Step [&#123;&#125;/&#123;&#125;], Loss: &#123;:.4f&#125;'</span>                  .format(epoch + <span class="hljs-number">1</span>, num_epochs, i + <span class="hljs-number">1</span>, total_step, loss.item()))<span class="hljs-comment"># Test the model</span><span class="hljs-comment"># In test phase, we don't need to compute gradients (for memory efficiency)</span><span class="hljs-keyword">with</span> torch.no_grad():    correct = <span class="hljs-number">0</span>    total = <span class="hljs-number">0</span>    <span class="hljs-keyword">for</span> images, labels <span class="hljs-keyword">in</span> test_loader:        images = images.reshape(<span class="hljs-number">-1</span>, <span class="hljs-number">28</span> * <span class="hljs-number">28</span>).to(device)        labels = labels.to(device)        outputs = model(images)        _, predicted = torch.max(outputs.data, <span class="hljs-number">1</span>)        total += labels.size(<span class="hljs-number">0</span>)        correct += (predicted == labels).sum().item()    print(<span class="hljs-string">'Accuracy of the network on the 10000 test images: &#123;&#125; %'</span>.format(<span class="hljs-number">100</span> * correct / total))<span class="hljs-comment"># Save the model checkpoint</span>torch.save(model.state_dict(), <span class="hljs-string">'DNN-model.ckpt'</span>)</code></pre>]]></content>
    
    
    <categories>
      
      <category>Pytorch</category>
      
    </categories>
    
    
    <tags>
      
      <tag>深度学习</tag>
      
      <tag>Pytorch</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>pytorch实现逻辑回归</title>
    <link href="/2019/04/pytorch-basic_function_logistics_regression/"/>
    <url>/2019/04/pytorch-basic_function_logistics_regression/</url>
    
    <content type="html"><![CDATA[<h3 id="pytorch实现逻辑回归"><a href="#pytorch实现逻辑回归" class="headerlink" title="pytorch实现逻辑回归"></a>pytorch实现逻辑回归</h3><pre><code class="hljs python"><span class="hljs-comment">#!/usr/bin/env python</span><span class="hljs-comment"># -*- coding: utf-8 -*-</span><span class="hljs-comment"># @Time    : 2019/3/6 19:21</span><span class="hljs-comment"># @Author  : Seven</span><span class="hljs-comment"># @File    : LogisticRegression.py</span><span class="hljs-comment"># @Software: PyCharm</span><span class="hljs-comment"># function : pytorch实现逻辑回归</span><span class="hljs-keyword">import</span> torch<span class="hljs-keyword">import</span> torch.nn <span class="hljs-keyword">as</span> nn<span class="hljs-keyword">import</span> torchvision<span class="hljs-keyword">import</span> torchvision.transforms <span class="hljs-keyword">as</span> transforms<span class="hljs-comment"># Hyper-parameters</span>input_size = <span class="hljs-number">784</span>num_classes = <span class="hljs-number">10</span>num_epochs = <span class="hljs-number">5</span>batch_size = <span class="hljs-number">100</span>learning_rate = <span class="hljs-number">0.001</span><span class="hljs-comment"># MNIST dataset (images and labels)</span>train_dataset = torchvision.datasets.MNIST(root=<span class="hljs-string">'data'</span>,                                           train=<span class="hljs-literal">True</span>,                                           transform=transforms.ToTensor(),                                           download=<span class="hljs-literal">True</span>)test_dataset = torchvision.datasets.MNIST(root=<span class="hljs-string">'data'</span>,                                          train=<span class="hljs-literal">False</span>,                                          transform=transforms.ToTensor())<span class="hljs-comment"># Data loader (input pipeline)</span>train_loader = torch.utils.data.DataLoader(dataset=train_dataset,                                           batch_size=batch_size,                                           shuffle=<span class="hljs-literal">True</span>)test_loader = torch.utils.data.DataLoader(dataset=test_dataset,                                          batch_size=batch_size,                                          shuffle=<span class="hljs-literal">False</span>)<span class="hljs-comment"># Logistic regression model</span>model = nn.Linear(input_size, num_classes)<span class="hljs-comment"># Loss and optimizer</span><span class="hljs-comment"># nn.CrossEntropyLoss() computes softmax internally</span>criterion = nn.CrossEntropyLoss()optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)<span class="hljs-comment"># Train the model</span>total_step = len(train_loader)<span class="hljs-keyword">for</span> epoch <span class="hljs-keyword">in</span> range(num_epochs):    <span class="hljs-keyword">for</span> i, (images, labels) <span class="hljs-keyword">in</span> enumerate(train_loader):        <span class="hljs-comment"># Reshape images to (batch_size, input_size)</span>        images = images.reshape(<span class="hljs-number">-1</span>, <span class="hljs-number">28</span> * <span class="hljs-number">28</span>)        <span class="hljs-comment"># Forward pass</span>        outputs = model(images)        loss = criterion(outputs, labels)        <span class="hljs-comment"># Backward and optimize</span>        optimizer.zero_grad()        loss.backward()        optimizer.step()        <span class="hljs-keyword">if</span> (i + <span class="hljs-number">1</span>) % <span class="hljs-number">100</span> == <span class="hljs-number">0</span>:            print(<span class="hljs-string">'Epoch [&#123;&#125;/&#123;&#125;], Step [&#123;&#125;/&#123;&#125;], Loss: &#123;:.4f&#125;'</span>                  .format(epoch + <span class="hljs-number">1</span>, num_epochs, i + <span class="hljs-number">1</span>, total_step, loss.item()))<span class="hljs-comment"># Test the model</span><span class="hljs-comment"># In test phase, we don't need to compute gradients (for memory efficiency)</span><span class="hljs-keyword">with</span> torch.no_grad():    correct = <span class="hljs-number">0</span>    total = <span class="hljs-number">0</span>    <span class="hljs-keyword">for</span> images, labels <span class="hljs-keyword">in</span> test_loader:        images = images.reshape(<span class="hljs-number">-1</span>, <span class="hljs-number">28</span> * <span class="hljs-number">28</span>)        outputs = model(images)        _, predicted = torch.max(outputs.data, <span class="hljs-number">1</span>)        total += labels.size(<span class="hljs-number">0</span>)        correct += (predicted == labels).sum()    print(<span class="hljs-string">'Accuracy of the model on the 10000 test images: &#123;&#125; %'</span>.format(<span class="hljs-number">100</span> * correct / total))<span class="hljs-comment"># Save the model checkpoint</span>torch.save(model.state_dict(), <span class="hljs-string">'logistic.ckpt'</span>)</code></pre>]]></content>
    
    
    <categories>
      
      <category>Pytorch</category>
      
    </categories>
    
    
    <tags>
      
      <tag>深度学习</tag>
      
      <tag>Pytorch</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>pytorch实现线性回归</title>
    <link href="/2019/03/pytorch-basic_function_lineregression/"/>
    <url>/2019/03/pytorch-basic_function_lineregression/</url>
    
    <content type="html"><![CDATA[<h3 id="pytorch实现线性回归"><a href="#pytorch实现线性回归" class="headerlink" title="pytorch实现线性回归"></a>pytorch实现线性回归</h3><pre><code class="hljs python"><span class="hljs-comment">#!/usr/bin/env python</span><span class="hljs-comment"># -*- coding: utf-8 -*-</span><span class="hljs-comment"># @Time    : 2019/3/6 19:12</span><span class="hljs-comment"># @Author  : Seven</span><span class="hljs-comment"># @File    : LinearRegression.py</span><span class="hljs-comment"># @Software: PyCharm</span><span class="hljs-comment"># function : pytorch实现线性回归</span><span class="hljs-keyword">import</span> torch<span class="hljs-keyword">import</span> torch.nn <span class="hljs-keyword">as</span> nn<span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt<span class="hljs-comment"># 设置超参数</span>input_size = <span class="hljs-number">1</span>output_size = <span class="hljs-number">1</span>num_epochs = <span class="hljs-number">60</span>learning_rate = <span class="hljs-number">0.001</span><span class="hljs-comment"># 初始化数据集</span>x_train = np.array([[<span class="hljs-number">3.3</span>], [<span class="hljs-number">4.4</span>], [<span class="hljs-number">5.5</span>], [<span class="hljs-number">6.71</span>], [<span class="hljs-number">6.93</span>], [<span class="hljs-number">4.168</span>],                    [<span class="hljs-number">9.779</span>], [<span class="hljs-number">6.182</span>], [<span class="hljs-number">7.59</span>], [<span class="hljs-number">2.167</span>], [<span class="hljs-number">7.042</span>],                    [<span class="hljs-number">10.791</span>], [<span class="hljs-number">5.313</span>], [<span class="hljs-number">7.997</span>], [<span class="hljs-number">3.1</span>]], dtype=np.float32)y_train = np.array([[<span class="hljs-number">1.7</span>], [<span class="hljs-number">2.76</span>], [<span class="hljs-number">2.09</span>], [<span class="hljs-number">3.19</span>], [<span class="hljs-number">1.694</span>], [<span class="hljs-number">1.573</span>],                    [<span class="hljs-number">3.366</span>], [<span class="hljs-number">2.596</span>], [<span class="hljs-number">2.53</span>], [<span class="hljs-number">1.221</span>], [<span class="hljs-number">2.827</span>],                    [<span class="hljs-number">3.465</span>], [<span class="hljs-number">1.65</span>], [<span class="hljs-number">2.904</span>], [<span class="hljs-number">1.3</span>]], dtype=np.float32)<span class="hljs-comment"># 初始化线性模型</span>model = nn.Linear(input_size, output_size)<span class="hljs-comment"># 初始化损失函数和优化器</span>criterion = nn.MSELoss()optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)<span class="hljs-comment"># 训练模型</span><span class="hljs-keyword">for</span> epoch <span class="hljs-keyword">in</span> range(num_epochs):    <span class="hljs-comment"># Convert numpy arrays to torch tensors</span>    inputs = torch.from_numpy(x_train)    targets = torch.from_numpy(y_train)    <span class="hljs-comment"># Forward pass</span>    outputs = model(inputs)    loss = criterion(outputs, targets)    <span class="hljs-comment"># Backward and optimize</span>    optimizer.zero_grad()    loss.backward()    optimizer.step()    <span class="hljs-keyword">if</span> (epoch + <span class="hljs-number">1</span>) % <span class="hljs-number">5</span> == <span class="hljs-number">0</span>:        print(<span class="hljs-string">'Epoch [&#123;&#125;/&#123;&#125;], Loss: &#123;:.4f&#125;'</span>.format(epoch + <span class="hljs-number">1</span>, num_epochs, loss.item()))<span class="hljs-comment"># Plot the graph</span>predicted = model(torch.from_numpy(x_train)).detach().numpy()plt.plot(x_train, y_train, <span class="hljs-string">'ro'</span>, label=<span class="hljs-string">'Original data'</span>)plt.plot(x_train, predicted, label=<span class="hljs-string">'Fitted line'</span>)plt.legend()plt.show()<span class="hljs-comment"># Save the model checkpoint</span>torch.save(model.state_dict(), <span class="hljs-string">'linear.ckpt'</span>)</code></pre><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20190323161456.png" srcset="/img/loading.gif" alt=""></p>]]></content>
    
    
    <categories>
      
      <category>Pytorch</category>
      
    </categories>
    
    
    <tags>
      
      <tag>深度学习</tag>
      
      <tag>Pytorch</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>pytorch自动求导、numpy的转换、模型的存取</title>
    <link href="/2019/03/pytorch-basic_function_2/"/>
    <url>/2019/03/pytorch-basic_function_2/</url>
    
    <content type="html"><![CDATA[<h3 id="autograd自动求导一"><a href="#autograd自动求导一" class="headerlink" title="autograd自动求导一"></a>autograd自动求导一</h3><pre><code class="hljs python"><span class="hljs-keyword">import</span> torch<span class="hljs-keyword">import</span> torchvision<span class="hljs-keyword">import</span> torch.nn <span class="hljs-keyword">as</span> nn<span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<span class="hljs-comment"># 创建张量</span>x = torch.tensor(<span class="hljs-number">1.</span>, requires_grad=<span class="hljs-literal">True</span>)w = torch.tensor(<span class="hljs-number">2.</span>, requires_grad=<span class="hljs-literal">True</span>)b = torch.tensor(<span class="hljs-number">3.</span>, requires_grad=<span class="hljs-literal">True</span>)<span class="hljs-comment"># 构建计算图。</span>y = w * x + b    <span class="hljs-comment"># y = 2 * x + 3</span><span class="hljs-comment"># # 计算渐变。</span><span class="hljs-comment"># y.backward()</span><span class="hljs-comment"># </span><span class="hljs-comment"># # 打印渐变。</span><span class="hljs-comment"># print(x.grad)    # x.grad = 2</span><span class="hljs-comment"># print(w.grad)    # w.grad = 1</span><span class="hljs-comment"># print(b.grad)    # b.grad = 1</span>grads = torch.autograd.grad(y, [x, b, w])print(grads)</code></pre><blockquote><p>(tensor(2.), tensor(1.), tensor(1.))</p></blockquote><h3 id="autograd自动求导二"><a href="#autograd自动求导二" class="headerlink" title="autograd自动求导二"></a>autograd自动求导二</h3><pre><code class="hljs python"><span class="hljs-comment"># Create tensors of shape (10, 3) and (10, 2).</span>x = torch.randn(<span class="hljs-number">10</span>, <span class="hljs-number">3</span>)y = torch.randn(<span class="hljs-number">10</span>, <span class="hljs-number">2</span>)<span class="hljs-comment"># Build a fully connected layer.</span>linear = nn.Linear(<span class="hljs-number">3</span>, <span class="hljs-number">2</span>)print(<span class="hljs-string">'w: '</span>, linear.weight)print(<span class="hljs-string">'b: '</span>, linear.bias)<span class="hljs-comment"># Build loss function and optimizer.</span>criterion = nn.MSELoss()optimizer = torch.optim.SGD(linear.parameters(), lr=<span class="hljs-number">0.01</span>)<span class="hljs-comment"># Forward pass.</span>pred = linear(x)<span class="hljs-comment"># Compute loss.</span>loss = criterion(pred, y)print(<span class="hljs-string">'loss: '</span>, loss.item())<span class="hljs-comment"># Backward pass.</span>loss.backward()<span class="hljs-comment"># Print out the gradients.</span>print(<span class="hljs-string">'dL/dw: '</span>, linear.weight.grad)print(<span class="hljs-string">'dL/db: '</span>, linear.bias.grad)<span class="hljs-comment"># 1-step gradient descent.</span>optimizer.step()<span class="hljs-comment"># Print out the loss after 1-step gradient descent.</span>pred = linear(x)loss = criterion(pred, y)print(<span class="hljs-string">'loss after 1 step optimization: '</span>, loss.item())</code></pre><blockquote><p>w:  Parameter containing:<br>tensor([[ 0.1648,  0.3432,  0.2555],<br>        [ 0.5305, -0.2595,  0.5334]], requires_grad=True)<br>b:  Parameter containing:<br>tensor([-0.2244,  0.5731], requires_grad=True)<br>loss:  0.8838511109352112<br>dL/dw:  tensor([[ 0.3066,  0.4282,  0.3613],<br>        [ 0.0505, -0.4339,  0.6579]])<br>dL/db:  tensor([-0.1423,  0.3835])<br>loss after 1 step optimization:  0.8719408512115479</p></blockquote><h3 id="从numpy加载数据"><a href="#从numpy加载数据" class="headerlink" title="从numpy加载数据"></a>从numpy加载数据</h3><pre><code class="hljs python"><span class="hljs-comment"># Create a numpy array.</span>x = np.array([[<span class="hljs-number">1</span>, <span class="hljs-number">2</span>], [<span class="hljs-number">3</span>, <span class="hljs-number">4</span>]])<span class="hljs-comment"># Convert the numpy array to a torch tensor.</span>y = torch.from_numpy(x)<span class="hljs-comment"># Convert the torch tensor to a numpy array.</span>z = y.numpy()print(z)</code></pre><blockquote><p>[[1 2]<br> [3 4]]</p></blockquote><h3 id="预训练模型"><a href="#预训练模型" class="headerlink" title="预训练模型"></a>预训练模型</h3><pre><code class="hljs python"><span class="hljs-comment"># Download and load the pretrained ResNet-18.</span>resnet = torchvision.models.resnet18(pretrained=<span class="hljs-literal">True</span>)<span class="hljs-comment"># If you want to finetune only the top layer of the model, set as below.</span><span class="hljs-keyword">for</span> param <span class="hljs-keyword">in</span> resnet.parameters():    param.requires_grad = <span class="hljs-literal">False</span><span class="hljs-comment"># Replace the top layer for finetuning.</span>resnet.fc = nn.Linear(resnet.fc.in_features, <span class="hljs-number">100</span>)  <span class="hljs-comment"># 100 is an example.</span><span class="hljs-comment"># Forward pass.</span>images = torch.randn(<span class="hljs-number">64</span>, <span class="hljs-number">3</span>, <span class="hljs-number">224</span>, <span class="hljs-number">224</span>)outputs = resnet(images)print(outputs.size())     <span class="hljs-comment"># (64, 100)</span></code></pre><h3 id="保存和读取模型"><a href="#保存和读取模型" class="headerlink" title="保存和读取模型"></a>保存和读取模型</h3><pre><code class="hljs python"><span class="hljs-comment"># Save and load the entire model.</span>torch.save(resnet, <span class="hljs-string">'./model.ckpt'</span>)model = torch.load(<span class="hljs-string">'model.ckpt'</span>)<span class="hljs-comment"># Save and load only the model parameters (recommended).</span>torch.save(resnet.state_dict(), <span class="hljs-string">'params.ckpt'</span>)resnet.load_state_dict(torch.load(<span class="hljs-string">'params.ckpt'</span>))</code></pre>]]></content>
    
    
    <categories>
      
      <category>Pytorch</category>
      
    </categories>
    
    
    <tags>
      
      <tag>深度学习</tag>
      
      <tag>Pytorch</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>pytorch基本数据类型及常用计算API的使用</title>
    <link href="/2019/03/pytorch-basic_function_1/"/>
    <url>/2019/03/pytorch-basic_function_1/</url>
    
    <content type="html"><![CDATA[<h3 id="Tensor的数据类型"><a href="#Tensor的数据类型" class="headerlink" title="Tensor的数据类型"></a>Tensor的数据类型</h3><h4 id="torch-FloatTensor-："><a href="#torch-FloatTensor-：" class="headerlink" title="torch.FloatTensor ："></a><strong>torch.FloatTensor ：</strong></h4><blockquote><p> 用于生成数据类型为浮点型的Tensor，传递给torch.FloatTensor的参数可以是一个列表，也可以是一个维度值</p></blockquote><pre><code class="hljs python">a = torch.FloatTensor(<span class="hljs-number">2</span>, <span class="hljs-number">3</span>)  <span class="hljs-comment"># 随机生成</span>b = torch.FloatTensor([<span class="hljs-number">2</span>, <span class="hljs-number">3</span>, <span class="hljs-number">4</span>, <span class="hljs-number">5</span>])  <span class="hljs-comment"># 指定生成</span>print(a)print(b)</code></pre><blockquote><p>tensor([[0, 0, 0],[0, 0, 0]], dtype=torch.int32)<br>tensor([2, 3, 4, 5], dtype=torch.int32)</p></blockquote><h4 id="torch-IntTensor-："><a href="#torch-IntTensor-：" class="headerlink" title="torch.IntTensor ："></a><strong>torch.IntTensor ：</strong></h4><blockquote><p>用于生成数据类型为整型的 Tensor，传递给 torch.IntTensor的参数可以是一个列表，也可以是一个维度值</p></blockquote><pre><code class="hljs python">a = torch.IntTensor(<span class="hljs-number">2</span>, <span class="hljs-number">3</span>)  <span class="hljs-comment"># 随机生成</span>b = torch.IntTensor([<span class="hljs-number">2</span>, <span class="hljs-number">3</span>, <span class="hljs-number">4</span>, <span class="hljs-number">5</span>])  <span class="hljs-comment"># 指定生成</span>print(a)print(b)</code></pre><blockquote><p>tensor([[0, 0, 0],<br>        [0, 0, 0]], dtype=torch.int32)<br>tensor([2, 3, 4, 5], dtype=torch.int32)</p></blockquote><h4 id="torch-rand-："><a href="#torch-rand-：" class="headerlink" title="torch.rand ："></a><strong>torch.rand ：</strong></h4><blockquote><p> 用于生成数据类型为浮点型且维度指定的随机Tensor，和在NumPy中使用numpy.rand生成随机数的方法类似，随机生成的浮点数据在0～1区间均匀分布</p></blockquote><pre><code class="hljs python">a = torch.rand(<span class="hljs-number">2</span>, <span class="hljs-number">3</span>)print(a)</code></pre><blockquote><p>tensor([[0.0584, 0.8775, 0.6964],<br>        [0.4844, 0.4144, 0.8594]])</p></blockquote><h4 id="torch-randn-："><a href="#torch-randn-：" class="headerlink" title="torch.randn ："></a><strong>torch.randn ：</strong></h4><blockquote><p> 用于生成数据类型为浮点型且维度指定的随机Tensor，和在NumPy中使用numpy.randn生成随机数的方法类似，随机生成的浮点数的取值满足均值为0、方差为1的正太分布</p></blockquote><pre><code class="hljs python">a = torch.randn(<span class="hljs-number">2</span>, <span class="hljs-number">3</span>)print(a)</code></pre><blockquote><p>tensor([[-0.8214, -0.0331,  0.7489],<br>        [-0.3789, -1.0754, -2.1049]])</p></blockquote><h4 id="torch-range-："><a href="#torch-range-：" class="headerlink" title="torch.range ："></a><strong>torch.range</strong> ：</h4><blockquote><p> 用于生成数据类型为浮点型且自定义起始范围和结束范围的Tensor，所以传递给torch.range的参数有三个，分别是范围的起始值、范围的结束值和步长，其中，步长用于指定从起始值到结束值的每步的数据间隔</p></blockquote><pre><code class="hljs python">a = torch.arange(<span class="hljs-number">1</span>, <span class="hljs-number">20</span>)print(a)</code></pre><blockquote><p>tensor([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17, 18,<br>        19])</p></blockquote><h4 id="torch-zeros-："><a href="#torch-zeros-：" class="headerlink" title="torch.zeros ："></a><strong>torch.zeros ：</strong></h4><blockquote><p> 用于生成数据类型为浮点型且维度指定的Tensor，不过这个浮点型的Tensor中的元素值全部为0</p></blockquote><pre><code class="hljs python">a = torch.zeros(<span class="hljs-number">2</span>, <span class="hljs-number">3</span>)print(a)</code></pre><blockquote><p>tensor([[0., 0., 0.],<br>        [0., 0., 0.]])</p></blockquote><h3 id="Tensor的运算"><a href="#Tensor的运算" class="headerlink" title="Tensor的运算"></a>Tensor的运算</h3><h4 id="torch-abs-："><a href="#torch-abs-：" class="headerlink" title="torch.abs ："></a><strong>torch.abs ：</strong></h4><blockquote><p> 将参数传递到torch.abs后返回输入参数的绝对值作为输出，输入参数必须是一个Tensor数据类型的变量</p></blockquote><pre><code class="hljs python">a = torch.randn(<span class="hljs-number">2</span>, <span class="hljs-number">3</span>)print(a)b = torch.abs(a)print(b)</code></pre><blockquote><p>tensor([[ 1.0926,  1.1767, -1.2057],<br>        [-0.5718,  0.8411,  0.4633]])<br>tensor([[1.0926, 1.1767, 1.2057],<br>        [0.5718, 0.8411, 0.4633]])</p></blockquote><h4 id="torch-add-："><a href="#torch-add-：" class="headerlink" title="torch.add ："></a><strong>torch.add ：</strong></h4><blockquote><p>将参数传递到 torch.add后返回输入参数的求和结果作为输出，输入参数既可以全部是Tensor数据类型的变量，也可以一个是Tensor数据类型的变量，另一个是标量</p></blockquote><pre><code class="hljs python">a = torch.randn(<span class="hljs-number">2</span>, <span class="hljs-number">3</span>)print(a)b = torch.randn(<span class="hljs-number">2</span>, <span class="hljs-number">3</span>)print(b)c = torch.add(a, b)print(c)d = torch.randn(<span class="hljs-number">2</span>, <span class="hljs-number">3</span>)print(d)e = torch.add(c, d)print(e)</code></pre><blockquote><p>tensor([[ 0.5688, -1.0098,  0.0875],<br>        [-1.1604,  0.4908, -0.0372]])<br>tensor([[-0.9039,  1.5682,  0.4096],<br>        [ 1.2196,  0.9986, -0.2263]])<br>tensor([[-0.3351,  0.5584,  0.4971],<br>        [ 0.0593,  1.4895, -0.2635]])<br>tensor([[-1.3956, -0.3117,  0.2534],<br>        [-0.1104,  0.1590, -1.3836]])<br>tensor([[-1.7307,  0.2467,  0.7505],<br>        [-0.0512,  1.6484, -1.6471]])</p></blockquote><h4 id="torch-clamp-："><a href="#torch-clamp-：" class="headerlink" title="torch.clamp ："></a><strong>torch.clamp</strong> ：</h4><blockquote><p>对输入参数按照自定义的范围进行裁剪，最后将参数裁剪的结果作为输出。</p><ul><li>所以输入参数一共有三个，分别是需要进行裁剪的Tensor数据类型的变量、裁剪的上边界和裁剪的下边界，</li><li>具体的裁剪过程是：使用变量中的每个元素分别和裁剪的上边界及裁剪的下边界的值进行比较，如果元素的值小于裁剪的下边界的值，该元素就被重写成裁剪的下边界的值；</li><li>同理，如果元素的值大于裁剪的上边界的值，该元素就被重写成裁剪的上边界的值。</li></ul></blockquote><pre><code class="hljs python">a = torch.randn(<span class="hljs-number">2</span>, <span class="hljs-number">3</span>)print(a)b = torch.clamp(a, <span class="hljs-number">-0.1</span>, <span class="hljs-number">0.1</span>)print(b)</code></pre><blockquote><p>tensor([[ 0.2453, -2.7730, -0.1356],<br>        [-0.1726, -0.7038, -0.8896]])<br>tensor([[ 0.1000, -0.1000, -0.1000],<br>        [-0.1000, -0.1000, -0.1000]])</p></blockquote><h4 id="torch-div-："><a href="#torch-div-：" class="headerlink" title="torch.div ："></a><strong>torch.div ：</strong></h4><blockquote><p>将参数传递到torch.div后返回输入参数的求商结果作为输出，同样，参与运算的参数可以全部是Tensor数据类型的变量，也可以是Tensor数据类型的变量和标量的组合</p></blockquote><pre><code class="hljs python">a = torch.randn(<span class="hljs-number">2</span>, <span class="hljs-number">3</span>)print(a)b = torch.randn(<span class="hljs-number">2</span>, <span class="hljs-number">3</span>)print(b)c = torch.div(a, b)print(c)d = torch.randn(<span class="hljs-number">2</span>, <span class="hljs-number">3</span>)print(d)e = torch.div(d, <span class="hljs-number">10</span>)print(e)</code></pre><blockquote><p>tensor([[-1.7551,  1.1126, -0.4721],<br>        [-1.3093, -0.6008,  0.8263]])<br>tensor([[ 0.3355, -1.0047,  0.5603],<br>        [-0.4188,  1.9017,  0.7473]])<br>tensor([[-5.2315, -1.1074, -0.8427],<br>        [ 3.1263, -0.3159,  1.1058]])<br>tensor([[ 1.5984, -0.5883,  0.1418],<br>        [-0.8639, -0.7141, -1.2127]])<br>tensor([[ 0.1598, -0.0588,  0.0142],<br>        [-0.0864, -0.0714, -0.1213]])</p></blockquote><h4 id="torch-mul-："><a href="#torch-mul-：" class="headerlink" title="torch.mul ："></a><strong>torch.mul ：</strong></h4><blockquote><p>将参数传递到 torch.mul后返回输入参数求积的结果作为输出，参与运算的参数可以全部是Tensor数据类型的变量，也可以是Tensor数据类型的变量和标量的组合</p></blockquote><pre><code class="hljs python">a = torch.randn(<span class="hljs-number">2</span>, <span class="hljs-number">3</span>)print(a)b = torch.randn(<span class="hljs-number">2</span>, <span class="hljs-number">3</span>)print(b)c = torch.mul(a, b)print(c)d = torch.randn(<span class="hljs-number">2</span>, <span class="hljs-number">3</span>)print(d)e = torch.mul(d, <span class="hljs-number">10</span>)print(e)</code></pre><blockquote><p>tensor([[ 1.8059, -0.1371,  0.3777],<br>        [-0.4319,  1.4183,  2.4060]])<br>tensor([[ 0.9212, -1.1527,  0.1132],<br>        [-0.9209, -0.3348,  0.7959]])<br>tensor([[ 1.6636,  0.1581,  0.0428],<br>        [ 0.3977, -0.4748,  1.9149]])<br>tensor([[ 0.6092,  0.0987,  1.0230],<br>        [ 0.5443,  1.9293, -0.0953]])<br>tensor([[ 6.0925,  0.9873, 10.2297],<br>        [ 5.4433, 19.2932, -0.9529]])</p></blockquote><h4 id="torch-pow-："><a href="#torch-pow-：" class="headerlink" title="torch.pow ："></a><strong>torch.pow ：</strong></h4><blockquote><p>将参数传递到torch.pow后返回输入参数的求幂结果作为输出，参与运算的参数可以全部是Tensor数据类型的变量，也可以是Tensor数据类型的变量和标量的组合</p></blockquote><pre><code class="hljs python">a = torch.randn(<span class="hljs-number">2</span>, <span class="hljs-number">3</span>)print(a)b = torch.pow(a, <span class="hljs-number">2</span>)print(a)</code></pre><blockquote><p>tensor([[0.4679, 0.7738, 0.0807],<br>        [0.7510, 1.6373, 0.1373]])<br>tensor([[0.4679, 0.7738, 0.0807],<br>        [0.7510, 1.6373, 0.1373]])</p></blockquote><p><strong>torch.mm ：</strong></p><blockquote><h4 id="将参数传递到-torch-mm后返回输入参数的求积结果作为输出，不过这个求积的方式和之前的torch-mul运算方式不太一样，torch-mm运用矩阵之间的乘法规则进行计算，所以被传入的参数会被当作矩阵进行处理，参数的维度自然也要满足矩阵乘法的前提条件，即前一个矩阵的行数必须和后一个矩阵的列数相等，否则不能进行计算。"><a href="#将参数传递到-torch-mm后返回输入参数的求积结果作为输出，不过这个求积的方式和之前的torch-mul运算方式不太一样，torch-mm运用矩阵之间的乘法规则进行计算，所以被传入的参数会被当作矩阵进行处理，参数的维度自然也要满足矩阵乘法的前提条件，即前一个矩阵的行数必须和后一个矩阵的列数相等，否则不能进行计算。" class="headerlink" title="将参数传递到 torch.mm后返回输入参数的求积结果作为输出，不过这个求积的方式和之前的torch.mul运算方式不太一样，torch.mm运用矩阵之间的乘法规则进行计算，所以被传入的参数会被当作矩阵进行处理，参数的维度自然也要满足矩阵乘法的前提条件，即前一个矩阵的行数必须和后一个矩阵的列数相等，否则不能进行计算。"></a>将参数传递到 torch.mm后返回输入参数的求积结果作为输出，不过这个求积的方式和之前的torch.mul运算方式不太一样，torch.mm运用矩阵之间的乘法规则进行计算，所以被传入的参数会被当作矩阵进行处理，参数的维度自然也要满足矩阵乘法的前提条件，即前一个矩阵的行数必须和后一个矩阵的列数相等，否则不能进行计算。</h4></blockquote><pre><code class="hljs python">a = torch.randn(<span class="hljs-number">2</span>, <span class="hljs-number">3</span>)print(a)b = torch.randn(<span class="hljs-number">3</span>, <span class="hljs-number">2</span>)print(b)c = torch.mm(a, b)print(c)</code></pre><blockquote><p>tensor([[ 0.1439, -0.0668, -1.4954],<br>        [-0.4558,  0.1897,  0.6573]])<br>tensor([[ 0.4398,  1.0332],<br>        [ 2.2942, -1.1032],<br>        [-0.1976, -0.3176]])<br>tensor([[ 0.2056,  0.6973],<br>        [ 0.1049, -0.8890]])</p></blockquote><h4 id="torch-mv-："><a href="#torch-mv-：" class="headerlink" title="torch.mv ："></a><strong>torch.mv ：</strong></h4><blockquote><p> 将参数传递到torch.mv后返回输入参数的求积结果作为输出，torch.mv运用矩阵与向量之间的乘法规则进行计算，被传入的参数中的第1个参数代表矩阵，第2个参数代表向量，顺序不能颠倒。</p></blockquote><pre><code class="hljs python">a = torch.randn(<span class="hljs-number">2</span>, <span class="hljs-number">3</span>)print(a)b = torch.randn(<span class="hljs-number">3</span>)print(b)c = torch.mv(a, b)print(c)</code></pre><blockquote><p>tensor([[ 0.8297, -0.2407, -0.2188],<br>        [-1.0946, -1.5037, -0.0340]])<br>tensor([2.6157, 0.4282, 1.2846])<br>tensor([ 1.7861, -3.5507])</p></blockquote>]]></content>
    
    
    <categories>
      
      <category>Pytorch</category>
      
    </categories>
    
    
    <tags>
      
      <tag>深度学习</tag>
      
      <tag>Pytorch</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>pytorch基本数据类型和常用的操作</title>
    <link href="/2019/03/pytorch-basic_function/"/>
    <url>/2019/03/pytorch-basic_function/</url>
    
    <content type="html"><![CDATA[<h3 id="基本数据类型"><a href="#基本数据类型" class="headerlink" title="基本数据类型"></a>基本数据类型</h3><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20190305195731.png" srcset="/img/loading.gif" alt=""></p><blockquote><p>注意：pytorch 是不支持string类型的， 但是可以用其他方式来表示string类型</p><ul><li>one-hot</li><li>Embedding<ul><li>Word2vec</li><li>glove</li></ul></li></ul></blockquote><h4 id="【Data-type】"><a href="#【Data-type】" class="headerlink" title="【Data type】"></a>【Data type】</h4><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20190305200915.png" srcset="/img/loading.gif" alt=""></p><h4 id="【代码演示】"><a href="#【代码演示】" class="headerlink" title="【代码演示】"></a>【代码演示】</h4><ul><li><a href="https://0520.tech/2019/03/29/2019-03-29-pytorch-basic_function_1/">pytorch基本数据类型及常用计算API的使用</a></li><li><a href="https://0520.tech/2019/03/30/2019-03-30-pytorch-basic_function_2/">pytorch自动求导、numpy的转换、模型的存取</a></li><li><a href="https://0520.tech/2019/03/31/2019-03-31-pytorch-basic_function_lineregression/">pytorch实现线性回归</a></li><li><a href="https://0520.tech/2019/04/01/2019-04-01-pytorch-basic_function_logistics_regression/">pytorch实现逻辑回归</a></li></ul>]]></content>
    
    
    <categories>
      
      <category>Pytorch</category>
      
    </categories>
    
    
    <tags>
      
      <tag>深度学习</tag>
      
      <tag>Pytorch</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>PyTorch概述、环境部署及简单使用</title>
    <link href="/2019/03/pytorch-introduction/"/>
    <url>/2019/03/pytorch-introduction/</url>
    
    <content type="html"><![CDATA[<h3 id="PyTorch概述"><a href="#PyTorch概述" class="headerlink" title="PyTorch概述"></a>PyTorch概述</h3><p>【<strong>官网</strong>】</p><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20190305150727.png" srcset="/img/loading.gif" alt=""></p><blockquote><p>网址<a href="https://pytorch.org/" target="_blank" rel="noopener">pytorch</a> ：<a href="https://pytorch.org/" target="_blank" rel="noopener">https://pytorch.org/</a></p></blockquote><p>【<strong>简介</strong>】</p><p>​    PyTorch是一个非常有可能改变深度学习领域前景的Python库。<a href="https://github.com/pytorch/pytorch" target="_blank" rel="noopener">PyTorch</a>是一个较新的深度学习框架。从名字可以看出，其和Torch不同之处在于PyTorch使用了Python作为开发语言，所谓“Python first”。一方面，使用者可以将其作为加入了GPU支持的<code>numpy</code>，另一方面，PyTorch也是强大的深度学习框架。</p><p>​    目前有很多深度学习框架，PyTorch主推的功能是<code>动态网络模型</code>。例如在TensorFlow中，使用者通过预先编写网络结构，网络结构是不能变化的（但是TensorFlow2.0加入了动态网络的支持）。而PyTorch中的网络结构不再只能是一成不变的。同时PyTorch实现了多达上百种op的自动求导（AutoGrad）。</p><p>​    在我使用过的各种深度学习库中，PyTorch是最灵活、最容易掌握的。</p><p>【<strong>发展路线</strong>】</p><ul><li>002年发布<code>Torch</code></li><li>2011年发布<code>Torch7</code></li></ul><blockquote><p>最开始的torch框架是使用<code>lua</code>语言开发的，代码的可读性和实现起来都挺麻烦，所以热度一直就不高。</p><pre><code class="hljs lua">torch.Tensor&#123;<span class="hljs-number">1</span>,<span class="hljs-number">2</span>,<span class="hljs-number">3</span>&#125;<span class="hljs-function"><span class="hljs-keyword">function</span> <span class="hljs-title">gradUpdate</span><span class="hljs-params">(mlp,x,y,learningRate)</span></span>  <span class="hljs-keyword">local</span> criterion = nn.ClassNLLCriterion()  pred = mlp:forward(x)  <span class="hljs-keyword">local</span> err = criterion:forward(pred, y);   mlp:zeroGradParameters();  <span class="hljs-keyword">local</span> t = criterion:backward(pred, y);  mlp:backward(x, t);  mlp:updateParameters(learningRate);<span class="hljs-keyword">end</span></code></pre></blockquote><ul><li>2016年10月发布0.1，后端还是基于原来的<code>torch</code>的Python接口发布的的<code>PyTorch</code></li><li>2018年12月发布1.0，后端改为了<code>caffe2</code>的pytorch稳定版1.0</li></ul><p>【<strong>框架</strong>】</p><p>  <img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20190221140942.png" srcset="/img/loading.gif" alt=""></p><p>【<strong>评分</strong>】</p><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20190305152746.png" srcset="/img/loading.gif" alt=""></p><p>【<strong>招聘描述</strong>】</p><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20190305152857.png" srcset="/img/loading.gif" alt=""></p><p>【<strong>github情况</strong>】</p><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20190305153024.png" srcset="/img/loading.gif" alt=""></p><blockquote><p>综合来说，pytorch也是一个非常热门的趋势。</p></blockquote><h3 id="PyTorch环境配置"><a href="#PyTorch环境配置" class="headerlink" title="PyTorch环境配置"></a>PyTorch环境配置</h3><p><a href="https://sevenold.github.io/2018/08/TensorFlow-windows/" target="_blank" rel="noopener">GPU环境配置</a></p><p><a href="https://sevenold.github.io/2018/11/anaconda-windows/" target="_blank" rel="noopener">Pycharm-Python环境配置</a></p><p>【<a href="https://www.python.org/" target="_blank" rel="noopener">Python版本：3.6.x</a>】</p><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20190221154911.png" srcset="/img/loading.gif" alt=""></p><p>【<a href="https://developer.nvidia.com/cuda-downloads" target="_blank" rel="noopener">cuda版本：9.0</a>】</p><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20190221154939.png" srcset="/img/loading.gif" alt=""></p><p>【<a href="https://pytorch.org/" target="_blank" rel="noopener">torch版本：1.0</a>】</p><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20190221154743.png" srcset="/img/loading.gif" alt=""></p><p>【<a href="https://www.jetbrains.com/pycharm/" target="_blank" rel="noopener">pycharm：commit</a>】</p><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20190221155001.png" srcset="/img/loading.gif" alt=""></p><p>【<strong>测试</strong>】</p><pre><code class="hljs python"><span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch<span class="hljs-meta">&gt;&gt;&gt; </span>torch.__version__<span class="hljs-string">'1.0.1'</span><span class="hljs-meta">&gt;&gt;&gt; </span>torch.cuda.is_available()<span class="hljs-literal">True</span></code></pre><blockquote><p>输出为true，则表示环境配置成功并可以支持GPU运算。</p></blockquote><h3 id="PyTorch功能简介"><a href="#PyTorch功能简介" class="headerlink" title="PyTorch功能简介"></a>PyTorch功能简介</h3><p>【<strong>PyTorch动态图</strong>】</p><blockquote><p>每一步都是构建图的过程</p><ul><li>适合debug</li></ul></blockquote><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/dynamic_graph.gif" srcset="/img/loading.gif" alt=""></p><p>【<strong>tensorflow静态图</strong>】</p><blockquote><p>先建好图，然后计算图</p><ul><li>不需要中间修改</li><li>自建命名体系</li><li>自建时序控制</li><li>很难debug</li><li>2.0支持动态图优先</li></ul></blockquote><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20190305153526.png" srcset="/img/loading.gif" alt=""></p><p>【<strong>深度学习计算库</strong>】</p><ul><li><p><strong><code>GPU加速</code></strong></p><blockquote><p>代码实例：</p><pre><code class="hljs python"><span class="hljs-keyword">import</span> torch<span class="hljs-keyword">import</span> timeprint(torch.__version__)print(torch.cuda.is_available())  <span class="hljs-comment"># 判断是否支持GPU运算</span><span class="hljs-comment"># 初始化计算变量</span>a = torch.randn(<span class="hljs-number">10000</span>, <span class="hljs-number">1000</span>)b = torch.randn(<span class="hljs-number">1000</span>, <span class="hljs-number">2000</span>)start = time.time()<span class="hljs-comment"># 计算矩阵的乘法</span>c = torch.matmul(a, b)end = time.time()print(<span class="hljs-string">"cpu计算时间及结果"</span>, a.device, end-start, c.norm(<span class="hljs-number">2</span>))device = torch.device(<span class="hljs-string">'cuda'</span>)a = a.to(device)b = b.to(device)start = time.time()c = torch.matmul(a, b)end = time.time()print(<span class="hljs-string">"GPU初始化"</span>, a.device, end-start, c.norm(<span class="hljs-number">2</span>))start = time.time()c = torch.matmul(a, b)end = time.time()print(<span class="hljs-string">"GPU计算时间及结果"</span>, a.device, end-start, c.norm(<span class="hljs-number">2</span>))</code></pre><p>运行结果：</p><pre><code class="hljs python"><span class="hljs-number">1.0</span><span class="hljs-number">.1</span><span class="hljs-literal">True</span>cpu计算时间及结果 cpu <span class="hljs-number">0.21739435195922852</span> tensor(<span class="hljs-number">141500.7031</span>)GPU初始化 cuda:<span class="hljs-number">0</span> <span class="hljs-number">1.1588668823242188</span> tensor(<span class="hljs-number">141500.7188</span>, device=<span class="hljs-string">'cuda:0'</span>)GPU计算时间及结果 cuda:<span class="hljs-number">0</span> <span class="hljs-number">0.009351253509521484</span> tensor(<span class="hljs-number">141500.7188</span>, device=<span class="hljs-string">'cuda:0'</span>)</code></pre><p>使用GPU运算，在我的电脑上运算速度快了20多倍。</p></blockquote></li><li><p><strong><code>自动求导</code></strong></p><blockquote><p>代码实例：</p><p>公式：$y = a^2x+bx+c$</p><p>数学计算结果：2a、1、1</p><pre><code class="hljs python"><span class="hljs-keyword">import</span> torch<span class="hljs-keyword">import</span> time<span class="hljs-keyword">from</span> torch <span class="hljs-keyword">import</span> autogradprint(torch.__version__)print(torch.cuda.is_available())  <span class="hljs-comment"># 判断是否支持GPU运算</span>x = torch.tensor(<span class="hljs-number">1.</span>)a = torch.tensor(<span class="hljs-number">1.</span>, requires_grad=<span class="hljs-literal">True</span>)b = torch.tensor(<span class="hljs-number">2.</span>, requires_grad=<span class="hljs-literal">True</span>)c = torch.tensor(<span class="hljs-number">3.</span>, requires_grad=<span class="hljs-literal">True</span>)y = a**<span class="hljs-number">2</span> * x + b * x + cprint(<span class="hljs-string">'before:'</span>, a.grad, b.grad, c.grad)grads = autograd.grad(y, [a, b, c])print(<span class="hljs-string">'after :'</span>, grads[<span class="hljs-number">0</span>], grads[<span class="hljs-number">1</span>], grads[<span class="hljs-number">2</span>])</code></pre><p>运行结果</p><pre><code class="hljs python"><span class="hljs-number">1.0</span><span class="hljs-number">.1</span><span class="hljs-literal">True</span>before: <span class="hljs-literal">None</span> <span class="hljs-literal">None</span> <span class="hljs-literal">None</span>after : tensor(<span class="hljs-number">2.</span>) tensor(<span class="hljs-number">1.</span>) tensor(<span class="hljs-number">1.</span>)</code></pre></blockquote></li><li><p>常用网络层API</p><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20190221152531.png" srcset="/img/loading.gif" alt=""></p></li></ul>]]></content>
    
    
    <categories>
      
      <category>Pytorch</category>
      
    </categories>
    
    
    <tags>
      
      <tag>深度学习</tag>
      
      <tag>Pytorch</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>图片拼接</title>
    <link href="/2019/03/cv-IMAGE_JOIN/"/>
    <url>/2019/03/cv-IMAGE_JOIN/</url>
    
    <content type="html"><![CDATA[<pre><code class="hljs python"><span class="hljs-comment">#!/usr/bin/env python</span><span class="hljs-comment"># -*- coding: utf-8 -*-</span><span class="hljs-comment"># @Time    : 2019/2/16 21:29</span><span class="hljs-comment"># @Author  : Seven</span><span class="hljs-comment"># @File    : ImageJoin.py</span><span class="hljs-comment"># @Software: PyCharm</span><span class="hljs-comment"># function : 图片拼接</span><span class="hljs-keyword">import</span> cv2<span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">get_stitched_image</span><span class="hljs-params">(img1, img2, M)</span>:</span>    <span class="hljs-string">"""</span><span class="hljs-string">    使用关键点来缝合图像</span><span class="hljs-string">    :param img1:</span><span class="hljs-string">    :param img2:</span><span class="hljs-string">    :param M: 对应矩阵</span><span class="hljs-string">    :return: 拼接后的图片数据</span><span class="hljs-string">    """</span>    <span class="hljs-comment"># 获取输入图片的维度</span>    w1, h1 = img1.shape[:<span class="hljs-number">2</span>]    w2, h2 = img2.shape[:<span class="hljs-number">2</span>]    <span class="hljs-comment"># 生成对应维度的画布</span>    img1_dims = np.float32([[<span class="hljs-number">0</span>, <span class="hljs-number">0</span>], [<span class="hljs-number">0</span>, w1], [h1, w1], [h1, <span class="hljs-number">0</span>]]).reshape(<span class="hljs-number">-1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">2</span>)    img2_dims_temp = np.float32([[<span class="hljs-number">0</span>, <span class="hljs-number">0</span>], [<span class="hljs-number">0</span>, w2], [h2, w2], [h2, <span class="hljs-number">0</span>]]).reshape(<span class="hljs-number">-1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">2</span>)    <span class="hljs-comment"># 获取透视变换的交换矩阵</span>    img2_dims = cv2.perspectiveTransform(img2_dims_temp, M)    <span class="hljs-comment"># 最终的维度</span>    result_dims = np.concatenate((img1_dims, img2_dims), axis=<span class="hljs-number">0</span>)    <span class="hljs-comment"># 开始拼接图片</span>    <span class="hljs-comment"># 通过匹配点来计算维度</span>    [x_min, y_min] = np.int32(result_dims.min(axis=<span class="hljs-number">0</span>).ravel() - <span class="hljs-number">0.5</span>)    [x_max, y_max] = np.int32(result_dims.max(axis=<span class="hljs-number">0</span>).ravel() + <span class="hljs-number">0.5</span>)    <span class="hljs-comment"># 变换后创建输出矩阵</span>    transform_dist = [-x_min, -y_min]    transform_array = np.array([[<span class="hljs-number">1</span>, <span class="hljs-number">0</span>, transform_dist[<span class="hljs-number">0</span>]],                                [<span class="hljs-number">0</span>, <span class="hljs-number">1</span>, transform_dist[<span class="hljs-number">1</span>]],                                [<span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">1</span>]])    <span class="hljs-comment"># 修正图像以获得结果图像</span>    result_img = cv2.warpPerspective(img2, transform_array.dot(M),                                     (x_max - x_min, y_max - y_min))    result_img[transform_dist[<span class="hljs-number">1</span>]:w1 + transform_dist[<span class="hljs-number">1</span>], transform_dist[<span class="hljs-number">0</span>]:h1 + transform_dist[<span class="hljs-number">0</span>]] = img1    <span class="hljs-comment"># 返回最终结果</span>    <span class="hljs-keyword">return</span> result_img<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">get_sift_homography</span><span class="hljs-params">(img1, img2)</span>:</span>    <span class="hljs-string">"""</span><span class="hljs-string">    使用SIFT方法获取两张图片的关键点</span><span class="hljs-string">    :param img1:</span><span class="hljs-string">    :param img2:</span><span class="hljs-string">    :return: 关键点组成的对应矩阵</span><span class="hljs-string">    """</span>    <span class="hljs-comment"># 初始化SIFT方法</span>    sift = cv2.xfeatures2d_SIFT.create()    <span class="hljs-comment"># 获取关键点和描述子</span>    k1, d1 = sift.detectAndCompute(img1, <span class="hljs-literal">None</span>)    k2, d2 = sift.detectAndCompute(img2, <span class="hljs-literal">None</span>)    <span class="hljs-comment"># 初始化Bruteforce匹配器</span>    bf = cv2.BFMatcher()    <span class="hljs-comment"># 通过KNN匹配两张图片的描述子</span>    matches = bf.knnMatch(d1, d2, k=<span class="hljs-number">2</span>)    <span class="hljs-comment"># 确定最好的匹配</span>    verified_matches = []    <span class="hljs-keyword">for</span> m1, m2 <span class="hljs-keyword">in</span> matches:        <span class="hljs-comment"># 把匹配的较好的添加到矩阵中</span>        <span class="hljs-keyword">if</span> m1.distance &lt; <span class="hljs-number">0.8</span> * m2.distance:            verified_matches.append(m1)    <span class="hljs-comment"># 最小匹配数</span>    min_matches = <span class="hljs-number">8</span>    <span class="hljs-keyword">if</span> len(verified_matches) &gt; min_matches:        <span class="hljs-comment"># 存储匹配点</span>        img1_pts = []        img2_pts = []        <span class="hljs-comment"># 将匹配点加入矩阵中</span>        <span class="hljs-keyword">for</span> match <span class="hljs-keyword">in</span> verified_matches:            img1_pts.append(k1[match.queryIdx].pt)            img2_pts.append(k2[match.trainIdx].pt)        img1_pts = np.float32(img1_pts).reshape(<span class="hljs-number">-1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">2</span>)        img2_pts = np.float32(img2_pts).reshape(<span class="hljs-number">-1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">2</span>)        <span class="hljs-comment"># 计算单应矩阵</span>        M, mask = cv2.findHomography(img1_pts, img2_pts, cv2.RANSAC, <span class="hljs-number">5.0</span>)        <span class="hljs-keyword">return</span> M    <span class="hljs-keyword">else</span>:        print(<span class="hljs-string">'Error: Not enough matches'</span>)        exit()<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">changeImage</span><span class="hljs-params">(img1, img2)</span>:</span>    <span class="hljs-comment"># 使用SIFT查找关键点并返回关键点组成的矩阵</span>    M = get_sift_homography(img1, img2)    <span class="hljs-comment"># 使用关键点矩阵将图像缝合在一起</span>    result_image = get_stitched_image(img2, img1, M)    <span class="hljs-keyword">return</span> result_image<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">main</span><span class="hljs-params">()</span>:</span>    <span class="hljs-comment"># 加载图片</span>    img1 = cv2.imread(<span class="hljs-string">"images/csdn_A.jpg"</span>)    img2 = cv2.imread(<span class="hljs-string">"images/csdn_B.jpg"</span>)    img3 = cv2.imread(<span class="hljs-string">"images/csdn_C.jpg"</span>)    <span class="hljs-comment"># 显示加载的图片</span>    input_images = np.hstack((img1, img2, img3))    cv2.imshow(<span class="hljs-string">'Input Images'</span>, input_images)    out1 = changeImage(img1, img2)    out2 = changeImage(img1, img3)    result_image = changeImage(out1, out2)    <span class="hljs-comment"># 保存文件</span>    result_image_name = <span class="hljs-string">'results/result_tree.jpg'</span>    cv2.imwrite(result_image_name, result_image)    <span class="hljs-comment"># 显示拼接的结果</span>    cv2.imshow(<span class="hljs-string">'Result'</span>, result_image)    cv2.waitKey()<span class="hljs-keyword">if</span> __name__ == <span class="hljs-string">'__main__'</span>:    main()</code></pre><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20190323145021.png" srcset="/img/loading.gif" alt=""></p>]]></content>
    
    
    <categories>
      
      <category>机器视觉</category>
      
    </categories>
    
    
    <tags>
      
      <tag>图片拼接</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>基于SURF的特征匹配</title>
    <link href="/2019/03/cv-SURF_feature/"/>
    <url>/2019/03/cv-SURF_feature/</url>
    
    <content type="html"><![CDATA[<pre><code class="hljs python"><span class="hljs-comment">#!/usr/bin/env python</span><span class="hljs-comment"># -*- coding: utf-8 -*-</span><span class="hljs-comment"># @Time    : 2019/3/2 21:36</span><span class="hljs-comment"># @Author  : Seven</span><span class="hljs-comment"># @File    : ImageMatching-SURF.py</span><span class="hljs-comment"># @Software: PyCharm</span><span class="hljs-comment"># function : 使用SURF进行提取特征点进行图片匹配</span><span class="hljs-keyword">import</span> cv2<span class="hljs-comment"># 加载图片</span>imgL = cv2.imread(<span class="hljs-string">'image/A.jpg'</span>)imgR = cv2.imread(<span class="hljs-string">'image/B.jpg'</span>)<span class="hljs-comment"># 转换为灰度图</span>grayL = cv2.cvtColor(imgL, cv2.COLOR_BGR2GRAY)grayR = cv2.cvtColor(imgR, cv2.COLOR_BGR2GRAY)<span class="hljs-comment"># 提取特征点</span>surf = cv2.xfeatures2d.SURF_create(hessianThreshold=<span class="hljs-number">800</span>)   <span class="hljs-comment"># hessian矩阵阈值，在这里调整精度，值越大，点越少，越精准</span>kL, dL = surf.detectAndCompute(grayL, <span class="hljs-literal">None</span>)kR, dR = surf.detectAndCompute(grayR, <span class="hljs-literal">None</span>)<span class="hljs-comment"># 创建 BFMatcher 对象</span>bf = cv2.BFMatcher(cv2.NORM_L2)<span class="hljs-comment"># 根据描述子匹配特征点.</span>matches = bf.match(dL, dR)<span class="hljs-comment"># 画出匹配点</span>img3 = cv2.drawMatches(imgL, kL, imgR, kR, matches, <span class="hljs-literal">None</span>, flags=<span class="hljs-number">2</span>)cv2.imshow(<span class="hljs-string">"SURF"</span>, img3)<span class="hljs-comment"># ------------------------------------------------------------------------------------------------</span><span class="hljs-comment"># 设置FLANN 超参数</span>FLANN_INDEX_KDTREE = <span class="hljs-number">0</span><span class="hljs-comment"># K-D树索引超参数</span>index_params = dict(algorithm=FLANN_INDEX_KDTREE, trees=<span class="hljs-number">5</span>)<span class="hljs-comment"># 搜索超参数</span>search_params = dict(checks=<span class="hljs-number">50</span>)<span class="hljs-comment"># 初始化FlannBasedMatcher匹配器</span>flann = cv2.FlannBasedMatcher(index_params, search_params)<span class="hljs-comment"># 通过KNN的方式匹配两张图的描述子</span>matches = flann.knnMatch(dL, dR, k=<span class="hljs-number">2</span>)<span class="hljs-comment"># 筛选比较好的匹配点</span>good = []<span class="hljs-keyword">for</span> i, (m, n) <span class="hljs-keyword">in</span> enumerate(matches):    <span class="hljs-keyword">if</span> m.distance &lt; <span class="hljs-number">0.6</span> * n.distance:        good.append(m)<span class="hljs-comment"># 画出匹配点</span>img3 = cv2.drawMatches(imgL, kL, imgR, kR, good, <span class="hljs-literal">None</span>, flags=<span class="hljs-number">2</span>)cv2.imshow(<span class="hljs-string">"SURF-FLANN"</span>, img3)cv2.waitKey(<span class="hljs-number">0</span>)</code></pre><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20190323144253.png" srcset="/img/loading.gif" alt=""></p>]]></content>
    
    
    <categories>
      
      <category>机器视觉</category>
      
    </categories>
    
    
    <tags>
      
      <tag>SURF</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>基于SIFT的特征匹配</title>
    <link href="/2019/03/cv-SIFT_feature/"/>
    <url>/2019/03/cv-SIFT_feature/</url>
    
    <content type="html"><![CDATA[<pre><code class="hljs python"><span class="hljs-comment">#!/usr/bin/env python</span><span class="hljs-comment"># -*- coding: utf-8 -*-</span><span class="hljs-comment"># @Time    : 2019/3/2 22:26</span><span class="hljs-comment"># @Author  : Seven</span><span class="hljs-comment"># @File    : ImageMatching-SIFT.py</span><span class="hljs-comment"># @Software: PyCharm</span><span class="hljs-comment"># function : 使用SIFT进行提取特征点进行图片匹配</span><span class="hljs-keyword">import</span> cv2<span class="hljs-comment"># 加载图片</span>imgL = cv2.imread(<span class="hljs-string">'image/A.jpg'</span>)imgR = cv2.imread(<span class="hljs-string">'image/B.jpg'</span>)<span class="hljs-comment"># 转换为灰度图</span>grayL = cv2.cvtColor(imgL, cv2.COLOR_BGR2GRAY)grayR = cv2.cvtColor(imgR, cv2.COLOR_BGR2GRAY)<span class="hljs-comment"># 提取特征点</span>sift = cv2.xfeatures2d_SIFT.create()kL, dL = sift.detectAndCompute(grayL, <span class="hljs-literal">None</span>)kR, dR = sift.detectAndCompute(grayR, <span class="hljs-literal">None</span>)<span class="hljs-comment"># 创建 BFMatcher 对象</span>bf = cv2.BFMatcher(cv2.NORM_L2)<span class="hljs-comment"># 根据描述子匹配特征点.</span>matches = bf.match(dL, dR)<span class="hljs-comment"># 画出匹配点</span>img3 = cv2.drawMatches(imgL, kL, imgR, kR, matches, <span class="hljs-literal">None</span>, flags=<span class="hljs-number">2</span>)cv2.imshow(<span class="hljs-string">"SIFT"</span>, img3)<span class="hljs-comment"># ------------------------------------------------------------------------------------------------</span><span class="hljs-comment"># 设置FLANN 超参数</span>FLANN_INDEX_KDTREE = <span class="hljs-number">0</span><span class="hljs-comment"># K-D树索引超参数</span>index_params = dict(algorithm=FLANN_INDEX_KDTREE, trees=<span class="hljs-number">5</span>)<span class="hljs-comment"># 搜索超参数</span>search_params = dict(checks=<span class="hljs-number">50</span>)<span class="hljs-comment"># 初始化FlannBasedMatcher匹配器</span>flann = cv2.FlannBasedMatcher(index_params, search_params)<span class="hljs-comment"># 通过KNN的方式匹配两张图的描述子</span>matches = flann.knnMatch(dL, dR, k=<span class="hljs-number">2</span>)<span class="hljs-comment"># 筛选比较好的匹配点</span>good = []<span class="hljs-keyword">for</span> i, (m, n) <span class="hljs-keyword">in</span> enumerate(matches):    <span class="hljs-keyword">if</span> m.distance &lt; <span class="hljs-number">0.6</span> * n.distance:        good.append(m)<span class="hljs-comment"># 画出匹配点</span>img3 = cv2.drawMatches(imgL, kL, imgR, kR, good, <span class="hljs-literal">None</span>, flags=<span class="hljs-number">2</span>)cv2.imshow(<span class="hljs-string">"SIFT-FLANN"</span>, img3)cv2.waitKey(<span class="hljs-number">0</span>)</code></pre><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20190323144134.png" srcset="/img/loading.gif" alt=""></p>]]></content>
    
    
    <categories>
      
      <category>机器视觉</category>
      
    </categories>
    
    
    <tags>
      
      <tag>SIFT</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>基于Harris的特征匹配</title>
    <link href="/2019/03/cv-Harris_feature/"/>
    <url>/2019/03/cv-Harris_feature/</url>
    
    <content type="html"><![CDATA[<pre><code class="hljs python"><span class="hljs-comment">#!/usr/bin/env python</span><span class="hljs-comment"># -*- coding: utf-8 -*-</span><span class="hljs-comment"># @Time    : 2019/3/4 14:35</span><span class="hljs-comment"># @Author  : Seven</span><span class="hljs-comment"># @File    : ImageMatching-Harris.py</span><span class="hljs-comment"># @Software: PyCharm</span><span class="hljs-comment"># function : 使用Harris进行提取特征点进行图片匹配</span><span class="hljs-comment"># 选择了一种速度、特征点数量和精度都比较好的组合方案：</span><span class="hljs-comment"># FAST角点检测算法+SURF特征描述子+FLANN(Fast Library for Approximate Nearest Neighbors) 匹配算法。</span><span class="hljs-keyword">import</span> cv2<span class="hljs-comment"># 加载图片</span>imgL = cv2.imread(<span class="hljs-string">'image/A.jpg'</span>)imgR = cv2.imread(<span class="hljs-string">'image/B.jpg'</span>)<span class="hljs-comment"># 转换为灰度图</span>grayL = cv2.cvtColor(imgL, cv2.COLOR_BGR2GRAY)grayR = cv2.cvtColor(imgR, cv2.COLOR_BGR2GRAY)<span class="hljs-comment"># 提取特征点</span>harris = cv2.xfeatures2d_HarrisLaplaceFeatureDetector.create()kL = harris.detect(grayL, <span class="hljs-literal">None</span>)kR = harris.detect(grayR, <span class="hljs-literal">None</span>)<span class="hljs-comment"># 提取描述子</span>br = cv2.BRISK_create()kL, dL = br.compute(grayL, kL)kR, dR = br.compute(grayR, kR)<span class="hljs-comment"># 创建 BFMatcher 对象</span>bf = cv2.BFMatcher(cv2.NORM_L2)<span class="hljs-comment"># 根据描述子匹配特征点.</span>matches = bf.match(dL, dR)<span class="hljs-comment"># 画出匹配点</span>img3 = cv2.drawMatches(imgL, kL, imgR, kR, matches, <span class="hljs-literal">None</span>, flags=<span class="hljs-number">2</span>)cv2.imshow(<span class="hljs-string">"Harris"</span>, img3)<span class="hljs-comment"># ------------------------------------------------------------------------------------------------</span><span class="hljs-comment"># 初始化Bruteforce匹配器</span>bf = cv2.BFMatcher()<span class="hljs-comment"># 通过KNN匹配两张图片的描述子</span>matches = bf.knnMatch(dL, dR, k=<span class="hljs-number">2</span>)<span class="hljs-comment"># 筛选比较好的匹配点</span>good = []<span class="hljs-keyword">for</span> i, (m, n) <span class="hljs-keyword">in</span> enumerate(matches):    <span class="hljs-keyword">if</span> m.distance &lt; <span class="hljs-number">0.6</span> * n.distance:        good.append(m)<span class="hljs-comment"># 画出匹配点</span>img3 = cv2.drawMatches(imgL, kL, imgR, kR, good, <span class="hljs-literal">None</span>, flags=<span class="hljs-number">2</span>)cv2.imshow(<span class="hljs-string">"Harris-BF"</span>, img3)cv2.waitKey(<span class="hljs-number">0</span>)</code></pre><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20190323143639.png" srcset="/img/loading.gif" alt=""></p>]]></content>
    
    
    <categories>
      
      <category>机器视觉</category>
      
    </categories>
    
    
    <tags>
      
      <tag>ORB</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>基于ORB的特征匹配</title>
    <link href="/2019/03/cv-ORB_feature/"/>
    <url>/2019/03/cv-ORB_feature/</url>
    
    <content type="html"><![CDATA[<pre><code class="hljs python"><span class="hljs-comment">#!/usr/bin/env python</span><span class="hljs-comment"># -*- coding: utf-8 -*-</span><span class="hljs-comment"># @Time    : 2019/3/4 12:54</span><span class="hljs-comment"># @Author  : Seven</span><span class="hljs-comment"># @File    : ImageMatching-ORB.py</span><span class="hljs-comment"># @Software: PyCharm</span><span class="hljs-comment"># function : 使用ORB进行提取特征点进行图片匹配</span><span class="hljs-keyword">import</span> cv2<span class="hljs-comment"># 加载图片</span>imgL = cv2.imread(<span class="hljs-string">'image/A.jpg'</span>)imgR = cv2.imread(<span class="hljs-string">'image/B.jpg'</span>)<span class="hljs-comment"># 转换为灰度图</span>grayL = cv2.cvtColor(imgL, cv2.COLOR_BGR2GRAY)grayR = cv2.cvtColor(imgR, cv2.COLOR_BGR2GRAY)<span class="hljs-comment"># 提取特征点</span>orb = cv2.ORB_create()kL, dL = orb.detectAndCompute(grayL, <span class="hljs-literal">None</span>)kR, dR = orb.detectAndCompute(grayR, <span class="hljs-literal">None</span>)<span class="hljs-comment"># 创建 BFMatcher 对象</span>bf = cv2.BFMatcher(cv2.NORM_L2)<span class="hljs-comment"># 根据描述子匹配特征点.</span>matches = bf.match(dL, dR)<span class="hljs-comment"># 画出匹配点</span>img3 = cv2.drawMatches(imgL, kL, imgR, kR, matches, <span class="hljs-literal">None</span>, flags=<span class="hljs-number">2</span>)cv2.imshow(<span class="hljs-string">"ORB"</span>, img3)<span class="hljs-comment"># ------------------------------------------------------------------------------------------------</span><span class="hljs-comment"># 初始化Bruteforce匹配器</span>bf = cv2.BFMatcher()<span class="hljs-comment"># 通过KNN匹配两张图片的描述子</span>matches = bf.knnMatch(dL, dR, k=<span class="hljs-number">2</span>)<span class="hljs-comment"># 筛选比较好的匹配点</span>good = []<span class="hljs-keyword">for</span> i, (m, n) <span class="hljs-keyword">in</span> enumerate(matches):    <span class="hljs-keyword">if</span> m.distance &lt; <span class="hljs-number">0.6</span> * n.distance:        good.append(m)<span class="hljs-comment"># 画出匹配点</span>img3 = cv2.drawMatches(imgL, kL, imgR, kR, good, <span class="hljs-literal">None</span>, flags=<span class="hljs-number">2</span>)cv2.imshow(<span class="hljs-string">"ORB-BF"</span>, img3)cv2.waitKey(<span class="hljs-number">0</span>)</code></pre><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20190323143807.png" srcset="/img/loading.gif" alt=""></p>]]></content>
    
    
    <categories>
      
      <category>机器视觉</category>
      
    </categories>
    
    
    <tags>
      
      <tag>ORB</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>基于FAST的特征匹配</title>
    <link href="/2019/03/cv-fast_feature/"/>
    <url>/2019/03/cv-fast_feature/</url>
    
    <content type="html"><![CDATA[<pre><code class="hljs python"><span class="hljs-comment">#!/usr/bin/env python</span><span class="hljs-comment"># -*- coding: utf-8 -*-</span><span class="hljs-comment"># @Time    : 2019/3/4 13:57</span><span class="hljs-comment"># @Author  : Seven</span><span class="hljs-comment"># @File    : ImageMatching-FAST.py</span><span class="hljs-comment"># @Software: PyCharm</span><span class="hljs-comment"># function : 使用FAST进行提取特征点进行图片匹配</span><span class="hljs-keyword">import</span> cv2<span class="hljs-comment"># 加载图片</span>imgL = cv2.imread(<span class="hljs-string">'image/A.jpg'</span>)imgR = cv2.imread(<span class="hljs-string">'image/B.jpg'</span>)<span class="hljs-comment"># 转换为灰度图</span>grayL = cv2.cvtColor(imgL, cv2.COLOR_BGR2GRAY)grayR = cv2.cvtColor(imgR, cv2.COLOR_BGR2GRAY)<span class="hljs-comment"># 提取特征点</span>fast = cv2.FastFeatureDetector_create(<span class="hljs-number">50</span>)kL = fast.detect(grayL, <span class="hljs-literal">None</span>)kR = fast.detect(grayR, <span class="hljs-literal">None</span>)<span class="hljs-comment"># 提取描述子</span>br = cv2.BRISK_create()kL, dL = br.compute(grayL, kL)kR, dR = br.compute(grayR, kR)<span class="hljs-comment"># 创建 BFMatcher 对象</span>bf = cv2.BFMatcher(cv2.NORM_L2)<span class="hljs-comment"># 根据描述子匹配特征点.</span>matches = bf.match(dL, dR)<span class="hljs-comment"># 画出匹配点</span>img3 = cv2.drawMatches(imgL, kL, imgR, kR, matches, <span class="hljs-literal">None</span>, flags=<span class="hljs-number">2</span>)cv2.imshow(<span class="hljs-string">"FAST"</span>, img3)<span class="hljs-comment"># ------------------------------------------------------------------------------------------------</span><span class="hljs-comment"># 初始化Bruteforce匹配器</span>bf = cv2.BFMatcher()<span class="hljs-comment"># 通过KNN匹配两张图片的描述子</span>matches = bf.knnMatch(dL, dR, k=<span class="hljs-number">2</span>)<span class="hljs-comment"># 筛选比较好的匹配点</span>good = []<span class="hljs-keyword">for</span> i, (m, n) <span class="hljs-keyword">in</span> enumerate(matches):    <span class="hljs-keyword">if</span> m.distance &lt; <span class="hljs-number">0.6</span> * n.distance:        good.append(m)<span class="hljs-comment"># 画出匹配点</span>img3 = cv2.drawMatches(imgL, kL, imgR, kR, good, <span class="hljs-literal">None</span>, flags=<span class="hljs-number">2</span>)cv2.imshow(<span class="hljs-string">"FAST-BF"</span>, img3)cv2.waitKey(<span class="hljs-number">0</span>)</code></pre><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20190323143205.png" srcset="/img/loading.gif" alt=""></p>]]></content>
    
    
    <categories>
      
      <category>机器视觉</category>
      
    </categories>
    
    
    <tags>
      
      <tag>FAST</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>【二十】特征匹配</title>
    <link href="/2019/03/cv-image_feature/"/>
    <url>/2019/03/cv-image_feature/</url>
    
    <content type="html"><![CDATA[<h3 id="特征匹配"><a href="#特征匹配" class="headerlink" title="特征匹配"></a>特征匹配</h3><h4 id="【基本问题】"><a href="#【基本问题】" class="headerlink" title="【基本问题】"></a>【基本问题】</h4><ul><li><p>特征点匹配</p><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20190302143508.png" srcset="/img/loading.gif" alt=""></p></li><li><p>暴力搜索与2NN判据</p><blockquote><p>设两帧图像中的对应特征点集${𝑥<em>𝑖,𝑦_𝑖}$和${𝑥_𝑖^′,𝑦_𝑖^′}$，共N个特征点 对应点 $𝐱_𝒋=(𝑥_𝑗,𝑦_𝑗)$，匹配点为距离最小点 $𝐱_𝑗^′=𝑥_𝑗^′∗,𝑦_𝑗^′∗=𝑚𝑖𝑛</em>{𝑘=1}^𝑁||x_𝑗−𝑥_𝑘^′||$ ，对应距离$𝑑_𝑗^∗$ 进一步得到次小点𝐱′′，对应距离$𝑑_𝑗^{∗′}$, 则匹配点满足：$𝑑_𝑗^∗&lt;𝛼∗𝑑_𝑗^{∗′}$, 认为正常匹配.</p><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20190302144003.png" srcset="/img/loading.gif" alt=""></p></blockquote></li></ul><h4 id="【特征点匹配方法】"><a href="#【特征点匹配方法】" class="headerlink" title="【特征点匹配方法】"></a>【特征点匹配方法】</h4><ul><li><p>快速搜索方式—-二叉树</p><blockquote><ul><li>算法复杂度o(N2)。特征点数量多时，匹配效率低。</li><li>二叉搜索树(BST)提供了高效搜索方式</li><li>以下是一颗一维的二叉搜索树，尝试搜索和11最近的点。</li></ul><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20190302144354.png" srcset="/img/loading.gif" alt=""></p></blockquote></li><li><p>K-D树</p><blockquote><p>对于每一层，可以指定一个划分维度（轴垂直分区面axis-aligned splitting planes）。最简单的就是按照关键字轮流划分（例如：奇数层按照x轴划分，也即第一个关键字；偶数层按照y轴划分，也即第二个关键字）。</p><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20190302144540.png" srcset="/img/loading.gif" alt=""></p></blockquote></li><li><p>K-D树的建立方式</p><blockquote><p>对于所有的样本点，统计它们在每个维上的方差，挑选出方差中的最大值，对应的维就是分裂域的值。数据方差最大表明沿该维度数据点分散得比较开，这个方向上进行数据分割可以获得最好的分辨率；然后再将所有样本点按其第该维的值迕行排序，位于正中间的那个数据点选为分裂结点对应域。重复上述过程直至获得所有叶子节点显示了构建返棵二叉树的所有步骤。</p><p>下面以一个简单的例子来解释上述k-d tree的构建过程。</p><p>假设样本集为：{(2,3), (5,4), (9,6), (4,7), (8,1), (7,2)}。</p><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20190302144826.png" srcset="/img/loading.gif" alt=""></p></blockquote></li><li><p>K-D树最近邻查询算法</p><blockquote><ol><li>首先通过将查找点数据根结点数据对应维 上的值相比较，按照二叉搜索的方式，顺着“搜索路径”找到最近邻的近似点，也就是 与查询点处于同一个子空间的叶子节点；</li><li>为了防止漏查与查找点 跟进的距离的点，回溯搜索路径，并且判断搜索路径上节点的其他子节点空 间中是否还有距离查询点更近的数据点，如果有，则需要跳到其他子节点空间中去搜索。</li><li>重复返个过程直到搜索路径为空。</li></ol><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20190302145139.png" srcset="/img/loading.gif" alt=""></p></blockquote></li><li><p>BBF(Best Bin First)</p><blockquote><p>​    BBF的查询思路就是将“查询路径”上的节点进行排序，如按各自分割超平面（称为Bin）与查询点的距离排序,优先考虑距离小的点。BBF还设置了一个运行超时限制，当优先级队列中的所有节点都经过检查或者超出时间限制时，算法返回当前找到的最好结果作为近似的最近邻。</p></blockquote></li><li><p>随机化K-D森林</p><blockquote><p>同时独立建立多个k-d树，每棵树在具有大方差的各维中(如top-5)随机选择。查询时，并行查询多个k-d树，按照BBF准侧将候选节点放在同一队列中。</p><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20190302145410.png" srcset="/img/loading.gif" alt=""></p></blockquote></li></ul><h4 id="【RANSAC】"><a href="#【RANSAC】" class="headerlink" title="【RANSAC】"></a>【RANSAC】</h4><ul><li><p>稳健(robust): 对数据噪声的敏感性</p><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20190302145622.png" srcset="/img/loading.gif" alt=""></p></li><li><p>基本思想</p><blockquote><p>RANSAC通过反复选择数据中的一组随机子集来达成目标。被选取的子集被假设为局内点，并用下述方法进行验证：</p><ol><li>有一个模型适应于假设的局内点，即所有的未知参数都能从假设的局内点计算得出。 </li><li>用1中得到的模型去测试所有的其它数据，如果某个点适用于估计的模型，认为它也是局内点。 </li><li>如果有足够多的点被归类为假设的局内点，那么估计的模型就足够合理。 </li><li>然后，用所有假设的局内点去重新估计模型，因为它仅仅被初始的假设局内点估计过。 </li><li>最后，通过估计局内点与模型的错误率来评估模型。 </li></ol><p>这个过程被重复执行固定的次数，每次产生的模型要么因为局内点太少而被舍弃，要么因为比现有的模型更好而被选用</p></blockquote></li><li><p>SIFT和RANSAC结合</p><blockquote><p>RANSAC算法在SIFT特征筛选中的主要流程：</p><ol><li>从样本集中随机抽选一个RANSAC样本，即4个匹配点对</li><li>根据返4个匹配点对计算变换矩阵M</li><li>根据样本集，变换矩阵M，和误差度量函数计算满足当前变换矩阵的一致集consensus，并返回一致集中元素个数</li><li>根据当前一致集中元素个数判断是否最优(最大)一致集，若是则更新当前最优一致集</li><li>更新当前错误概率p，若p大于允许的最小错误概率则重复(1)至(4)继续迭代，直到当前错误概率p小于最小错误概率</li></ol></blockquote></li><li><p>结果比较：2NN vs 2NN+RANSAC</p><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20190302150106.png" srcset="/img/loading.gif" alt=""></p></li></ul><h4 id="【总结】"><a href="#【总结】" class="headerlink" title="【总结】"></a>【总结】</h4><blockquote><ul><li>特征匹配是立体视觉、全景视觉、由运动到结构的重要环节</li><li>使用基于k-d树的特征匹配方法，能有效提高搜索效率</li><li>RANSAC是一类随机稳健估计方法，可有效滤除误配点</li></ul></blockquote>]]></content>
    
    
    <categories>
      
      <category>机器视觉</category>
      
    </categories>
    
    
    <tags>
      
      <tag>特征匹配</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>【十九】立体视觉</title>
    <link href="/2019/03/cv-image_Stereo/"/>
    <url>/2019/03/cv-image_Stereo/</url>
    
    <content type="html"><![CDATA[<h3 id="立体视觉"><a href="#立体视觉" class="headerlink" title="立体视觉"></a>立体视觉</h3><h4 id="【基本概念】"><a href="#【基本概念】" class="headerlink" title="【基本概念】"></a>【基本概念】</h4><ul><li><p>我们在二维视角中，结构和深度是不确定的</p><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20190302140557.png" srcset="/img/loading.gif" alt=""></p></li><li><p>立体视觉：第2个照相机可以解决这种歧义性，通过三角化实现深度测量</p><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20190301202859.png" srcset="/img/loading.gif" alt=""></p></li></ul><h4 id="【平行双目视觉】"><a href="#【平行双目视觉】" class="headerlink" title="【平行双目视觉】"></a>【平行双目视觉】</h4><ul><li><p>假设双目完全平行</p><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20190302140858.png" srcset="/img/loading.gif" alt=""></p></li><li><p>空间点三维座标位置求解</p><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20190302161109.png" srcset="/img/loading.gif" alt=""></p></li><li><p>视差和深度成反比关系: $z_1 = \frac{bf_x}{u_1-u_2}$</p><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20190302141519.png" srcset="/img/loading.gif" alt=""></p></li><li><p>空间点三维座标位置求解：一般情况</p><blockquote><ul><li>二摄像机坐标系与世界坐标系位姿关系已知</li></ul><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20190302141616.png" srcset="/img/loading.gif" alt=""></p><ul><li>共4个方程，三个未知数，可求解坐标</li></ul></blockquote></li></ul><h4 id="【三维重构】"><a href="#【三维重构】" class="headerlink" title="【三维重构】"></a>【三维重构】</h4><ul><li><p>三维重构步骤</p><blockquote><ol><li>提取特征点，建立特征匹配</li><li>计算视差</li><li>计算世界坐标</li><li>三角剖分</li><li>三维重构 </li></ol></blockquote></li><li><p>提取特征点并建立匹配</p><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20190302142133.png" srcset="/img/loading.gif" alt=""></p></li><li><p>特征匹配方式</p><blockquote><ul><li>特征点提取+特征匹配</li><li>光流匹配</li><li>块匹配</li><li>立体矫正+平行匹配</li></ul></blockquote></li><li><p>视差计算</p><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20190302142321.png" srcset="/img/loading.gif" alt=""></p></li><li><p>计算世界坐标–形成点云数据</p><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20190302142340.png" srcset="/img/loading.gif" alt=""></p></li><li><p>三角剖分：采用经典的Delauney算法</p><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20190302142431.png" srcset="/img/loading.gif" alt=""></p><blockquote><p>Delauney算法示意：</p><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20190302142536.png" srcset="/img/loading.gif" alt=""></p><p>Delaunay三角网是唯一的（任意四点不能共圆），在Delaunay三角形网中任一三角形的外接圆范围内不会有其它点存在。</p></blockquote></li><li><p>三维重构：基于计算坐标，采用OpenGL绘制三角片</p><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20190302142640.png" srcset="/img/loading.gif" alt=""></p><blockquote><p>效果不好的主要原因，是图像中深度变化较大，同时灰度变换，而特征点选取的比较稀疏.</p></blockquote></li></ul><h4 id="【总结】"><a href="#【总结】" class="headerlink" title="【总结】"></a>【总结】</h4><blockquote><ul><li>立体视觉可计算空间点的三维坐标。基线越长，距离越近，精度越高</li><li>根据双目视觉进行三维重构包括特征点提取、匹配、坐标计算、三角剖分、三维重构等几个步骤</li></ul></blockquote>]]></content>
    
    
    <categories>
      
      <category>机器视觉</category>
      
    </categories>
    
    
    <tags>
      
      <tag>立体视觉</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>【十八】极线几何</title>
    <link href="/2019/03/cv-image_PolarGeometry/"/>
    <url>/2019/03/cv-image_PolarGeometry/</url>
    
    <content type="html"><![CDATA[<h3 id="极线几何"><a href="#极线几何" class="headerlink" title="极线几何"></a>极线几何</h3><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20190301202859.png" srcset="/img/loading.gif" alt=""></p><blockquote><p>有时候我们使用一个相机进行拍摄目标物体的时候，会 发现几个物体都重合了，但是当我们再放台相机的时候，就可以把这些物体的特征获取到。</p><p>也就是双目视觉对应关系，同时也可用于相邻两帧间的运动估计。</p></blockquote><h4 id="【基本概念】"><a href="#【基本概念】" class="headerlink" title="【基本概念】"></a>【基本概念】</h4><blockquote><p>极线几何(Epipolar geometry)</p><ul><li>(匹配)极线约束：匹配点必须在极线上</li><li>基线：左右像机光心连线</li><li>极平面：空间点，两像机光心决定的平面</li><li>极点：基线与两摄像机图像平面的交点</li><li>极线：极平面与图片平面的交线</li></ul></blockquote><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20190301203308.png" srcset="/img/loading.gif" alt=""></p><h4 id="【本质矩阵】"><a href="#【本质矩阵】" class="headerlink" title="【本质矩阵】"></a>【本质矩阵】</h4><blockquote><p><strong>本质矩阵</strong> <strong>E</strong>（Essential Matrix）：反映【<strong>空间一点</strong> <strong>P</strong> <strong>的像点】</strong>在【<strong>不同视角摄像机】</strong>下【<strong>摄像机坐标系】</strong>中的表示之间的关系。</p></blockquote><ul><li><p>前面我们已经知道了各个坐标系之前的转换</p><ul><li>相机坐标系与世界坐标系</li></ul><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20190301204745.png" srcset="/img/loading.gif" alt=""></p><ul><li>相机坐标系与图像坐标系</li></ul><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/1551444580057.png" srcset="/img/loading.gif" alt=""></p></li><li><p>两相机坐标系某点与对应图像坐标系的关系：</p><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20190301205229.png" srcset="/img/loading.gif" alt=""></p></li><li><p>同一点在两相机坐标系之间的关系：</p><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20190301205312.png" srcset="/img/loading.gif" alt=""></p></li><li><p>两边同时叉积$t$：</p><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20190301205438.png" srcset="/img/loading.gif" alt=""></p></li><li><p>再与$p_r^\sim$点积：</p><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/1551445425466.png" srcset="/img/loading.gif" alt=""></p></li></ul><h4 id="【本质矩阵求解】"><a href="#【本质矩阵求解】" class="headerlink" title="【本质矩阵求解】"></a>【本质矩阵求解】</h4><ul><li><p>基本方程</p><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20190301210555.png" srcset="/img/loading.gif" alt=""></p></li><li><p>线性方程求解</p><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20190301210713.png" srcset="/img/loading.gif" alt=""></p><blockquote><p>有九个点(非共面)时，可获得线性解：</p><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20190301210826.png" srcset="/img/loading.gif" alt=""></p><p>注意：解与真实解相差一个比例系数</p></blockquote></li><li><p>使用SVD分解求解平移和旋转矩阵</p><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20190301210911.png" srcset="/img/loading.gif" alt=""></p><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20190301210925.png" srcset="/img/loading.gif" alt=""></p><blockquote><p>可以证明，本质矩阵有2个相同的非零特征值</p></blockquote><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/1551446959523.png" srcset="/img/loading.gif" alt=""></p><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20190301212931.png" srcset="/img/loading.gif" alt=""></p><blockquote><p>因此，最终可以得到4个解，但仅有一个合理解</p></blockquote></li></ul><h4 id="【扩展】"><a href="#【扩展】" class="headerlink" title="【扩展】"></a>【扩展】</h4><blockquote><p><strong>基本矩阵(Fundamental matrix)</strong>： 反映【<strong>空间一点</strong> <strong>P</strong> <strong>的像素点】</strong>在【<strong>不同视角摄像机】</strong>下【<strong>图像坐标系</strong>】中的表示之间的关系。</p></blockquote><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20190301213256.png" srcset="/img/loading.gif" alt=""></p><blockquote><p>两个对应点在像素座标系的对应关系(包含相机内参数信息)</p></blockquote><h4 id="【总结】"><a href="#【总结】" class="headerlink" title="【总结】"></a>【总结】</h4><blockquote><ul><li>极线是极平面和像平面交线，极点是极线和基线交点</li><li>本质矩阵确定了两帧图像中对应点的约束关系</li><li>可以通过8个对应点求解本质矩阵，进一步分解得到R和t</li></ul></blockquote><h3 id="代码实例"><a href="#代码实例" class="headerlink" title="代码实例"></a>代码实例</h3><pre><code class="hljs python"><span class="hljs-comment">#!/usr/bin/env python</span><span class="hljs-comment"># -*- coding: utf-8 -*-</span><span class="hljs-comment"># @Time    : 2019/3/1 22:03</span><span class="hljs-comment"># @Author  : Seven</span><span class="hljs-comment"># @File    : PolarGeometry.py</span><span class="hljs-comment"># @Software: PyCharm</span><span class="hljs-comment"># function : 极线几何，测量极线</span><span class="hljs-keyword">import</span> cv2<span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt<span class="hljs-comment"># 加载图片</span>img1 = cv2.imread(<span class="hljs-string">'image/l.jpg'</span>, <span class="hljs-number">0</span>)img2 = cv2.imread(<span class="hljs-string">'image/r.jpg'</span>, <span class="hljs-number">0</span>)<span class="hljs-comment"># 初始化SIFT方法</span>sift = cv2.xfeatures2d_SIFT.create()<span class="hljs-comment"># 获取关键点和描述子</span>k1, d1 = sift.detectAndCompute(img1, <span class="hljs-literal">None</span>)k2, d2 = sift.detectAndCompute(img2, <span class="hljs-literal">None</span>)<span class="hljs-comment"># 设置FLANN 超参数</span>FLANN_INDEX_KDTREE = <span class="hljs-number">0</span><span class="hljs-comment"># K-D树索引超参数</span>index_params = dict(algorithm=FLANN_INDEX_KDTREE, trees=<span class="hljs-number">5</span>)<span class="hljs-comment"># 搜索超参数</span>search_params = dict(checks=<span class="hljs-number">50</span>)<span class="hljs-comment"># 初始化FlannBasedMatcher匹配器</span>flann = cv2.FlannBasedMatcher(index_params, search_params)<span class="hljs-comment"># 通过KNN的方式匹配两张图的描述子</span>matches = flann.knnMatch(d1, d2, k=<span class="hljs-number">2</span>)good = []pts1 = []pts2 = []<span class="hljs-comment"># 筛选比较好的匹配点</span><span class="hljs-keyword">for</span> i, (m, n) <span class="hljs-keyword">in</span> enumerate(matches):    <span class="hljs-keyword">if</span> m.distance &lt; <span class="hljs-number">0.8</span> * n.distance:        good.append(m)        pts2.append(k2[m.trainIdx].pt)        pts1.append(k1[m.queryIdx].pt)<span class="hljs-comment"># 计算基础矩阵</span>pts1 = np.int32(pts1)pts2 = np.int32(pts2)<span class="hljs-comment"># F为基本矩阵、mask是返回基本矩阵的值：没有找到矩阵，返回0，找到一个矩阵返回1，多个矩阵返回3</span>F, mask = cv2.findFundamentalMat(pts1, pts2, cv2.FM_LMEDS)<span class="hljs-comment"># 只选择有效数据</span>pts1 = pts1[mask.ravel() == <span class="hljs-number">1</span>]pts2 = pts2[mask.ravel() == <span class="hljs-number">1</span>]<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">drawlines</span><span class="hljs-params">(img1, img2, lines, pts1, pts2)</span>:</span>    <span class="hljs-string">"""</span><span class="hljs-string">    绘制图像极线</span><span class="hljs-string">    :param img1:</span><span class="hljs-string">    :param img2:</span><span class="hljs-string">    :param lines:</span><span class="hljs-string">    :param pts1:</span><span class="hljs-string">    :param pts2:</span><span class="hljs-string">    :return:</span><span class="hljs-string">    """</span>    r, c = img1.shape    img1 = cv2.cvtColor(img1, cv2.COLOR_GRAY2BGR)    img2 = cv2.cvtColor(img2, cv2.COLOR_GRAY2BGR)    <span class="hljs-keyword">for</span> r, pt1, pt2 <span class="hljs-keyword">in</span> zip(lines, pts1, pts2):        color = tuple(np.random.randint(<span class="hljs-number">0</span>, <span class="hljs-number">255</span>, <span class="hljs-number">3</span>).tolist())        x0, y0 = map(int, [<span class="hljs-number">0</span>, -r[<span class="hljs-number">2</span>] / r[<span class="hljs-number">1</span>]])        x1, y1 = map(int, [c, -(r[<span class="hljs-number">2</span>] + r[<span class="hljs-number">0</span>] * c) / r[<span class="hljs-number">1</span>]])        img1 = cv2.line(img1, (x0, y0), (x1, y1), color, <span class="hljs-number">1</span>)        img1 = cv2.circle(img1, tuple(pt1), <span class="hljs-number">5</span>, color, <span class="hljs-number">-1</span>)        img2 = cv2.circle(img2, tuple(pt2), <span class="hljs-number">5</span>, color, <span class="hljs-number">-1</span>)    <span class="hljs-keyword">return</span> img1, img2<span class="hljs-comment"># 在右图（第二图）中找到与点相对应的极线，并在左图上画出它的线。</span>lines1 = cv2.computeCorrespondEpilines(pts2.reshape(<span class="hljs-number">-1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">2</span>), <span class="hljs-number">2</span>, F)lines1 = lines1.reshape(<span class="hljs-number">-1</span>, <span class="hljs-number">3</span>)img5, img6 = drawlines(img1, img2, lines1, pts1, pts2)<span class="hljs-comment"># 找到与左图像（第一个图像）中的点对应的极线，以及在右图上画线</span>lines2 = cv2.computeCorrespondEpilines(pts1.reshape(<span class="hljs-number">-1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">2</span>), <span class="hljs-number">1</span>, F)lines2 = lines2.reshape(<span class="hljs-number">-1</span>, <span class="hljs-number">3</span>)img3, img4 = drawlines(img2, img1, lines2, pts2, pts1)plt.subplot(<span class="hljs-number">121</span>), plt.imshow(img5)plt.subplot(<span class="hljs-number">122</span>), plt.imshow(img3)plt.show()</code></pre><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20190323142108.png" srcset="/img/loading.gif" alt=""></p>]]></content>
    
    
    <categories>
      
      <category>机器视觉</category>
      
    </categories>
    
    
    <tags>
      
      <tag>极线几何</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>【十七】相机标定</title>
    <link href="/2019/03/cv-image_calibration/"/>
    <url>/2019/03/cv-image_calibration/</url>
    
    <content type="html"><![CDATA[<h3 id="相机标定"><a href="#相机标定" class="headerlink" title="相机标定"></a>相机标定</h3><h4 id="【基本问题】"><a href="#【基本问题】" class="headerlink" title="【基本问题】"></a>【基本问题】</h4><ul><li><p><strong>相机内外参数标定</strong></p><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20190227152254.png" srcset="/img/loading.gif" alt=""></p><blockquote><ul><li>计算$m_{ij}$的解</li><li>分解内、外参数</li><li>考虑非线性项</li></ul></blockquote></li></ul><h4 id="【Zhang方法】"><a href="#【Zhang方法】" class="headerlink" title="【Zhang方法】"></a>【Zhang方法】</h4><ul><li><p>Zhang方法：由张正友提出，OpenCV等广泛使用</p><blockquote><ul><li>特点：使用平面靶标摆多个pose(可未知)</li><li>标定步骤<ul><li>对一个pose,计算单应矩阵(类似M矩阵)</li><li>有三个以上Pose，根据各单应矩阵计算线性相机参数；</li><li>使用非线性优化方法计算非线性参数</li></ul></li></ul></blockquote></li><li><p>第一步：求解单应矩阵——基本方程</p><ul><li><p>特点：使用平面靶标摆多个pose(可未知)</p><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20190227152710.png" srcset="/img/loading.gif" alt=""></p></li><li><p>平面靶标有四个点或更多时，可求解H(差一比例因子)</p></li></ul></li><li><p>第二步：求解单应矩阵——建立内参数方程</p><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20190227152857.png" srcset="/img/loading.gif" alt=""></p><ul><li><p>根据R约束</p><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20190227152920.png" srcset="/img/loading.gif" alt=""></p></li><li><p>对应每一个pose,可得到上述两个方程</p></li></ul></li><li><p>第三步：求解内参数——建立方程</p><ul><li><p>令 $B=(b_{ij})=M_1^{-T}M_1^{-1}$</p></li><li><p>根据B对称，定义参数向量</p><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20190227153140.png" srcset="/img/loading.gif" alt=""></p></li></ul></li><li><p>第四步：求解参数——建立内参数方程</p><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20190227153428.png" srcset="/img/loading.gif" alt=""></p></li><li><p>第五步：求解参数——内参数求解</p><ul><li><p>当n&gt;=3 时,可求解b</p><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20190227153504.png" srcset="/img/loading.gif" alt=""></p></li><li><p>解为：</p><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20190227153528.png" srcset="/img/loading.gif" alt=""></p></li><li><p>前式代入，可知：</p><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20190227153547.png" srcset="/img/loading.gif" alt=""></p></li><li><p>进一步可确定$M_1$各参数</p></li></ul></li><li><p>第六步：求解参数——外参数求解</p><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20190227153634.png" srcset="/img/loading.gif" alt=""></p><ul><li><p>系数</p><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20190227153717.png" srcset="/img/loading.gif" alt=""></p></li></ul></li><li><p>最后一步：非线性畸变参数求解</p><ul><li><p>以已有解为初值，求解下式：</p><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20190227153752.png" srcset="/img/loading.gif" alt=""></p><blockquote><p>可使用Levenberg-Marquardt算法求解(原方法中只含径向畸变)</p></blockquote></li></ul></li></ul><p><strong>外参数标定结果示意</strong></p><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20190227153933.png" srcset="/img/loading.gif" alt=""></p><p><strong>重投影示意</strong></p><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20190227153950.png" srcset="/img/loading.gif" alt=""></p><h4 id="【总结】"><a href="#【总结】" class="headerlink" title="【总结】"></a>【总结】</h4><blockquote><ul><li>Zhang方法从多个角度拍摄平面标定物，进一步通过特征点计算内、外参数及非线性畸变</li><li>与已有方法相比，Zhang方法简单，不需要已知目标特征点三维坐标，因而得到广泛应用</li><li>注意：参与标定的数据一般为30张左右。</li></ul></blockquote><h3 id="代码演示"><a href="#代码演示" class="headerlink" title="代码演示"></a>代码演示</h3><pre><code class="hljs python"><span class="hljs-comment">#!/usr/bin/env python</span><span class="hljs-comment"># -*- coding: utf-8 -*-</span><span class="hljs-comment"># @Time    : 2019/2/26 20:15</span><span class="hljs-comment"># @Author  : Seven</span><span class="hljs-comment"># @File    : CameraCalibration.py</span><span class="hljs-comment"># @Software: PyCharm</span><span class="hljs-comment"># function : 摄像机标定</span><span class="hljs-keyword">import</span> cv2<span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<span class="hljs-keyword">import</span> glob<span class="hljs-comment"># 设置寻找亚像素角点的参数，采用的停止准则是最大循环次数30和最大误差容限0.001</span>criteria = (cv2.TERM_CRITERIA_EPS + cv2.TERM_CRITERIA_MAX_ITER, <span class="hljs-number">30</span>, <span class="hljs-number">0.001</span>)<span class="hljs-comment"># 准备目标点，例如 (0,0,0), (1,0,0), (2,0,0) ....,(6,5,0)</span>objp = np.zeros((<span class="hljs-number">6</span> * <span class="hljs-number">7</span>, <span class="hljs-number">3</span>), np.float32)<span class="hljs-comment"># 将世界坐标系建在标定板上，所有点的Z坐标全部为0，所以只需要赋值x和y</span>objp[:, :<span class="hljs-number">2</span>] = np.mgrid[<span class="hljs-number">0</span>:<span class="hljs-number">7</span>, <span class="hljs-number">0</span>:<span class="hljs-number">6</span>].T.reshape(<span class="hljs-number">-1</span>, <span class="hljs-number">2</span>)<span class="hljs-comment"># 用于存储所有图像中的对象点和图像点的数组。</span>objpoints = []  <span class="hljs-comment"># 存储在现实世界空间的3d点</span>imgpoints = []  <span class="hljs-comment"># 储存图像平面中的2d点。</span>images = glob.glob(<span class="hljs-string">'image/*.jpg'</span>)<span class="hljs-keyword">for</span> fname <span class="hljs-keyword">in</span> images:    img = cv2.imread(fname)    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)    <span class="hljs-comment"># 获取XY坐标</span>    size = gray.shape[::<span class="hljs-number">-1</span>]    <span class="hljs-comment"># 找到棋盘角点</span>    ret, corners = cv2.findChessboardCorners(gray, (<span class="hljs-number">7</span>, <span class="hljs-number">6</span>), <span class="hljs-literal">None</span>)    <span class="hljs-comment"># 如果找到，添加3D点，2D点</span>    <span class="hljs-keyword">if</span> ret:        objpoints.append(objp)        <span class="hljs-comment"># 增加角点的准确度</span>        corners2 = cv2.cornerSubPix(gray, corners, (<span class="hljs-number">11</span>, <span class="hljs-number">11</span>), (<span class="hljs-number">-1</span>, <span class="hljs-number">-1</span>), criteria)        imgpoints.append(corners2)        <span class="hljs-comment"># 画出并显示角点</span>        img = cv2.drawChessboardCorners(img, (<span class="hljs-number">7</span>, <span class="hljs-number">6</span>), corners2, ret)        cv2.imshow(<span class="hljs-string">'img'</span>, img)        cv2.waitKey(<span class="hljs-number">500</span>)<span class="hljs-comment"># 相机标定</span>ret, mtx, dist, rvecs, tvecs = cv2.calibrateCamera(objpoints, imgpoints, size, <span class="hljs-literal">None</span>, <span class="hljs-literal">None</span>)<span class="hljs-comment"># 保存相机参数</span>np.savez(<span class="hljs-string">'C.npz'</span>, mtx=mtx, dist=dist, rvecs=rvecs, tvecs=tvecs)print(<span class="hljs-string">"ret:"</span>, ret)print(<span class="hljs-string">"内参数矩阵:\n"</span>, mtx)print(<span class="hljs-string">"畸变系数:\n"</span>, dist)print(<span class="hljs-string">"旋转向量:"</span>, rvecs)  <span class="hljs-comment"># 外参数</span>print(<span class="hljs-string">"平移向量:"</span>, tvecs)  <span class="hljs-comment"># 外参数</span></code></pre><p>运行结果：</p><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20190228215354.png" srcset="/img/loading.gif" alt=""></p>]]></content>
    
    
    <categories>
      
      <category>机器视觉</category>
      
    </categories>
    
    
    <tags>
      
      <tag>相机标定</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>【十六】相对位姿测量算法</title>
    <link href="/2019/03/cv-image_position/"/>
    <url>/2019/03/cv-image_position/</url>
    
    <content type="html"><![CDATA[<h3 id="相对位姿测量算法"><a href="#相对位姿测量算法" class="headerlink" title="相对位姿测量算法"></a>相对位姿测量算法</h3><h4 id="【基于空间多点】"><a href="#【基于空间多点】" class="headerlink" title="【基于空间多点】"></a>【基于空间多点】</h4><ul><li><p><strong>相对位姿估计的基本问题</strong></p><blockquote><ul><li>已知：相机内参数；多个空间上的特征点(非共面)在目标坐标系(3D)和相平面坐标系(2D)坐标。</li><li>输出：目标坐标系相对相机坐标系的位置和姿态。</li></ul></blockquote><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20190226190648.png" srcset="/img/loading.gif" alt=""></p></li><li><p><strong>基本思想示意</strong></p><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20190226190725.png" srcset="/img/loading.gif" alt=""></p></li><li><p><strong>线性求解</strong></p><ul><li><p>对每一个特征点，均有：</p><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20190226191229.png" srcset="/img/loading.gif" alt=""></p></li><li><p>对每一个特征点，均有：</p><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20190226194028.png" srcset="/img/loading.gif" alt=""></p></li><li><p>展开第一行</p><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20190226194052.png" srcset="/img/loading.gif" alt=""></p></li><li><p>类似展开第二、第三行：</p><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20190226194126.png" srcset="/img/loading.gif" alt=""></p></li><li><p>消去$Z_c$</p><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20190226194214.png" srcset="/img/loading.gif" alt=""></p></li><li><p>上二式右侧分母移到左边，得：</p><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20190226194247.png" srcset="/img/loading.gif" alt=""></p></li><li><p>整理为矩阵形式</p><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20190226194444.png" srcset="/img/loading.gif" alt=""></p></li><li><p>对于每一个点都可以形成如上两个方程，对于多个点，可进行堆叠，并记成矩阵形式：</p><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20190226194513.png" srcset="/img/loading.gif" alt=""></p></li><li><p>有六个或以上特征点且非共面时，可求解：</p><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20190226194611.png" srcset="/img/loading.gif" alt=""></p></li><li><p>上面求出的只有11个参数，且多一个/t3. 最后一个变量可利用如下约束求出</p><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20190226194640.png" srcset="/img/loading.gif" alt=""></p></li><li><p>线性求解总结</p><blockquote><p>利用矩阵的QR分解，得到最终的旋转矩阵非奇异矩阵P的正交三角分解：P=QR,  其中Q(维数n*s): 正交阵； R :上三角阵</p><p>证明思路：对P 中各向量进行正交化</p><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20190226194856.png" srcset="/img/loading.gif" alt=""></p></blockquote></li></ul></li></ul><h4 id="【扩展】"><a href="#【扩展】" class="headerlink" title="【扩展】"></a>【扩展】</h4><ul><li><p>根据旋转矩阵计算旋转角</p><blockquote><p>相机坐标系想要转到与世界坐标系完全平行（即坐标轴完全平行，且方向相同），需要旋转3次，设原始相机坐标系为C0<br>1、 C0绕其Z轴旋转，得到新的坐标系C1；<br>2、 C1绕其Y轴旋转，得到新的坐标系C2（注意旋转轴为C1的Y轴，而非C0的Y轴）；<br>3、 C2绕其X轴旋转，得到新的坐标系C3。此时C3与世界坐标系完全平行。</p><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20190226220707.png" srcset="/img/loading.gif" alt=""></p></blockquote></li><li><p>Rodrigues旋转</p><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20190226220856.png" srcset="/img/loading.gif" alt=""></p></li><li><p>空间的任何一个旋转，可表达为一个向量绕旋转轴旋转给定角度。可用四元数表达：</p><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20190226221704.png" srcset="/img/loading.gif" alt=""></p></li><li><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20190226221722.png" srcset="/img/loading.gif" alt=""></p></li><li><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20190226221732.png" srcset="/img/loading.gif" alt=""></p></li></ul><h4 id="【基于平面多特征点】"><a href="#【基于平面多特征点】" class="headerlink" title="【基于平面多特征点】"></a>【基于平面多特征点】</h4><ul><li><p><strong>基本问题</strong></p><blockquote><p>已知：相机内参数；多个平面上的特征点在目标坐标系(3D)和相平面坐标系(2D)坐标。</p><p>输出：目标坐标系相对相机坐标系的位置和姿态。</p></blockquote></li></ul><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20190227151524.png" srcset="/img/loading.gif" alt=""></p><ul><li><p>平面特征点相对位姿估计——线性求解</p><ul><li><p>设$Z_t=0$(特征共面), 则对每一个特征点，均有：</p><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20190227151704.png" srcset="/img/loading.gif" alt=""></p></li><li><p>得到两个方程</p><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20190227151740.png" srcset="/img/loading.gif" alt=""></p></li><li><p>未知数线性求解</p><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20190227151810.png" srcset="/img/loading.gif" alt=""></p></li><li><p>对于每一个点都可以形成如上两个方程，对于&gt;=4个点，可使用类似PnP方法求得解：</p><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20190227151918.png" srcset="/img/loading.gif" alt=""></p></li></ul></li></ul><h4 id="【总结】"><a href="#【总结】" class="headerlink" title="【总结】"></a>【总结】</h4><blockquote><ul><li>在已知至少六个空间点三维点坐标的条件下，可通过点的图像坐标及相对位姿估计算法计算相对位姿。</li><li>在已知至少四个平面点三维点坐标的条件下，可通过点的图像坐标及相对位姿估计算法计算相对位姿。</li></ul></blockquote><h3 id="代码演示"><a href="#代码演示" class="headerlink" title="代码演示"></a>代码演示</h3><pre><code class="hljs python"><span class="hljs-comment">#!/usr/bin/env python</span><span class="hljs-comment"># -*- coding: utf-8 -*-</span><span class="hljs-comment"># @Time    : 2019/2/26 21:25</span><span class="hljs-comment"># @Author  : Seven</span><span class="hljs-comment"># @File    : PositionMeasurement.py</span><span class="hljs-comment"># @Software: PyCharm</span><span class="hljs-comment"># function : 实现位姿测量算法</span><span class="hljs-keyword">import</span> cv2<span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<span class="hljs-keyword">import</span> glob<span class="hljs-comment"># 加载相机标定的数据</span><span class="hljs-keyword">with</span> np.load(<span class="hljs-string">'C.npz'</span>) <span class="hljs-keyword">as</span> X:    mtx, dist, _, _ = [X[i] <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> (<span class="hljs-string">'mtx'</span>, <span class="hljs-string">'dist'</span>, <span class="hljs-string">'rvecs'</span>, <span class="hljs-string">'tvecs'</span>)]<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">draw</span><span class="hljs-params">(img, corners, imgpts)</span>:</span>    <span class="hljs-string">"""</span><span class="hljs-string">    在图片上画出三维坐标轴</span><span class="hljs-string">    :param img: 图片原数据</span><span class="hljs-string">    :param corners: 图像平面点坐标点</span><span class="hljs-string">    :param imgpts: 三维点投影到二维图像平面上的坐标</span><span class="hljs-string">    :return:</span><span class="hljs-string">    """</span>    corner = tuple(corners[<span class="hljs-number">0</span>].ravel())    cv2.line(img, corner, tuple(imgpts[<span class="hljs-number">0</span>].ravel()), (<span class="hljs-number">255</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>), <span class="hljs-number">5</span>)    cv2.line(img, corner, tuple(imgpts[<span class="hljs-number">1</span>].ravel()), (<span class="hljs-number">0</span>, <span class="hljs-number">255</span>, <span class="hljs-number">0</span>), <span class="hljs-number">5</span>)    cv2.line(img, corner, tuple(imgpts[<span class="hljs-number">2</span>].ravel()), (<span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">255</span>), <span class="hljs-number">5</span>)    <span class="hljs-keyword">return</span> img<span class="hljs-comment"># 初始化目标坐标系的3D点</span>objp = np.zeros((<span class="hljs-number">6</span> * <span class="hljs-number">7</span>, <span class="hljs-number">3</span>), np.float32)objp[:, :<span class="hljs-number">2</span>] = np.mgrid[<span class="hljs-number">0</span>:<span class="hljs-number">7</span>, <span class="hljs-number">0</span>:<span class="hljs-number">6</span>].T.reshape(<span class="hljs-number">-1</span>, <span class="hljs-number">2</span>)<span class="hljs-comment"># 初始化三维坐标系</span>axis = np.float32([[<span class="hljs-number">3</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>], [<span class="hljs-number">0</span>, <span class="hljs-number">3</span>, <span class="hljs-number">0</span>], [<span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">-3</span>]]).reshape(<span class="hljs-number">-1</span>, <span class="hljs-number">3</span>)  <span class="hljs-comment"># 坐标轴</span><span class="hljs-comment"># 加载打包所有图片数据</span>images = glob.glob(<span class="hljs-string">'image/*.jpg'</span>)<span class="hljs-keyword">for</span> fname <span class="hljs-keyword">in</span> images:    img = cv2.imread(fname)    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)    <span class="hljs-comment"># 找到图像平面点坐标点</span>    ret, corners = cv2.findChessboardCorners(gray, (<span class="hljs-number">7</span>, <span class="hljs-number">6</span>), <span class="hljs-literal">None</span>)    <span class="hljs-keyword">if</span> ret:        <span class="hljs-comment"># PnP计算得出旋转向量和平移向量</span>        _, rvecs, tvecs, _ = cv2.solvePnPRansac(objp, corners, mtx, dist)        print(<span class="hljs-string">"旋转变量"</span>, rvecs)        print(<span class="hljs-string">"平移变量"</span>, tvecs)        <span class="hljs-comment"># 计算三维点投影到二维图像平面上的坐标</span>        imgpts, jac = cv2.projectPoints(axis, rvecs, tvecs, mtx, dist)        <span class="hljs-comment"># 把坐标显示图片上</span>        img = draw(img, corners, imgpts)        cv2.imshow(<span class="hljs-string">'img'</span>, img)        cv2.waitKey(<span class="hljs-number">500</span>)cv2.destroyAllWindows()</code></pre><p>运行结果</p><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20190228221322.png" srcset="/img/loading.gif" alt=""></p>]]></content>
    
    
    <categories>
      
      <category>机器视觉</category>
      
    </categories>
    
    
    <tags>
      
      <tag>相对位姿测量</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>【十五】坐标变换与摄像机模型</title>
    <link href="/2019/03/cv-image_camera/"/>
    <url>/2019/03/cv-image_camera/</url>
    
    <content type="html"><![CDATA[<h3 id="坐标变换与摄像机模型"><a href="#坐标变换与摄像机模型" class="headerlink" title="坐标变换与摄像机模型"></a>坐标变换与摄像机模型</h3><h4 id="图像变换模型"><a href="#图像变换模型" class="headerlink" title="图像变换模型"></a>图像变换模型</h4><h5 id="【平移变换】"><a href="#【平移变换】" class="headerlink" title="【平移变换】"></a>【平移变换】</h5><blockquote><p>只改变图形位置，不改变图形的大小和形状.</p></blockquote><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20190225203846.png" srcset="/img/loading.gif" alt=""></p><p><strong><code>代码及演示</code></strong></p><pre><code class="hljs python"><span class="hljs-keyword">import</span> cv2<span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<span class="hljs-comment"># 加载图片</span>source_image = cv2.imread(<span class="hljs-string">'lena.jpg'</span>, <span class="hljs-number">0</span>)cv2.imshow(<span class="hljs-string">'source'</span>, source_image)<span class="hljs-comment"># 平移图片</span>rows, cols = source_image.shape<span class="hljs-comment"># 创建交换矩阵，按（100,50）平移</span>M = np.float32([[<span class="hljs-number">1</span>, <span class="hljs-number">0</span>, <span class="hljs-number">100</span>], [<span class="hljs-number">0</span>, <span class="hljs-number">1</span>, <span class="hljs-number">50</span>]])<span class="hljs-comment"># 使用仿射变换的api进行平移变换</span>translation_image = cv2.warpAffine(source_image, M, (cols, rows))cv2.imshow(<span class="hljs-string">'translation'</span>, translation_image)cv2.waitKey(<span class="hljs-number">0</span>)</code></pre><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20190225212105.png" srcset="/img/loading.gif" alt=""></p><h5 id="【旋转变换】"><a href="#【旋转变换】" class="headerlink" title="【旋转变换】"></a>【旋转变换】</h5><blockquote><p>将输入图像绕笛卡尔坐标系的原点逆时针旋转$\theta$角度， 则变换后图像坐标为</p><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20190225204017.png" srcset="/img/loading.gif" alt=""></p></blockquote><p><strong><code>代码及演示</code></strong></p><pre><code class="hljs python"><span class="hljs-keyword">import</span> cv2<span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<span class="hljs-comment"># 加载图片</span>source_image = cv2.imread(<span class="hljs-string">'lena.jpg'</span>, <span class="hljs-number">0</span>)cv2.imshow(<span class="hljs-string">'source'</span>, source_image)<span class="hljs-comment"># 获取形状</span>rows, cols = source_image.shape<span class="hljs-comment"># 旋转图片</span><span class="hljs-comment"># 创建交换矩阵， 旋转90度</span>M = cv2.getRotationMatrix2D(((cols<span class="hljs-number">-1</span>)/<span class="hljs-number">2.</span>, (rows<span class="hljs-number">-1</span>)/<span class="hljs-number">2.</span>), <span class="hljs-number">90</span>, <span class="hljs-number">1</span>)<span class="hljs-comment"># 使用仿射变换的api进行旋转变换</span>rotation_image = cv2.warpAffine(source_image, M, (cols, rows))cv2.imshow(<span class="hljs-string">'rotation'</span>, rotation_image)cv2.waitKey(<span class="hljs-number">0</span>)</code></pre><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20190225212849.png" srcset="/img/loading.gif" alt=""></p><h5 id="【比例变换】"><a href="#【比例变换】" class="headerlink" title="【比例变换】"></a>【比例变换】</h5><blockquote><p>若图像坐标$(x, y)$ 缩放到$(S_x, S_y)$倍，则变换函数为：</p><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20190225204208.png" srcset="/img/loading.gif" alt=""></p><p>其中$S_y$, 分别为 x和y 坐标的缩放因子，其大于1表示放大，小于1表示缩小。</p></blockquote><p><strong><code>代码及演示</code></strong></p><pre><code class="hljs python"><span class="hljs-keyword">import</span> cv2<span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<span class="hljs-comment"># 加载图片</span>source_image = cv2.imread(<span class="hljs-string">'lena.jpg'</span>, <span class="hljs-number">0</span>)cv2.imshow(<span class="hljs-string">'source'</span>, source_image)<span class="hljs-comment"># 获取形状</span>rows, cols = source_image.shape<span class="hljs-comment"># 比例变换</span><span class="hljs-comment"># 缩小一半</span>proportion_image = cv2.resize(source_image, (cols//<span class="hljs-number">2</span>, rows//<span class="hljs-number">2</span>), cv2.INTER_CUBIC)cv2.imshow(<span class="hljs-string">'proportion'</span>, proportion_image)cv2.waitKey(<span class="hljs-number">0</span>)</code></pre><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20190225213626.png" srcset="/img/loading.gif" alt=""></p><h5 id="【仿射变换】"><a href="#【仿射变换】" class="headerlink" title="【仿射变换】"></a>【仿射变换】</h5><blockquote><p>简单的来说就是一个<code>线性变换加上平移</code></p><p>仿射变换的一般表达式为:</p><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20190225205543.png" srcset="/img/loading.gif" alt=""></p><p>平移、比例缩放和旋转变换都是一种称为仿射变换的特殊情况。</p></blockquote><p><strong><code>仿射变换的性质</code></strong></p><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20190227160734.png" srcset="/img/loading.gif" alt=""></p><blockquote><ul><li>仿射变换有6个自由度（对应变换中的6个系数），因此，仿射变换后互相平行直线仍然为平行直线，三角形映射后仍是三角形。但却不能保证将四边形以上的多边形映射为等边数的多边形。</li><li>仿射变换，可以保持原来的线共点，点共线的关系不变，保持原来相互平行的的先仍然相互平行，保持原来在一直线上几段线段之间的比例 关系不变。但是，仿射变换不能保持原来的线段长度不变，也不能保持原来的夹角角度不变。</li><li>仿射变换的乘积和逆变换仍是仿射变换。</li><li>仿射变换能够实现平移、旋转、缩放等几何变换。</li></ul></blockquote><p><strong><code>代码及演示</code></strong></p><pre><code class="hljs python"><span class="hljs-keyword">import</span> cv2<span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt<span class="hljs-comment"># 加载图片</span>source_image = cv2.imread(<span class="hljs-string">'lena.jpg'</span>, <span class="hljs-number">0</span>)cv2.imshow(<span class="hljs-string">'source'</span>, source_image)<span class="hljs-comment"># 获取形状</span>rows, cols = source_image.shape<span class="hljs-comment"># 仿射变换</span><span class="hljs-comment"># 定义交换矩阵</span>pts1 = np.float32([[<span class="hljs-number">50</span>, <span class="hljs-number">50</span>], [<span class="hljs-number">200</span>, <span class="hljs-number">50</span>], [<span class="hljs-number">50</span>, <span class="hljs-number">200</span>]])pts2 = np.float32([[<span class="hljs-number">10</span>, <span class="hljs-number">100</span>], [<span class="hljs-number">200</span>, <span class="hljs-number">50</span>], [<span class="hljs-number">100</span>, <span class="hljs-number">250</span>]])M = cv2.getAffineTransform(pts1, pts2)<span class="hljs-comment"># 开始变换</span>affine_image = cv2.warpAffine(source_image, M, (cols, rows))plt.subplot(<span class="hljs-number">121</span>), plt.imshow(source_image), plt.title(<span class="hljs-string">'Input'</span>)plt.subplot(<span class="hljs-number">122</span>), plt.imshow(affine_image), plt.title(<span class="hljs-string">'Output'</span>)plt.show()cv2.imshow(<span class="hljs-string">'affine'</span>, affine_image)cv2.waitKey(<span class="hljs-number">0</span>)</code></pre><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20190225214526.png" srcset="/img/loading.gif" alt=""></p><h5 id="【透视变换】"><a href="#【透视变换】" class="headerlink" title="【透视变换】"></a>【透视变换】</h5><blockquote><ul><li>把物体的三维图像表示转变为二维表示的过程，称为透视变换，也称为投影映射，其表达式为:</li></ul><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20190225205814.png" srcset="/img/loading.gif" alt=""></p><ul><li>透视变换也是一种平面映射 ，并且可以保证任意方向上的直线经过透视变换后仍然保持是直线。</li><li>透视变换具有9个自由度（其变换系数为9个），故可以实现平面四边形到四边形的映射。</li></ul></blockquote><p><strong><code>透视变化示意</code></strong></p><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20190225205928.png" srcset="/img/loading.gif" alt=""></p><blockquote><p>对于透视投影，一束平行于投影面的平行线的投影可保持平行，而不平行于投影面的平行线的投影会聚集到一个点，该点称<code>灭点</code>.</p></blockquote><p><strong><code>代码及演示</code></strong></p><pre><code class="hljs python"><span class="hljs-keyword">import</span> cv2<span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt<span class="hljs-comment"># 加载图片</span>source_image = cv2.imread(<span class="hljs-string">'lena.jpg'</span>, <span class="hljs-number">0</span>)cv2.imshow(<span class="hljs-string">'source'</span>, source_image)<span class="hljs-comment"># 获取形状</span>rows, cols = source_image.shape<span class="hljs-comment"># 透视变换</span><span class="hljs-comment"># 定义交换矩阵</span>pts1 = np.float32([[<span class="hljs-number">56</span>, <span class="hljs-number">65</span>], [<span class="hljs-number">368</span>, <span class="hljs-number">52</span>], [<span class="hljs-number">28</span>, <span class="hljs-number">387</span>], [<span class="hljs-number">389</span>, <span class="hljs-number">390</span>]])pts2 = np.float32([[<span class="hljs-number">0</span>, <span class="hljs-number">0</span>], [<span class="hljs-number">300</span>, <span class="hljs-number">0</span>], [<span class="hljs-number">0</span>, <span class="hljs-number">300</span>], [<span class="hljs-number">300</span>, <span class="hljs-number">300</span>]])M = cv2.getPerspectiveTransform(pts1, pts2)<span class="hljs-comment"># 开始变换</span>perspective_image = cv2.warpPerspective(source_image, M, (<span class="hljs-number">300</span>, <span class="hljs-number">300</span>))plt.subplot(<span class="hljs-number">121</span>), plt.imshow(source_image), plt.title(<span class="hljs-string">'Input'</span>)plt.subplot(<span class="hljs-number">122</span>), plt.imshow(perspective_image), plt.title(<span class="hljs-string">'Output'</span>)plt.show()cv2.imshow(<span class="hljs-string">'perspective'</span>, proportion_image)cv2.waitKey(<span class="hljs-number">0</span>)</code></pre><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20190225215028.png" srcset="/img/loading.gif" alt=""></p><h4 id="坐标系和坐标变换"><a href="#坐标系和坐标变换" class="headerlink" title="坐标系和坐标变换"></a>坐标系和坐标变换</h4><ul><li><p>不同坐标系及坐标变换关系</p><blockquote><p>当物体旋转时，其上的点在固定坐标系坐标值变化。</p></blockquote></li><li><p>任意两个三维坐标系之间的变换关系</p><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20190226133821.png" srcset="/img/loading.gif" alt=""></p><blockquote><p>注意：R满足旋转矩阵正交性约束</p></blockquote></li><li><p>坐标系变换及旋转矩阵生成示意图</p><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20190226134034.png" srcset="/img/loading.gif" alt=""></p></li></ul><h5 id="【像素坐标系】"><a href="#【像素坐标系】" class="headerlink" title="【像素坐标系】"></a>【像素坐标系】</h5><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20190226185321.png" srcset="/img/loading.gif" alt=""></p><blockquote><p><strong>像素坐标系</strong>uov是一个二维直角坐标系，反映了相机CCD/CMOS芯片中像素的排列情况。原点o位于图像的左上角，u轴、v轴分别于像面的两边平行。像素坐标系中坐标轴的单位是像素（整数）。</p></blockquote><h5 id="【图像坐标系】"><a href="#【图像坐标系】" class="headerlink" title="【图像坐标系】"></a>【图像坐标系】</h5><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20190226134217.png" srcset="/img/loading.gif" alt=""></p><blockquote><p><strong>图像坐标系</strong>(x,y)：像素坐标系不利于坐标变换，因此需要建立图像坐标系XOY，其坐标轴的单位通常为毫米（mm），原点是相机光轴与相面的交点（称为主点），即图像的中心点，X轴、Y轴分别与u轴、v轴平行。故两个坐标系实际是平移关系，即可以通过平移就可得到。</p></blockquote><h5 id="【摄像机坐标系】"><a href="#【摄像机坐标系】" class="headerlink" title="【摄像机坐标系】"></a>【摄像机坐标系】</h5><blockquote><p><strong>相机坐标系</strong>（camera coordinate）($O_cX_cY_cZ_c (camera frame)$)，也是一个三维直角坐标系，原点位于镜头光心处，x、y轴分别与相面的两边平行，z轴为镜头光轴，与像平面垂直。</p></blockquote><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20190226134644.png" srcset="/img/loading.gif" alt=""></p><h5 id="【世界坐标系】"><a href="#【世界坐标系】" class="headerlink" title="【世界坐标系】"></a>【世界坐标系】</h5><blockquote><p><strong>世界坐标系</strong>（world coordinate）($O_wX_wY_wZ_w$)，也称为测量坐标系，是一个三维直角坐标系，以其为基准可以描述相机和待测物体的空间位置。世界坐标系的位置可以根据实际情况自由确定。</p></blockquote><h5 id="【手端坐标系或平台坐标系】"><a href="#【手端坐标系或平台坐标系】" class="headerlink" title="【手端坐标系或平台坐标系】"></a>【手端坐标系或平台坐标系】</h5><blockquote><p>$O_eX_eY_eZ_e$</p></blockquote><h5 id="【目标坐标系】"><a href="#【目标坐标系】" class="headerlink" title="【目标坐标系】"></a>【目标坐标系】</h5><blockquote><p>$O_tX_tY_tZ_t$</p></blockquote><h5 id="【坐标系转换】"><a href="#【坐标系转换】" class="headerlink" title="【坐标系转换】"></a>【坐标系转换】</h5><ul><li><p><strong>世界坐标系转换为相机坐标系</strong></p><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20190226160556.png" srcset="/img/loading.gif" alt=""></p></li></ul><blockquote><p>其中R为3*3的旋转矩阵，t为3*1的平移矢量，即相机外参数</p></blockquote><ul><li><p><strong>相机坐标系转换为图像坐标系</strong></p><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20190226160628.png" srcset="/img/loading.gif" alt=""></p><blockquote><p>s为比例因子（s不为0），f为有效焦距（光心到图像平面的距离），(x,y,z,1)是空间点P在相机坐标系oxyz中的齐次坐标，(X,Y,1)是像点p在图像坐标系OXY中的齐次坐标。</p></blockquote></li><li><p><strong>图像坐标系转换为像素坐标系</strong></p><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20190226160706.png" srcset="/img/loading.gif" alt=""></p><blockquote><p>其中，dX、dY分别为像素在X、Y轴方向上的物理尺寸，$u_0,v_0$为主点（图像原点）坐标</p></blockquote></li><li><p><strong>世界坐标系转换为像素坐标系</strong></p><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20190226160855.png" srcset="/img/loading.gif" alt=""></p><blockquote><p>其中，$m_1、m_2$即为相机的内参和外参数。</p><p>注意：R,t参数矩阵为不可逆矩阵。</p></blockquote></li></ul><h4 id="线性及非线性摄像机模型"><a href="#线性及非线性摄像机模型" class="headerlink" title="线性及非线性摄像机模型"></a>线性及非线性摄像机模型</h4><h5 id="【线性摄像机模型】"><a href="#【线性摄像机模型】" class="headerlink" title="【线性摄像机模型】"></a>【线性摄像机模型】</h5><ul><li><p>考虑简化的针孔模型</p><blockquote><p>​    针孔模型是各种相机模型中最简单的一种，它是相机的一个近似线性模型。在相机坐标系下，任一点$P(X_c,Y_c,Z_c)$在像平面的投影位置,也就是说，任一点$P(X_c,Y_c,Z_c)$的投影点p(x,y)都是OP（即光心（投影中心）与点$P(X_c,Y_c,Z_c)$的连线）与像平面的交点如下图。</p><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20190226184641.png" srcset="/img/loading.gif" alt=""></p></blockquote><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20190226140023.png" srcset="/img/loading.gif" alt=""></p></li><li><p>加入相机坐标系与世界坐标系变换关系，得到</p><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20190226140120.png" srcset="/img/loading.gif" alt=""></p><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20190226140311.png" srcset="/img/loading.gif" alt=""></p></li></ul><blockquote><p>说明：上述公式中完成了从<code>世界坐标系</code>到<code>图像坐标系</code>的转变，中间经过了<code>相机坐标系</code>的过度，$X_w$中的w表示world世界，单位为毫米，而u,v是的 单位为像素，即完成了从毫米——像素的转换。</p><p>所以，为了得到空间物体的三维世界坐标，就必须有两个或更多的相机构成立体视觉系统模型才能实现。</p></blockquote><p><strong>成像畸变示意</strong></p><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20190226140930.png" srcset="/img/loading.gif" alt=""></p><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20190226140938.png" srcset="/img/loading.gif" alt=""></p><h5 id="【非线性摄像机模型】"><a href="#【非线性摄像机模型】" class="headerlink" title="【非线性摄像机模型】"></a>【非线性摄像机模型】</h5><blockquote><p>在实际的成像过程中，考虑镜头的失真，一般都存在非线性畸变，所以线性模型不能准确描述成像几何关系。非线性畸可用下列公式描述：</p></blockquote><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20190226185028.png" srcset="/img/loading.gif" alt=""></p><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20190226185040.png" srcset="/img/loading.gif" alt=""></p><ul><li><p>径向畸变，离心畸变，薄棱镜畸变</p><blockquote><p>通常只考虑径向畸变：</p><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/1551161540304.png" srcset="/img/loading.gif" alt=""></p></blockquote></li><li><p>若考虑非线性畸变，则对相机标定时需要使用非线性优化算法。而有研究表明引入过多的非线性参入（如离心畸变和薄棱畸变）不仅不能提高精度，还会引起解的不稳定。一般情况下径向畸变就足以描述非线性畸变，所有本课题只是考虑径向畸变。则将式(2.9)中的径向畸变代入式(2.8)可得：</p><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20190226185128.png" srcset="/img/loading.gif" alt=""></p></li></ul><h4 id="总结"><a href="#总结" class="headerlink" title="总结"></a><strong>总结</strong></h4><blockquote><ul><li>图像变换包括平秱、旋转、仿射、透视变换等</li><li>常用的坐标系包括像素坐标系、相机坐标系、世界坐标系等，任一点在世界坐标系和相机坐标系中的坐标通过投影矩阵M相关联，M包含了内参数和外参数矩阵</li><li>一般相机成像存在非线性畸变，重点需要考虑径向畸变</li></ul></blockquote>]]></content>
    
    
    <categories>
      
      <category>机器视觉</category>
      
    </categories>
    
    
    <tags>
      
      <tag>坐标变换</tag>
      
      <tag>摄像机模型</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>【十四】光流估计</title>
    <link href="/2019/03/cv-image_LK/"/>
    <url>/2019/03/cv-image_LK/</url>
    
    <content type="html"><![CDATA[<h3 id="光流估计"><a href="#光流估计" class="headerlink" title="光流估计"></a>光流估计</h3><h4 id="【简介】"><a href="#【简介】" class="headerlink" title="【简介】"></a>【<strong>简介</strong>】</h4><blockquote><p>​    在计算机视觉中，Lucas–Kanade光流算法是一种两帧差分的光流估计算法。它由Bruce D. Lucas 和 Takeo Kanade提出。</p></blockquote><h4 id="【光流的概念】"><a href="#【光流的概念】" class="headerlink" title="【光流的概念】"></a>【<strong>光流的概念</strong>】</h4><blockquote><p>(Optical flow or optic flow)它是一种运动模式，这种运动模式指的是一个物体、表面、边缘在一个视角下由一个观察者（比如眼睛、摄像头等）和背景之间形成的明显移动。</p><p>​    光流技术，如运动检测和图像分割，时间碰撞，运动补偿编码，三维立体视差，都是利用了这种边缘或表面运动的技术。</p><p>​    二维图像的移动相对于观察者而言是三维物体移动的在图像平面的投影。</p><p>​    有序的图像可以估计出二维图像的瞬时图像速率或离散图像转移。</p></blockquote><h4 id="【光流算法】"><a href="#【光流算法】" class="headerlink" title="【光流算法】"></a>【光流算法】</h4><blockquote><p>​    它评估了两幅图像的之间的变形，它的<code>基本假设是体素和图像像素守恒</code>。它假设一个物体的颜色在前后两帧没有巨大而明显的变化。</p><p>​    基于这个思路，我们可以得到图像约束方程。不同的光流算法解决了假定了不同附加条件的光流问题。</p></blockquote><h4 id="【基本模型】"><a href="#【基本模型】" class="headerlink" title="【基本模型】"></a>【基本模型】</h4><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20190123154113.png" srcset="/img/loading.gif" alt=""></p><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20190123160039.png" srcset="/img/loading.gif" alt=""></p><blockquote><p>基本思想：虽然$t+1$的时候位置进行了移动，如果我们知道<code>灰度是连续变化</code>的，那么我们就可以通过像素的变化推演当前这个中心点大概移动了多少。</p><p>通过当前位置亮度在$I_x，I_y$以及相邻两帧中灰度的变化值$I_t$，来推演$\delta _x，\delta_y$.</p></blockquote><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20190123154311.png" srcset="/img/loading.gif" alt=""></p><blockquote><p><code>我们在检测物体上建立一个小方格里的所有像素位移相同，建立矩阵方程组，解出u、v两个未知数。</code></p></blockquote><ul><li><p>最优化问题（超定方程求解）</p><p>$min |Au-b|$</p></li><li><p>最小二乘解</p><p>$u=(A^TA)^{-1}A^Tb$</p></li></ul><blockquote><ul><li>区域像素只有2个时，就是2元1次方程组求解！</li><li>多个像素，比如3∗3时，则是求上述最小二乘解.</li></ul></blockquote><h4 id="【Lucas–Kanade方法】"><a href="#【Lucas–Kanade方法】" class="headerlink" title="【Lucas–Kanade方法】"></a>【Lucas–Kanade方法】</h4><blockquote><p>​    这个算法是最常见，最流行的。它计算两帧在时间t 到t + δt之间每个每个像素点位置的移动。 由于它是基于图像信号的泰勒级数，这种方法称为差分，这就是对于空间和时间坐标使用偏导数。</p></blockquote><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20190123161352.png" srcset="/img/loading.gif" alt=""></p><blockquote><p>可信度判断：矩阵求逆是否能实现？</p></blockquote><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20190123161715.png" srcset="/img/loading.gif" alt=""></p><blockquote><ul><li>该表达式是和沿着x和y的梯度是密切相关的，只要其中一个为0时，$A^TA$就不可逆。只要当该区域的像素点比较平滑，矩阵就可逆</li><li>通过求特征值来判断计算可信。<ul><li>两个特征值都远大于0，那么就可逆，否则就不可逆或者很难求逆。</li></ul></li></ul></blockquote><h4 id="【金字塔L-K方法】"><a href="#【金字塔L-K方法】" class="headerlink" title="【金字塔L-K方法】"></a>【金字塔L-K方法】</h4><blockquote><p>针对目标运动很快的情况，用原始的L-K方法就没有办法处理了， 需要采用图像金字塔的方法。把图像从小到大的顺序排列。</p></blockquote><blockquote><p>​    金字塔特征跟踪算法描述如下：首先，光流和仿射变换矩阵在最高一层的图像上计算出；将上一层的计算结果作为初始值传递给下一层图像，这一层的图像在这个初始值的基础上，计算这一层的光流和仿射变化矩阵；再将这一层的光流和仿射矩阵作为初始值传递给下一层图像，直到传递给最后一层，即原始图像层，这一层计算出来的光流和仿射变换矩阵作为最后的光流和仿射变换矩阵的结果。</p></blockquote><ul><li><p>用不同像素的图片建立金字塔</p><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20190123185018.png" srcset="/img/loading.gif" alt=""></p></li><li><p>运行L-K方法</p><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20190123185057.png" srcset="/img/loading.gif" alt=""></p></li></ul><blockquote><p>优点：可处理大位移；对噪声更不敏感。</p></blockquote><h4 id="【数学模型】"><a href="#【数学模型】" class="headerlink" title="【数学模型】"></a>【<strong>数学模型</strong>】</h4><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20190123190149.png" srcset="/img/loading.gif" alt=""></p><h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><blockquote><ol><li>光流估计基于恒定亮度假设模型</li><li>L-K光流估计方法利用了邻域内的运动不变性</li><li>金字塔L-K方法可有效提高光流计算对大位移的鲁棒性</li></ol></blockquote><h3 id="代码实例"><a href="#代码实例" class="headerlink" title="代码实例"></a>代码实例</h3><pre><code class="hljs python"><span class="hljs-comment">#!/usr/bin/env python</span><span class="hljs-comment"># -*- coding: utf-8 -*-</span><span class="hljs-comment"># @Time    : 2019/1/22 22:20</span><span class="hljs-comment"># @Author  : Seven</span><span class="hljs-comment"># @File    : sightDemo.py</span><span class="hljs-comment"># @Software: PyCharm</span><span class="hljs-keyword">import</span> cv2<span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<span class="hljs-comment"># 加载视频</span>cap = cv2.VideoCapture()cap.open(<span class="hljs-string">'768x576.avi'</span>)<span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> cap.isOpened():    print(<span class="hljs-string">"无法打开视频文件"</span>)lk_params = dict(winSize=(<span class="hljs-number">15</span>, <span class="hljs-number">15</span>),                 maxLevel=<span class="hljs-number">2</span>,                 criteria=(cv2.TERM_CRITERIA_EPS | cv2.TERM_CRITERIA_COUNT, <span class="hljs-number">10</span>, <span class="hljs-number">0.03</span>))feature_params = dict(maxCorners=<span class="hljs-number">500</span>,                      qualityLevel=<span class="hljs-number">0.3</span>,                      minDistance=<span class="hljs-number">7</span>,                      blockSize=<span class="hljs-number">7</span>)<span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">App</span>:</span>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span><span class="hljs-params">(self, video_src)</span>:</span>  <span class="hljs-comment"># 构造方法，初始化一些参数和视频路径</span>        self.track_len = <span class="hljs-number">10</span>        self.detect_interval = <span class="hljs-number">5</span>        self.tracks = []        self.cam = cv2.VideoCapture(video_src)        self.frame_idx = <span class="hljs-number">0</span>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">run</span><span class="hljs-params">(self)</span>:</span>  <span class="hljs-comment"># 光流运行方法</span>        <span class="hljs-keyword">while</span> <span class="hljs-literal">True</span>:            ret, frame = self.cam.read()  <span class="hljs-comment"># 读取视频帧</span>            <span class="hljs-keyword">if</span> ret:                frame_gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)  <span class="hljs-comment"># 转化为灰度虚图像</span>                vis = frame.copy()                <span class="hljs-keyword">if</span> len(self.tracks) &gt; <span class="hljs-number">0</span>:  <span class="hljs-comment"># 检测到角点后进行光流跟踪</span>                    img0, img1 = self.prev_gray, frame_gray                    p0 = np.float32([tr[<span class="hljs-number">-1</span>] <span class="hljs-keyword">for</span> tr <span class="hljs-keyword">in</span> self.tracks]).reshape(<span class="hljs-number">-1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">2</span>)                    p1, st, err = cv2.calcOpticalFlowPyrLK(img0, img1, p0, <span class="hljs-literal">None</span>,                                                           **lk_params)  <span class="hljs-comment"># 前一帧的角点和当前帧的图像作为输入来得到角点在当前帧的位置</span>                    p0r, st, err = cv2.calcOpticalFlowPyrLK(img1, img0, p1, <span class="hljs-literal">None</span>,                                                            **lk_params)  <span class="hljs-comment"># 当前帧跟踪到的角点及图像和前一帧的图像作为输入来找到前一帧的角点位置</span>                    d = abs(p0 - p0r).reshape(<span class="hljs-number">-1</span>, <span class="hljs-number">2</span>).max(<span class="hljs-number">-1</span>)  <span class="hljs-comment"># 得到角点回溯与前一帧实际角点的位置变化关系</span>                    good = d &lt; <span class="hljs-number">1</span>  <span class="hljs-comment"># 判断d内的值是否小于1，大于1跟踪被认为是错误的跟踪点</span>                    new_tracks = []                    <span class="hljs-keyword">for</span> tr, (x, y), good_flag <span class="hljs-keyword">in</span> zip(self.tracks, p1.reshape(<span class="hljs-number">-1</span>, <span class="hljs-number">2</span>), good):  <span class="hljs-comment"># 将跟踪正确的点列入成功跟踪点</span>                        <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> good_flag:                            <span class="hljs-keyword">continue</span>                        tr.append((x, y))                        <span class="hljs-keyword">if</span> len(tr) &gt; self.track_len:                            <span class="hljs-keyword">del</span> tr[<span class="hljs-number">0</span>]                        new_tracks.append(tr)                        cv2.circle(vis, (x, y), <span class="hljs-number">2</span>, (<span class="hljs-number">0</span>, <span class="hljs-number">255</span>, <span class="hljs-number">0</span>), <span class="hljs-number">-1</span>)                    self.tracks = new_tracks                    cv2.polylines(vis, [np.int32(tr) <span class="hljs-keyword">for</span> tr <span class="hljs-keyword">in</span> self.tracks], <span class="hljs-literal">False</span>,                                  (<span class="hljs-number">0</span>, <span class="hljs-number">255</span>, <span class="hljs-number">0</span>))  <span class="hljs-comment"># 以上一振角点为初始点，当前帧跟踪到的点为终点划线</span>                    <span class="hljs-comment"># draw_str(vis, (20, 20), 'track count: %d' % len(self.tracks))</span>                <span class="hljs-keyword">if</span> self.frame_idx % self.detect_interval == <span class="hljs-number">0</span>:  <span class="hljs-comment"># 每5帧检测一次特征点</span>                    mask = np.zeros_like(frame_gray)  <span class="hljs-comment"># 初始化和视频大小相同的图像</span>                    mask[:] = <span class="hljs-number">255</span>  <span class="hljs-comment"># 将mask赋值255也就是算全部图像的角点</span>                    <span class="hljs-keyword">for</span> x, y <span class="hljs-keyword">in</span> [np.int32(tr[<span class="hljs-number">-1</span>]) <span class="hljs-keyword">for</span> tr <span class="hljs-keyword">in</span> self.tracks]:  <span class="hljs-comment"># 跟踪的角点画圆</span>                        cv2.circle(mask, (x, y), <span class="hljs-number">5</span>, <span class="hljs-number">0</span>, <span class="hljs-number">-1</span>)                    p = cv2.goodFeaturesToTrack(frame_gray, mask=mask, **feature_params)  <span class="hljs-comment"># 像素级别角点检测</span>                    <span class="hljs-keyword">if</span> p <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:                        <span class="hljs-keyword">for</span> x, y <span class="hljs-keyword">in</span> np.float32(p).reshape(<span class="hljs-number">-1</span>, <span class="hljs-number">2</span>):                            self.tracks.append([(x, y)])  <span class="hljs-comment"># 将检测到的角点放在待跟踪序列中</span>                self.frame_idx += <span class="hljs-number">1</span>                self.prev_gray = frame_gray                cv2.imshow(<span class="hljs-string">'lk_track'</span>, vis)            ch = <span class="hljs-number">0xFF</span> &amp; cv2.waitKey(<span class="hljs-number">100</span>)            <span class="hljs-keyword">if</span> ch == <span class="hljs-number">27</span>:                <span class="hljs-keyword">break</span>App(<span class="hljs-string">'768x576.avi'</span>).run()</code></pre><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/123.gif" srcset="/img/loading.gif" alt=""></p>]]></content>
    
    
    <categories>
      
      <category>机器视觉</category>
      
    </categories>
    
    
    <tags>
      
      <tag>LK</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>【十三】背景建模</title>
    <link href="/2019/03/cv-image_background/"/>
    <url>/2019/03/cv-image_background/</url>
    
    <content type="html"><![CDATA[<h3 id="背景建模"><a href="#背景建模" class="headerlink" title="背景建模"></a>背景建模</h3><h4 id="相对运动的基本方式"><a href="#相对运动的基本方式" class="headerlink" title="相对运动的基本方式"></a>相对运动的基本方式</h4><blockquote><ul><li>相机静止，目标运劢——背景提取(减除)</li><li>相机运劢，目标静止——光流估计(全局运劢)</li><li>相机和目标均运劢——光流估计</li></ul></blockquote><h4 id="帧差法运动目标检测"><a href="#帧差法运动目标检测" class="headerlink" title="帧差法运动目标检测"></a>帧差法运动目标检测</h4><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20190123141057.png" srcset="/img/loading.gif" alt=""></p><p>​    如图可见，由目标运动引起的运动变化区域包括运动目标在前后两帧中的共同位置(图中黑色区域)、在当前帧中新显露出的背景区域和新覆盖的背景区域三部分。</p><h5 id="【数学模型】"><a href="#【数学模型】" class="headerlink" title="【数学模型】"></a>【<strong>数学模型</strong>】</h5><p>$sign(x)=\begin{cases}<br>1,&amp;如果I(x， y， t)-I(x, y, t-1)&gt;T \ 0,&amp;如果其他情况<br>\end{cases}$</p><blockquote><ul><li>D(x, y): 帧差</li><li>I(x,y,t): 当前帧(t时刻)图像</li><li>I(x,y,t): 上一帧(t-1时刻)图像</li><li>T: 像素灰度差阈值</li></ul></blockquote><h4 id="混合高斯背景建模"><a href="#混合高斯背景建模" class="headerlink" title="混合高斯背景建模"></a>混合高斯背景建模</h4><h5 id="【高斯背景】"><a href="#【高斯背景】" class="headerlink" title="【高斯背景】"></a>【<strong>高斯背景</strong>】</h5><blockquote><p>像素灰度值随时间变化符合高斯分布.</p><p>$I(x, y) \thicksim N(u, \sigma^2)$</p></blockquote><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20190123143447.png" srcset="/img/loading.gif" alt=""></p><blockquote><ul><li>如果$I(x, y, t)-u&gt;3\sigma$ 为<code>前景</code>，否则就是<code>背景</code>。<ul><li>$I(x, y, t)$是当前帧。</li><li>$u$为均值、$\sigma$是方差。</li></ul></li></ul><p>注意：当前帧的灰度值变化落在$3\sigma$间的就是背景，否则就是前景。</p></blockquote><h5 id="【混合高斯模型】"><a href="#【混合高斯模型】" class="headerlink" title="【混合高斯模型】"></a>【<strong>混合高斯模型</strong>】</h5><blockquote><p>任何一种分布函数都可以看做是多个高斯分布的组合.</p></blockquote><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20190123144119.png" srcset="/img/loading.gif" alt=""></p><p>​    混合高斯背景建模是<code>基于像素样本统计信息的背景表示方法</code>，利用像素在较长时间内大量样本值的概率密度等统计信息(如模式数量、每个模式的均值和标准差)表示背景，然后使用<code>统计差分(如3σ原则)进行目标像素判断</code>，可以对复杂动态背景进行建模，计算量较大。<br>​    在混合高斯背景模型中，认为<code>像素之间的颜色信息互不相关，对各像素点的处理都是相互独立的</code>。对于视频图像中的每一个像素点，其值在序列图像中的变化可看作是不断产生像素值的随机过程，即<code>用高斯分布来描述每个像素点的颜色呈现规律{单模态(单峰)，多模态(多峰)}</code>。</p><p>​    对于多峰高斯分布模型，<code>图像的每一个像素点按不同权值的多个高斯分布的叠加来建模，每种高斯分布对应一个可能产生像素点所呈现颜色的状态，各个高斯分布的权值和分布参数随时间更新</code>。当处理彩色图像时，假定图像像素点R、G、B三色通道相互独立并具有相同的方差。对于随机变量X的观测数据集${x_1,x_2,…,x_N}，x_t=(r_t,g_t,b_t)$为t时刻像素的样本，则单个采样点$x_t$其服从的混合高斯分布概率密度函数：</p><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20190123144742.png" srcset="/img/loading.gif" alt=""></p><blockquote><p>​    其中Q为分布模式总数，$N(I,u_q, \sigma_q^2)$为高斯分布，$μ_q$为其均值，$δ_q$为方差，I为三维单位矩阵，$ω_q$为高斯分布的权重。</p></blockquote><h5 id="【混合高斯背景建模步骤】"><a href="#【混合高斯背景建模步骤】" class="headerlink" title="【混合高斯背景建模步骤】"></a>【<strong>混合高斯背景建模步骤</strong>】</h5><blockquote><p>任务：在线计算：$μ_q$，$δ_q$，$ω_q$</p></blockquote><blockquote><ol><li>模型初始化 将采到的第一帧图像的每个象素的灰度值作为均值，再赋以较大的方差。初值Q=1, w=1.0。</li><li>模型学习 将当前帧的对应点象素的灰度值与已有的Q个高斯模型作比较，若满足$|x_k-u_{q, k}|&lt;2.5\sigma_{q,k}$ ，则按概率密度公式调整第q个高斯模型的参数和权重；否则转入(3)：</li><li>增加/替换高斯分量 若不满足条件，且q&lt;Q，则增加一个新分量；若q=Q，则替换</li><li>判断背景  $B = argmin_b(\sum_{q=1}^bw_q&gt;T)$</li><li>判断前景</li></ol></blockquote><h5 id="【混合高斯模型迭代计算原理】"><a href="#【混合高斯模型迭代计算原理】" class="headerlink" title="【混合高斯模型迭代计算原理】"></a>【<strong>混合高斯模型迭代计算原理</strong>】</h5><p><strong>迭代计算：</strong></p><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20190123150252.png" srcset="/img/loading.gif" alt=""></p><blockquote><ul><li>$M_q(k)$为二值化函数，仅当像素值匹配第q类时取1，其余取0</li><li>类别数值取值不大于5</li></ul></blockquote><h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><blockquote><ol><li>背景静止时，可以使用基于背景提取的运动估计斱法计算运动目标。</li><li>混合高斯模型可模拟任意概率密度函数，是背景建模的主流方法</li><li>混合高斯模型参数采用迭代方式计算。</li></ol></blockquote><h3 id="代码实例"><a href="#代码实例" class="headerlink" title="代码实例"></a>代码实例</h3><pre><code class="hljs python"><span class="hljs-comment">#!/usr/bin/env python</span><span class="hljs-comment"># -*- coding: utf-8 -*-</span><span class="hljs-comment"># @Time    : 2019/1/22 21:09</span><span class="hljs-comment"># @Author  : Seven</span><span class="hljs-comment"># @File    : backgroundDemo.py</span><span class="hljs-comment"># @Software: PyCharm</span><span class="hljs-comment"># function : 基于背景提取的运动估计</span><span class="hljs-keyword">import</span> cv2<span class="hljs-comment"># 加载视频</span>cap = cv2.VideoCapture()cap.open(<span class="hljs-string">'768x576.avi'</span>)<span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> cap.isOpened():    print(<span class="hljs-string">"无法打开视频文件"</span>)pBgModel = cv2.createBackgroundSubtractorMOG2()<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">labelTargets</span><span class="hljs-params">(img, mask, threshold)</span>:</span>    seg = mask.copy()    cnts = cv2.findContours(seg, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)    count = <span class="hljs-number">0</span>    <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> cnts[<span class="hljs-number">1</span>]:        area = cv2.contourArea(i)        <span class="hljs-keyword">if</span> area &lt; threshold:            <span class="hljs-keyword">continue</span>        count += <span class="hljs-number">1</span>        rect = cv2.boundingRect(i)        print(<span class="hljs-string">"矩形：X:&#123;&#125; Y:&#123;&#125; 宽：&#123;&#125; 高：&#123;&#125;"</span>.format(rect[<span class="hljs-number">0</span>], rect[<span class="hljs-number">1</span>], rect[<span class="hljs-number">2</span>], rect[<span class="hljs-number">3</span>]))        cv2.drawContours(img, [i], <span class="hljs-number">-1</span>, (<span class="hljs-number">255</span>, <span class="hljs-number">255</span>, <span class="hljs-number">0</span>), <span class="hljs-number">1</span>)        cv2.rectangle(img, (rect[<span class="hljs-number">0</span>], rect[<span class="hljs-number">1</span>]), (rect[<span class="hljs-number">0</span>] + rect[<span class="hljs-number">2</span>], rect[<span class="hljs-number">1</span>] + rect[<span class="hljs-number">3</span>]), (<span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">255</span>), <span class="hljs-number">1</span>)        cv2.putText(img, str(count), (rect[<span class="hljs-number">0</span>], rect[<span class="hljs-number">1</span>]), cv2.FONT_HERSHEY_PLAIN, <span class="hljs-number">0.5</span>, (<span class="hljs-number">0</span>, <span class="hljs-number">255</span>, <span class="hljs-number">0</span>))    <span class="hljs-keyword">return</span> count<span class="hljs-keyword">while</span> <span class="hljs-literal">True</span>:    flag, source = cap.read()    <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> flag:        <span class="hljs-keyword">break</span>    image = cv2.pyrDown(source)    fgMask = pBgModel.apply(image)    kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (<span class="hljs-number">5</span>, <span class="hljs-number">5</span>))    morphImage_open = cv2.morphologyEx(fgMask, cv2.MORPH_OPEN, kernel, iterations=<span class="hljs-number">5</span>)    mask = fgMask - morphImage_open    _, Mask = cv2.threshold(mask, <span class="hljs-number">30</span>, <span class="hljs-number">255</span>, cv2.THRESH_BINARY + cv2.THRESH_OTSU)    <span class="hljs-comment"># Mask = cv2.GaussianBlur(Mask, (5, 5), 0)</span>    targets = labelTargets(image, Mask, <span class="hljs-number">30</span>)    print(<span class="hljs-string">"共检测%s个目标"</span> % targets)    backGround = pBgModel.getBackgroundImage()    foreGround = image - backGround    cv2.imshow(<span class="hljs-string">'source'</span>, image)    cv2.imshow(<span class="hljs-string">'background'</span>, backGround)    cv2.imshow(<span class="hljs-string">'foreground'</span>, Mask)    key = cv2.waitKey(<span class="hljs-number">10</span>)    <span class="hljs-keyword">if</span> key == <span class="hljs-number">27</span>:        <span class="hljs-keyword">break</span></code></pre><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/GIF.gif" srcset="/img/loading.gif" alt=""></p>]]></content>
    
    
    <categories>
      
      <category>机器视觉</category>
      
    </categories>
    
    
    <tags>
      
      <tag>背景建模</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>【十二】ORB角点检测</title>
    <link href="/2019/03/cv-image_orb/"/>
    <url>/2019/03/cv-image_orb/</url>
    
    <content type="html"><![CDATA[<h3 id="ORB角点检测"><a href="#ORB角点检测" class="headerlink" title="ORB角点检测"></a>ORB角点检测</h3><h4 id="【简介】"><a href="#【简介】" class="headerlink" title="【简介】"></a>【简介】</h4><blockquote><ul><li>ORB（Oriented FAST and Rotated BRIEF）是一种快速特征点提取和描述的算法。</li><li>这个算法是由Ethan Rublee, Vincent Rabaud, Kurt Konolige以及Gary R.Bradski在2011年一篇名为“ORB：An Efficient Alternative to SIFT or SURF”的文章中提出。</li><li>ORB算法分为两部分，分别是特征点提取和特征点描述。特征提取是由FAST（Features from Accelerated Segment Test）算法发展来的，特征点描述是根据BRIEF（Binary Robust Independent Elementary Features）特征描述算法改进的。</li><li>ORB = oFast + rBRIEF。据称ORB算法的速度是sift的100倍，是surf的10倍。</li><li>ORB算子在SLAM及无人机视觉等领域得到广泛应用</li></ul></blockquote><h4 id="【oFAST特征提取】"><a href="#【oFAST特征提取】" class="headerlink" title="【oFAST特征提取】"></a>【oFAST特征提取】</h4><blockquote><ul><li>ORB算法的特征提取是由FAST算法改进的，这里称为oFAST（FAST keypoint Orientation）。</li><li>在使用FAST提取出特征点之后，给其定义一个特征点方向，以此来实现特征点的旋转不变性。</li></ul></blockquote><h5 id="粗提取"><a href="#粗提取" class="headerlink" title="粗提取"></a><code>粗提取</code></h5><blockquote><ul><li>判断特征点：从图像中选取一点P，以P为圆心画一个半径为3像素的圆。圆周上如果有连续N个像素点的灰度值比P点的灰度值大戒小，则认为P为特征点。这就是大家经常说的FAST-N。有FAST-9、FAST-10、FAST-11、FAST-12，大家使用比较多的是FAST-9和FAST-12。</li><li>快速算法：为了加快特征点的提取，快速排出非特征点，首先检测1、5、9、13位置上的灰度值，如果P是特征点，那么这四个位置上有3个或3个以上的的像素值都大于或者小于P点的灰度值。如果不满足，则直接排出此点。</li></ul></blockquote><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20190120215840.png" srcset="/img/loading.gif" alt="粗提取"></p><h5 id="筛选最优特征点"><a href="#筛选最优特征点" class="headerlink" title="筛选最优特征点"></a><code>筛选最优特征点</code></h5><p>机器学习的方法筛选最优特征点。简单来说就是使用ID3算法训练一个决策树，将特征点圆周上的16个像素输入决策树中，以此来筛选出最优的FAST特征点。具体步骤如下：</p><blockquote><ol><li>选取进行角点提取的应用场景下的一组训练图像。</li><li>使用FAST角点检测算法找出训练图像上的所有角点。</li><li>对于每一个角点，将其周围的16个像素存储成一个向量。对所有图像都这样做构建一个特征向量。</li><li>每一个角点的16像素点都属于下列三类中的一种，像素点因此被分成三个子集:$P_d、P_s、P_b$</li></ol><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20190120220130.png" srcset="/img/loading.gif" alt="子集"></p><ol start="5"><li>定义一个新的布尔变量Kp ，如果是角点就设置为True，否则就设置为False。</li><li>使用ID3算法来查询每一个子集。</li><li>递归计算所有的子集直到它的熵为0。</li></ol><p>注意：被构建好的决策树可用于其它图像的FAST检测。</p></blockquote><h5 id="使用非极大值抑制算法去除临近位置多个特征点"><a href="#使用非极大值抑制算法去除临近位置多个特征点" class="headerlink" title="使用非极大值抑制算法去除临近位置多个特征点"></a><code>使用非极大值抑制算法去除临近位置多个特征点</code></h5><blockquote><ol><li>计算特征点出的FAST得分值s（像素点不周围16个像素点差值的绝对值之和）</li><li>以特征点p为中心的一个邻域（如3x3或5x5）内，若有多个特征点，则判断每个特征点的s值</li><li>若p是邻域所有特征点中响应值最大的，则保留；否则，抑制。若邻域内只有一个特征点，则保留。得分计算公式如下（公式中用V表示得分，t表示阈值）：</li></ol><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20190120220601.png" srcset="/img/loading.gif" alt="抑制"></p></blockquote><h5 id="建立金字塔以实现特征点多尺度不变性"><a href="#建立金字塔以实现特征点多尺度不变性" class="headerlink" title="建立金字塔以实现特征点多尺度不变性"></a><code>建立金字塔以实现特征点多尺度不变性</code></h5><blockquote><ul><li>设置一个比例因子scaleFactor（opencv默认为1.2）和金字塔的层数nlevels（Opencv默认为8）。</li><li>将原图像按比例因子缩小成nlevels幅图像。</li><li>缩放后的图像为：I’= I/scaleFactork(k=1,2,…, nlevels)。nlevels幅不同比例的图像提取特征点总和作为这幅图像的oFAST特征点。</li></ul><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20190120220736.png" srcset="/img/loading.gif" alt=""></p><ul><li>特征点的旋转不变性。ORB算法提出使用矩（moment）法来确定FAST特征点的方向。也就是说通过矩来计算特征点以r为半径范围内的质心，特征点坐标到质心形成一个向量作为该特征点的方向。矩定义如下:$m_{pq}=\Sigma_{x,y} x^p y^q I(x,y), x,y \in [-r,r]$</li><li>其中，I(x,y)为图像灰度表达式。该矩的质心为：$C = ( \dfrac{m_{10}}{m_{00}}, \dfrac{m_{01}}{m_{00}})$</li><li>假设角点坐标为O，则向量的角度即为该特征点的方向。计算公式如下：$\theta = arctan(m_{01}/m_{10})$</li></ul></blockquote><h5 id="算法特点"><a href="#算法特点" class="headerlink" title="算法特点"></a><code>算法特点</code></h5><blockquote><ul><li>FAST算法比其他角点检测算法要快</li><li>受图像噪声以及设定阈值影响较大</li><li>当设置n&lt;12n&lt;12时，不能用快速方法过滤非角点</li><li>FAST不产生多尺度特征，不具备旋转不变性，而且检测到的角点不是最优</li></ul></blockquote><h4 id="【rBRIEF】"><a href="#【rBRIEF】" class="headerlink" title="【rBRIEF】"></a>【rBRIEF】</h4><blockquote><p>ORB算法的特征描述是由BRIEF算法改进的，这里称为rBRIEF（Rotation-Aware Brief）。也就是说，在BRIEF特征描述的基础上加入旋转因子从而改进BRIEF算法。</p></blockquote><h5 id="算法描述"><a href="#算法描述" class="headerlink" title="算法描述"></a><code>算法描述</code></h5><blockquote><p>得到特征点后我们需要以某种方式描述这些特征点的属性。这些属性的输出我们称之为该特征点的描述子（Feature Descritors）.ORB采用BRIEF算法来计算一个特征点的描述子。BRIEF算法的<strong>核心思想</strong>是在关键点P的周围以一定模式选取N个点对，把这N个点对的比较结果组合起来作为描述子。</p><ul><li>BRIEF算法计算出来的是一个二进制串的特征描述符。它是在一个特征点的邻域内，选择n对像素点$p_i、q_i（i=1,2,…,n）$。</li><li>比较每个点对的灰度值的大小，如果$I(p_i)&gt; I(q_i)$，则生成二进制串中的1，否则为0。</li><li>所有的点对都进行比较，则生成长度为n的二进制串。一般n取128、256戒512，opencv默认为256。</li></ul></blockquote><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20190120221620.png" srcset="/img/loading.gif" alt=""></p><h5 id="算法步骤"><a href="#算法步骤" class="headerlink" title="算法步骤"></a><code>算法步骤</code></h5><blockquote><ul><li>以关键点P为圆心，以d为半径做圆O。</li><li>在圆O内某一模式选取N个点对。这里为方便说明，N=4，实际应用中N可以取512.</li></ul><p>假设当前选取的4个点对如上图所示分别标记为：<img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20190120221752.png" srcset="/img/loading.gif" alt="1"></p><ul><li>定义T：</li></ul><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20190120221847.png" srcset="/img/loading.gif" alt=""></p><ul><li>分别对已选取的点对进行T操作，将得到的结果进行组合。</li></ul><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20190120221906.png" srcset="/img/loading.gif" alt="1"></p><ul><li>则最终的描述子为：1011</li></ul></blockquote><h5 id="具有旋转不变性的BRIEF"><a href="#具有旋转不变性的BRIEF" class="headerlink" title="具有旋转不变性的BRIEF"></a><code>具有旋转不变性的BRIEF</code></h5><p>​    ORB并没有解决<strong>尺度一致性</strong>问题，在OpenCV的ORB实现中采用了图像金字塔来改善这方面的性能。ORB主要解决BRIEF描述子不具备<strong>旋转不变性</strong>的问题。</p><blockquote><p>steered BRIEF（旋转不变性改进）</p><ul><li>在使用oFast算法计算出的特征点中包括了特征点的方向角度。假设原始的BRIEF算法在特征点SxS（一般S取31）邻域内选取n对点集。</li></ul><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20190120222241.png" srcset="/img/loading.gif" alt=""></p><ul><li>经过旋转角度θ旋转，得到新的点对$D_\theta=R_\theta D$</li><li>在新的点集位置上比较点对的大小形成二进制串的描述符。</li></ul></blockquote><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20190120222336.png" srcset="/img/loading.gif" alt=""></p><h5 id="描述子的区分性"><a href="#描述子的区分性" class="headerlink" title="描述子的区分性"></a><code>描述子的区分性</code></h5><p>  通过上述方法得到的特征描述子具有旋转不变性，称为steered BRIEF(sBRIEF)，但匹配效果却不如原始BRIEF算法，因为可区分性减弱了。特征描述子的一个要求就是要尽可能地表达特征点的独特性，便于区分不同的特征点。如下图所示，为几种特征描述子的均值分布，横轴为均值与0.5之间的距离，纵轴为相应均值下特征点的统计数量。可以看出，BRIEF描述子所有比特位的均值接近于0.5，且方差很大；方差越大表明可区分性越好。不同特征点的描述子表现出较大的差异性，不易造成无匹配。但steered BRIEF进行了坐标旋转，损失了这个特性，导致可区分性减弱，相关性变强，不利于匹配。</p><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20190120222445.png" srcset="/img/loading.gif" alt=""></p><p>为了解决steered BRIEF可区分性降低的问题，ORB使用了一种基于学习的方法来选择一定数量的随机点对。</p><blockquote><ul><li>首先建立一个大约300k特征点的数据集(特征点来源于PASCAL2006中的图像)，对每个特征点，考虑其31×31的邻域Patch，为了去除噪声的干扰，选择5×5的子窗口的灰度均值代替单个像素的灰度，这样每个Patch内就有N=(31−5+1)×(31−5+1)=27×27=729个子窗口，从中随机选取2个非重复的子窗口，一共有$M=C_N^2$中方法。</li><li>这样，每个特征点便可提取出一个长度为M的二进制串，所有特征点可构成一个300k×M的二进制矩阵Q，矩阵中每个元素取值为0或1。现在需要从M个点对中选取256个<code>相关性最小、可区分性最大的点对，作为最终的二进制编码</code>。筛选方法如下：<ul><li>对矩阵Q的每一列求取均值，并根据均值与0.5之间的距离从小到大的顺序，依次对所有列向量进行重新排序，得到矩阵T</li><li>将T中的第一列向量放到结果矩阵R中</li><li>取出T中的下一列向量，计算其与矩阵R中所有列向量的相关性，如果相关系数小于给定阈值，则将T中的该列向量移至矩阵R中，否则丢弃</li><li>循环执行上一步，直到R中有256个列向量；如果遍历T中所有列，R中向量列数还不满256，则增大阈值，重复以上步骤。</li></ul></li></ul></blockquote><p>这样，最后得到的就是相关性最小的256对随机点，该方法称为<code>rBRIEF</code>。</p><h4 id="【总结】"><a href="#【总结】" class="headerlink" title="【总结】"></a>【总结】</h4><blockquote><ol><li>ORB = oFAST + rBRIEF</li><li>oFAST是一类快速角点检测算法，并具备旋转不变性</li><li>rBRIEF是一类角点描述(编码算法)，并且编码具有良好的可区分性</li></ol></blockquote><h3 id="代码实例"><a href="#代码实例" class="headerlink" title="代码实例"></a>代码实例</h3><pre><code class="hljs python"><span class="hljs-comment">#!/usr/bin/env python</span><span class="hljs-comment"># -*- coding: utf-8 -*-</span><span class="hljs-comment"># @Time    : 2019/1/19 13:30</span><span class="hljs-comment"># @Author  : Seven</span><span class="hljs-comment"># @File    : ORBDemo.py</span><span class="hljs-comment"># @Software: PyCharm</span><span class="hljs-keyword">import</span> cv2<span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> npimg1 = cv2.imread(<span class="hljs-string">'csdn.png'</span>)img2 = np.rot90(img1)orb = cv2.ORB_create(<span class="hljs-number">50</span>)<span class="hljs-comment"># 特征点检测和提取特征点描述子</span>kp1, des1 = orb.detectAndCompute(img1, <span class="hljs-literal">None</span>)kp2, des2 = orb.detectAndCompute(img2, <span class="hljs-literal">None</span>)<span class="hljs-comment"># 创建 BFMatcher 对象</span>bf = cv2.BFMatcher(cv2.NORM_L2)<span class="hljs-comment"># 根据描述子匹配特征点.</span>matches = bf.match(des1, des2)<span class="hljs-comment"># 画出50个匹配点</span>img3 = cv2.drawMatches(img1, kp1, img2, kp2, matches[:<span class="hljs-number">50</span>], <span class="hljs-literal">None</span>, flags=<span class="hljs-number">2</span>)cv2.imshow(<span class="hljs-string">"ORB"</span>, img3)cv2.waitKey(<span class="hljs-number">0</span>)</code></pre><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20190322220304.png" srcset="/img/loading.gif" alt=""></p>]]></content>
    
    
    <categories>
      
      <category>机器视觉</category>
      
    </categories>
    
    
    <tags>
      
      <tag>ORB</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>【十一】SIFT角点检测</title>
    <link href="/2019/03/cv-image_sift/"/>
    <url>/2019/03/cv-image_sift/</url>
    
    <content type="html"><![CDATA[<h3 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h3><blockquote><ul><li>SIFT的全称是Scale Invariant Feature Transform（尺度不变特征变换），是由加拿大教授David G.Lowe在1999年在会议文章中提出，2004年发表在IJCV上。</li><li>是计算机视觉界近二十年来引用率最高的文章之一。</li><li>SIFT特征对旋转、尺度缩放、亮度变化等保持不发性，是一种稳定的局部特征。</li><li>SIFT的特征提取方面对计算机视觉近年来的发展影响深远，特别是几乎影响到了后续所有的角点提取和匹配算法。</li></ul></blockquote><h3 id="算法特点"><a href="#算法特点" class="headerlink" title="算法特点"></a>算法特点</h3><blockquote><ul><li>图像的局部特征，对旋转、尺度缩放、亮度变化保持不变，对视角变化、仿射变换、噪声也保持一定程度的稳定性。</li><li>独特性好，信息量丰富，适用于海量特征库进行快速、准确的匹配。</li><li>多量性：即使是很少几个物体也可以产生大量的SIFT特征</li><li>高速性：改进的SIFT匹配算法甚至可以达到实时性</li><li>扩展性：可以很方便的与其他的特征向量进行联合。</li></ul></blockquote><h3 id="尺度空间"><a href="#尺度空间" class="headerlink" title="尺度空间"></a>尺度空间</h3><h4 id="概述-1"><a href="#概述-1" class="headerlink" title="概述"></a><code>概述</code></h4><blockquote><ul><li>人眼可自动调节尺度，完成对物体的检测和识别</li><li>模仿人的视觉认知，把物体不同尺度下的图像都提供给机器，让机器能够对物体在不同的尺度下综合信息识别。</li><li>因此，首先需要建立尺度空间。</li><li>通过高斯函数与原图像卷积，并经过下采样，可建立原始图像的尺度空间模型。</li></ul></blockquote><h4 id="理论"><a href="#理论" class="headerlink" title="理论"></a><code>理论</code></h4><blockquote><ul><li>在图像信息处理模型中引入一个被视为尺度的参数，通过连续变化尺度参数获得多尺度下的尺度空间表示序列，对这些序列进行尺度空间主轮廓的提取，并以该主轮廓作为一种特征向量，实现边缘、角点检测和不同分辨率上的特征提取等。</li><li>尺度空间方法将传统的单尺度图像信息处理技术纳入尺度不断变化的动态分析框架中，更容易获取图像的本质特征。尺度空间中各尺度图像的模糊程度逐渐发大，能够模拟人在距离目标由近到远时目标在视网膜上的形成过程。</li></ul></blockquote><h4 id="定义"><a href="#定义" class="headerlink" title="定义"></a><code>定义</code></h4><blockquote><ul><li>高斯卷积核是实现尺度变换的唯一变换核</li><li>一幅图像的尺度空间被定义为：高斯卷积核与该图像的卷积，它是高斯卷积核中的参数$\sigma$的函数，这里用到“空间”这个词，是因为$\sigma$是可以连续变化的，具体地，图像$I(x,y)$的尺度空间为： $L(x,y,\sigma) = G(x,y,\sigma)*I(x,y)$<ul><li>$G(x,y,\sigma) = \frac{1}{2\pi\sigma^2}*e^{-\frac{x^2+y^2}{2\sigma^2}}$ 是高斯核函数。</li><li>$\sigma$是尺度因子，$\sigma$的大小决定图像的平滑程度，大尺度对应图像的概貌特征，小尺度对应图像的细节特征。大的$\sigma$值对应粗糙尺度(低分辨率)，小的$\sigma$值对应精细尺度(高分辨率)。</li><li>*是卷积操作。</li><li>$L(x,y,\sigma) $就是输入图像$I(x,y)$的尺度空间。</li></ul></li></ul></blockquote><h4 id="栗子"><a href="#栗子" class="headerlink" title="栗子"></a><code>栗子</code></h4><blockquote><p>​    比如要观察一棵树，所选择的尺度应该是“米”级；如果观察树上的树叶，所选择的尺度则应该是“厘米”级。一般来说，摄像设备所能呈现的画面分辨率是固定的。要以同样的分辨率分别观察树和树叶，我们需要调整摄像设备的摄像位置。因此，视觉问题中的“尺度”概念也可以被形象地理解为摄像设备与被观察物体之间的距离：较远的距离对应较大的尺度，较近的距离对应较小的尺度。</p><p>概况的说：<code>“尺度空间”的概念就是在多个尺度下观察目标，然后加以综合的分析和理解。</code></p></blockquote><h3 id="特征提取"><a href="#特征提取" class="headerlink" title="特征提取"></a>特征提取</h3><p>一幅图像的SIFT特征提取，分为4个步骤：</p><blockquote><ul><li>尺度空间极值检测.</li><li>关键点位置及尺度确定</li><li>关键点方向确定</li><li>特征向量生成</li></ul></blockquote><h4 id="尺度空间极值检测"><a href="#尺度空间极值检测" class="headerlink" title="尺度空间极值检测"></a><code>尺度空间极值检测</code></h4><blockquote><p>SIFT特征点其实就是尺度空间中稳定的点/极值点，那么，为了得到这些稳定点：</p><ul><li>首先，需要对输入图像建立尺度空间（通过图像高斯金字塔）</li><li>然后，需要从建立得到的尺度空间中检测极值点（转换为图像差分高斯金字塔中极值点的检测）<ul><li>获取差分高斯金字塔</li><li>DoG中极值点的检测</li></ul></li></ul></blockquote><h5 id="【构建高斯金字塔】"><a href="#【构建高斯金字塔】" class="headerlink" title="【构建高斯金字塔】"></a>【<strong>构建高斯金字塔</strong>】</h5><blockquote><ul><li>尺度空间在实现时使用高斯金字塔表示，高斯金字塔的构建分为两部分：<ul><li>对图像做不同尺度的高斯模糊；</li><li>对图像做降采样(隔点采样)。</li></ul></li><li>图像的金字塔模型是指，将原始图像不断降阶采样，得到一系列大小不一的图像，由大到小，从下到上构成的塔状模型。</li></ul></blockquote><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20190120200046.png" srcset="/img/loading.gif" alt="高斯金字塔"></p><p>​    对于一幅输入图像，为了进行sift特征检测、实现scale-invariant（任何尺度下都能够有对应的特征点），需要对该图像的尺度空间进行分析，即建立高斯金字塔图像、得到不同scale的图像，这里的高斯金字塔与最原始的高斯金字塔稍微有点区别，因为它在构造尺度空间时，将这些不同尺度图像分为了多个Octave、每个Octave又分为了多层。下图左侧框内给出了Sift中的高斯金字塔的结构图。</p><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20190120202543.png" srcset="/img/loading.gif" alt="结构图"></p><blockquote><ul><li>高斯金字塔分为S组（即S个塔、或者称为S个octave），每个塔中有s层；具体有几个塔、每个塔包含几层由输入图像的尺寸决定！对于上图而言：有S=2组，每组由s=5层；</li><li>同一塔中图像尺寸相同但尺度不同、不同塔中图像尺寸和尺度都不同（注意：这里的尺寸和尺度概念不同！尺寸对应图像分辨率、尺度为高斯核大小）<ul><li>每一个塔中图像尺寸确定方法为：第一个塔的尺寸为原图大小，后面每个塔的尺寸为上一个塔的降采样的结果（即长宽分别减半的结果）</li><li>每个塔中各层图像的尺度不同，即$\sigma$取值不同，各层之间相差一个k值。例如：上图中第一个塔中各层的尺度因子分别为$\sigma、k\sigma、 k^2\sigma、k^3\sigma、k^4\sigma$；而第二个塔中各层的尺度因子分别为$2\sigma、2k\sigma、 2k^2\sigma、2k^3\sigma、2k^4\sigma$</li></ul></li><li>如何确定相邻层之间的比例因子k呢？下图是同一Octave中不同层和不同Octave之间的尺度大小关系，为了满足尺度变化的连续性，即某一组的最后一层对应的尺度与下一组的第一层对应的尺度应该相差一个k倍，所以应该有<strong>$k^{s-1}\sigma_0 <em>k = 2</em>\sigma_0$</strong>， 所以<strong>$k^s=2$</strong>，即<strong>$k=2^{1/2}$</strong>， 其中<strong>，$\sigma_0$</strong>为基础尺度因子。 </li></ul></blockquote><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20190120204121.png" srcset="/img/loading.gif" alt="1"></p><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20190120204133.png" srcset="/img/loading.gif" alt="2"></p><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20190120204145.png" srcset="/img/loading.gif" alt="3"></p><blockquote><p>注意：关于尺度变化的连续性应该就是指：金字塔图像中所有图像之间的尺度因子都相差一个k倍：每个octave中相邻图像之间尺度因子相差一个k，相邻2个octave的最后一层与第一层的图像之间尺度因子相差一个k！<br>注2：英文Octave是音乐上一个八度的意思，在这里指的是一组图像。这一组图像的分辨率是相同的，但是采用了不同尺度的高斯函数进行滤波，因此从模糊程度上看（或者说从关注的尺度上看）是有区别的。而不同组的图像具有不同的分辨率，在尺度上的差别就更大。</p></blockquote><p>​    得到了图像的尺度空间后，需要在尺度中间内检测稳定特征点，从而需要比较不同尺度图像之间的差别，实现极值点的检测，实际上，Laplacian of Gaussian和Difference of Gaussian都能够实现这样的目的，但LoG需要大量的计算，而DoG的计算相对简单。</p><h5 id="【建立图像差分高斯金字塔（DoG）】"><a href="#【建立图像差分高斯金字塔（DoG）】" class="headerlink" title="【建立图像差分高斯金字塔（DoG）】"></a>【建立图像差分高斯金字塔（DoG）】</h5><p>​    对于高斯金字塔中的每一个塔的不同层，可以计算得到<strong>相邻层</strong>之间的差值，从而可以得到差分高斯，对高斯金字塔中每一个塔都进行同样的操作，从而得到差分高斯金字塔，如下图右侧所示，即显示了由左侧的高斯金字塔构造得到的差分高斯金字塔，该差分高斯金字塔包含2个塔，每个塔都有四层 </p><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20190120204441.png" srcset="/img/loading.gif" alt="1"></p><p>差分高斯表征了相邻尺度的高斯图像之前的差别，大值表示区别大，小值表示区别小，后续的特征点检测都是差分高斯图像金字塔中进行的！</p><h5 id="【尺度空间中特征点的检测（DoG中极值点的检测）】"><a href="#【尺度空间中特征点的检测（DoG中极值点的检测）】" class="headerlink" title="【尺度空间中特征点的检测（DoG中极值点的检测）】"></a>【尺度空间中特征点的检测（DoG中极值点的检测）】</h5><blockquote><p>构造完尺度空间（差分高斯金字塔）后，接下来的任务就是“在尺度中间中检测出图像中的稳定特征点”.</p></blockquote><p>​    对于DoG中每一个采样点（每一个Octave中每一层），将其与它邻域内所有像素点（8+18=26）进行比较，判断其是否为局部极值点（极大或者极小），更加具体地：如下图所示，中间的检测点和它同尺度的8个相邻点和上下相邻尺度对应的9×2个点共26个点比较，以确保在尺度空间和二维图像空间都检测到极值点。 一个点如果在DOG尺度空间本层以及上下两层的26个领域中是最大或最小值时，就认为该点是图像在该尺度下的一个特征点。但要注意：这种相邻层之间的极值点的寻找是在同一Octave中的相邻尺度之间进行寻找的，而不要跨组！ </p><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20190120204725.png" srcset="/img/loading.gif" alt=""></p><p>同时，应该注意到一个问题，在极值比较的过程中，每一Octave图像的首末两层是无法进行极值比较的，为了满足尺度变化的连续性，需要进行一些修正：在高斯金字塔中生成S+3层图像，具体解释如下：假设s=3，也就是每个塔里有3层，则k=21/s=21/3：</p><ul><li><p>那么按照上图可得Gauss Space和DoG space 分别有3个（s个）和2个（s-1个）分量，在DoG space中，1st-octave两项分别是σ,kσ; 2nd-octave两项分别是2σ,2kσ; </p><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20190120204813.png" srcset="/img/loading.gif" alt="1"></p></li><li><p>由于无法比较极值，我们必须在高斯空间继续添加高斯模糊项，使得DoG中第一个Octave形成σ,kσ,k2σ,k3σ,k4σ，这样就可以选择中间三项kσ,k2σ,k3σ（只有左右都有才能有极值）；那么下一octave中（由上一层降采样获得）所得三项即为2kσ,2k2σ,2k3σ，其首项2kσ=24/3。刚好与上一octave末项k3σ=23/3尺度变化连续起来，所以每次要在Gaussian space添加3项，每组（塔）共S+3层图像，相应的DoG金字塔有S+2层图像。 </p><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20190120204844.png" srcset="/img/loading.gif" alt=""></p><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20190120204854.png" srcset="/img/loading.gif" alt="1"></p></li></ul><h4 id="关键点位置及尺度确定"><a href="#关键点位置及尺度确定" class="headerlink" title="关键点位置及尺度确定"></a>关键点位置及尺度确定</h4><p>​    通过拟和“三维二次函数”可以精确确定关键点的位置和尺度（达到亚像素精度），具体方法还未知，可以得到一系列的SIFT候选特征点集合，但由于这些关键点中有些具有较低的对比对，有些输属于不稳定的边缘响应点(因为DoG算子会产生较强的边缘响应)，所以，为了增强匹配稳定性、提高抗噪声能力，应该将这2类关键点去除，实现对候选SIFT特征点集合的进一步净化：</p><blockquote><ul><li>剔除对比度低的点</li><li>剔除边缘点</li></ul></blockquote><h4 id="关键点方向确定"><a href="#关键点方向确定" class="headerlink" title="关键点方向确定"></a>关键点方向确定</h4><ul><li><p>算关键点的方向，需要利用以该关键点为中心的某邻域窗口内所有像素点的梯度分布特性（目的是为了使的sift算子具备旋转不变性），所以，下面首先给出计算尺度空间中每个像素点的梯度模值和方向的方法，按照下面公式计算</p><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20190120205319.png" srcset="/img/loading.gif" alt="1"></p><ul><li><p>其中L所用的尺度为每个关键点各自所在的尺度。 </p><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20190120205422.png" srcset="/img/loading.gif" alt="2"></p></li></ul></li><li><p>接下来，对于每个关键点（假设尺度为sigma），利用<strong>直方图</strong>统计其相邻窗口内的像素的梯度分布，从而，确定该关键点的方向，具体方法如下：</p><ul><li><p>分别计算以该关键点为中心的相邻窗口中像素点的梯度方向和模值.</p></li><li><p>为该窗口内每一个像素点赋予一个权值：由每个像素点对应的梯度模值和以关键点为中心尺度为1.5sigma的高斯核决定</p></li><li><p>设定某种规则，开始统计直方图，例如，这里将方向（0~360°）分为8份，那么，梯度方向直方图将有8个柱，窗口内每个像素到底属于哪个柱由该像素点的梯度方向确定（见下图右侧子图所示） </p><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20190120205527.png" srcset="/img/loading.gif" alt="1"><br><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20190120205537.png" srcset="/img/loading.gif" alt="3"></p></li><li><p>在该关键点对应的梯度方向直方图中，找到包含像素点最多的那个方向作为该关键点的方向，对于上图而言，方向角(0.25π,0.5π)这个区间内的像素点最多，所以，以(0.25π+0.5π)/2 = 0.375π作为该关键点的方向 。</p></li></ul></li></ul><blockquote><p>至此，得到了图像中所有关键点的方向！实际上，关键点方向的确定是为了接下来的特征向量生成中的坐标旋转使用的！</p></blockquote><h4 id="特征向量生成"><a href="#特征向量生成" class="headerlink" title="特征向量生成"></a>特征向量生成</h4><p>上面只是得到了每个关键点的方向，接下来，需要确定每个关键点的特征向量，具体方式如下：</p><ul><li><p>将坐标轴旋转到关键点的方向 </p><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20190120205842.png" srcset="/img/loading.gif" alt="1"></p></li><li><p>对于某一个关键点，取其周围窗口，如下图绿色区域所示，其中，窗口内每个小矩形框为一个像素，箭头表示该像素位置的梯度方向和模值 </p><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20190120205921.png" srcset="/img/loading.gif" alt=""></p></li><li><p>在该$8<em>8$窗口对应的4个（称为4个种子点）$4</em>4$的小块儿上，分别计算该小块儿包含的16个像素点的梯度直方图（8个柱），并进行累加（每个柱对应的所有像素点的梯度模值累加），每个小块儿可以得到8个特征（8个方向对应的模值的累加），从而，4个种子点将得到关键点的4*8=32个特征，如下图右侧所示，4个种子点，每个种子点产生8个特征 .</p><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20190120210008.png" srcset="/img/loading.gif" alt="2"></p></li><li><p>实际中，Lowe建议使用$4<em>4$个子块儿（称为16个种子点）进行特征计算，那么，每个关键点将具有16\</em>8=128个特征，如下图所示，此时，需要在特征点周围取$16<em>16$的窗口，分割为$4</em>4$个子块儿（这样做的目的是为了增强后续匹配的稳健性）。 </p><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20190120210208.png" srcset="/img/loading.gif" alt="3"></p></li></ul><p>至此，关键点特征向量完全确定！此时SIFT特征向量已经去除了尺度变化、旋转等几何变形因素的影响，再继续将特征向量的长度归一化，则可以进一步去除光照变化的影响。 </p><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20190120210234.png" srcset="/img/loading.gif" alt=""></p><h3 id="特征匹配"><a href="#特征匹配" class="headerlink" title="特征匹配"></a>特征匹配</h3><p>现有A、B两幅图像，分别利用上面的方法从各幅图像中提取到了k1个sift特征点和k2个特征点及其对应的特征描述子，即$k1<em>128$维和$k2</em>128$维的特征，现在需要将两图中各个scale（所有scale）的描述子进行匹配。</p><p>接下来采用关键点特征向量的欧式距离来作为两幅图像中关键点的相似性判定度量。</p><ul><li>取图像A中的某个关键点，并找出其与图像B中欧式距离最近的前两个关键点<ul><li>在这两个关键点中，如果最近的距离除以次近的距离少于某个比例阈值，则接受这一对匹配点。降低这个比例阈值，SIFT匹配点数目会减少，但更加稳定。</li><li>利用2个近邻点比较的目的是为了排除因为图像遮挡和背景混乱而产生的无匹配关系的关键点，所以Lowe提出了比较最近邻距离与次近邻距离的方法,距离比率ratio小于某个阈值的认为是正确匹配。因为对于错误匹配,由于特征空间的高维性,相似的距离可能有大量其他的错误匹配,从而它的ratio值比较高。Lowe推荐ratio的阈值为0.8。但作者对大量任意存在尺度、旋转和亮度变化的两幅图片进行匹配，结果表明ratio取值在0. 4~0. 6之间最佳，小于0. 4的很少有匹配点，大于0. 6的则存在大量错误匹配点。(如果这个地方你要改进，最好给出一个匹配率和ration之间的关系图，这样才有说服力)，作者建议ratio的取值原则如下:<ul><li>ratio=0. 4　对于准确度要求高的匹配；</li><li>ratio=0. 6　对于匹配点数目要求比较多的匹配；</li><li>ratio=0. 5　一般情况下。</li><li>也可按如下原则: 当最近邻距离&lt;200时，ratio=0. 6；反之，ratio=0. 4。ratio的取值策略能排分错误匹配点。</li></ul></li></ul></li></ul><h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><blockquote><ul><li>尺度空间的核心思想是通过不同分辨率看同一个图像，可通过不同尺度的高斯函数不原始图像卷积实现</li><li>极大值点的精确定位可通过解含有二阶寻数项的方程得到</li><li>借助在关键点附近区域的梯度方向统计不直方图生成，SIFT最终可得到对应每个关键点的128维向量描述。</li></ul></blockquote><h3 id="代码实例"><a href="#代码实例" class="headerlink" title="代码实例"></a>代码实例</h3><pre><code class="hljs python"><span class="hljs-comment">#!/usr/bin/env python</span><span class="hljs-comment"># -*- coding: utf-8 -*-</span><span class="hljs-comment"># @Time    : 2019/1/18 22:04</span><span class="hljs-comment"># @Author  : Seven</span><span class="hljs-comment"># @File    : surfDemo.py</span><span class="hljs-comment"># @Software: PyCharm</span><span class="hljs-keyword">import</span> cv2<span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> npimg1 = cv2.imread(<span class="hljs-string">'csdn.png'</span>)img2 = np.rot90(img1)<span class="hljs-comment"># 特征点检测和提取特征点描述子</span>surf = cv2.xfeatures2d.SURF_create(<span class="hljs-number">50</span>)kp1, des1 = surf.detectAndCompute(img1, <span class="hljs-literal">None</span>)kp2, des2 = surf.detectAndCompute(img2, <span class="hljs-literal">None</span>)<span class="hljs-comment"># 创建 BFMatcher 对象</span>bf = cv2.BFMatcher(cv2.NORM_L2)<span class="hljs-comment"># 根据描述子匹配特征点.</span>matches = bf.match(des1, des2)<span class="hljs-comment"># 画出50个匹配点</span>img3 = cv2.drawMatches(img1, kp1, img2, kp2, matches[:<span class="hljs-number">50</span>], <span class="hljs-literal">None</span>, flags=<span class="hljs-number">2</span>)cv2.imshow(<span class="hljs-string">"img"</span>, img3)cv2.waitKey(<span class="hljs-number">0</span>)</code></pre><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20190322220230.png" srcset="/img/loading.gif" alt=""></p>]]></content>
    
    
    <categories>
      
      <category>机器视觉</category>
      
    </categories>
    
    
    <tags>
      
      <tag>SIFT</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>【十】Harris角点检测</title>
    <link href="/2019/03/cv-image_harris/"/>
    <url>/2019/03/cv-image_harris/</url>
    
    <content type="html"><![CDATA[<h3 id="Harris角点检测"><a href="#Harris角点检测" class="headerlink" title="Harris角点检测"></a>Harris角点检测</h3><h4 id="【原理】"><a href="#【原理】" class="headerlink" title="【原理】"></a>【原理】</h4><blockquote><p>人眼对角点的识别通常是在一个局部的小区域或小窗口完成的。如果在各个方向上移动这个特征的小窗口，窗口内区域的灰度发生了较大的变化，那么就认为在窗口内遇到了角点。如果这个特定的窗口在图像各个方向上移动时，窗口内图像的灰度没有发生变化，那么窗口内就不存在角点；如果窗口在某一个方向移动时，窗口内图像的灰度发生了较大的变化，而在另一些方向上没有发生变化，那么，窗口内的图像可能就是一条直线的线段。</p></blockquote><blockquote><ul><li>在灰度变化平缓区域，窗口内像素灰度积分近似保持不变</li><li>在 边缘区域，边缘方向：灰度积分近似不变， 其余任意方向：剧烈变化。</li><li>在角点出：任意方向剧烈变化。</li></ul></blockquote><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20190118163820.png" srcset="/img/loading.gif" alt="原理"></p><h4 id="【数学推导】"><a href="#【数学推导】" class="headerlink" title="【数学推导】"></a>【<strong>数学推导</strong>】</h4><ul><li><p>定义灰度积分变化</p><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20190118183652.png" srcset="/img/loading.gif" alt="灰度积分"></p></li><li><p>定义灰度积分变化</p><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20190118183813.png" srcset="/img/loading.gif" alt="2"></p></li><li><p>如果$u、v$很小， 则有：</p><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20190118183858.png" srcset="/img/loading.gif" alt="2"></p></li><li><p>其中</p><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20190118183922.png" srcset="/img/loading.gif" alt="2"></p></li><li><p>注意$E$是一个二次型，即：</p><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20190118184508.png" srcset="/img/loading.gif" alt="3"></p></li><li><p>$\lambda_1^{-\frac12}、\lambda_2^{-\frac12}$是椭圆的长短轴</p><ul><li>当$\lambda_1、\lambda_2$都比较小时，点(x, y)处于灰度变化平缓区域.</li><li>当$\lambda_1&gt;\lambda_2$或者$\lambda_1&lt;\lambda_2$时，点(x, y)为边界元素.</li><li>当$\lambda_1、\lambda_2$都比较大时，且近似相等，点(x, y)为角点.</li></ul><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20190118184902.png" srcset="/img/loading.gif" alt="3"></p></li><li><p>角点响应函数</p><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20190118185002.png" srcset="/img/loading.gif" alt="4"></p></li><li><p>当R接近于零时，处于灰度变化平缓区域</p></li><li><p>当R&lt;0时，点为边界像素</p></li><li><p>当R&gt;0时，点为角点</p></li></ul><h3 id="代码实例"><a href="#代码实例" class="headerlink" title="代码实例"></a>代码实例</h3><pre><code class="hljs python"><span class="hljs-comment">#!/usr/bin/env python</span><span class="hljs-comment"># -*- coding: utf-8 -*-</span><span class="hljs-comment"># @Time    : 2019/1/18 18:51</span><span class="hljs-comment"># @Author  : Seven</span><span class="hljs-comment"># @File    : HarrisDemo.py</span><span class="hljs-comment"># @Software: PyCharm</span><span class="hljs-keyword">import</span> cv2<span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> npimg = cv2.imread(<span class="hljs-string">'chess.png'</span>)img = cv2.pyrDown(img)cv2.imshow(<span class="hljs-string">'source'</span>, img)<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">harrisDemo</span><span class="hljs-params">(img)</span>:</span>    <span class="hljs-string">"""</span><span class="hljs-string">    cornerHarris(src, blockSize, ksize, k, dst=None, borderType=None)</span><span class="hljs-string">    src，输入图像，即源图像，填Mat类的对象即可，且需为单通道8位或者浮点型图像。</span><span class="hljs-string">    dst，函数调用后的运算结果存在这里，即这个参数用于存放Harris角点检测的输出结果，和源图片有一样的尺寸和类型。</span><span class="hljs-string">    blockSize，表示邻域的大小，更多的详细信息在cornerEigenValsAndVecs中有讲到。</span><span class="hljs-string">    ksize，表示Sobel()算子的孔径大小。</span><span class="hljs-string">    k，Harris 角点检测方程中的自由参数，取值参数为[0,04，0.06]</span><span class="hljs-string">    borderType，图像像素的边界模式，注意它有默认值BORDER_DEFAULT。更详细的解释，参考borderInterpolate函数。</span><span class="hljs-string">    :param img:</span><span class="hljs-string">    :return:</span><span class="hljs-string">    """</span>    grayImage = np.float32(cv2.cvtColor(img, cv2.COLOR_BGR2GRAY))    <span class="hljs-comment"># 输入图像必须是float32，最后一个参数在0.04 到0.05 之间</span>    <span class="hljs-comment"># harris角点检测</span>    harrisImg = cv2.cornerHarris(grayImage, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>, <span class="hljs-number">0.04</span>)    <span class="hljs-comment"># 膨胀</span>    dst = cv2.dilate(harrisImg, <span class="hljs-literal">None</span>)    <span class="hljs-comment"># 0.01是人为设定的阈值</span>    <span class="hljs-comment"># 把角点设为红色</span>    img[dst &gt; <span class="hljs-number">0.01</span> * dst.max()] = [<span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">255</span>]    cv2.imshow(<span class="hljs-string">'harris'</span>, img)<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">harrisSubPix</span><span class="hljs-params">(img)</span>:</span>    <span class="hljs-string">"""</span><span class="hljs-string">    cornerSubPix(image, corners, winSize, zeroZone, criteria)</span><span class="hljs-string">    image：输入图像</span><span class="hljs-string"></span><span class="hljs-string">    corners：输入角点的初始坐标以及精准化后的坐标用于输出。</span><span class="hljs-string"></span><span class="hljs-string">    winSize：搜索窗口边长的一半，例如如果winSize=Size(5,5)，则一个大小为(5*2+1)*(5*2+1)=11*11的搜索窗口将被使用。</span><span class="hljs-string"></span><span class="hljs-string">    zeroZone：搜索区域中间的dead region边长的一半，有时用于避免自相关矩阵的奇异性。如果值设为(-1,-1)则表示没有这个区域。</span><span class="hljs-string">    criteria：角点精准化迭代过程的终止条件。也就是当迭代次数超过criteria.maxCount，或者角点位置变化小于criteria.epsilon时，停止迭代过程。</span><span class="hljs-string"></span><span class="hljs-string">    :param img:</span><span class="hljs-string">    :return:</span><span class="hljs-string">    """</span>    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)    <span class="hljs-comment"># harris角点检测</span>    gray = np.float32(gray)    dst = cv2.cornerHarris(gray, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>, <span class="hljs-number">0.04</span>)    dst = cv2.dilate(dst, <span class="hljs-literal">None</span>)    ret, dst = cv2.threshold(dst, <span class="hljs-number">0.01</span> * dst.max(), <span class="hljs-number">255</span>, <span class="hljs-number">0</span>)    dst = np.uint8(dst)    <span class="hljs-comment"># 获取质心坐标-centroids 是每个域的质心坐标</span>    ret, labels, stats, centroids = cv2.connectedComponentsWithStats(dst)    <span class="hljs-comment"># 设置停止和转换条件</span>    criteria = (cv2.TERM_CRITERIA_EPS + cv2.TERM_CRITERIA_MAX_ITER, <span class="hljs-number">100</span>, <span class="hljs-number">0.001</span>)    corners = cv2.cornerSubPix(gray, np.float32(centroids), (<span class="hljs-number">5</span>, <span class="hljs-number">5</span>), (<span class="hljs-number">-1</span>, <span class="hljs-number">-1</span>), criteria)    <span class="hljs-comment"># Now draw them</span>    res = np.hstack((centroids, corners))    res = np.int0(res)    img[res[:, <span class="hljs-number">1</span>], res[:, <span class="hljs-number">0</span>]] = [<span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">255</span>]    img[res[:, <span class="hljs-number">3</span>], res[:, <span class="hljs-number">2</span>]] = [<span class="hljs-number">0</span>, <span class="hljs-number">255</span>, <span class="hljs-number">0</span>]    cv2.imshow(<span class="hljs-string">'SubPix'</span>, img)<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">harrisImage</span><span class="hljs-params">(img)</span>:</span>    grayImage = np.float32(cv2.cvtColor(img, cv2.COLOR_BGR2GRAY))    <span class="hljs-comment"># 输入图像必须是float32，最后一个参数在0.04 到0.05 之间</span>    <span class="hljs-comment"># harris角点检测</span>    harrisImg = cv2.cornerHarris(grayImage, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>, <span class="hljs-number">0.04</span>)    <span class="hljs-comment"># 膨胀</span>    dst = cv2.dilate(harrisImg, <span class="hljs-literal">None</span>)    <span class="hljs-comment"># 对角点图像进行阈值化</span>    ret, dst = cv2.threshold(dst, <span class="hljs-number">0.01</span> * dst.max(), <span class="hljs-number">255</span>, <span class="hljs-number">0</span>)    <span class="hljs-comment"># 转换为8位图像</span>    dst = np.uint8(dst)    <span class="hljs-comment"># 获取质心坐标-centroids 是每个域的质心坐标</span>    ret, labels, stats, centroids = cv2.connectedComponentsWithStats(dst)    criteria = (cv2.TERM_CRITERIA_EPS + cv2.TERM_CRITERIA_MAX_ITER, <span class="hljs-number">100</span>, <span class="hljs-number">0.001</span>)    <span class="hljs-comment">#  计算得出角点坐标</span>    corners = cv2.cornerSubPix(grayImage, np.float32(centroids), (<span class="hljs-number">5</span>, <span class="hljs-number">5</span>), (<span class="hljs-number">-1</span>, <span class="hljs-number">-1</span>), criteria)    <span class="hljs-comment"># 对所有角点用0圈出来</span>    <span class="hljs-keyword">for</span> point <span class="hljs-keyword">in</span> corners:        cv2.circle(img, tuple(point), <span class="hljs-number">5</span>, (<span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">255</span>), <span class="hljs-number">2</span>)    cv2.imshow(<span class="hljs-string">'harris-2'</span>, img)harrisDemo(img)harrisSubPix(img)harrisImage(img)cv2.waitKey(<span class="hljs-number">0</span>)</code></pre><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20190322214936.png" srcset="/img/loading.gif" alt=""></p>]]></content>
    
    
    <categories>
      
      <category>机器视觉</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Harris</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>【九】直线检测</title>
    <link href="/2019/03/cv-image_hough/"/>
    <url>/2019/03/cv-image_hough/</url>
    
    <content type="html"><![CDATA[<h3 id="Hough变换"><a href="#Hough变换" class="headerlink" title="Hough变换"></a>Hough变换</h3><h4 id="【简介】"><a href="#【简介】" class="headerlink" title="【简介】"></a>【简介】</h4><blockquote><p>Hough变换是图像处理中从图像中识别几何形状的基本方法之一。Hough变换的基本原理在于利用点与线的对偶性，将原始图像空间的给定的曲线通过曲线表达形式变为参数空间的一个点。这样就把原始图像中给定曲线的检测问题转化为寻找参数空间中的峰值问题。也即把检测整体特性转化为检测局部特性。比如直线、椭圆、圆、弧线等。</p></blockquote><h4 id="【原理】"><a href="#【原理】" class="headerlink" title="【原理】"></a>【<strong>原理</strong>】</h4><blockquote><ul><li>采用参数空间变换的方法，对噪声和不间断直线的检测具有鲁棒性。</li><li>可用于检测圆和其他参数形状。</li><li>核心思想：直线$y = kx +b $, 每一条直线对应一个k,b，极坐标下对应一个点($\rho, \theta$).</li></ul></blockquote><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20190118142323.png" srcset="/img/loading.gif" alt="hough"></p><h4 id="【直线】"><a href="#【直线】" class="headerlink" title="【直线】"></a>【<strong>直线</strong>】</h4><blockquote><ul><li>直角坐标系的一点$(x, y)$，对应极坐标系下的一条正弦曲线$\rho= x cos\theta+y\sin\theta$</li><li>同一条直线上的多个点，在极坐标下必相交于一点。</li></ul></blockquote><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20190118143525.png" srcset="/img/loading.gif" alt="直线"></p><h4 id="【步骤】"><a href="#【步骤】" class="headerlink" title="【步骤】"></a>【<strong>步骤</strong>】</h4><blockquote><ul><li>将($\rho, \theta$)空间量化为许多小格。</li><li>根据$x-y$平面每一个直线点代入$\theta$的量化值，算出各个$\rho$，将对应格计数累加。</li><li>当全部点变换后，对小格进行检验。设置累计阈值T，计数器大于T的小格对应于共线点，其可以用作直线拟合参数。小于T的反映非共线点，丢弃不用。</li></ul><p>注意：$\theta$的取值为$(0,2\pi)$.</p></blockquote><h4 id="【总结】"><a href="#【总结】" class="headerlink" title="【总结】"></a>【<strong>总结</strong>】</h4><blockquote><ol><li>Hough变换的核心思想基于参数空间下的直线模型及投票原理。</li><li>Hough变换通过特征值判断灰度是否发生突变。</li><li>Hough变换想法直接，易于实现，是其它角点提取算法的基础。</li></ol></blockquote><h3 id="代码实例"><a href="#代码实例" class="headerlink" title="代码实例"></a>代码实例</h3><pre><code class="hljs python"><span class="hljs-comment">#!/usr/bin/env python</span><span class="hljs-comment"># -*- coding: utf-8 -*-</span><span class="hljs-comment"># @Time    : 2019/1/18 14:39</span><span class="hljs-comment"># @Author  : Seven</span><span class="hljs-comment"># @File    : HoughDemo.py</span><span class="hljs-comment"># @Software: PyCharm</span><span class="hljs-keyword">import</span> cv2<span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> npimg = cv2.imread(<span class="hljs-string">'sudoku.png'</span>)edgeImage = cv2.Canny(img, <span class="hljs-number">50</span>, <span class="hljs-number">200</span>, apertureSize=<span class="hljs-number">3</span>)grayImage = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">basicHough</span><span class="hljs-params">(edgeImage)</span>:</span>    <span class="hljs-string">"""</span><span class="hljs-string">    HoughLines(image, rho, theta, threshold, lines=None, srn=None, stn=None, min_theta=None, max_theta=None)</span><span class="hljs-string">    image:边缘检测的输出图像. 它应该是个灰度图 (但事实上是个二值化图)</span><span class="hljs-string"></span><span class="hljs-string">    lines:储存着检测到的直线的参数对  的容器 </span><span class="hljs-string"></span><span class="hljs-string">    rho:参数极径  以像素值为单位的分辨率. 我们使用 1 像素.</span><span class="hljs-string"></span><span class="hljs-string">    theta:参数极角  以弧度为单位的分辨率. 我们使用 1度 (即CV_PI/180)</span><span class="hljs-string"></span><span class="hljs-string">    theta:要”检测” 一条直线所需最少的的曲线交点</span><span class="hljs-string"></span><span class="hljs-string">    srn and stn: 参数默认为0.</span><span class="hljs-string"></span><span class="hljs-string">    :param img:</span><span class="hljs-string">    :return:</span><span class="hljs-string">    """</span>    cv2.imshow(<span class="hljs-string">'edge'</span>, edgeImage)    lines = cv2.HoughLines(edgeImage, <span class="hljs-number">1</span>, np.pi / <span class="hljs-number">180</span>, <span class="hljs-number">150</span>)    <span class="hljs-keyword">for</span> rho, theta <span class="hljs-keyword">in</span> lines[:, <span class="hljs-number">0</span>, :]:        a = np.cos(theta)        b = np.sin(theta)        x0 = a * rho        y0 = b * rho        x1 = int(x0 + <span class="hljs-number">1000</span> * (-b))        y1 = int(y0 + <span class="hljs-number">1000</span> * a)        x2 = int(x0 - <span class="hljs-number">1000</span> * (-b))        y2 = int(y0 - <span class="hljs-number">1000</span> * a)        cv2.line(img, (x1, y1), (x2, y2), (<span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">255</span>), <span class="hljs-number">2</span>)    cv2.imshow(<span class="hljs-string">"hough"</span>, img)<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">uphough</span><span class="hljs-params">(edgeImage)</span>:</span>    <span class="hljs-string">"""</span><span class="hljs-string">    HoughLinesP(image, rho, theta, threshold, lines=None, minLineLength=None, maxLineGap=None)</span><span class="hljs-string">    image： 必须是二值图像，推荐使用canny边缘检测的结果图像； </span><span class="hljs-string">    rho: 线段以像素为单位的距离精度，double类型的，推荐用1.0 </span><span class="hljs-string">    theta： 线段以弧度为单位的角度精度，推荐用numpy.pi/180 </span><span class="hljs-string">    threshod: 累加平面的阈值参数，int类型，超过设定阈值才被检测出线段，值越大，基本上意味着检出的线段越长，检出的线段个数越少。根据情况推荐先用100试试</span><span class="hljs-string">    lines：这个参数的意义未知，发现不同的lines对结果没影响，但是不要忽略了它的存在 </span><span class="hljs-string">    minLineLength：线段以像素为单位的最小长度，根据应用场景设置 </span><span class="hljs-string">    maxLineGap：同一方向上两条线段判定为一条线段的最大允许间隔（断裂），超过了设定值，则把两条线段当成一条线段，值越大，允许线段上的断裂越大，越有可能检出潜在的直线段</span><span class="hljs-string">    :param img:</span><span class="hljs-string">    :return:</span><span class="hljs-string">    """</span>    cv2.imshow(<span class="hljs-string">'ep'</span>, edgeImage)    linesP = cv2.HoughLinesP(edgeImage, <span class="hljs-number">1</span>, np.pi/<span class="hljs-number">180</span>, threshold=<span class="hljs-number">100</span>, maxLineGap=<span class="hljs-number">100</span>, minLineLength=<span class="hljs-number">10</span>)    <span class="hljs-keyword">for</span> x1, x2, y1, y2 <span class="hljs-keyword">in</span> linesP[:, <span class="hljs-number">0</span>, :]:        cv2.line(img, (x1, x2), (y1, y2), (<span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">255</span>), <span class="hljs-number">2</span>, cv2.LINE_AA)    cv2.imshow(<span class="hljs-string">'houghP'</span>, img)basicHough(edgeImage)uphough(edgeImage)cv2.waitKey(<span class="hljs-number">0</span>)</code></pre><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20190322214512.png" srcset="/img/loading.gif" alt=""></p>]]></content>
    
    
    <categories>
      
      <category>机器视觉</category>
      
    </categories>
    
    
    <tags>
      
      <tag>直线检测</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>【八】图像分割</title>
    <link href="/2019/03/cv-image_split/"/>
    <url>/2019/03/cv-image_split/</url>
    
    <content type="html"><![CDATA[<h3 id="灰度阈值分割"><a href="#灰度阈值分割" class="headerlink" title="灰度阈值分割"></a><strong>灰度阈值分割</strong></h3><blockquote><ul><li>假设：图像中的目标区和背景区之间或者不同目标区之间，存在不同的灰度或平均灰度。</li><li>凡是灰度值包含于z的像素都变成某一灰度值，其他的变成另一个灰度值，则该图像就以z为界被分<br>成两个区域。</li></ul><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20190115154608.png" srcset="/img/loading.gif" alt="灰度分割公式"></p><ul><li>如果=1和=0，分割后的图像为二值图像</li></ul><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20190115154652.png" srcset="/img/loading.gif" alt="二值"></p><ul><li>确定最佳阈值，使背景和目标之间的差异最大。</li></ul></blockquote><h4 id="大津（Otsu）算法"><a href="#大津（Otsu）算法" class="headerlink" title="大津（Otsu）算法"></a>大津（Otsu）算法</h4><ul><li><p>确定最佳阈值，使背景和目标之间的类间方差最大 (因为二者差异最大)。</p><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20190115154854.png" srcset="/img/loading.gif" alt="otsu"></p><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/1547539080340.png" srcset="/img/loading.gif" alt="图示"></p></li><li><p>算法实现：遍历灰度取值。</p></li></ul><h3 id="区域生长法分割"><a href="#区域生长法分割" class="headerlink" title="区域生长法分割"></a><strong>区域生长法分割</strong></h3><p>实现步骤：</p><blockquote><ol><li>对图像顺序扫描!找到第1个还没有归属的像素, 设该像素为(x0, y0);</li><li>以(x0, y0)为中心, 考虑(x0, y0)的8邻域像素(x, y)，如果(x, y)满足生长准则, 将(x, y)与 (x0, y0)合并, 同时将(x, y)压入堆栈;</li><li>从堆栈中取出一个像素, 把它当作(x0, y0)返回到步骤2;</li><li>当堆栈为空时，返回到步骤1;</li><li>重复步骤1 - 4直到图像中的每个点都有归属时。生长结束。</li></ol></blockquote><h3 id="【总结】"><a href="#【总结】" class="headerlink" title="【总结】"></a>【总结】</h3><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20190116163011.png" srcset="/img/loading.gif" alt="图像分割"></p><h3 id="【基于阈值的分割】"><a href="#【基于阈值的分割】" class="headerlink" title="【基于阈值的分割】"></a>【基于阈值的分割】</h3><blockquote><p>基于阈值的分割算法是最简单直接的分割算法。由于图像中目标位置和其他区域之间具有不同的灰度值，具有这种性质的目标区域通过阈值分割能够取得非常好的效果。通过阈值进行分割通常可以需要一个或多个灰度值作为阈值使图像分成不同的目标区域与背景区域。</p></blockquote><blockquote><p>如何找到合适的阈值进行分割是基于阈值的分割算法中最核心的问题。学者们针对这一问题进行了深入的研究。大津法（OTSU）、最大熵法等算法是其中比较突出的算法，这类算法在图像中适用固定的阈值。但是也有一类算法使用局部阈值，这类算法称为自适应阈值算法。这类算法根据图像的局部特征计算局部的阈值，通常情况下这类算法在有阴影或者图像灰度不均匀的情况下，具有比全局阈值更加好的分割效果。由于基于阈值的分割算法对噪声敏感，通常情况图像在分割之前需要进行图像去噪的操作。       </p></blockquote><blockquote><p>OTSU算法通过计算前景和背景的两个类的最大类间方差得到阈值，阈值将整张图像分为前景和背景两个部分。</p></blockquote><h3 id="【基于区域的分割】"><a href="#【基于区域的分割】" class="headerlink" title="【基于区域的分割】"></a>【基于区域的分割】</h3><blockquote><p>​    基于区域生长的分割算法将具有相似特征的像素集合聚集构成一个区域，这个区域中的相邻像素之间具有相似的性质。算法首先在每个区域中寻找一个像素点作为种子点，然后人工设定合适的生长规则与停止规则，这些规则可以是灰度级别的特征、纹理级别的特征、梯度级别的特征等，生长规则可以根据实际需要具体设置。满足生长规则的像素点视为具有相似特征，将这些像素点划分到种子点所在区域中。将新的像素点作为种子点重复上面的步骤，直到所有的种子点都执行的一遍，生成的区域就是该种子点所在的区域。</p><p>​    区域生长法的优势是整个算法计算简单，对于区域内部较为平滑的连通目标能分割得到很好的结果，同时算法对噪声不那么敏感。而它的缺点也非常明显，需要人为选定合适的区域生长种子点，而且生长规则选取的不合适可能导致区域内有空洞，甚至在复杂的图像中使用区域生长算法可以导致欠分割或者过分割。最后，作为一种串行的算法，当目标区域较大时，目标分割的速度较慢。</p></blockquote><h3 id="【基于边缘的分割】"><a href="#【基于边缘的分割】" class="headerlink" title="【基于边缘的分割】"></a>【基于边缘的分割】</h3><blockquote><p>​    通过区域的边缘来实现图像的分割是图像分割中常见的一种算法。由于不同区域中通常具有结构突变或者不连续的地方，这些地方往往能够为图像分割提供了有效的依据。这些不连续或者结构突变的地方称为边缘。图像中不同区域通常具有明显的边缘，利用边缘信息能够很好的实现对不同区域的分割。</p><p>​        基于边缘的图像分割算法最重要的是边缘的检测。图像的边缘通常是图像颜色、灰度性质不连续的位置。对于图像边缘的检测，通常使用边缘检测算子计算得出。常用的图像边缘检测算子有：Laplace算子、Sobel算子、Canny算子等。</p></blockquote><h3 id="代码演示"><a href="#代码演示" class="headerlink" title="代码演示"></a>代码演示</h3><pre><code class="hljs python"><span class="hljs-keyword">import</span> cv2<span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt<span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<span class="hljs-keyword">import</span> random<span class="hljs-comment"># 加载图片</span>histImage = cv2.imread(<span class="hljs-string">'lena.jpg'</span>)image_1 = cv2.imread(<span class="hljs-string">'pic2.png'</span>)image_2 = cv2.imread(<span class="hljs-string">'pic6.png'</span>)cv2.imshow(<span class="hljs-string">"source-1"</span>, image_1)cv2.imshow(<span class="hljs-string">"source-2"</span>, image_2)<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">GrayHist</span><span class="hljs-params">(img)</span>:</span>    <span class="hljs-string">"""</span><span class="hljs-string">    calcHist(images, channels, mask, histSize, ranges, hist=None, accumulate=None)</span><span class="hljs-string">    images — 源矩阵指针（输入图像，可以多张）。 但都应当有同样的深度（depth）, 比如CV_8U 或者 CV_32F ，</span><span class="hljs-string">             和同样的大小。每张图片可以拥有任意数量的通道。</span><span class="hljs-string">    channels — 通道的数量，每个通道都有编号，灰度图为一通道，即channel[1]=0;对应着一维。</span><span class="hljs-string">                BGR三通道图像编号为channels[3]=&#123;0,1,2&#125;;分别对应着第一维，第二维，第三维。</span><span class="hljs-string">    mask — 掩码图像：要对图像处理时，先看看这个像素对应的掩码位是否为屏蔽，如果为屏蔽，就是说该像素不处理（掩码值为0的像素都将被忽略）</span><span class="hljs-string">    hist — 返回的直方图。</span><span class="hljs-string">    histSize — 直方图每一维的条目个数的数组，灰度图一维有256个条目。</span><span class="hljs-string">    ranges — 每一维的像素值的范围，传递数组，与维数相对应，灰度图一维像素值范围为0~255。</span><span class="hljs-string">    :param name: 命名标志</span><span class="hljs-string">    :param img: 图片数据</span><span class="hljs-string">    :return:</span><span class="hljs-string">    """</span>    grayHist = cv2.calcHist([img],                            [<span class="hljs-number">0</span>],                            <span class="hljs-literal">None</span>,                            [<span class="hljs-number">256</span>],                            [<span class="hljs-number">0.0</span>, <span class="hljs-number">255.0</span>])    print(grayHist)    plt.plot(grayHist)    plt.show()<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">ThresholdImage</span><span class="hljs-params">(image, name)</span>:</span>    <span class="hljs-string">"""</span><span class="hljs-string">    threshold(src, thresh, maxval, type, dst=None)</span><span class="hljs-string">    src： 输入图，只能输入单通道图像，通常来说为灰度图</span><span class="hljs-string">    dst： 输出图</span><span class="hljs-string">    thresh： 阈值</span><span class="hljs-string">    maxval： 当像素值超过了阈值（或者小于阈值，根据type来决定），所赋予的值</span><span class="hljs-string">    type：二值化操作的类型，包含以下5种类型： cv2.THRESH_BINARY； cv2.THRESH_BINARY_INV；</span><span class="hljs-string">                                        cv2.THRESH_TRUNC； cv2.THRESH_TOZERO；cv2.THRESH_TOZERO_INV</span><span class="hljs-string">    :param img:</span><span class="hljs-string">    :return:</span><span class="hljs-string">    """</span>    img = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)    listType = [cv2.THRESH_BINARY, cv2.THRESH_BINARY_INV, cv2.THRESH_TRUNC, cv2.THRESH_TOZERO, cv2.THRESH_TOZERO_INV]    <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> listType:        thresholdImage = cv2.threshold(img, <span class="hljs-number">125</span>, <span class="hljs-number">255</span>, i)        cv2.imshow(<span class="hljs-string">"threshold-%s-%s"</span> % (name, i), thresholdImage[<span class="hljs-number">1</span>])<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">floodFillImage</span><span class="hljs-params">(img, name, lo, up)</span>:</span>    <span class="hljs-string">"""</span><span class="hljs-string">    floodFill(image, mask, seedPoint, newVal, loDiff=None, upDiff=None, flags=None)</span><span class="hljs-string">    image 【输入/输出】 1或者3通道、 8bit或者浮点图像。仅当参数flags的FLOODFILL_MASK_ONLY标志位被设置时image不会被修改，否则会被修改。</span><span class="hljs-string"></span><span class="hljs-string">    mask 【输入/输出】 操作掩码，必须为单通道、8bit，且比image宽2个像素、高2个像素。使用前必须先初始化。</span><span class="hljs-string">                        Flood-filling无法跨越mask中的非0像素。例如，一个边缘检测的结果可以作为mask来阻止边缘填充。</span><span class="hljs-string">                        在输出中，mask中与image中填充像素对应的像素点被设置为1，或者flags标志位中设置的值(详见flags标志位的解释)。</span><span class="hljs-string">                        此外，该函数还用1填充了mask的边缘来简化内部处理。因此，可以在多个调用中使用同一mask，以确保填充区域不会重叠。</span><span class="hljs-string">    seedPoint 起始像素点</span><span class="hljs-string">    newVal    重绘像素区域的新的填充值(颜色)</span><span class="hljs-string">    loDiff    当前选定像素与其连通区中相邻像素中的一个像素，或者与加入该连通区的一个seedPoint像素，二者之间的最大下行差异值。</span><span class="hljs-string">    upDiff    当前选定像素与其连通区中相邻像素中的一个像素，或者与加入该连通区的一个seedPoint像素，二者之间的最大上行差异值。</span><span class="hljs-string">    flags     flags标志位是一个32bit的int类型数据，其由3部分组成： 0-7bit表示邻接性(4邻接、8邻接)；</span><span class="hljs-string">                    8-15bit表示mask的填充颜色；16-31bit表示填充模式（详见填充模式解释）</span><span class="hljs-string">    :param img:</span><span class="hljs-string">    :param name:</span><span class="hljs-string">    :return:</span><span class="hljs-string">    """</span>    seed = (<span class="hljs-number">10</span>, <span class="hljs-number">30</span>)  <span class="hljs-comment"># 起始位置</span>    <span class="hljs-comment"># 构建mask,根据mask参数的介绍,其size必须为宽img+2,高img+2</span>    mask = np.zeros((img.shape[<span class="hljs-number">0</span>] + <span class="hljs-number">2</span>, img.shape[<span class="hljs-number">1</span>] + <span class="hljs-number">2</span>), dtype=np.uint8)    newVal = (<span class="hljs-number">255</span>, <span class="hljs-number">64</span>, <span class="hljs-number">64</span>)  <span class="hljs-comment"># img fill的填充颜色值</span>    <span class="hljs-comment"># 执行floodFill操作</span>    <span class="hljs-comment"># ret, image, mask, rect = cv2.floodFill(img, mask, seed, newVal)</span>    cv2.floodFill(img, mask, seed, newVal,                  (lo, )*<span class="hljs-number">3</span>, (up, )*<span class="hljs-number">3</span>,                  cv2.FLOODFILL_FIXED_RANGE)    cv2.imshow(<span class="hljs-string">'floodfill-%s'</span> % name, img)<span class="hljs-comment"># 计算并显示直方图</span>GrayHist(image_1)<span class="hljs-comment"># 大津算法</span>ThresholdImage(image_1, <span class="hljs-string">'1'</span>)ThresholdImage(image_2, <span class="hljs-string">'2'</span>)<span class="hljs-comment"># riceImage = cv2.imread('rice.png')</span><span class="hljs-comment"># ThresholdImage(riceImage, 'rice')</span>floodFillImage(image_1, <span class="hljs-string">'1'</span>, <span class="hljs-number">200</span>, <span class="hljs-number">255</span>)floodFillImage(image_2, <span class="hljs-string">'2'</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>)cv2.waitKey(<span class="hljs-number">0</span>)</code></pre><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20190116150300.png" srcset="/img/loading.gif" alt="diamante"></p>]]></content>
    
    
    <categories>
      
      <category>机器视觉</category>
      
    </categories>
    
    
    <tags>
      
      <tag>图像分割</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>【七】图像特征描述</title>
    <link href="/2019/03/cv-image_description/"/>
    <url>/2019/03/cv-image_description/</url>
    
    <content type="html"><![CDATA[<h3 id="简单描述"><a href="#简单描述" class="headerlink" title="简单描述"></a>简单描述</h3><h4 id="【简单描述符】"><a href="#【简单描述符】" class="headerlink" title="【简单描述符】"></a>【简单描述符】</h4><ul><li><p>区域面积：区域包含的像素数</p></li><li><p>区域重心：</p><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20190116161656.png" srcset="/img/loading.gif" alt="区域重心"></p></li></ul><blockquote><p>注意：区域重心可能不是整数。</p></blockquote><h4 id="【形状描述符】"><a href="#【形状描述符】" class="headerlink" title="【形状描述符】"></a>【形状描述符】</h4><ul><li><p>形状参数</p><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20190116161905.png" srcset="/img/loading.gif" alt="形状参数"></p><blockquote><p>注意：形状为圆形时：F=1；形状为其他时，F&gt;1</p></blockquote></li><li><p>偏心率：等效椭圆宽高比。</p></li><li><p>欧拉数：$E = C-H$</p></li><li><p>圆形性：</p><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20190116162134.png" srcset="/img/loading.gif" alt="圆形性"></p></li></ul><h3 id="一般化描述"><a href="#一般化描述" class="headerlink" title="一般化描述"></a>一般化描述</h3><ul><li><p>最小包围矩形（MER）</p><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20190116162239.png" srcset="/img/loading.gif" alt="mer"></p></li><li><p>方向和离心率</p><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20190116162315.png" srcset="/img/loading.gif" alt="方向和离心率"></p></li></ul><h3 id="不变矩"><a href="#不变矩" class="headerlink" title="不变矩"></a>不变矩</h3><ul><li><p>首先定义归一化的中心矩</p><blockquote><p>图像f(x,y)的p+q阶矩定义为：</p><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20190116162435.png" srcset="/img/loading.gif" alt="1"></p><p>f(x,y)的p+q阶中心矩定义为：</p><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20190116162502.png" srcset="/img/loading.gif" alt="2"></p><p>其中：</p><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20190116162528.png" srcset="/img/loading.gif" alt="3"></p><p>即前面定义的重心。</p><p>f(x,y)的归一化中心矩定义为：</p><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20190116162603.png" srcset="/img/loading.gif" alt="4"></p></blockquote></li><li><p>然后定义不变矩</p><blockquote><p>常用的有七个不变矩，即对平移、旋转和尺度变化保持不变。这些可由归一化的二阶和三阶中心矩得到：</p><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20190116162647.png" srcset="/img/loading.gif" alt="6"></p></blockquote></li><li><p>计算示例</p><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20190116162804.png" srcset="/img/loading.gif" alt="计算示例"></p></li></ul><h3 id="图像特征描述总结"><a href="#图像特征描述总结" class="headerlink" title="图像特征描述总结"></a>图像特征描述总结</h3><blockquote><ol><li>区域的基本特征包括面积、质心、圆形性等.</li><li>最小包围矩形和多边形拟合能有效的描述区域形状</li><li>七个不变矩特征可以有效的度量区域形状，对平移、旋转和比例放缩变换不敏感</li></ol></blockquote><h3 id="代码实例"><a href="#代码实例" class="headerlink" title="代码实例"></a>代码实例</h3><pre><code class="hljs python"><span class="hljs-comment">#!/usr/bin/env python</span><span class="hljs-comment"># -*- coding: utf-8 -*-</span><span class="hljs-comment"># @Time    : 2019/1/16 16:39</span><span class="hljs-comment"># @Author  : Seven</span><span class="hljs-comment"># @File    : featureDescription.py</span><span class="hljs-comment"># @Software: PyCharm</span><span class="hljs-keyword">import</span> cv2<span class="hljs-comment"># 加载图片</span>img = cv2.imread(<span class="hljs-string">'rice.png'</span>)cv2.imshow(<span class="hljs-string">'source'</span>, img)<span class="hljs-comment"># 转换为灰度图</span>grayImage = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)<span class="hljs-comment"># 形态学处理--五次开运算</span>kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (<span class="hljs-number">5</span>, <span class="hljs-number">5</span>))morphImage_open = cv2.morphologyEx(grayImage, cv2.MORPH_OPEN, kernel, iterations=<span class="hljs-number">5</span>)<span class="hljs-comment"># 原图减去背景图</span>riceImage = grayImage - morphImage_open<span class="hljs-comment"># 大津算法阈值化</span>thresholdImage = cv2.threshold(riceImage, <span class="hljs-number">0</span>, <span class="hljs-number">255</span>, cv2.THRESH_BINARY + cv2.THRESH_OTSU)<span class="hljs-comment"># 图像分割</span>partition = cv2.findContours(thresholdImage[<span class="hljs-number">1</span>], cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)count = <span class="hljs-number">0</span><span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> partition[<span class="hljs-number">1</span>]:    area = cv2.contourArea(i)    <span class="hljs-comment"># print("area:", area)</span>    <span class="hljs-keyword">if</span> area &lt;= <span class="hljs-number">10</span>:        <span class="hljs-keyword">continue</span>    <span class="hljs-keyword">else</span>:        count += <span class="hljs-number">1</span>        print(<span class="hljs-string">"blob:&#123;&#125;:&#123;&#125;"</span>.format(count, area))        <span class="hljs-comment"># 得到米粒的坐标--x、y、w、h</span>        rect = cv2.boundingRect(i)        <span class="hljs-comment"># 在img中画出最大面积米粒</span>        cv2.drawContours(img, [i], <span class="hljs-number">-1</span>, (<span class="hljs-number">255</span>, <span class="hljs-number">255</span>, <span class="hljs-number">0</span>), <span class="hljs-number">1</span>)        cv2.rectangle(img, (rect[<span class="hljs-number">0</span>], rect[<span class="hljs-number">1</span>]), (rect[<span class="hljs-number">0</span>]+rect[<span class="hljs-number">2</span>], rect[<span class="hljs-number">1</span>]+rect[<span class="hljs-number">3</span>]),  (<span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">255</span>), <span class="hljs-number">1</span>)        cv2.putText(img, str(count), (rect[<span class="hljs-number">0</span>], rect[<span class="hljs-number">1</span>]), cv2.FONT_HERSHEY_PLAIN, <span class="hljs-number">0.5</span>, (<span class="hljs-number">0</span>, <span class="hljs-number">255</span>, <span class="hljs-number">0</span>))print(<span class="hljs-string">"米粒数量：%s"</span> % count)cv2.imshow(<span class="hljs-string">"area"</span>, img)cv2.waitKey(<span class="hljs-number">0</span>)</code></pre><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20190322212743.png" srcset="/img/loading.gif" alt=""></p>]]></content>
    
    
    <categories>
      
      <category>机器视觉</category>
      
    </categories>
    
    
    <tags>
      
      <tag>图像特征描述</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>【六】直方图</title>
    <link href="/2019/03/cv-image_hist/"/>
    <url>/2019/03/cv-image_hist/</url>
    
    <content type="html"><![CDATA[<h3 id="图像直方图"><a href="#图像直方图" class="headerlink" title="图像直方图"></a><strong>图像直方图</strong></h3><blockquote><p>通过灰度直方图看到图像的照明效果</p></blockquote><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20190115154350.png" srcset="/img/loading.gif" alt="![](https://eveseven.oss-cn-shanghai.aliyuncs.com/20190115154324.png)"></p><h3 id="代码实例"><a href="#代码实例" class="headerlink" title="代码实例"></a>代码实例</h3><pre><code class="hljs python"><span class="hljs-comment">#!/usr/bin/env python</span><span class="hljs-comment"># -*- coding: utf-8 -*-</span><span class="hljs-comment"># @Time    : 2019/3/9 21:28</span><span class="hljs-comment"># @Author  : Seven</span><span class="hljs-comment"># @File    : HistDemo.py</span><span class="hljs-comment"># @Software: PyCharm</span><span class="hljs-comment"># function : 图像直方图</span><span class="hljs-keyword">import</span> cv2<span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt<span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">GrayHist</span><span class="hljs-params">(img)</span>:</span>    <span class="hljs-string">"""</span><span class="hljs-string">    calcHist(images, channels, mask, histSize, ranges, hist=None, accumulate=None)</span><span class="hljs-string">    images — 源矩阵指针（输入图像，可以多张）。 但都应当有同样的深度（depth）, 比如CV_8U 或者 CV_32F ，</span><span class="hljs-string">             和同样的大小。每张图片可以拥有任意数量的通道。</span><span class="hljs-string">    channels — 通道的数量，每个通道都有编号，灰度图为一通道，即channel[1]=0;对应着一维。</span><span class="hljs-string">                BGR三通道图像编号为channels[3]=&#123;0,1,2&#125;;分别对应着第一维，第二维，第三维。</span><span class="hljs-string">    mask — 掩码图像：要对图像处理时，先看看这个像素对应的掩码位是否为屏蔽，如果为屏蔽，就是说该像素不处理（掩码值为0的像素都将被忽略）</span><span class="hljs-string">    hist — 返回的直方图。</span><span class="hljs-string">    histSize — 直方图每一维的条目个数的数组，灰度图一维有256个条目。</span><span class="hljs-string">    ranges — 每一维的像素值的范围，传递数组，与维数相对应，灰度图一维像素值范围为0~255。</span><span class="hljs-string">    :param name: 命名标志</span><span class="hljs-string">    :param img: 图片数据</span><span class="hljs-string">    :return:</span><span class="hljs-string">    """</span>    img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)    hist = cv2.calcHist([img],                            [<span class="hljs-number">0</span>],                            <span class="hljs-literal">None</span>,                            [<span class="hljs-number">256</span>],                            [<span class="hljs-number">0.0</span>, <span class="hljs-number">255.0</span>])    plt.plot(hist)    plt.xlim([<span class="hljs-number">0</span>, <span class="hljs-number">256</span>])    plt.show()<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">colorHist</span><span class="hljs-params">(img)</span>:</span>    <span class="hljs-comment"># 颜色直方图</span>    channels = cv2.split(img)    colors = (<span class="hljs-string">'b'</span>, <span class="hljs-string">'g'</span>, <span class="hljs-string">'r'</span>)    <span class="hljs-keyword">for</span> (chan, color) <span class="hljs-keyword">in</span> zip(channels, colors):        hist = cv2.calcHist([chan], [<span class="hljs-number">0</span>], <span class="hljs-literal">None</span>, [<span class="hljs-number">256</span>], [<span class="hljs-number">0</span>, <span class="hljs-number">256</span>])        plt.plot(hist, color=color)        plt.xlim([<span class="hljs-number">0</span>, <span class="hljs-number">256</span>])    plt.show()<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">channelsHist</span><span class="hljs-params">(img)</span>:</span>    b, g, r = cv2.split(img)    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">calcAndDrawHist</span><span class="hljs-params">(image, color)</span>:</span>        hist = cv2.calcHist([image], [<span class="hljs-number">0</span>], <span class="hljs-literal">None</span>, [<span class="hljs-number">256</span>], [<span class="hljs-number">0.0</span>, <span class="hljs-number">255.0</span>])        minVal, maxVal, minLoc, maxLoc = cv2.minMaxLoc(hist)        histImg = np.zeros([<span class="hljs-number">256</span>, <span class="hljs-number">256</span>, <span class="hljs-number">3</span>], np.uint8)        hpt = int(<span class="hljs-number">0.9</span> * <span class="hljs-number">256</span>);        <span class="hljs-keyword">for</span> h <span class="hljs-keyword">in</span> range(<span class="hljs-number">256</span>):            intensity = int(hist[h] * hpt / maxVal)            cv2.line(histImg, (h, <span class="hljs-number">256</span>), (h, <span class="hljs-number">256</span> - intensity), color)        <span class="hljs-keyword">return</span> histImg;    histImgB = calcAndDrawHist(b, [<span class="hljs-number">255</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>])    histImgG = calcAndDrawHist(g, [<span class="hljs-number">0</span>, <span class="hljs-number">255</span>, <span class="hljs-number">0</span>])    histImgR = calcAndDrawHist(r, [<span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">255</span>])    plt.figure(figsize=(<span class="hljs-number">8</span>, <span class="hljs-number">6</span>))    plt.subplot(<span class="hljs-number">221</span>)    plt.imshow(img[:, :, ::<span class="hljs-number">-1</span>]);    plt.title(<span class="hljs-string">'origin'</span>)    plt.subplot(<span class="hljs-number">222</span>)    plt.imshow(histImgB[:, :, ::<span class="hljs-number">-1</span>]);    plt.title(<span class="hljs-string">'histImgB'</span>)    plt.subplot(<span class="hljs-number">223</span>)    plt.imshow(histImgG[:, :, ::<span class="hljs-number">-1</span>]);    plt.title(<span class="hljs-string">'histImgG'</span>)    plt.subplot(<span class="hljs-number">224</span>)    plt.imshow(histImgR[:, :, ::<span class="hljs-number">-1</span>]);    plt.title(<span class="hljs-string">'histImgR'</span>)    plt.tight_layout()    plt.show()<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">grayEqualize</span><span class="hljs-params">(img)</span>:</span>    img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)    equ = cv2.equalizeHist(img)    <span class="hljs-comment"># 两个图片的像素分布连接在一起，拍成一维数组</span>    res = np.hstack((img, equ))    plt.figure(figsize=(<span class="hljs-number">8</span>, <span class="hljs-number">6</span>))    ax1 = plt.subplot2grid((<span class="hljs-number">2</span>, <span class="hljs-number">2</span>), (<span class="hljs-number">0</span>, <span class="hljs-number">0</span>))    ax1.imshow(img, cmap=plt.cm.gray)    ax1.set_title(<span class="hljs-string">'orignal image'</span>)    ax1 = plt.subplot2grid((<span class="hljs-number">2</span>, <span class="hljs-number">2</span>), (<span class="hljs-number">0</span>, <span class="hljs-number">1</span>))    ax1.imshow(equ, cmap=plt.cm.gray)    ax1.set_title(<span class="hljs-string">'equalization'</span>)    ax1 = plt.subplot2grid((<span class="hljs-number">2</span>, <span class="hljs-number">2</span>), (<span class="hljs-number">1</span>, <span class="hljs-number">0</span>), colspan=<span class="hljs-number">3</span>, rowspan=<span class="hljs-number">1</span>)    ax1.imshow(res, cmap=plt.cm.gray)    ax1.set_title(<span class="hljs-string">'horizational'</span>)    plt.tight_layout()    plt.show()<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">colorEqualize</span><span class="hljs-params">(img)</span>:</span>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">hisEqulColor</span><span class="hljs-params">(img)</span>:</span>        ycrcb = cv2.cvtColor(img, cv2.COLOR_BGR2YCR_CB)        channels = cv2.split(ycrcb)        print(len(channels))        cv2.equalizeHist(channels[<span class="hljs-number">0</span>], channels[<span class="hljs-number">0</span>])        cv2.merge(channels, ycrcb)        cv2.cvtColor(ycrcb, cv2.COLOR_YCR_CB2BGR, img)        <span class="hljs-keyword">return</span> img    equ = hisEqulColor(img)    h_stack_img = np.hstack((img, equ))    plt.figure(figsize=(<span class="hljs-number">10</span>, <span class="hljs-number">8</span>))    ax1 = plt.subplot2grid((<span class="hljs-number">2</span>, <span class="hljs-number">2</span>), (<span class="hljs-number">0</span>, <span class="hljs-number">0</span>))    ax1.imshow(img[:, :, ::<span class="hljs-number">-1</span>])    ax1.set_title(<span class="hljs-string">'orignal image'</span>)    ax1 = plt.subplot2grid((<span class="hljs-number">2</span>, <span class="hljs-number">2</span>), (<span class="hljs-number">0</span>, <span class="hljs-number">1</span>))    ax1.imshow(equ[:, :, ::<span class="hljs-number">-1</span>])    ax1.set_title(<span class="hljs-string">'equalization'</span>)    ax1 = plt.subplot2grid((<span class="hljs-number">2</span>, <span class="hljs-number">2</span>), (<span class="hljs-number">1</span>, <span class="hljs-number">0</span>), colspan=<span class="hljs-number">3</span>, rowspan=<span class="hljs-number">1</span>)    ax1.imshow(h_stack_img[:, :, ::<span class="hljs-number">-1</span>])    ax1.set_title(<span class="hljs-string">'horizational'</span>)    plt.tight_layout()    plt.show()plt.figure()plt.title(<span class="hljs-string">"Grayscale Histogram"</span>)plt.xlabel(<span class="hljs-string">"Bins"</span>)plt.ylabel(<span class="hljs-string">"# of Pixels"</span>)<span class="hljs-comment"># 灰度直方图</span>img = cv2.imread(<span class="hljs-string">'lena.jpg'</span>)GrayHist(img)<span class="hljs-comment"># 颜色直方图</span>colorHist(img)<span class="hljs-comment"># 多通道直方图</span>channelsHist(img)<span class="hljs-comment"># 灰度直方图均衡</span>grayEqualize(img)<span class="hljs-comment"># 彩色图像直方图均衡</span>colorEqualize(img)</code></pre><h4 id="灰度直方图"><a href="#灰度直方图" class="headerlink" title="灰度直方图"></a>灰度直方图</h4><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20190322210855.png" srcset="/img/loading.gif" alt=""></p><h4 id="颜色直方图"><a href="#颜色直方图" class="headerlink" title="颜色直方图"></a>颜色直方图</h4><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20190322210923.png" srcset="/img/loading.gif" alt=""></p><h4 id="多通道直方图"><a href="#多通道直方图" class="headerlink" title="多通道直方图"></a>多通道直方图</h4><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20190322210948.png" srcset="/img/loading.gif" alt=""></p><h4 id="灰度直方图均衡"><a href="#灰度直方图均衡" class="headerlink" title="灰度直方图均衡"></a>灰度直方图均衡</h4><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20190322211034.png" srcset="/img/loading.gif" alt=""></p><h4 id="彩色图像直方图均衡"><a href="#彩色图像直方图均衡" class="headerlink" title="彩色图像直方图均衡"></a>彩色图像直方图均衡</h4><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20190322211109.png" srcset="/img/loading.gif" alt=""></p>]]></content>
    
    
    <categories>
      
      <category>机器视觉</category>
      
    </categories>
    
    
    <tags>
      
      <tag>直方图</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>【五】图像边缘检测</title>
    <link href="/2019/03/cv-image_edge/"/>
    <url>/2019/03/cv-image_edge/</url>
    
    <content type="html"><![CDATA[<h3 id="基本思想"><a href="#基本思想" class="headerlink" title="基本思想"></a>基本思想</h3><blockquote><p>边缘检测的基本思想是通过检测每个像素和其邻域的状态，以决定该像素是否位于一个物体的边界上。如果一个像素位于一个物体的边界上，则其邻域像素的灰度值的变化就比较大。假如可以应用某种算法检测出这种变化并进行量化表示，那么就可以确定物体的边界。</p><ul><li>边缘检测的实质是微分。</li><li>实际中常用差分， X方向、Y方向。</li></ul></blockquote><h3 id="基本算子"><a href="#基本算子" class="headerlink" title="基本算子"></a>基本算子</h3><h4 id="【Robert算子：一阶微分算子】"><a href="#【Robert算子：一阶微分算子】" class="headerlink" title="【Robert算子：一阶微分算子】"></a>【<strong>Robert算子：一阶微分算子</strong>】</h4><p>对于图像来说，是一个二维的离散型数集，通过推广二维连续型求函数偏导的方法，来求得图像的偏导数，即在(x,y)处的最大变化率，也就是这里的梯度：</p><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20190114161922.png" srcset="/img/loading.gif" alt="robert算子梯度公式"></p><p>梯度是一个矢量，则(x,y)处的梯度表示为：</p><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20190114162158.png" srcset="/img/loading.gif" alt="梯度公式"></p><p>其大小为：</p><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20190114162250.png" srcset="/img/loading.gif" alt="梯度公式"></p><p>因为平方和平方根需要大量的计算开销，所以使用绝对值来近似梯度幅值：</p><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20190114162454.png" srcset="/img/loading.gif" alt="梯度简化计算"></p><p>方向与α(x,y)正交：</p><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20190114162526.png" srcset="/img/loading.gif" alt="正交"></p><p>其对应的模板为：</p><blockquote><p>图像的垂直和水平梯度</p></blockquote><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20190114162619.png" srcset="/img/loading.gif" alt=""></p><p>我们有时候也需要对角线方向的梯度，定义如下：</p><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20190114162732.png" srcset="/img/loading.gif" alt="梯度"></p><p>对应模板为：</p><blockquote><p>Roberts 交叉梯度算子。</p></blockquote><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20190114162815.png" srcset="/img/loading.gif" alt="Robert算子"></p><p>模板推导</p><p><code>垂直或水平算子模板</code></p><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20190114165059.png" srcset="/img/loading.gif" alt="垂直或水平模板"></p><p><code>交叉算子模板</code></p><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20190114164252.png" srcset="/img/loading.gif" alt="算子模板推导"></p><blockquote><p>2*2 大小的模板在概念上很简单，但是他们对于用关于中心点对称的模板来计算边缘方向不是很有用，其最小模板大小为3*3。3*3 模板考虑了中心点对段数据的性质，并携带有关于边缘方向的更多信息。</p></blockquote><h4 id="【Prewitt算子：一阶微分算子】"><a href="#【Prewitt算子：一阶微分算子】" class="headerlink" title="【Prewitt算子：一阶微分算子】"></a>【<strong>Prewitt算子：一阶微分算子</strong>】</h4><blockquote><p>prewitt算子一般使用的是3*3的模板</p></blockquote><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20190114165808.png" srcset="/img/loading.gif" alt="prewitt"></p><p>我如下定义水平、垂直和两对角线方向的梯度:</p><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20190114165914.png" srcset="/img/loading.gif" alt="prewitt梯度"></p><p>Prewitt算子：</p><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20190114170149.png" srcset="/img/loading.gif" alt="Prewitt算子"></p><p>数学推导：</p><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20190114170225.png" srcset="/img/loading.gif" alt="数学推导prewitt"></p><blockquote><p>Prewitt 算子是由两部分组成，检测水平边缘的模板和检测垂直边缘的模板， Prewitt 算子一个方向求微分，一个方向求平均，所以对噪声相对不敏感。</p></blockquote><h4 id="【Sobel算子：一阶微分算子】"><a href="#【Sobel算子：一阶微分算子】" class="headerlink" title="【Sobel算子：一阶微分算子】"></a>【<strong>Sobel算子：一阶微分算子</strong>】</h4><blockquote><p>Sobel 算子是在Prewitt算子的基础上改进的， 在中心系数上使用一个权值2， 相比较Prewitt算子，Sobel模板能够较好的抑制(平滑)噪声。</p></blockquote><p>计算公式：</p><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20190114175731.png" srcset="/img/loading.gif" alt="sobel 算子"></p><p>sobel算子：</p><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20190114175830.png" srcset="/img/loading.gif" alt="sobel算子"></p><p>数学推导：</p><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20190114180216.png" srcset="/img/loading.gif" alt="sobel数学推"></p><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20190122203820.png" srcset="/img/loading.gif" alt=""></p><blockquote><ul><li>sobel算子也有两个，一个是检测水平边缘的模板基于sobel算子的边缘检测，另一个是检测垂直边缘的模板基于sobel算子的边缘检测。</li><li>与Prewitt算子相比，sobel算子对于像素位置的影响做了加权，因此效果更好。</li><li>sobel算子的另外一种形式是各向同性Sobel算子， 也有两个模板组成，一个是检测水平边缘的基于sobel算子的边缘检测，另一个是检测垂直边缘的基于sobel算子的边缘检测。</li><li>各向同性Sobel算子和普通sobel算子相比，位置加权系数更为准确，在检测不同方向的边缘是梯度的幅度是一致的。</li></ul></blockquote><h4 id="【Laplace算子：二阶微分算子】"><a href="#【Laplace算子：二阶微分算子】" class="headerlink" title="【Laplace算子：二阶微分算子】"></a>【<strong>Laplace算子：二阶微分算子</strong>】</h4><blockquote><p>Laplace 算子是一种各向同性算子，在只关心边缘的位置而不考虑其周围的像素灰度差值时比较合适。Laplace算子对孤立像素的响应要比对边缘或线的响应要更强烈， 因此只适合用于<code>无噪声图像</code>。存在噪声情况下， 使用Laplace算子检测边缘之前需要先进行<code>低通滤波</code>。</p></blockquote><p>一阶导数：</p><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20190114181441.png" srcset="/img/loading.gif" alt="一阶导数"></p><p>二阶导数：</p><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20190114181509.png" srcset="/img/loading.gif" alt="二阶导数"></p><p>我们需要注意的是关于点x的二阶导数，故将上式中的变量减去1后，得到：</p><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20190114181750.png" srcset="/img/loading.gif" alt="二阶导"></p><p>在图像处理中通过拉普拉斯模板求二阶导数， 其定义如下：</p><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20190114181852.png" srcset="/img/loading.gif" alt="laplace"></p><p>写成差分形式为</p><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20190114182154.png" srcset="/img/loading.gif" alt="差分"></p><p>laplace卷积核：</p><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20190114182254.png" srcset="/img/loading.gif" alt="卷积核"></p><p>在用lapacian 算子图像进行卷积运算时，当响应的绝对值超过指定阈值时，那么该点就是被检测出来的孤立点，具体输出如下：</p><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20190114182512.png" srcset="/img/loading.gif" alt="孤立点"></p><h4 id="【LoG算子：二阶微分算子】"><a href="#【LoG算子：二阶微分算子】" class="headerlink" title="【LoG算子：二阶微分算子】"></a>【<strong>LoG算子：二阶微分算子</strong>】</h4><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20190114182657.png" srcset="/img/loading.gif" alt="log算子"></p><blockquote><p>Log 边缘检测则是先进行高斯滤波再进行拉普拉斯算子检测, 然后找过零点来确定边缘位置，很多时候我们只是知道Log 5*5 模板如上图所示。</p></blockquote><p>二维高斯公式：</p><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20190114182751.png" srcset="/img/loading.gif" alt="二维高斯公式"></p><p>按拉普拉斯算子公式求X，Y方向的二阶偏导后：</p><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20190114182855.png" srcset="/img/loading.gif" alt="二阶偏导"></p><p>这里x，y 不能看成模板位置，应看成是模板其他位置到中心位置的距离。那么写成：</p><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20190114183026.png" srcset="/img/loading.gif" alt="1"></p><p>这里x0，y0 就是模板中心位置，x，y 是模板其他位置，对于5*5 模板， 则$x_0,=2 , y_0=2$，那对于模板中（0,0）位置的权值，即把x= 0，y= 0，x0= 2，y0 = 2 带入上式， 令σ= 1，得到约等于0.0175，这样得到</p><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20190114183327.png" srcset="/img/loading.gif" alt="2"></p><p>通过取整变符号，且模板总和为0，得到下图所示的模板。</p><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20190114183403.png" srcset="/img/loading.gif" alt=""></p><blockquote><p>通常高斯分布中，在（-3σ，3σ）的范围内就覆盖了绝大部分区域，所以模板大小一般取dim = 1 + 6σ（在SIFT 特征中，其中的高斯模糊也是这样取），dim 如果为小数，则取不小于dim 的最小整数，当然实际使用时没有这么严格，如上面我们取σ=1 时，模板大小取5*5。那同一个尺寸的模板中的权值调整就是σ的变化得到的，变化到一定程度，模板尺寸大小改变。</p></blockquote><h3 id="Canny算子：非微分算子"><a href="#Canny算子：非微分算子" class="headerlink" title="Canny算子：非微分算子"></a>Canny算子：非微分算子</h3><h4 id="【基本原理】"><a href="#【基本原理】" class="headerlink" title="【基本原理】"></a>【<strong>基本原理</strong>】</h4><blockquote><ul><li>图象边缘检测必须满足两个条件:<ul><li>能有效的抑制噪声。</li><li>必须尽量精确确定边缘的位置。</li></ul></li><li>根据对信噪比与定位乘积进行测度，，得到最优化逼近算子。这就是Canny边缘检测算子。</li><li>类似与LoG边缘检测方法，也属于先平滑后求导数的方法。</li></ul></blockquote><h4 id="【算法步骤】"><a href="#【算法步骤】" class="headerlink" title="【算法步骤】"></a>【<strong>算法步骤</strong>】</h4><blockquote><ol><li>使用高斯滤波器，以平滑图像，滤除噪声。</li><li>计算图像中每个像素点的梯度强度和方向。</li><li>应用非极大值（Non-Maximum Suppression）抑制， 以消除边缘检测带着的杂散响应。</li><li>应用双阈值（Double-Threshold）检测来确定真实的和潜在的边缘。</li><li>通过抑制孤立的弱边缘最终完成边缘检测。</li></ol></blockquote><h5 id="第一步：同时平滑与微分"><a href="#第一步：同时平滑与微分" class="headerlink" title="第一步：同时平滑与微分"></a>第一步：同时平滑与微分</h5><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20190114184750.png" srcset="/img/loading.gif" alt=""></p><blockquote><p>使用高斯函数的一阶导数同时完成平滑和微分。</p></blockquote><h5 id="第二步：计算梯度和方向"><a href="#第二步：计算梯度和方向" class="headerlink" title="第二步：计算梯度和方向"></a>第二步：计算梯度和方向</h5><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20190114185109.png" srcset="/img/loading.gif" alt="2"></p><ul><li>梯度幅值和方向</li></ul><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20190114185153.png" srcset="/img/loading.gif" alt="2"></p><ul><li>方向离散化：离散化为上下左右和斜45°共4个方向</li></ul><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20190114185232.png" srcset="/img/loading.gif" alt=""></p><h5 id="第三步：梯度幅值非极大值抑制"><a href="#第三步：梯度幅值非极大值抑制" class="headerlink" title="第三步：梯度幅值非极大值抑制"></a>第三步：梯度幅值非极大值抑制</h5><p>细化梯度幅值图像中的屋脊带，只保留幅值局部变化最大的点</p><blockquote><p>使用一个3*3邻域作用于幅值阵列的所有点。在每一点上, 邻域的中心像素与沿梯度方向的两个梯度幅值的插值结果进行较，仅保留极大值点</p></blockquote><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20190114185542.png" srcset="/img/loading.gif" alt="3"></p><h5 id="第四步：边缘连接"><a href="#第四步：边缘连接" class="headerlink" title="第四步：边缘连接"></a>第四步：边缘连接</h5><p>对上一步得到的图像使用低、高阈值t1, t2 阈值化，得到三幅图像</p><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20190114185713.png" srcset="/img/loading.gif" alt=""></p><blockquote><ul><li>T1对应假边缘，去除</li><li>T3对应真边缘，全部保留</li><li>T2连接：临接像素中是否有属于T3的像素</li></ul></blockquote><h5 id="第五步：抑制孤立低阈值点"><a href="#第五步：抑制孤立低阈值点" class="headerlink" title="第五步：抑制孤立低阈值点"></a>第五步：抑制孤立低阈值点</h5><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20190114185951.png" srcset="/img/loading.gif" alt="5"></p><h3 id="边缘检测总结"><a href="#边缘检测总结" class="headerlink" title="边缘检测总结"></a>边缘检测总结</h3><blockquote><ol><li>边缘检测即图像差分</li><li>常见边缘检测算子包括Robert算子，Sobel算子，LoG算子等，其中Sobel算子最为常用</li><li>Canny算子的基本优点在于检测准确、对噪声稳健，在实际中广泛应用.</li></ol></blockquote><h3 id="代码演示"><a href="#代码演示" class="headerlink" title="代码演示"></a>代码演示</h3><pre><code class="hljs python"><span class="hljs-keyword">import</span> cv2<span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<span class="hljs-comment"># 加载图片</span>image = cv2.imread(<span class="hljs-string">'lena.jpg'</span>)img = cv2.pyrDown(image)cv2.imshow(<span class="hljs-string">"source image"</span>, img)<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">RobertImage</span><span class="hljs-params">(img, name)</span>:</span>    <span class="hljs-string">"""</span><span class="hljs-string">    robert算子的实现</span><span class="hljs-string">    :param img:</span><span class="hljs-string">    :param name:</span><span class="hljs-string">    :return:</span><span class="hljs-string">    """</span>    r_sunnzi = np.array([[<span class="hljs-number">-1</span>, <span class="hljs-number">1</span>], [<span class="hljs-number">1</span>, <span class="hljs-number">-1</span>]], np.float32)    robertImage = cv2.filter2D(img, <span class="hljs-number">-1</span>, r_sunnzi)    cv2.imshow(<span class="hljs-string">"Robert-%s"</span> % name, robertImage)<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">sobelImage</span><span class="hljs-params">(img, name)</span>:</span>    <span class="hljs-string">"""</span><span class="hljs-string">    Sobel算子</span><span class="hljs-string">    Sobel(src, ddepth, dx, dy, dst=None, ksize=None, scale=None, delta=None, borderType=None)</span><span class="hljs-string">    前四个是必须的参数：</span><span class="hljs-string">    第一个参数是需要处理的图像；</span><span class="hljs-string">    第二个参数是图像的深度，-1表示采用的是与原图像相同的深度。目标图像的深度必须大于等于原图像的深度；</span><span class="hljs-string">    dx和dy表示的是求导的阶数，0表示这个方向上没有求导，一般为0、1、2。</span><span class="hljs-string">    其后是可选的参数：</span><span class="hljs-string">    dst不用解释了；</span><span class="hljs-string">    ksize是Sobel算子的大小，必须为1、3、5、7。</span><span class="hljs-string">    scale是缩放导数的比例常数，默认情况下没有伸缩系数；</span><span class="hljs-string">    delta是一个可选的增量，将会加到最终的dst中，同样，默认情况下没有额外的值加到dst中；</span><span class="hljs-string">    borderType是判断图像边界的模式。这个参数默认值为cv2.BORDER_DEFAULT。</span><span class="hljs-string">    :param img: 图片数据</span><span class="hljs-string">    :param name: 命名标志</span><span class="hljs-string">    :return:</span><span class="hljs-string">    """</span>    <span class="hljs-comment"># 先检测xy方向上的边缘</span>    Sobel_Ex = cv2.Sobel(src=img, ddepth=cv2.CV_16S, dx=<span class="hljs-number">1</span>, dy=<span class="hljs-number">0</span>, ksize=<span class="hljs-number">3</span>)    Sobel_Ey = cv2.Sobel(src=img, ddepth=cv2.CV_16S, dx=<span class="hljs-number">0</span>, dy=<span class="hljs-number">1</span>, ksize=<span class="hljs-number">3</span>)    <span class="hljs-comment"># 即Sobel函数求完导数后会有负值，还有会大于255的值。而原图像是uint8，即8位无符号数，</span>    <span class="hljs-comment"># 所以Sobel建立的图像位数不够，会有截断。因此要使用16位有符号的数据类型，即cv2.CV_16S。</span>    <span class="hljs-comment"># cv2.imshow("Sobel_Ex-%s" % name, Sobel_Ex)</span>    <span class="hljs-comment"># 图像的每一个像素的横向及纵向梯度近似值结合</span>    <span class="hljs-comment"># 用convertScaleAbs()函数将其转回原来的uint8形式。否则将无法显示图像，而只是一副灰色的窗口。</span>    absX = cv2.convertScaleAbs(Sobel_Ex)    absY = cv2.convertScaleAbs(Sobel_Ey)    <span class="hljs-comment"># cv2.imshow("absX-%s" % name, absX)</span>    <span class="hljs-comment"># cv2.imshow("absY-%s" % name, absY)</span>    SobelImage = cv2.addWeighted(absX, <span class="hljs-number">0.5</span>, absY, <span class="hljs-number">0.5</span>, <span class="hljs-number">0</span>)    cv2.imshow(<span class="hljs-string">"sobel-%s"</span> % name, SobelImage)    <span class="hljs-comment"># ddepth=-1表示图像深度和原图深度一致</span>    <span class="hljs-comment"># Ex = cv2.Sobel(img, -1, 1, 0, ksize=3)</span>    <span class="hljs-comment"># Ey = cv2.Sobel(img, -1, 0, 1, ksize=3)</span>    <span class="hljs-comment"># # cv2.imshow("Ex-%s" % name, Ex)</span>    <span class="hljs-comment"># SobelImg = cv2.addWeighted(Ex, 0.5, Ey, 0.5, 0)</span>    <span class="hljs-comment"># cv2.imshow("soImg-%s" % name, SobelImg)</span><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">LaplaceImage</span><span class="hljs-params">(img, name)</span>:</span>    <span class="hljs-string">"""</span><span class="hljs-string">    Laplace算子</span><span class="hljs-string">    Laplacian(src, ddepth, dst=None, ksize=None, scale=None, delta=None, borderType=None)</span><span class="hljs-string">    前两个是必须的参数：</span><span class="hljs-string">    第一个参数是需要处理的图像；</span><span class="hljs-string">    第二个参数是图像的深度，-1表示采用的是与原图像相同的深度。目标图像的深度必须大于等于原图像的深度；</span><span class="hljs-string">    其后是可选的参数：</span><span class="hljs-string">    dst不用解释了；</span><span class="hljs-string">    ksize是算子的大小，必须为1、3、5、7。默认为1。</span><span class="hljs-string">    scale是缩放导数的比例常数，默认情况下没有伸缩系数；</span><span class="hljs-string">    delta是一个可选的增量，将会加到最终的dst中，同样，默认情况下没有额外的值加到dst中；</span><span class="hljs-string">    borderType是判断图像边界的模式。这个参数默认值为cv2.BORDER_DEFAULT。</span><span class="hljs-string">    :param img: 图片数据</span><span class="hljs-string">    :param name: 命名标志</span><span class="hljs-string">    :return:</span><span class="hljs-string">    """</span>    <span class="hljs-comment"># ddepth=-1表示图像深度和原图深度一致</span>    <span class="hljs-comment"># laplaceImage = cv2.Laplacian(img, ddepth=-1, ksize=3)</span>    <span class="hljs-comment"># cv2.imshow("laplace-%s" % name, laplaceImage)</span>    <span class="hljs-comment"># ddepth=cv2.CV_16S表示图像深度</span>    laplaceImg = cv2.Laplacian(img, ddepth=cv2.CV_16S, ksize=<span class="hljs-number">3</span>)    lapimg = cv2.convertScaleAbs(laplaceImg)    cv2.imshow(<span class="hljs-string">"laplace-%s"</span> % name, lapimg)<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">LoG</span><span class="hljs-params">(img, name)</span>:</span>    <span class="hljs-string">"""</span><span class="hljs-string">    LoG算子的实现</span><span class="hljs-string">    :param img:</span><span class="hljs-string">    :param name:</span><span class="hljs-string">    :return:</span><span class="hljs-string">    """</span>    gaussImage = cv2.GaussianBlur(img, (<span class="hljs-number">5</span>, <span class="hljs-number">5</span>), <span class="hljs-number">0.01</span>)    laplaceImg = cv2.Laplacian(gaussImage, ddepth=cv2.CV_16S, ksize=<span class="hljs-number">3</span>)    logImg = cv2.convertScaleAbs(laplaceImg)    cv2.imshow(<span class="hljs-string">"LoG-%s"</span> % name, logImg)<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">CannyImage</span><span class="hljs-params">(img, name)</span>:</span>    <span class="hljs-string">"""</span><span class="hljs-string">    必要参数：</span><span class="hljs-string">    第一个参数是需要处理的原图像，该图像必须为单通道的灰度图；</span><span class="hljs-string">    第二个参数是阈值1；</span><span class="hljs-string">    第三个参数是阈值2。</span><span class="hljs-string">    其中较大的阈值2用于检测图像中明显的边缘，但一般情况下检测的效果不会那么完美，</span><span class="hljs-string">    边缘检测出来是断断续续的。所以这时候用较小的第一个阈值用于将这些间断的边缘连接起来。</span><span class="hljs-string">    :param img: 图片数据</span><span class="hljs-string">    :param name: 命名标志</span><span class="hljs-string">    :return:</span><span class="hljs-string">    """</span>    <span class="hljs-comment"># 先进行高斯平滑</span>    gaussImage = cv2.GaussianBlur(img, (<span class="hljs-number">3</span>, <span class="hljs-number">3</span>), <span class="hljs-number">0</span>)    <span class="hljs-comment"># 边缘检测，最大阈值为150，最小阈值为50</span>    <span class="hljs-comment"># Canny 推荐的 高:低 阈值比在 2:1 到3:1之间。</span>    cannyImage = cv2.Canny(gaussImage, <span class="hljs-number">50</span>, <span class="hljs-number">150</span>)    cv2.imshow(<span class="hljs-string">"canny-%s"</span> % name, cannyImage)<span class="hljs-comment"># 彩色图像进行Robert边缘检测</span>RobertImage(img, <span class="hljs-string">'rgb'</span>)<span class="hljs-comment"># 灰度图像进行robert边缘检测</span>grayImage = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)RobertImage(grayImage, <span class="hljs-string">'gray'</span>)<span class="hljs-comment"># 彩色图像进行sobel边缘检测</span>sobelImage(img, <span class="hljs-string">'rgb'</span>)<span class="hljs-comment"># 灰度图像进行sobel边缘检测</span>grayImage = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)sobelImage(grayImage, <span class="hljs-string">'gray'</span>)<span class="hljs-comment"># 彩色图像进行laplace边缘检测</span>LaplaceImage(img, <span class="hljs-string">'rgb'</span>)<span class="hljs-comment"># 灰度图像进行laplace边缘检测</span>grayImage = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)LaplaceImage(grayImage, <span class="hljs-string">'gray'</span>)<span class="hljs-comment"># 彩色图像进行LoG边缘检测</span>LoG(img, <span class="hljs-string">'rgb'</span>)<span class="hljs-comment"># 灰度图像进行LoG边缘检测</span>grayImage = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)LoG(grayImage, <span class="hljs-string">'gray'</span>)<span class="hljs-comment"># 彩色图像进行canny边缘检测</span>CannyImage(img, <span class="hljs-string">'rgb'</span>)<span class="hljs-comment"># 灰度图像进行canny边缘检测</span>grayImage = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)CannyImage(grayImage, <span class="hljs-string">'gray'</span>)cv2.waitKey(<span class="hljs-number">0</span>)</code></pre><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20190115192824.png" srcset="/img/loading.gif" alt="代码演示"></p><blockquote><ul><li>sobel 产生的边缘有强弱，抗噪性好</li><li>laplace 对边缘敏感，可能有些是噪声的边缘，也被算进来了</li><li>canny 产生的边缘很细，可能就一个像素那么细，没有强弱之分。</li></ul></blockquote>]]></content>
    
    
    <categories>
      
      <category>机器视觉</category>
      
    </categories>
    
    
    <tags>
      
      <tag>图像边缘检测</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>【四】图像滤波</title>
    <link href="/2019/03/cv-image_filter/"/>
    <url>/2019/03/cv-image_filter/</url>
    
    <content type="html"><![CDATA[<h3 id="图像滤波基本原理"><a href="#图像滤波基本原理" class="headerlink" title="图像滤波基本原理"></a>图像滤波基本原理</h3><blockquote><p>图像信息在采集过程中往往受到各种噪声源的干扰，这些噪声在图像上的常常表现为一些孤立像素点，这可理解为像素的灰度是空间相关的，即噪声点像素灰度与它们临近像素的灰度有着显著不同。通常，一般的前置图像处理后的信息仍然带有后续所不希望夹带的孤立像素点，这种干扰或孤立像素点如不经过滤波处理，会对以后的图像区域分割、分析和判断带来影响。</p></blockquote><h3 id="基本图像预处理滤波方法"><a href="#基本图像预处理滤波方法" class="headerlink" title="基本图像预处理滤波方法"></a>基本图像预处理滤波方法</h3><h4 id="图像滤波与卷积"><a href="#图像滤波与卷积" class="headerlink" title="图像滤波与卷积"></a><strong>图像滤波与卷积</strong></h4><h5 id="【公式定义】"><a href="#【公式定义】" class="headerlink" title="【公式定义】"></a>【<strong>公式定义</strong>】</h5><blockquote><p>与1维信号滤波类似，图像滤波由卷积定义。</p></blockquote><p><img src="https://s2.ax1x.com/2019/01/13/FvLb6O.png" srcset="/img/loading.gif" alt="卷积公式"></p><blockquote><p>在图像中， 以模板形式定义。</p></blockquote><p><img src="https://s2.ax1x.com/2019/01/13/FvLX0H.png" srcset="/img/loading.gif" alt="卷积公式模板"></p><blockquote><p>注意：如果滤波器是对称的，那么两个公式就是等价的。</p><p>​        $f(x, y)$为图片原始数据、$g(x, y)$为卷积核。</p></blockquote><h5 id="【计算方式】"><a href="#【计算方式】" class="headerlink" title="【计算方式】"></a>【<strong>计算方式</strong>】</h5><p><strong>举个栗子</strong>：</p><p>假设一张图像有 5x5 个像素，1 代表白，0 代表黑，这幅图像被视为 5x5 的单色图像。现在用一个由随机地 0 和 1 组成的 3x3 矩阵去和图像中的子区域做Hadamard乘积，每次迭代移动一个像素，这样该乘法会得到一个新的 3x3 的矩阵。下面的动图展示了这个过程。 </p><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20200603133041.png" srcset="/img/loading.gif" alt="images"></p><p><strong>直观上来理解</strong>：</p><ul><li>用一个小的权重矩阵去覆盖输入数据，对应位置元素加权相乘，其和作为结果的一个像素点。</li><li>这个权重在输入数据上滑动，形成一张新的矩阵</li><li>这个权重矩阵就被称为<code>卷积核</code>（convolution kernel）</li><li>其覆盖的位置称为<code>感受野</code>（receptive fileld ）</li><li>生成的新矩阵叫做<code>特征图</code>（feature map）</li></ul><p>分解开来，就如下图所示：</p><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20200603133042.gif" srcset="/img/loading.gif" alt="images"></p><p><strong>其中：</strong></p><ul><li><p>滑动的像素数量就叫做<code>步长</code>（stride），步长为1，表示跳过1个像素，步长为2，就表示跳过2个像素，以此类推</p><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20200603133043.png" srcset="/img/loading.gif" alt="images"></p></li><li><p>以卷积核的边还是中心点作为开始/结束的依据，决定了卷积的<code>补齐</code>（padding）方式。前面我们所举的栗子是<code>valid</code>方式，而<code>same</code>方式则会在图像的边缘用0补齐，如将前面的<code>valid</code>改为<code>same</code>方式，如图所示：</p></li></ul><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20200603133044.gif" srcset="/img/loading.gif" alt="images"></p><p>其采样方式对应变换为：</p><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20200603133045.gif" srcset="/img/loading.gif" alt="images"></p><ul><li>我们前面所提到的输入图像都是灰色的，只有一个通道，但是我们一般会遇到输入通道不只有一个，那么卷积核是三阶的，也就是说所有的通道的结果做累加。</li></ul><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20200603133046.gif" srcset="/img/loading.gif" alt="images"></p><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20200603133047.gif" srcset="/img/loading.gif" alt="images"></p><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20200603133048.gif" srcset="/img/loading.gif" alt="images"></p><p>当然，最后，这里有一个术语：“<code>偏置</code>（bias）”，每个输出滤波器都有一个偏置项，偏置被添加到输出通道产生最终输出通道。 </p><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20200603133049.png" srcset="/img/loading.gif" alt="images"></p><h4 id="图像去噪"><a href="#图像去噪" class="headerlink" title="图像去噪"></a><strong>图像去噪</strong></h4><h5 id="【平均滤波】"><a href="#【平均滤波】" class="headerlink" title="【平均滤波】"></a>【<strong>平均滤波</strong>】</h5><blockquote><p>在一个小区域内（通常是3*3）像素值平均。</p></blockquote><p><strong>数学公式定义</strong></p><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20190113205514.png" srcset="/img/loading.gif" alt="平均滤波公式"></p><blockquote><p>说明：如果在3*3的一个区域内，如果存在5个有效像素值，那么整体就除以5，以此类推。</p></blockquote><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20190113205855.png" srcset="/img/loading.gif" alt="平均滤波示例"></p><h5 id="【加权平均滤波】"><a href="#【加权平均滤波】" class="headerlink" title="【加权平均滤波】"></a>【<strong>加权平均滤波</strong>】</h5><blockquote><p>在一个小区域内（通常是3*3）像素值加权平均。</p></blockquote><p><strong>数学公式定义</strong></p><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20190113210305.png" srcset="/img/loading.gif" alt="滤波加权平均"></p><blockquote><p>在进行基本平均滤波前，先把原始数据乘以一个指定的权重值，然后再进行平均。</p></blockquote><p><strong>普通卷积核</strong></p><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20190113211941.png" srcset="/img/loading.gif" alt="普通卷积核"></p><p><strong>高斯卷积核</strong></p><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20190113212008.png" srcset="/img/loading.gif" alt="高斯卷积核"></p><blockquote><p>权重的分布符合高斯分布，所以叫高斯卷积核。</p></blockquote><h5 id="【中值滤波】"><a href="#【中值滤波】" class="headerlink" title="【中值滤波】"></a>【<strong>中值滤波</strong>】</h5><blockquote><ol><li>将滤波模板（含有若干个点的滑动窗口）在图像中漫游，并将模板中心与图中某个像素位置重合</li><li>读取模板中各对应像素的灰度值；</li><li>将这些灰度值从小到大排列；</li><li>取这一列数据的中间数据，将其赋给对应模板中心位置的像素。如果窗口中有奇数个元素，中值取元素按灰度值大小排序后的中间元素灰度值。如果窗口中有偶数个元素，中值取元素按灰度值大小排序后，中间两个元素灰度的平均值。因为图像为二维信号，中值滤波的窗口形状和尺寸对滤波器效果影响很大，不同图像内容和不同应用要求往往选用不同的窗口形状和尺寸。</li></ol></blockquote><p><strong>示例</strong></p><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20190113212631.png" srcset="/img/loading.gif" alt="中值滤波示例"></p><p><strong>常用选取形式</strong></p><p><code>3*3</code></p><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20190113213612.png" srcset="/img/loading.gif" alt="中值滤波选取形式"></p><p><code>5*5</code></p><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20190113213654.png" srcset="/img/loading.gif" alt="中值滤波选取形式"></p><blockquote><p>中值滤波对椒盐噪声有效。</p></blockquote><h3 id="数学形态学滤波"><a href="#数学形态学滤波" class="headerlink" title="数学形态学滤波"></a>数学形态学滤波</h3><h4 id="【概述】"><a href="#【概述】" class="headerlink" title="【概述】"></a>【<strong>概述</strong>】</h4><blockquote><p>形态学（morphology）一词通常表示生物学的一个分支，该分支主要研究动植物的形态和结构。而我们图像处理中指的形态学，往往表示的是数学形态学。</p><p>数学形态学（Mathematical morphology） 是一门建立在格论和拓扑学基础之上的图像分析学科，是数学形态学图像处理的基本理论。其基本的运算包括：二值腐蚀和膨胀、二值开闭运算、骨架抽取、极限腐蚀、击中击不中变换、形态学梯度、Top-hat变换、颗粒分析、流域变换、灰值腐蚀和膨胀、灰值开闭运算、灰值形态学梯度等。</p></blockquote><h4 id="【膨胀和腐蚀】"><a href="#【膨胀和腐蚀】" class="headerlink" title="【膨胀和腐蚀】"></a>【<strong>膨胀和腐蚀</strong>】</h4><blockquote><p>按数学方面来说，膨胀或者腐蚀操作就是将图像（或图像的一部分区域，我们称之为A）与核（我们称之为B）进行卷积。</p><p>核可以是任何的形状和大小，它拥有一个单独定义出来的参考点，我们称其为锚点（anchorpoint）。多数情况下，核是一个小的中间带有参考点和实心正方形或者圆盘，其实，我们可以把核视为模板或者掩码。</p></blockquote><h5 id="膨胀"><a href="#膨胀" class="headerlink" title="膨胀"></a><code>膨胀</code></h5><blockquote><p>膨胀就是求局部最大值的操作，核B与图形卷积，即计算核B覆盖的区域的像素点的最大值，并把这个最大值赋值给参考点指定的像素。这样就会使图像中的高亮区域逐渐增长。</p></blockquote><p><strong>数学定义</strong></p><blockquote><p>$A\oplus B$表示集合A用<code>结构元素B</code>膨胀。</p></blockquote><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20190113214217.png" srcset="/img/loading.gif" alt="膨胀公式"></p><p><strong>示例</strong></p><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20190113221916.png" srcset="/img/loading.gif" alt="膨胀示例"></p><h5 id="腐蚀"><a href="#腐蚀" class="headerlink" title="腐蚀"></a><code>腐蚀</code></h5><blockquote><p><code>腐蚀</code>就是<code>膨胀</code>的相反操作，腐蚀就是求局部最小值的操作。</p></blockquote><p><strong>数学定义</strong></p><blockquote><p>$A\ominus B$表示集合A用<code>结构元素B</code>腐蚀。</p></blockquote><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20190113215646.png" srcset="/img/loading.gif" alt="腐蚀数学定义"></p><p><strong>示例</strong></p><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20190113222014.png" srcset="/img/loading.gif" alt="腐蚀结果"></p><h4 id="【开闭运算】"><a href="#【开闭运算】" class="headerlink" title="【开闭运算】"></a>【<strong>开闭运算</strong>】</h4><blockquote><p><code>膨胀</code>和<code>腐蚀</code>并不互为<code>逆运算</code>，二者<code>级联</code>使用可生成新的形态学运算。</p></blockquote><p><code>开运算</code></p><blockquote><p>先腐蚀后膨胀</p></blockquote><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20190113220210.png" srcset="/img/loading.gif" alt="开运算"></p><p><code>闭运算</code></p><blockquote><p>先膨胀后腐蚀</p></blockquote><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20190113220258.png" srcset="/img/loading.gif" alt="闭运算"></p><p><code>先开后闭</code></p><blockquote><p>可有效的去除噪声。</p></blockquote><h3 id="滤波总结"><a href="#滤波总结" class="headerlink" title="滤波总结"></a>滤波总结</h3><blockquote><ol><li>滤波即卷积，由逐点乘积后累加得到</li><li>平滑滤波包括平均滤波、高斯滤波、中值滤波等方法，其中高斯滤波最为常用</li><li>数学形态学滤波基于腐蚀与膨胀两个基本操作</li></ol></blockquote><h3 id="代码演示"><a href="#代码演示" class="headerlink" title="代码演示"></a><strong>代码演示</strong></h3><pre><code class="hljs python"><span class="hljs-keyword">import</span> cv2<span class="hljs-comment"># 加载图片</span>image = cv2.imread(<span class="hljs-string">'lena.jpg'</span>)img = cv2.pyrDown(image)cv2.imshow(<span class="hljs-string">"source image"</span>, img)<span class="hljs-comment"># 高斯平滑滤波</span>gaussImage = cv2.GaussianBlur(img, (<span class="hljs-number">5</span>, <span class="hljs-number">5</span>), <span class="hljs-number">0</span>)cv2.imshow(<span class="hljs-string">"gauss image"</span>, gaussImage)<span class="hljs-comment"># 中值滤波</span>medianImage = cv2.medianBlur(img, <span class="hljs-number">5</span>)cv2.imshow(<span class="hljs-string">"median image"</span>, medianImage)<span class="hljs-comment"># 形态学滤波</span>kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (<span class="hljs-number">5</span>, <span class="hljs-number">5</span>))<span class="hljs-comment"># 腐蚀图像</span>eroded = cv2.erode(img, kernel)cv2.imshow(<span class="hljs-string">"Eroded Image"</span>, eroded)<span class="hljs-comment"># 膨胀图像</span>dilated = cv2.dilate(img, kernel)cv2.imshow(<span class="hljs-string">"dilated Image"</span>, dilated)<span class="hljs-comment"># 形态学滤波-开运算</span>morphImage_open = cv2.morphologyEx(img, cv2.MORPH_OPEN, kernel)cv2.imshow(<span class="hljs-string">"image-open"</span>, morphImage_open)<span class="hljs-comment"># 形态学滤波-闭运算</span>morphImage_close = cv2.morphologyEx(img, cv2.MORPH_CLOSE, kernel)cv2.imshow(<span class="hljs-string">"image-close"</span>, morphImage_close)<span class="hljs-comment"># 形态学滤波-先开后闭运算</span>morphImage_open_close = cv2.morphologyEx(morphImage_open, cv2.MORPH_CLOSE, kernel)cv2.imshow(<span class="hljs-string">"image-open-close"</span>, morphImage_open_close)cv2.waitKey(<span class="hljs-number">0</span>)</code></pre><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20190113221555.png" srcset="/img/loading.gif" alt="代码演示"></p>]]></content>
    
    
    <categories>
      
      <category>机器视觉</category>
      
    </categories>
    
    
    <tags>
      
      <tag>图像滤波</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>【三】数字成像系统</title>
    <link href="/2019/03/cv-digitaltoimage/"/>
    <url>/2019/03/cv-digitaltoimage/</url>
    
    <content type="html"><![CDATA[<h3 id="光通量"><a href="#光通量" class="headerlink" title="光通量"></a>光通量</h3><blockquote><p>指人眼所能感觉到的辐射功率，它等于单位时间内某一波段的辐射能量和该波段的相对视见率的乘积。</p><p>符号：Φ</p><p>单位：lm(流明)</p><p>1lm = 0.00146瓦</p></blockquote><h4 id="常见光源的光通量"><a href="#常见光源的光通量" class="headerlink" title="常见光源的光通量"></a>常见光源的光通量</h4><p><img src="http://eveseven.oss-cn-shanghai.aliyuncs.com/19-1-11/66671052.jpg" srcset="/img/loading.gif" alt="光通量"></p><h3 id="辐照度"><a href="#辐照度" class="headerlink" title="辐照度"></a>辐照度</h3><blockquote><p>指投射到一平方米表面上的辐射通量密度。也就是说是到达一平方米表面上，单位时间，单位面积上的辐射能。</p><p>符号：E</p><p>单位：lux(勒克斯)</p><p>1 lux = 1 lm/$m^2$</p></blockquote><h4 id="常见照明环境的辐照度"><a href="#常见照明环境的辐照度" class="headerlink" title="常见照明环境的辐照度"></a>常见照明环境的辐照度</h4><p><img src="http://eveseven.oss-cn-shanghai.aliyuncs.com/19-1-11/6673147.jpg" srcset="/img/loading.gif" alt="辐射度"></p><h3 id="光源类别"><a href="#光源类别" class="headerlink" title="光源类别"></a>光源类别</h3><blockquote><p>按方向</p><ul><li>直射光</li><li>漫射光</li></ul><p>按光谱</p><ul><li>可见光</li><li>近可见光</li></ul><p>其他</p><ul><li>偏振</li><li>其他</li></ul></blockquote><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20190309202114.png" srcset="/img/loading.gif" alt=""></p><h4 id="特点"><a href="#特点" class="headerlink" title="特点"></a>特点</h4><p>【<strong>透视轮廓–背光源</strong>】</p><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20190309202131.png" srcset="/img/loading.gif" alt=""></p><blockquote><p>发射面是一个漫射面， 均匀性好。可用于镜面反射材料，如晶片或者玻璃基底上的伤痕检测；LCD检测；微小电子元件尺寸、形状、靶标测试</p></blockquote><p>【<strong>表面照明–漫射光源</strong>】</p><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20190309202143.png" srcset="/img/loading.gif" alt=""></p><blockquote><p>在一定工作距离下，光束集中、亮度高、均匀性好、照射面积相对较小。常用于液晶校正、塑胶容器检查、工作螺孔定位、标签检查、管脚检查、集成电路印字检查等等</p></blockquote><p>【<strong>颜色光源</strong>】</p><p><img src="http://eveseven.oss-cn-shanghai.aliyuncs.com/19-1-11/88508873.jpg" srcset="/img/loading.gif" alt="互补光源"></p><blockquote><p>如果希望更加鲜明地突出某些颜色，则选择色环上相对应的互补颜色光源照明，这样就可以明显的提高图像的对比度。</p></blockquote><h3 id="RGB"><a href="#RGB" class="headerlink" title="RGB"></a>RGB</h3><p><img src="http://eveseven.oss-cn-shanghai.aliyuncs.com/19-1-11/81330727.jpg" srcset="/img/loading.gif" alt="rgb"></p><blockquote><p>为生活中所最常见的三基色，也是和视觉感受一一致的，很多颜色都是由这三基色产生。</p><p>三基色: 红、绿、蓝</p><p>三色相交互是白色。</p></blockquote><h3 id="CYMK"><a href="#CYMK" class="headerlink" title="CYMK"></a>CYMK</h3><p><img src="http://eveseven.oss-cn-shanghai.aliyuncs.com/19-1-11/84796285.jpg" srcset="/img/loading.gif" alt="cymk"></p><blockquote><p>补色模型</p><p>基色变为基本三基色的补色–红色–&gt;青色、绿色–&gt;黄色、蓝色–&gt;品红</p><p>三基色：青、黄、品红</p><p>三色交互是黑色。</p></blockquote><h3 id="HSI"><a href="#HSI" class="headerlink" title="HSI"></a>HSI</h3><p><img src="http://eveseven.oss-cn-shanghai.aliyuncs.com/19-1-11/29068085.jpg" srcset="/img/loading.gif" alt="hsi"></p><blockquote><ul><li>色调H：描述纯色的属性（红、黄）。</li><li>饱和度S：表示的是一种纯色被白光稀释的程度的度量。</li><li>亮度I：体现了无色的光照强度的概念，是一个主观的描述。</li></ul></blockquote><p>【与RGB<strong>的换算关系</strong>】</p><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20190309202246.png" srcset="/img/loading.gif" alt=""></p><p><code>代码演示</code></p><pre><code class="hljs python"><span class="hljs-keyword">import</span> cv2<span class="hljs-comment"># 加载图片</span>img = cv2.imread(<span class="hljs-string">'2.jpg'</span>)<span class="hljs-comment"># 缩放+灰度化</span>sizeImage = cv2.pyrDown(img)grayImage = cv2.cvtColor(sizeImage, cv2.COLOR_BGR2GRAY)cv2.imshow(<span class="hljs-string">"source image"</span>, sizeImage)cv2.imshow(<span class="hljs-string">"gray"</span>, grayImage)<span class="hljs-comment"># RGB通道分离--opencv中，RGB三个通道是反过来的</span>rgbImage = cv2.split(sizeImage)cv2.imshow(<span class="hljs-string">"R"</span>, rgbImage[<span class="hljs-number">2</span>])cv2.imshow(<span class="hljs-string">"G"</span>, rgbImage[<span class="hljs-number">1</span>])cv2.imshow(<span class="hljs-string">"B"</span>, rgbImage[<span class="hljs-number">0</span>])<span class="hljs-comment"># 分离后为单通道，相当于分离通道的同时把其他两个通道填充了相同的数值。</span><span class="hljs-comment"># 比如红色通道，分离出红色通道的同时，绿色和蓝色被填充为和红色相同的数值，这样一来就只有黑白灰了。</span><span class="hljs-comment"># 可以进行观察，会发现原图中颜色越接近红色的地方在红色通道越接近白色。</span><span class="hljs-comment"># 在纯红的地方在红色通道会出现纯白。</span><span class="hljs-comment"># R值为255 -&gt; RGB(255，255，255)，为纯白</span><span class="hljs-comment"># HSI颜色模型+通道分离</span>hsv = cv2.cvtColor(sizeImage, cv2.COLOR_BGR2HSV)hsvChannels = cv2.split(hsv)cv2.imshow(<span class="hljs-string">"Hue"</span>, hsvChannels[<span class="hljs-number">0</span>])cv2.imshow(<span class="hljs-string">"Saturation"</span>, hsvChannels[<span class="hljs-number">1</span>])cv2.imshow(<span class="hljs-string">"Value"</span>, hsvChannels[<span class="hljs-number">2</span>])cv2.waitKey(<span class="hljs-number">0</span>)</code></pre><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20190309210844.png" srcset="/img/loading.gif" alt=""></p><h3 id="CCD传感器基本原理"><a href="#CCD传感器基本原理" class="headerlink" title="CCD传感器基本原理"></a>CCD传感器基本原理</h3><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20190309202350.png" srcset="/img/loading.gif" alt=""></p><h3 id="彩色图像传感器的基本原理"><a href="#彩色图像传感器的基本原理" class="headerlink" title="彩色图像传感器的基本原理"></a>彩色图像传感器的基本原理</h3><p><img src="http://eveseven.oss-cn-shanghai.aliyuncs.com/19-1-11/45875188.jpg" srcset="/img/loading.gif" alt="彩色  图像传感器"></p><blockquote><p>最底层的感光区同样是采用CCD图像传感器</p><p>彩色滤色片阵列(Color Filter Array),也被称为拜尔滤色镜(Bayer Filter)，排列在感光区上方。</p><p>传感器不同，排列方式就不同。</p></blockquote><p><img src="http://eveseven.oss-cn-shanghai.aliyuncs.com/19-1-11/152560.jpg" srcset="/img/loading.gif" alt="Bayer"></p><blockquote><p>这个单元包含了三原色：红(R),绿(G),蓝(B)。对于单个像素点，只有一种特定的彩色滤色片放置在其上方。彩色图像的像素点不仅包含通常的感光区域还包括了一个彩色滤色片。因为人眼对绿色的敏感度很高，所以我们使用了更多的绿色滤色片，类似棋盘格的形式来摆放绿色滤色片，以期得到有着更高空间分辨率的图像。红色和蓝色滤色片每隔一行与绿色滤色片错落放置。</p></blockquote><h3 id="γ校正"><a href="#γ校正" class="headerlink" title="γ校正"></a>γ校正</h3><p><img src="http://eveseven.oss-cn-shanghai.aliyuncs.com/19-1-11/21721761.jpg" srcset="/img/loading.gif" alt="校正"></p><blockquote><p>图像传感器输出时经过γ校正，以符合人的视觉习惯；存储时还原回原有的RGB值。</p></blockquote><p><img src="http://eveseven.oss-cn-shanghai.aliyuncs.com/19-1-11/22073630.jpg" srcset="/img/loading.gif" alt="校正环节"></p><h3 id="图像传输的方式"><a href="#图像传输的方式" class="headerlink" title="图像传输的方式"></a>图像传输的方式</h3><blockquote><ul><li>模拟视频传输：采用同轴电缆等方式，将亮度和色度分离，在不同频带调制后在同一信号线上传输。常用的为同轴电缆，同轴电缆的中心导线用于传输信号，外层是金属屏蔽网。</li><li>RGB方式：显示器，投影</li><li>数字传输（长距离）：光纤高清信号，网线</li><li>数字传输（短距离）：USB，火线，HDMI</li></ul></blockquote><h3 id="模拟、数字视频传输接口"><a href="#模拟、数字视频传输接口" class="headerlink" title="模拟、数字视频传输接口"></a>模拟、数字视频传输接口</h3><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20190309202521.png" srcset="/img/loading.gif" alt=""></p><h3 id="常见的图像和视频压缩标准"><a href="#常见的图像和视频压缩标准" class="headerlink" title="常见的图像和视频压缩标准"></a>常见的图像和视频压缩标准</h3><p><img src="http://eveseven.oss-cn-shanghai.aliyuncs.com/19-1-11/87798349.jpg" srcset="/img/loading.gif" alt="标准"></p><h3 id="显示设备及参数（液晶显示）"><a href="#显示设备及参数（液晶显示）" class="headerlink" title="显示设备及参数（液晶显示）"></a>显示设备及参数（液晶显示）</h3><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20190309202658.png" srcset="/img/loading.gif" alt=""></p><blockquote><ul><li>分辨率（最佳分辨率）及显示器尺寸</li><li>亮度和对比度：LCD的亮度以流明/平方米（cd/m2）度量，对比度是直接反映LCD显示器能否现丰富的色阶的参数</li><li>响应时间：响应时间是LCD显示器的一个重要的参数，它指的是LCD显示器对于输入信号的反应时间。</li><li>坏点：如果液晶显示屏中某一个发光单元有问题就会出现总丌透光、总透光、半透光等现象，这就是所谓的“坏点”</li></ul></blockquote>]]></content>
    
    
    <categories>
      
      <category>机器视觉</category>
      
    </categories>
    
    
    <tags>
      
      <tag>数字成像系统</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>【二】视觉系统</title>
    <link href="/2019/03/cv-visualsystem/"/>
    <url>/2019/03/cv-visualsystem/</url>
    
    <content type="html"><![CDATA[<h3 id="视觉系统构成要素"><a href="#视觉系统构成要素" class="headerlink" title="视觉系统构成要素"></a>视觉系统构成要素</h3><blockquote><ul><li>照明设备：光源</li><li>成像设备：相机</li><li>处理设备：主机</li><li>算法软件：视觉处理系统</li></ul></blockquote><h4 id="【要素的关系】"><a href="#【要素的关系】" class="headerlink" title="【要素的关系】"></a>【要素的关系】</h4><p><img src="http://eveseven.oss-cn-shanghai.aliyuncs.com/19-1-9/95153635.jpg" srcset="/img/loading.gif" alt="视觉系统构成要素"></p><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20190309201004.png" srcset="/img/loading.gif" alt=""></p><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20190309201017.png" srcset="/img/loading.gif" alt=""></p><h4 id="【要素的理解】"><a href="#【要素的理解】" class="headerlink" title="【要素的理解】"></a>【要素的理解】</h4><blockquote><ul><li>光源：对场景进行照明，使能捕捉的范围更大，事物更清晰，一般就是各种光。</li><li>相机：抓取图片，保留信息，一般是指照相机，摄像机，一个或多个。</li><li>主机：处理图片信息，一般为台式计算机或嵌入式处理器。</li><li>算法软件：辅助主机处理图片信息，提取所需要的特征， 一般为C++或者其他编程语言编写的视觉识别算法及程序。</li></ul></blockquote><h4 id="【要素案例】"><a href="#【要素案例】" class="headerlink" title="【要素案例】"></a>【要素案例】</h4><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20190322201130.png" srcset="/img/loading.gif" alt=""></p><blockquote><ul><li>光源：室内光线或专用照明</li><li>相机：放在机械臂前端，单相机</li><li>主机：台式计算机或嵌入式处理器</li><li>算法软件：使用C++或其它语言编写的视觉识别算法及程序</li></ul></blockquote><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20190309201230.png" srcset="/img/loading.gif" alt=""></p><blockquote><ul><li>光源：红外和可见光</li><li>相机：RGB+红外</li><li>主机：类PC结构</li><li>软件：运行在SoC+后端主机软件</li></ul></blockquote><h3 id="Marr视觉计算机理论"><a href="#Marr视觉计算机理论" class="headerlink" title="Marr视觉计算机理论"></a>Marr视觉计算机理论</h3><blockquote><ul><li>目的：通过视觉系统，重构三维物体的形状和位置</li><li>初略图（2维）：过零点（zero-crossing）、短线段、端点等基本特征</li><li>$2 \frac12$维：对物体形状的一些初略描述</li><li>3维：对物体的三维描述</li></ul></blockquote><h4 id="【解释】"><a href="#【解释】" class="headerlink" title="【解释】"></a>【解释】</h4><blockquote><p>Marr视觉计算机理论就是通过视觉捕捉物体的图像和位置，然后通过技术重构物体的三维特征；这个过程包括：</p><ul><li>先通过图像提取出一些2维的初略图</li><li>然后对物体的形状的特征(法向量等)抽取并做一些初略的描述确定$2 \frac12$维图</li><li>最后综合所有特征形成物体的三维特征图。</li></ul></blockquote>]]></content>
    
    
    <categories>
      
      <category>机器视觉</category>
      
    </categories>
    
    
    <tags>
      
      <tag>视觉系统</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>【一】计算机视觉引论</title>
    <link href="/2019/03/cv-introduction/"/>
    <url>/2019/03/cv-introduction/</url>
    
    <content type="html"><![CDATA[<h3 id="计算机视觉"><a href="#计算机视觉" class="headerlink" title="计算机视觉"></a>计算机视觉</h3><blockquote><p>计算机视觉就是让计算机<code>看懂</code>图像和视频。</p></blockquote><ul><li><p>视觉是自然智能不可思议的结晶</p><blockquote><ul><li>猕猴的大脑皮层中视觉部分占据大约50%</li><li>人脑中有关视觉的部分所占比重最大</li></ul><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20190309200421.png" srcset="/img/loading.gif" alt=""></p></blockquote></li><li><p>人类大脑对视觉进行层次化的处理</p><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20190309200455.png" srcset="/img/loading.gif" alt=""></p><blockquote><p>人类采用神经网络对视觉信息进行深层次处理，和深度学习密切结合。</p></blockquote></li></ul><h3 id="计算机视觉的产生和发展都经历的阶段"><a href="#计算机视觉的产生和发展都经历的阶段" class="headerlink" title="计算机视觉的产生和发展都经历的阶段"></a>计算机视觉的产生和发展都经历的阶段</h3><blockquote><ul><li>起源：20世纪50年代统计模式识别，二维图像分析。</li><li>诞生：1974年 Minsky -&gt; David Marr 暑期， 1981年，人工智能“计算机视觉”专辑，Marr视觉计算理论 得到了迅速的发展。</li><li>发展：</li><li>80年代以后：<ul><li>随着计算能力的迅速增长，视觉计算成本极大降低。</li><li>以Marr理论为基础的视觉理论广泛研究。</li></ul></li><li>2000年后，特征提取和基于学习的视觉得到迅速发展。</li><li>2006年，Hinton 提出深度学习。</li><li>2010年， 微软使用深度学习在语音方面取得突破进展。</li><li>2015年后，深度学习在视觉个各邻域取得突破：<ul><li>2015年， 在imageNet上的识别准确率首次超越人类。</li><li>2016年，Tesla 创造了56亿公里的自动驾驶路测数据。</li><li>2017年，iPone x 宣布引入Face ID 高精度人脸识别技术。</li><li>2018年，OpenAI 2:1 战胜人类DOTA2高手队。</li></ul></li></ul></blockquote><h3 id="计算机视觉的应用"><a href="#计算机视觉的应用" class="headerlink" title="计算机视觉的应用"></a>计算机视觉的应用</h3><blockquote><ul><li>服务机器人</li><li>安防监控</li><li>自动驾驶</li><li>智能穿戴</li><li>无人机快递</li><li>等等</li></ul></blockquote><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20190309200616.png" srcset="/img/loading.gif" alt=""></p><h3 id="内容架构"><a href="#内容架构" class="headerlink" title="内容架构"></a>内容架构</h3><blockquote><p>遵循Marr视觉计算机理论</p><p>从初略图（2维）–&gt; $2 \frac12$维 –&gt;3维</p><p>初略图</p><ul><li>“看见”–照相，颜色，图像采集的过程。</li><li>“基本理解”– 滤波、边缘检测、灰度直方图、直线检测–基本特征提取</li><li>图像阈值分割、区域生长、图像描述</li><li>关键点及特征检测</li><li>背景建模及运动估计</li></ul><p>$2 \frac12$维</p><ul><li>视觉成像模型、视觉几何基础、相机标定</li></ul><p>3维</p><ul><li>图像拼接</li><li>立体视觉</li></ul><p>从底层–&gt;中层–&gt;高层</p><p>底层是由高等数学，线性代数，矩阵分析，概率论，最优化方法，物理(运动学)来进行架构 作为理论支撑。</p><p>中层：机器视觉是建立在数字图像处理和模式识别之上的。</p><p>高层：深度学习+计算机视觉</p></blockquote><h3 id="即将更新"><a href="#即将更新" class="headerlink" title="即将更新"></a>即将更新</h3><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20190322201253.png" srcset="/img/loading.gif" alt=""></p>]]></content>
    
    
    <categories>
      
      <category>机器视觉</category>
      
    </categories>
    
    
    <tags>
      
      <tag>计算机视觉</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>OpenCV环境配置（win10+opencv3.4.1+vs2017）</title>
    <link href="/2019/01/opencv-setup/"/>
    <url>/2019/01/opencv-setup/</url>
    
    <content type="html"><![CDATA[<p><strong>简介：</strong> OpenCV是一个基于BSD许可（开源）发行的跨平台计算机视觉库，可以运行在Linux、Windows、Android和Mac OS操作系统上。它轻量级而且高效——由一系列 C 函数和少量 C++ 类构成，同时提供了Python、Java、Ruby、MATLAB等语言的接口，实现了图像处理和计算机视觉方面的很多通用算法。</p><h3 id="系统环境"><a href="#系统环境" class="headerlink" title="系统环境"></a>系统环境</h3><p>系统：windows 10 - 1809</p><p>OpenCV版本：3.4.1</p><p>Microsoft Visio Studio：2017</p><h3 id="环境安装"><a href="#环境安装" class="headerlink" title="环境安装"></a>环境安装</h3><blockquote><p>默认vs2017已经安装。</p></blockquote><p><strong>下载路径</strong>：从<a href="https://link.jianshu.com/?t=https%3A%2F%2Fopencv.org%2F" target="_blank" rel="noopener">OpenCV Library</a>下载安装包并安装</p><ul><li><p>进入官网首页，点击【<strong>Releases</strong>】</p><p><img src="http://eveseven.oss-cn-shanghai.aliyuncs.com/19-1-7/71143712.jpg" srcset="/img/loading.gif" alt="opencv下载"></p></li><li><p>在<strong>Releases</strong> 界面下找到<strong>3.4.1</strong>版本, 因为是Windows平台且使用安装包方式，故点击<strong>【Win pack】</strong></p></li></ul><p><img src="http://eveseven.oss-cn-shanghai.aliyuncs.com/19-1-7/4000755.jpg" srcset="/img/loading.gif" alt="opencv3.4.1"></p><ul><li><p>下载完毕后，【双击安装包】开始安装</p><p><img src="http://eveseven.oss-cn-shanghai.aliyuncs.com/19-1-7/51001757.jpg" srcset="/img/loading.gif" alt="opencv安装"></p><blockquote><p>我这里是安装在D盘，不用再新建文件夹，解压后会自动创建一个【opencv】的文件夹。</p><p>注意：安装路径不要有中文，避免出现问题。</p></blockquote></li><li><p>等待，直到安装完毕。</p></li></ul><h3 id="配置OpenCV的系统环境"><a href="#配置OpenCV的系统环境" class="headerlink" title="配置OpenCV的系统环境"></a>配置OpenCV的系统环境</h3><ul><li><p>在【桌面】或者【资源管理器】-&gt; 【右击】【此电脑】-&gt; 选择【属性】</p><p><img src="http://eveseven.oss-cn-shanghai.aliyuncs.com/19-1-7/1812340.jpg" srcset="/img/loading.gif" alt="属性"></p></li><li><p>选择【高级系统设置】-&gt; 点击【环境变量】</p><p><img src="http://eveseven.oss-cn-shanghai.aliyuncs.com/19-1-7/3290596.jpg" srcset="/img/loading.gif" alt="环境变量"></p></li><li><p>环境变量界面分为两部分，上方为<strong>【用户变量】</strong>，下方为<strong>【系统变量】</strong></p><p><img src="http://eveseven.oss-cn-shanghai.aliyuncs.com/19-1-7/18148003.jpg" srcset="/img/loading.gif" alt="path"></p></li><li><p>在<strong>【系统变量】</strong>列表中找到<strong>【Path】</strong>，双击打开</p><p><img src="http://eveseven.oss-cn-shanghai.aliyuncs.com/19-1-7/43977585.jpg" srcset="/img/loading.gif" alt="系统环境变量"></p></li><li><p>在打开界面的右侧列表中选择<strong>【浏览】</strong>,进入<strong>opencv安装路径</strong>并选取<strong>XXXX\opencv\build\x64\vc14\bin</strong>目录，逐级<strong>【确定】</strong>保存。</p></li><li><p>系统环境变量设置完毕。</p></li></ul><h3 id="在VS2017项目中配置OpenCV"><a href="#在VS2017项目中配置OpenCV" class="headerlink" title="在VS2017项目中配置OpenCV"></a>在VS2017项目中配置OpenCV</h3><h4 id="【新建空项目】"><a href="#【新建空项目】" class="headerlink" title="【新建空项目】"></a>【<strong>新建空项目</strong>】</h4><p><img src="http://eveseven.oss-cn-shanghai.aliyuncs.com/19-1-7/70133617.jpg" srcset="/img/loading.gif" alt="新建项目"></p><p><img src="http://eveseven.oss-cn-shanghai.aliyuncs.com/19-1-7/85326264.jpg" srcset="/img/loading.gif" alt="空项目"></p><h4 id="【新建cpp文件】"><a href="#【新建cpp文件】" class="headerlink" title="【新建cpp文件】"></a>【<strong>新建cpp文件</strong>】</h4><p>右击-&gt;【<strong>源文件</strong>】-&gt; 【<strong>添加</strong>】-&gt; 【<strong>新建项</strong>】</p><p><img src="http://eveseven.oss-cn-shanghai.aliyuncs.com/19-1-7/31670791.jpg" srcset="/img/loading.gif" alt="新建文件"></p><p><img src="http://eveseven.oss-cn-shanghai.aliyuncs.com/19-1-7/71795105.jpg" srcset="/img/loading.gif" alt="cpp文件"></p><h4 id="【属性管理】"><a href="#【属性管理】" class="headerlink" title="【属性管理】"></a>【<strong>属性管理</strong>】</h4><p><strong>VS2017</strong>上方菜单栏找到<strong>【视图】</strong>–&gt;<strong>【其他窗口】</strong>–&gt;<strong>【属性管理器】</strong>，点击开启</p><p><img src="http://eveseven.oss-cn-shanghai.aliyuncs.com/19-1-7/12879883.jpg" srcset="/img/loading.gif" alt="属性管理器"></p><p><img src="http://eveseven.oss-cn-shanghai.aliyuncs.com/19-1-7/18122186.jpg" srcset="/img/loading.gif" alt="属性管理器"></p><h4 id="【Debug-X64设置】"><a href="#【Debug-X64设置】" class="headerlink" title="【Debug X64设置】"></a><strong>【Debug X64设置】</strong></h4><p>右击【<strong>Debug|x64</strong>】,点击【<strong>属性</strong>】</p><p><img src="http://eveseven.oss-cn-shanghai.aliyuncs.com/19-1-7/31611889.jpg" srcset="/img/loading.gif" alt="属性"></p><ul><li><p>在左侧<strong>【通用属性】</strong>菜单下找到<strong>【VC++目录】</strong>，点击，右侧找到<strong>【包含目录】</strong>，点击，右侧出现<strong>【倒三角】</strong>，点击，在弹出下拉列表中点击<strong>【编辑】</strong></p><p><img src="http://eveseven.oss-cn-shanghai.aliyuncs.com/19-1-7/83946786.jpg" srcset="/img/loading.gif" alt="编辑属性"></p></li><li><p>在弹出界面中通过点击右上角<strong>【新行】</strong>按钮，添加三条路径：<br><strong>【XXXX\opencv\build\include\opencv2】</strong><br><strong>【XXXX\opencv\build\include\opencv】</strong><br><strong>【XXXX\opencv\build\include】</strong></p><p><img src="http://eveseven.oss-cn-shanghai.aliyuncs.com/19-1-7/34773414.jpg" srcset="/img/loading.gif" alt="新行"></p></li><li><p>在【<strong>库目录</strong>】添加路径</p><p><strong>【XXXX\opencv\build\x64\vc14\lib】</strong></p><p><img src="http://eveseven.oss-cn-shanghai.aliyuncs.com/19-1-7/41741966.jpg" srcset="/img/loading.gif" alt="库目录"></p></li><li><p>在左侧<strong>【通用属性】</strong>菜单下找到<strong>【链接器】</strong>，展开菜单，选择<strong>【输入】</strong>，在右侧找到<strong>【附加依赖项】</strong>，以<strong>【第4步】</strong>中同样方式打开<strong>【编辑】</strong>，在上方空白处手动键入:<strong>【opencv_world341d.lib】</strong>。</p><p><img src="http://eveseven.oss-cn-shanghai.aliyuncs.com/19-1-7/9953370.jpg" srcset="/img/loading.gif" alt="依赖项"></p><p><img src="http://eveseven.oss-cn-shanghai.aliyuncs.com/19-1-7/3510846.jpg" srcset="/img/loading.gif" alt="附加依赖项"></p></li><li><p>保存退出</p></li></ul><blockquote><p> 此处键入的文件<strong>随版本号不同而不同</strong>，若非OpenCV 3.4.1版本可自行进入<strong>【XXXX\opencv\build\x64\vc14\lib】</strong>目录查看文件名并键入。</p></blockquote><h4 id="【Release-x64设置】"><a href="#【Release-x64设置】" class="headerlink" title="【Release|x64设置】"></a>【<strong>Release|x64设置</strong>】</h4><p>重复执行<strong>【Debug|X64设置】</strong>–&gt; 不同的就是【<strong>依赖项</strong>】的选择。</p><blockquote><p> 文件夹下会看到几个几乎同名的文件，区别仅仅为文件名<strong>末尾有无字母d</strong>，其中d代表debug版本，其他为release版本，以3.4.1版本为例，配置<strong>【Debug|x64】</strong>时使用<strong>opencv_world341d.lib</strong>，配置<strong>【Release|x64】</strong>时使用<strong>opencv_world341.lib</strong>.</p></blockquote><h4 id="【环境搭建完成】"><a href="#【环境搭建完成】" class="headerlink" title="【环境搭建完成】"></a>【<strong>环境搭建完成</strong>】</h4><h3 id="功能测试"><a href="#功能测试" class="headerlink" title="功能测试"></a>功能测试</h3><h4 id="【测试代码】"><a href="#【测试代码】" class="headerlink" title="【测试代码】"></a>【测试代码】</h4><pre><code class="hljs c++"><span class="hljs-meta">#<span class="hljs-meta-keyword">include</span> <span class="hljs-meta-string">&lt;opencv2/opencv.hpp&gt;</span></span><span class="hljs-meta">#<span class="hljs-meta-keyword">include</span> <span class="hljs-meta-string">&lt;iostream&gt;</span></span><span class="hljs-keyword">using</span> <span class="hljs-keyword">namespace</span> <span class="hljs-built_in">std</span>;<span class="hljs-keyword">using</span> <span class="hljs-keyword">namespace</span> cv;<span class="hljs-function"><span class="hljs-keyword">int</span> <span class="hljs-title">main</span><span class="hljs-params">()</span></span><span class="hljs-function"></span>&#123;    Mat <span class="hljs-built_in">image</span> = imread(<span class="hljs-string">"opencv.png"</span>);      imshow(<span class="hljs-string">"显示图像"</span>, <span class="hljs-built_in">image</span>);    waitKey(<span class="hljs-number">0</span>);    <span class="hljs-keyword">return</span> <span class="hljs-number">0</span>;&#125;</code></pre><h4 id="【运行环境】"><a href="#【运行环境】" class="headerlink" title="【运行环境】"></a>【运行环境】</h4><p><img src="http://eveseven.oss-cn-shanghai.aliyuncs.com/19-1-7/59828793.jpg" srcset="/img/loading.gif" alt="运行环境"></p><h4 id="【运行结果】"><a href="#【运行结果】" class="headerlink" title="【运行结果】"></a>【<strong>运行结果</strong>】</h4><p><img src="http://eveseven.oss-cn-shanghai.aliyuncs.com/19-1-7/83983634.jpg" srcset="/img/loading.gif" alt="运行结果"></p><h4 id="【环境搭建成功】"><a href="#【环境搭建成功】" class="headerlink" title="【环境搭建成功】"></a>【<strong>环境搭建成功</strong>】</h4><h3 id="配置属性表"><a href="#配置属性表" class="headerlink" title="配置属性表"></a>配置属性表</h3><blockquote><p>我们知道了如何对新建的项目进行属性配置，但若每次都从头进行一遍是十分繁琐的，所以可以通过<strong>【配置属性表并导出】</strong>，下次新建项目时<strong>【导入】</strong>，来减少这些不必要的重复劳动。</p></blockquote><h4 id="【新建属性表】"><a href="#【新建属性表】" class="headerlink" title="【新建属性表】"></a>【<strong>新建属性表</strong>】</h4><ul><li>以对【<strong>Debug|x64</strong>】操作为例，右击，选择<strong>【添加新项目属性表】</strong></li></ul><p><img src="http://eveseven.oss-cn-shanghai.aliyuncs.com/19-1-7/42846413.jpg" srcset="/img/loading.gif" alt="新建属性表"></p><ul><li>在跳出的窗口中选择<strong>【属性表】</strong>格式文件，对文件<strong>【命名】</strong>,<strong>【选择保存位置】</strong>，点击<strong>【添加】</strong>新建成功</li></ul><p><img src="http://eveseven.oss-cn-shanghai.aliyuncs.com/19-1-7/38050191.jpg" srcset="/img/loading.gif" alt="属性表"></p><ul><li><p>此时我们在<strong>【Debug|x64】</strong>目录下会看到创建好的属性表文件，对其双击打开，<code>以同样的方式配置</code><strong>【包含目录】</strong>、<strong>【库目录】</strong>、<strong>【附加依赖项】</strong>，保存，对于【<strong>Release|x64</strong>】同理</p><p><img src="http://eveseven.oss-cn-shanghai.aliyuncs.com/19-1-7/81378791.jpg" srcset="/img/loading.gif" alt="属性文件"></p></li><li><p>相应的位置即可找到这两个文件</p><p><img src="http://eveseven.oss-cn-shanghai.aliyuncs.com/19-1-7/31368933.jpg" srcset="/img/loading.gif" alt="属性文件"></p></li></ul><h4 id="【导入属性表】"><a href="#【导入属性表】" class="headerlink" title="【导入属性表】"></a>【<strong>导入属性表</strong>】</h4><p>以后<strong>再次新建项目</strong>只需在属性管理器中通过<strong>【添加现有属性表】</strong>导入即可完成配置</p><p><img src="http://eveseven.oss-cn-shanghai.aliyuncs.com/19-1-7/25516145.jpg" srcset="/img/loading.gif" alt="现有属性"></p>]]></content>
    
    
    <categories>
      
      <category>opencv</category>
      
    </categories>
    
    
    <tags>
      
      <tag>OpenCV</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>python第十五话之requests库及爬虫案例</title>
    <link href="/2018/12/python-reptile/"/>
    <url>/2018/12/python-reptile/</url>
    
    <content type="html"><![CDATA[<h3 id="Request库"><a href="#Request库" class="headerlink" title="Request库"></a>Request库</h3><blockquote><p>使用 Requests 发送网络请求非常简单。</p></blockquote><h4 id="requests-get"><a href="#requests-get" class="headerlink" title="requests.get()"></a>requests.get()</h4><pre><code class="hljs python">requests.get(url, params=<span class="hljs-literal">None</span>, **kwargs)</code></pre><blockquote><p>获取HTML网页的主要方法，对应于HTTP的GET.</p><p>构造一个向服务器请求资源的Request对象</p><p>返回一个包含服务器资源的Response对象.</p></blockquote><pre><code class="hljs python">help(requests.get)</code></pre><blockquote><p>Sends a GET request.<br>      :param url: URL for the new :class:<code>Request</code> object.<br>​    :param params: (optional) Dictionary or bytes to be sent in the query string for the :class:<code>Request</code>.<br>​    :param **kwargs: Optional arguments that <code>request</code> takes.<br>​    :return: :class:<code>Response &lt;Response&gt;</code> object<br>​    :rtype: requests.Response</p><hr><p>发送GET请求。</p><p>url：拟获取页面的url链接</p><p>params：url中的额外参数，字典或字节流格式，可选</p><p>**kwargs：12个控制访问的参数</p></blockquote><h5 id="普通get"><a href="#普通get" class="headerlink" title="普通get"></a>普通get</h5><pre><code class="hljs python">In [<span class="hljs-number">22</span>]: <span class="hljs-keyword">import</span> requestsIn [<span class="hljs-number">23</span>]: urlOut[<span class="hljs-number">23</span>]: <span class="hljs-string">'https://www.baidu.com'</span>In [<span class="hljs-number">24</span>]: r = requests.get(url)Out[<span class="hljs-number">24</span>]: &lt;Response [<span class="hljs-number">200</span>]&gt;In [<span class="hljs-number">27</span>]: r.urlOut[<span class="hljs-number">27</span>]: <span class="hljs-string">'https://www.baidu.com/'</span></code></pre><h5 id="带参get"><a href="#带参get" class="headerlink" title="带参get"></a>带参get</h5><pre><code class="hljs python">In [<span class="hljs-number">41</span>]: r = requests.get(url+<span class="hljs-string">'/s'</span>, params=&#123;<span class="hljs-string">'wd'</span>:<span class="hljs-string">'dog'</span>&#125;)In [<span class="hljs-number">42</span>]: r.urlOut[<span class="hljs-number">42</span>]: <span class="hljs-string">'https://www.baidu.com/s?wd=dog'</span></code></pre><h4 id="requests-head"><a href="#requests-head" class="headerlink" title="requests.head()"></a>requests.head()</h4><blockquote><p>获取HTML网页头信息的方法，对应于HTTP的HEAD</p></blockquote><pre><code class="hljs python">requests.head(url, **kwargs)</code></pre><blockquote><p>url：拟获取页面的url链接</p><p>  **kwargs：13个控制访问的参数</p></blockquote><h4 id="requests-post"><a href="#requests-post" class="headerlink" title="requests.post()"></a>requests.post()</h4><blockquote><p>向HTML网页提交POST请求的方法，对应于HTTP的POST</p></blockquote><pre><code class="hljs python">requests.post(url, data=<span class="hljs-literal">None</span>, json=<span class="hljs-literal">None</span>, **kwargs)</code></pre><blockquote><p>url：拟获取页面的url链接</p><p>  data：字典、字节序列或文件，Request的内容</p><p>  json：JSON格式的数据，Request的内容</p><p>  **kwargs：11个控制访问参数</p></blockquote><h4 id="requests-put"><a href="#requests-put" class="headerlink" title="requests.put()"></a>requests.put()</h4><blockquote><p>向HTML网页提交PUT请求的方法，对应于HTTP的PUT</p></blockquote><pre><code class="hljs python">requests.put(url, data=<span class="hljs-literal">None</span>, **kwargs)</code></pre><blockquote><p>url：拟更新页面的url链接</p><p>  data：字典、字节序列或文件，Request的内容</p><p>  **kwargs：12个控制访问参数</p></blockquote><h4 id="requests-patch"><a href="#requests-patch" class="headerlink" title="requests.patch()"></a>requests.patch()</h4><blockquote><p>向HTML网页提交局部修改请求，对应于HTTP的PATCH</p></blockquote><pre><code class="hljs python">requests.patch(url, data=<span class="hljs-literal">None</span>, **kwargs)</code></pre><blockquote><p>url：拟更新页面的url链接</p><p>  data：字典、字节序列或文件，Request的内容</p><p>  **kwargs：12个控制访问参数</p></blockquote><h4 id="requests-delete"><a href="#requests-delete" class="headerlink" title="requests.delete()"></a>requests.delete()</h4><blockquote><p>向HTML页面提交删除请求，对应于HTTP的DELETE</p></blockquote><pre><code class="hljs python">requests.delete(url, **kwargs)</code></pre><blockquote><p>url：拟删除页面的url链接</p><p>  **kwargs：13个控制访问参数</p></blockquote><h4 id="response对象的属性"><a href="#response对象的属性" class="headerlink" title="response对象的属性"></a>response对象的属性</h4><p><code>r.status_code</code></p><blockquote><p>HTTP请求的返回状态，200表示连接成功，404表示失败</p></blockquote><p><img src="http://eveseven.oss-cn-shanghai.aliyuncs.com/18-12-24/38874266.jpg" srcset="/img/loading.gif" alt="http状态码"></p><p><code>r.text</code></p><blockquote><p>HTTP响应内容的字符串形式，即，url对应的页面内容</p></blockquote><p><code>r.emcoding</code></p><blockquote><p>从HTTPheader中猜测的响应内容编码方式</p></blockquote><p><code>r.apparent_encoding</code></p><blockquote><p>从内容分析出的响应内容编码方式（备选编码方式）.</p></blockquote><p><code>r.content</code></p><blockquote><p>HTTP响应内容的二进制形式.</p></blockquote><h4 id="理解Response的编码"><a href="#理解Response的编码" class="headerlink" title="理解Response的编码"></a>理解Response的编码</h4><h5 id="r-encoding"><a href="#r-encoding" class="headerlink" title="r.encoding"></a>r.encoding</h5><blockquote><p>从HTTP header中猜测的响应内容编码方式</p><p>注意：如果header中不存在charset，则认为编码为ISO-8859-1。</p></blockquote><h5 id="r-apparent-encoding"><a href="#r-apparent-encoding" class="headerlink" title="r.apparent_encoding"></a>r.apparent_encoding</h5><blockquote><p>从内容中分析出的响应内容编码方式（备选编码方式）</p><p>注意：根据网页内容分析出的编码方式</p></blockquote><h4 id="理解Requests库的异常"><a href="#理解Requests库的异常" class="headerlink" title="理解Requests库的异常"></a>理解Requests库的异常</h4><h5 id="requests-ConnectionError"><a href="#requests-ConnectionError" class="headerlink" title="requests.ConnectionError"></a>requests.ConnectionError</h5><blockquote><p>网络连接错误异常，如DNS查询失败、拒绝连接等</p></blockquote><h5 id="requests-HTTPErrorHTTP"><a href="#requests-HTTPErrorHTTP" class="headerlink" title="requests.HTTPErrorHTTP"></a>requests.HTTPErrorHTTP</h5><blockquote><p>错误异常</p></blockquote><h5 id="requests-URLRequired"><a href="#requests-URLRequired" class="headerlink" title="requests.URLRequired"></a>requests.URLRequired</h5><blockquote><p>URL缺失异常</p></blockquote><h5 id="requests-TooMangRedirects"><a href="#requests-TooMangRedirects" class="headerlink" title="requests.TooMangRedirects"></a>requests.TooMangRedirects</h5><blockquote><p>超过最大重定向次数，产生重定向异常</p></blockquote><h5 id="requests-ConnectTimeout"><a href="#requests-ConnectTimeout" class="headerlink" title="requests.ConnectTimeout"></a>requests.ConnectTimeout</h5><blockquote><p>连接远程服务器超时异常</p></blockquote><h5 id="requests-Timeout"><a href="#requests-Timeout" class="headerlink" title="requests.Timeout"></a>requests.Timeout</h5><blockquote><p>请求URL超时，产生超时异常</p></blockquote><h5 id="r-raise-for-status"><a href="#r-raise-for-status" class="headerlink" title="r.raise_for_status()"></a>r.raise_for_status()</h5><blockquote><p>如果不是200，产生异常requests.HTTPError</p></blockquote><h4 id="爬取网页的通用代码框架"><a href="#爬取网页的通用代码框架" class="headerlink" title="爬取网页的通用代码框架"></a>爬取网页的通用代码框架</h4><pre><code class="hljs python"><span class="hljs-keyword">import</span> requests<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">getHTMLText</span><span class="hljs-params">(url)</span>:</span>    <span class="hljs-keyword">try</span>:        r = requests.get(url, timeout=<span class="hljs-number">30</span>)        r.raise_for_status() <span class="hljs-comment">#如果状态不是200，引发HTTPError异常#</span>        r.encoding = r.apparent_encoding        <span class="hljs-keyword">return</span> r.text    <span class="hljs-keyword">except</span>:        <span class="hljs-keyword">return</span> <span class="hljs-string">"产生异常"</span><span class="hljs-keyword">if</span> __name__ == <span class="hljs-string">"__main__"</span>:    url = <span class="hljs-string">"http://www.baidu.com"</span>    print(getHTMLText(url))</code></pre><h4 id="HTTP协议对资源的操作"><a href="#HTTP协议对资源的操作" class="headerlink" title="HTTP协议对资源的操作"></a>HTTP协议对资源的操作</h4><p><img src="http://eveseven.oss-cn-shanghai.aliyuncs.com/18-12-24/49709395.jpg" srcset="/img/loading.gif" alt="协议"></p><h4 id="request-request"><a href="#request-request" class="headerlink" title="request.request()"></a>request.request()</h4><blockquote><p>requests.request(method, url, **kwargs)</p><p>method：请求方式</p><p>url：拟获取页面的url链接</p></blockquote><pre><code class="hljs python">r = requests.request(<span class="hljs-string">'GET'</span>, url, **kwargs)r = requests.request(<span class="hljs-string">'HEAD'</span>, url, **kwargs)r = requests.request(<span class="hljs-string">'POST'</span>, url, **kwargs)r = requests.request(<span class="hljs-string">'PUT'</span>, url, **kwargs)r = requests.request(<span class="hljs-string">'PATCH'</span>, url, **kwargs)r = requests.request(<span class="hljs-string">'DELETE'</span>, url, **kwargs)r = requests.request(<span class="hljs-string">'OPTIONS'</span>, url, **kwargs)**kwargs：控制访问的参数，均为可选项，共<span class="hljs-number">13</span>个</code></pre><h4 id="超参数"><a href="#超参数" class="headerlink" title="超参数"></a>超参数</h4><h5 id="params"><a href="#params" class="headerlink" title="params"></a><code>params</code></h5><blockquote><p> 字典或字节序列，作为参数增加到url中</p></blockquote><pre><code class="hljs python">kv = &#123;<span class="hljs-string">'key1'</span>: <span class="hljs-string">'value1'</span>, <span class="hljs-string">'key2'</span>: <span class="hljs-string">'value2'</span>&#125;r = requests.request(<span class="hljs-string">'GET'</span>, <span class="hljs-string">'http://python123.io/ws'</span>,  params=kv)print(r.url)</code></pre><pre><code class="hljs python">https://python123.io/ws?key1=value1&amp;key2=value2</code></pre><h5 id="data"><a href="#data" class="headerlink" title="data"></a><code>data</code></h5><blockquote><p>字典、字节序列或文件对象，作为Request的对象</p></blockquote><pre><code class="hljs python">body = <span class="hljs-string">'主体内容'</span>.encode(<span class="hljs-string">'utf-8'</span>)r = requests.request(<span class="hljs-string">'POST'</span>, <span class="hljs-string">'http://python123.io/ws'</span>, data=body)print(r.url)</code></pre><pre><code class="hljs python">http://python123.io/ws</code></pre><h5 id="json"><a href="#json" class="headerlink" title="json"></a><code>json</code></h5><blockquote><p>JSON格式的数据，作为Request的内容</p></blockquote><pre><code class="hljs python">kv = &#123;<span class="hljs-string">'key1'</span>: <span class="hljs-string">'value1'</span>&#125;r = requests.request(<span class="hljs-string">'POST'</span>, <span class="hljs-string">'http://python123.io/ws'</span>, json=kv)print(r.url)</code></pre><pre><code class="hljs python">http://python123.io/ws</code></pre><p><code>headers</code></p><blockquote><p>字典，HTTP定制头</p></blockquote><pre><code class="hljs python">hd = &#123;<span class="hljs-string">'user-agent'</span>: <span class="hljs-string">'Chrome/10'</span>&#125;r = requests.request(<span class="hljs-string">'POST'</span>, <span class="hljs-string">'http://python123.io/ws'</span>, headers=hd)print(r.url)</code></pre><pre><code class="hljs python">http://python123.io/ws</code></pre><h5 id="cookies"><a href="#cookies" class="headerlink" title="cookies"></a><code>cookies</code></h5><blockquote><p>字典或CookieJar，Request中的cookie</p></blockquote><h5 id="auth"><a href="#auth" class="headerlink" title="auth"></a><code>auth</code></h5><blockquote><p>元组，支持HTTP认证功能</p></blockquote><h5 id="files"><a href="#files" class="headerlink" title="files"></a><code>files</code></h5><blockquote><p>字典类型，传输文件</p></blockquote><pre><code class="hljs python">fs = &#123;<span class="hljs-string">'file'</span>: open(<span class="hljs-string">'data.xls'</span>,<span class="hljs-string">'rb'</span>)&#125;r = requests.request(<span class="hljs-string">'POST'</span>, <span class="hljs-string">'http://python123.io/ws'</span>, files=fs)print(r.url)</code></pre><h5 id="timeout"><a href="#timeout" class="headerlink" title="timeout"></a><code>timeout</code></h5><blockquote><p> 设定超时时间，秒为单位</p></blockquote><pre><code class="hljs python">r = requests.request(<span class="hljs-string">'GET'</span>, <span class="hljs-string">'http://www.baidu.com'</span>, timeout=<span class="hljs-number">10</span>)</code></pre><h5 id="proxies"><a href="#proxies" class="headerlink" title="proxies"></a><code>proxies</code></h5><blockquote><p> 字典类型，设置访问代理服务器，可以增加登录认证</p></blockquote><pre><code class="hljs python">pxs = &#123;<span class="hljs-string">'http'</span>: <span class="hljs-string">'http://user:pass@10.10.10.1:1234'</span>                <span class="hljs-string">'https'</span>: <span class="hljs-string">'https://10.10.10.1:4321'</span>&#125;r = requests.request(<span class="hljs-string">'GET'</span>, <span class="hljs-string">'http://www.baidu.com'</span>, proxies=pxs)</code></pre><h5 id="allow-redirects"><a href="#allow-redirects" class="headerlink" title="allow_redirects"></a><code>allow_redirects</code></h5><blockquote><p>True/False，默认为Ture，重定向开关</p></blockquote><h5 id="stream"><a href="#stream" class="headerlink" title="stream"></a><code>stream</code></h5><blockquote><p>True/False，默认为True，获取内容立即下载开关</p></blockquote><h5 id="verify"><a href="#verify" class="headerlink" title="verify"></a><code>verify</code></h5><blockquote><p>True/False，默认为True，认证SSL证书开关</p></blockquote><h5 id="cert"><a href="#cert" class="headerlink" title="cert"></a><code>cert</code></h5><blockquote><p>本地SSL证书路径</p></blockquote><h3 id="爬虫案例"><a href="#爬虫案例" class="headerlink" title="爬虫案例"></a>爬虫案例</h3><h4 id="网站"><a href="#网站" class="headerlink" title="网站"></a>网站</h4><blockquote><p>实例网站：<a href="http://image.baidu.com/" target="_blank" rel="noopener">百度图片搜索</a></p></blockquote><h4 id="分析网页"><a href="#分析网页" class="headerlink" title="分析网页"></a>分析网页</h4><p>搜索关键字：猫</p><pre><code class="hljs python">https://image.baidu.com/search/index?tn=baiduimage&amp;ipn=r&amp;ct=201326592&amp;cl=2&amp;lm=-1&amp;st=-1&amp;sf=1&amp;fmq=&amp;pv=&amp;ic=0&amp;nc=1&amp;z=&amp;se=1&amp;showtab=0&amp;fb=0&amp;width=&amp;height=&amp;face=0&amp;istype=2&amp;ie=utf-8&amp;fm=index&amp;pos=history&amp;word=%E7%8C%AB</code></pre><p>搜索关键字：狗</p><pre><code class="hljs python">https://image.baidu.com/search/index?tn=baiduimage&amp;ipn=r&amp;ct=201326592&amp;cl=2&amp;lm=-1&amp;st=-1&amp;sf=1&amp;fmq=&amp;pv=&amp;ic=0&amp;nc=1&amp;z=&amp;se=1&amp;showtab=0&amp;fb=0&amp;width=&amp;height=&amp;face=0&amp;istype=2&amp;ie=utf-8&amp;fm=index&amp;pos=history&amp;word=%E7%8B%97</code></pre><blockquote><p>对比get的网址，发现只有搜索关键字不一样。</p></blockquote><h4 id="获得网页数据"><a href="#获得网页数据" class="headerlink" title="获得网页数据"></a>获得网页数据</h4><blockquote><p>我们就才有前面提到的通用的代码框架</p></blockquote><pre><code class="hljs python"><span class="hljs-keyword">import</span> requests<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">getHTMLText</span><span class="hljs-params">(url)</span>:</span>    <span class="hljs-keyword">try</span>:        r = requests.get(url, timeout=<span class="hljs-number">30</span>)        r.raise_for_status()  <span class="hljs-comment"># 如果状态不是200，引发HTTPError异常#</span>        r.encoding = r.apparent_encoding        <span class="hljs-keyword">return</span> r.text    <span class="hljs-keyword">except</span>:        <span class="hljs-keyword">return</span> <span class="hljs-string">"产生异常"</span><span class="hljs-keyword">if</span> __name__ == <span class="hljs-string">"__main__"</span>:    key_word = <span class="hljs-string">'小狗'</span>    url = <span class="hljs-string">'https://image.baidu.com/search/index?tn=baiduimage&amp;ipn=r&amp;ct=201326592&amp;cl=2&amp;lm=-1&amp;st=-1&amp;sf=1&amp;fmq=&amp;pv=&amp;ic=0'</span> \          <span class="hljs-string">'&amp;nc=1&amp;z=&amp;se=1&amp;showtab=0&amp;fb=0&amp;width=&amp;height=&amp;face=0&amp;istype=2&amp;ie=utf-8&amp;fm=index&amp;pos=history&amp;word='</span>    html = getHTMLText(url+key_word)    print(html)</code></pre><blockquote><p>通用代码框架加上我们刚刚分析网址get的请求网址，就可以获取到网页的数据。</p></blockquote><h4 id="处理数据"><a href="#处理数据" class="headerlink" title="处理数据"></a>处理数据</h4><blockquote><p>接下来就是处理数据，把我们所需要的图片网址匹配出来</p></blockquote><p>通过F12找到图片的网址–这个不是原图地址</p><p><img src="http://eveseven.oss-cn-shanghai.aliyuncs.com/18-12-24/44722566.jpg" srcset="/img/loading.gif" alt="F12"></p><p>右键 查看网页源代码，分析JSON数据可知，其中的字段objURL，即表示了原图的下载地址。</p><p>通过正则在代码里匹配出图片的网址</p><pre><code class="hljs python">pic_urls = re.findall(<span class="hljs-string">'"objURL":"(.*?)",'</span>, html, re.S)print(pic_urls)</code></pre><h4 id="展示结果"><a href="#展示结果" class="headerlink" title="展示结果"></a>展示结果</h4><pre><code class="hljs python"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">down</span><span class="hljs-params">(urls)</span>:</span>    <span class="hljs-keyword">for</span> i, url <span class="hljs-keyword">in</span> enumerate(urls):        <span class="hljs-keyword">try</span>:            pic = requests.get(url, timeout=<span class="hljs-number">15</span>)            string = str(i + <span class="hljs-number">1</span>) + <span class="hljs-string">'.jpg'</span>            <span class="hljs-keyword">with</span> open(string, <span class="hljs-string">'wb'</span>) <span class="hljs-keyword">as</span> f:                f.write(pic.content)                print(<span class="hljs-string">'成功下载第%s张图片: %s'</span> % (str(i + <span class="hljs-number">1</span>), str(url)))        <span class="hljs-keyword">except</span> Exception <span class="hljs-keyword">as</span> e:            print(<span class="hljs-string">'下载第%s张图片时失败: %s'</span> % (str(i + <span class="hljs-number">1</span>), str(url)))            print(e)            <span class="hljs-keyword">continue</span></code></pre><blockquote><p>通过访问我们获取到的图片网址，对图片进行保存。</p></blockquote><h4 id="完整代码"><a href="#完整代码" class="headerlink" title="完整代码"></a>完整代码</h4><pre><code class="hljs python"><span class="hljs-keyword">import</span> requests<span class="hljs-keyword">import</span> re<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">getHTMLText</span><span class="hljs-params">(url)</span>:</span>    <span class="hljs-string">"""</span><span class="hljs-string">    获取网页数据</span><span class="hljs-string">    :param url: 访问网址</span><span class="hljs-string">    :return: </span><span class="hljs-string">    """</span>    <span class="hljs-keyword">try</span>:        r = requests.get(url, timeout=<span class="hljs-number">30</span>)        r.raise_for_status()  <span class="hljs-comment"># 如果状态不是200，引发HTTPError异常#</span>        r.encoding = r.apparent_encoding        <span class="hljs-keyword">return</span> r.text    <span class="hljs-keyword">except</span>:        <span class="hljs-keyword">return</span> <span class="hljs-string">"产生异常"</span><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">down</span><span class="hljs-params">(urls)</span>:</span>    <span class="hljs-string">"""</span><span class="hljs-string">    下载图片</span><span class="hljs-string">    :param urls: 图片网址列表</span><span class="hljs-string">    :return: </span><span class="hljs-string">    """</span>    <span class="hljs-keyword">for</span> i, url <span class="hljs-keyword">in</span> enumerate(urls):        <span class="hljs-keyword">try</span>:            pic = requests.get(url, timeout=<span class="hljs-number">15</span>)            string = str(i + <span class="hljs-number">1</span>) + <span class="hljs-string">'.jpg'</span>            <span class="hljs-keyword">with</span> open(string, <span class="hljs-string">'wb'</span>) <span class="hljs-keyword">as</span> f:                f.write(pic.content)                print(<span class="hljs-string">'成功下载第%s张图片: %s'</span> % (str(i + <span class="hljs-number">1</span>), str(url)))        <span class="hljs-keyword">except</span> Exception <span class="hljs-keyword">as</span> e:            print(<span class="hljs-string">'下载第%s张图片时失败: %s'</span> % (str(i + <span class="hljs-number">1</span>), str(url)))            print(e)            <span class="hljs-keyword">continue</span><span class="hljs-keyword">if</span> __name__ == <span class="hljs-string">"__main__"</span>:    key_word = <span class="hljs-string">'小狗'</span>    url = <span class="hljs-string">'https://image.baidu.com/search/index?tn=baiduimage&amp;ipn=r&amp;ct=201326592&amp;cl=2&amp;lm=-1&amp;st=-1&amp;sf=1&amp;fmq=&amp;pv=&amp;ic=0'</span> \          <span class="hljs-string">'&amp;nc=1&amp;z=&amp;se=1&amp;showtab=0&amp;fb=0&amp;width=&amp;height=&amp;face=0&amp;istype=2&amp;ie=utf-8&amp;fm=index&amp;pos=history&amp;word='</span>    html = getHTMLText(url+key_word)    urls = re.findall(<span class="hljs-string">'"objURL":"(.*?)",'</span>, html, re.S)  <span class="hljs-comment"># 匹配原图地址</span>    print(len(urls))    down(urls)</code></pre>]]></content>
    
    
    <categories>
      
      <category>python</category>
      
    </categories>
    
    
    <tags>
      
      <tag>requests</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>python第十四话之正则</title>
    <link href="/2018/12/python-re/"/>
    <url>/2018/12/python-re/</url>
    
    <content type="html"><![CDATA[<h3 id="正则"><a href="#正则" class="headerlink" title="正则"></a>正则</h3><p>在实际的应用中，我们会经常得到用户的输入，在得到用户的输入之后，需要我们对输入进行判断时候合法，比如判断输入的手机号码，从形式上来看是争取的呢？</p><pre><code class="hljs python">mu = input(<span class="hljs-string">'请输入电话号码：'</span>)<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">phone_number</span><span class="hljs-params">(st)</span>:</span>    st = str(st)    <span class="hljs-keyword">if</span> len(st) == <span class="hljs-number">11</span> <span class="hljs-keyword">and</span> st.startswith(<span class="hljs-string">'1'</span>) <span class="hljs-keyword">and</span> st.isdigit() :        <span class="hljs-keyword">return</span> <span class="hljs-literal">True</span>    <span class="hljs-keyword">else</span>:        print(<span class="hljs-string">'Phone Number Error'</span>)        <span class="hljs-keyword">return</span> <span class="hljs-literal">False</span>print(phone_number(mu))</code></pre><pre><code class="hljs python">请输入电话号码：<span class="hljs-number">12332112345</span><span class="hljs-literal">True</span></code></pre><blockquote><p>那有什么简单的方法呢？</p></blockquote><h4 id="正则搜索"><a href="#正则搜索" class="headerlink" title="正则搜索"></a>正则搜索</h4><h5 id="match"><a href="#match" class="headerlink" title="match"></a><code>match</code></h5><pre><code class="hljs python"><span class="hljs-keyword">import</span> rea = <span class="hljs-string">'12345678900'</span>rm = re.match(<span class="hljs-string">r'1\d&#123;10&#125;'</span>,a)print(rm)</code></pre><pre><code class="hljs python">&lt;_sre.SRE_Match object; span=(<span class="hljs-number">0</span>, <span class="hljs-number">11</span>), match=<span class="hljs-string">'12345678900'</span>&gt;</code></pre><h5 id="search"><a href="#search" class="headerlink" title="search"></a><code>search</code></h5><pre><code class="hljs python"><span class="hljs-keyword">import</span> rea = <span class="hljs-string">'12345678900'</span>rs = re.search(<span class="hljs-string">r'1\d&#123;10&#125;'</span>,a)print(rs)</code></pre><pre><code class="hljs python">&lt;_sre.SRE_Match object; span=(<span class="hljs-number">0</span>, <span class="hljs-number">11</span>), match=<span class="hljs-string">'12345678900'</span>&gt;</code></pre><blockquote><p>通过对比，可以很明显的发现，下面这种方式能够简单快捷的匹配出电话号码</p></blockquote><h4 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h4><blockquote><p>正则表达式：正则表达式是一种通用的用来简洁表达一组字符串的表达式，因此，正则表达式是和python无关的，在其他的语言或者不同的系统中，是通用的。</p><p>匹配：通过正则表达式就可以去匹配现有的字符串。</p><p>应用：通过正则匹配，可以迅速的过滤出我们需要的全部或者一部分字符串，查找文本中的特质值(如：病毒)等等。</p></blockquote><h3 id="元字符"><a href="#元字符" class="headerlink" title="元字符"></a>元字符</h3><p>正则表达式该如何书写呢？</p><p>观察如下两个例子：</p><pre><code class="hljs python">In [<span class="hljs-number">3</span>]: re.search(<span class="hljs-string">'a'</span>, <span class="hljs-string">'abc'</span>)Out[<span class="hljs-number">3</span>]: &lt;_sre.SRE_Match object; span=(<span class="hljs-number">0</span>, <span class="hljs-number">1</span>), match=<span class="hljs-string">'a'</span>&gt;In [<span class="hljs-number">5</span>]: re.search(<span class="hljs-string">'.'</span>, <span class="hljs-string">'ab.cd.de'</span>)Out[<span class="hljs-number">5</span>]: &lt;_sre.SRE_Match object; span=(<span class="hljs-number">0</span>, <span class="hljs-number">1</span>), match=<span class="hljs-string">'a'</span>&gt;</code></pre><blockquote><p>在第一个例子中，可以匹配出a</p><p>但是下面这个不能匹配，但是下面这个没有匹配出点，而是匹配到 a </p><p>这个 .  不是不能匹配到点，而是匹配任意字符，这个点已经被赋予了特殊的含义，  .(点)就是一个元字符</p></blockquote><p><img src="http://eveseven.oss-cn-shanghai.aliyuncs.com/18-12-21/81430927.jpg" srcset="/img/loading.gif" alt="元字符"></p><blockquote><p>正因为有这些元字符的存在，正则表达式才变得强大.</p></blockquote><p><img src="http://eveseven.oss-cn-shanghai.aliyuncs.com/18-12-21/73314241.jpg" srcset="/img/loading.gif" alt="元字符含义"></p><h4 id="b"><a href="#b" class="headerlink" title="\b"></a>\b</h4><pre><code class="hljs python">In [<span class="hljs-number">23</span>]: re.search(<span class="hljs-string">'\bs\b'</span>, <span class="hljs-string">'abcdsd s we'</span>)In [<span class="hljs-number">24</span>]: re.search(<span class="hljs-string">r'\bs\b'</span>, <span class="hljs-string">'abcdsd s we'</span>)Out[<span class="hljs-number">24</span>]: &lt;_sre.SRE_Match object; span=(<span class="hljs-number">7</span>, <span class="hljs-number">8</span>), match=<span class="hljs-string">'s'</span>&gt;</code></pre><blockquote><p>在正则中，加上r 去掉字符串的转义，以免影响正则的使用</p></blockquote><blockquote><p>\b匹配一个单词边界，也就是指单词和空格间的位置。</p></blockquote><h4 id=""><a href="#" class="headerlink" title="."></a>.</h4><pre><code class="hljs python">In [<span class="hljs-number">26</span>]: re.search(<span class="hljs-string">r'.'</span>, <span class="hljs-string">'abcdsd s we'</span>)Out[<span class="hljs-number">26</span>]: &lt;_sre.SRE_Match object; span=(<span class="hljs-number">0</span>, <span class="hljs-number">1</span>), match=<span class="hljs-string">'a'</span>&gt;In [<span class="hljs-number">27</span>]: re.search(<span class="hljs-string">r'.'</span>, <span class="hljs-string">'\nabcdsd s we'</span>)Out[<span class="hljs-number">27</span>]: &lt;_sre.SRE_Match object; span=(<span class="hljs-number">1</span>, <span class="hljs-number">2</span>), match=<span class="hljs-string">'a'</span>&gt;</code></pre><blockquote><p>匹配除换行符之外的所有的字符</p></blockquote><h4 id="d"><a href="#d" class="headerlink" title="\d"></a>\d</h4><pre><code class="hljs python">In [<span class="hljs-number">28</span>]: re.search(<span class="hljs-string">r'\d'</span>,<span class="hljs-string">r'abc141342d'</span>)Out[<span class="hljs-number">28</span>]: &lt;_sre.SRE_Match object; span=(<span class="hljs-number">3</span>, <span class="hljs-number">4</span>), match=<span class="hljs-string">'1'</span>&gt;</code></pre><blockquote><p>匹配0~9的数字   </p></blockquote><h4 id="s"><a href="#s" class="headerlink" title="\s"></a>\s</h4><pre><code class="hljs python">In [<span class="hljs-number">30</span>]: re.search(<span class="hljs-string">r'\s'</span>,<span class="hljs-string">r'abc 141342d'</span>)Out[<span class="hljs-number">30</span>]: &lt;_sre.SRE_Match object; span=(<span class="hljs-number">3</span>, <span class="hljs-number">4</span>), match=<span class="hljs-string">' '</span>&gt;</code></pre><blockquote><p>匹配任意的空白符，包括空格，制表符(Tab)，换行符等</p></blockquote><h4 id="w"><a href="#w" class="headerlink" title="\w"></a>\w</h4><pre><code class="hljs python">In [<span class="hljs-number">31</span>]: re.search(<span class="hljs-string">r'\w'</span>,<span class="hljs-string">r'abc 141342d'</span>)Out[<span class="hljs-number">31</span>]: &lt;_sre.SRE_Match object; span=(<span class="hljs-number">0</span>, <span class="hljs-number">1</span>), match=<span class="hljs-string">'a'</span>&gt;</code></pre><blockquote><p>匹配字母或数字或下划线或汉字等 \b   表示单词的边界</p></blockquote><pre><code class="hljs python">In [<span class="hljs-number">32</span>]: re.search(<span class="hljs-string">r'\bc\b'</span>,<span class="hljs-string">r'abc c 342d'</span>)Out[<span class="hljs-number">32</span>]: &lt;_sre.SRE_Match object; span=(<span class="hljs-number">4</span>, <span class="hljs-number">5</span>), match=<span class="hljs-string">'c'</span>&gt;In [<span class="hljs-number">33</span>]: re.search(<span class="hljs-string">r'\bbcb\b'</span>,<span class="hljs-string">r'abc bcb 342d'</span>)Out[<span class="hljs-number">33</span>]: &lt;_sre.SRE_Match object; span=(<span class="hljs-number">4</span>, <span class="hljs-number">7</span>), match=<span class="hljs-string">'bcb'</span>&gt;</code></pre><h4 id="-1"><a href="#-1" class="headerlink" title="\."></a>\.</h4><pre><code class="hljs python">In [<span class="hljs-number">34</span>]: re.search(<span class="hljs-string">r'\.'</span>,<span class="hljs-string">r'ab.c .bcb 342d'</span>)Out[<span class="hljs-number">34</span>]: &lt;_sre.SRE_Match object; span=(<span class="hljs-number">2</span>, <span class="hljs-number">3</span>), match=<span class="hljs-string">'.'</span>&gt;</code></pre><blockquote><p>表示匹配点号本身</p></blockquote><h4 id="D、-S、-W、-B"><a href="#D、-S、-W、-B" class="headerlink" title="\D、\S、\W、\B"></a>\D、\S、\W、\B</h4><pre><code class="hljs python">In [<span class="hljs-number">35</span>]: re.search(<span class="hljs-string">r'\D'</span>,<span class="hljs-string">'abc.1213'</span>)Out[<span class="hljs-number">35</span>]: &lt;_sre.SRE_Match object; span=(<span class="hljs-number">0</span>, <span class="hljs-number">1</span>), match=<span class="hljs-string">'a'</span>&gt;In [<span class="hljs-number">36</span>]: re.search(<span class="hljs-string">r'\S'</span>,<span class="hljs-string">'abc.1213'</span>)Out[<span class="hljs-number">36</span>]: &lt;_sre.SRE_Match object; span=(<span class="hljs-number">0</span>, <span class="hljs-number">1</span>), match=<span class="hljs-string">'a'</span>&gt;In [<span class="hljs-number">37</span>]: re.search(<span class="hljs-string">r'\W'</span>,<span class="hljs-string">'abc.1213'</span>)Out[<span class="hljs-number">37</span>]: &lt;_sre.SRE_Match object; span=(<span class="hljs-number">3</span>, <span class="hljs-number">4</span>), match=<span class="hljs-string">'.'</span>&gt;In [<span class="hljs-number">38</span>]: re.search(<span class="hljs-string">r'\B'</span>,<span class="hljs-string">'abc.1213'</span>)Out[<span class="hljs-number">38</span>]: &lt;_sre.SRE_Match object; span=(<span class="hljs-number">1</span>, <span class="hljs-number">1</span>), match=<span class="hljs-string">''</span>&gt;</code></pre><blockquote><p>是与小写的相反的作用</p></blockquote><h4 id="D"><a href="#D" class="headerlink" title="\D"></a>\D</h4><pre><code class="hljs python">In [<span class="hljs-number">39</span>]: re.search(<span class="hljs-string">r'\D'</span>,<span class="hljs-string">'abc.1213'</span>)Out[<span class="hljs-number">39</span>]: &lt;_sre.SRE_Match object; span=(<span class="hljs-number">0</span>, <span class="hljs-number">1</span>), match=<span class="hljs-string">'a'</span>&gt;</code></pre><blockquote><p>除了数字以外的字符</p></blockquote><h4 id="-2"><a href="#-2" class="headerlink" title="^"></a>^</h4><pre><code class="hljs python">In [<span class="hljs-number">41</span>]: re.search(<span class="hljs-string">r'^ab'</span>,<span class="hljs-string">r'abc 141342d'</span>)Out[<span class="hljs-number">41</span>]: &lt;_sre.SRE_Match object; span=(<span class="hljs-number">0</span>, <span class="hljs-number">2</span>), match=<span class="hljs-string">'ab'</span>&gt;</code></pre><blockquote><p>脱字符，匹配输入字符串的开始的位置</p></blockquote><h4 id="-3"><a href="#-3" class="headerlink" title="$"></a>$</h4><pre><code class="hljs python">In [<span class="hljs-number">42</span>]: re.search(<span class="hljs-string">r'd$'</span>,<span class="hljs-string">r'abc 141342d'</span>)Out[<span class="hljs-number">42</span>]: &lt;_sre.SRE_Match object; span=(<span class="hljs-number">10</span>, <span class="hljs-number">11</span>), match=<span class="hljs-string">'d'</span>&gt;</code></pre><blockquote><p>匹配输入字符串的结束位置</p></blockquote><h4 id="-4"><a href="#-4" class="headerlink" title="{}"></a>{}</h4><pre><code class="hljs python">In [<span class="hljs-number">43</span>]: re.search(<span class="hljs-string">r'\d&#123;1,3&#125;'</span>,<span class="hljs-string">r'abc 141 qw 342d'</span>)  <span class="hljs-comment"># 对象，找到一个就不找了</span>Out[<span class="hljs-number">43</span>]: &lt;_sre.SRE_Match object; span=(<span class="hljs-number">4</span>, <span class="hljs-number">7</span>), match=<span class="hljs-string">'141'</span>&gt;In [<span class="hljs-number">44</span>]: re.findall(<span class="hljs-string">r'\d&#123;1,3&#125;'</span>,<span class="hljs-string">r'abc 141 qw 342d'</span>)  <span class="hljs-comment">#列表，全部找出来</span>Out[<span class="hljs-number">44</span>]: [<span class="hljs-string">'141'</span>, <span class="hljs-string">'342'</span>]In [<span class="hljs-number">45</span>]: re.findall(<span class="hljs-string">r'\d&#123;1,&#125;'</span>,<span class="hljs-string">r'abc 141 qw 34325252d'</span>)Out[<span class="hljs-number">45</span>]: [<span class="hljs-string">'141'</span>, <span class="hljs-string">'34325252'</span>]In [<span class="hljs-number">46</span>]: re.findall(<span class="hljs-string">r'\d&#123;,5&#125;'</span>,<span class="hljs-string">r'abc 141 qw 34325252d'</span>)Out[<span class="hljs-number">46</span>]: [<span class="hljs-string">''</span>, <span class="hljs-string">''</span>, <span class="hljs-string">''</span>, <span class="hljs-string">''</span>, <span class="hljs-string">'141'</span>, <span class="hljs-string">''</span>, <span class="hljs-string">''</span>, <span class="hljs-string">''</span>, <span class="hljs-string">''</span>, <span class="hljs-string">'34325'</span>, <span class="hljs-string">'252'</span>, <span class="hljs-string">''</span>, <span class="hljs-string">''</span>]In [<span class="hljs-number">47</span>]: re.findall(<span class="hljs-string">r'\d&#123;0,3&#125;'</span>,<span class="hljs-string">r'abc 141 qw 34325252d'</span>)Out[<span class="hljs-number">47</span>]: [<span class="hljs-string">''</span>, <span class="hljs-string">''</span>, <span class="hljs-string">''</span>, <span class="hljs-string">''</span>, <span class="hljs-string">'141'</span>, <span class="hljs-string">''</span>, <span class="hljs-string">''</span>, <span class="hljs-string">''</span>, <span class="hljs-string">''</span>, <span class="hljs-string">'343'</span>, <span class="hljs-string">'252'</span>, <span class="hljs-string">'52'</span>, <span class="hljs-string">''</span>, <span class="hljs-string">''</span>]</code></pre><blockquote><p>{M,N}   M和N 为非负整数，其中M&lt;=N 表示前面的匹配M~N次</p><p>{M，}   表示需要匹配M次</p><p>{，N}    等价于{0~N}</p><p>{N}       表示需要匹配N次</p></blockquote><h4 id="-5"><a href="#-5" class="headerlink" title="*"></a>*</h4><pre><code class="hljs python">In [<span class="hljs-number">49</span>]: re.findall(<span class="hljs-string">r'\d*'</span>,<span class="hljs-string">r'abc 141 qw 34325252d'</span>)Out[<span class="hljs-number">49</span>]: [<span class="hljs-string">''</span>, <span class="hljs-string">''</span>, <span class="hljs-string">''</span>, <span class="hljs-string">''</span>, <span class="hljs-string">'141'</span>, <span class="hljs-string">''</span>, <span class="hljs-string">''</span>, <span class="hljs-string">''</span>, <span class="hljs-string">''</span>, <span class="hljs-string">'34325252'</span>, <span class="hljs-string">''</span>, <span class="hljs-string">''</span>]In [<span class="hljs-number">50</span>]: re.findall(<span class="hljs-string">r'\d&#123;0,&#125;'</span>,<span class="hljs-string">r'abc 141 qw 34325252d'</span>)Out[<span class="hljs-number">50</span>]: [<span class="hljs-string">''</span>, <span class="hljs-string">''</span>, <span class="hljs-string">''</span>, <span class="hljs-string">''</span>, <span class="hljs-string">'141'</span>, <span class="hljs-string">''</span>, <span class="hljs-string">''</span>, <span class="hljs-string">''</span>, <span class="hljs-string">''</span>, <span class="hljs-string">'34325252'</span>, <span class="hljs-string">''</span>, <span class="hljs-string">''</span>]</code></pre><blockquote><p>匹配前面的子表达式零次或多次，等价于{0，}</p></blockquote><h4 id="-6"><a href="#-6" class="headerlink" title="+"></a>+</h4><pre><code class="hljs python">In [<span class="hljs-number">51</span>]: re.findall(<span class="hljs-string">r'\d+'</span>,<span class="hljs-string">r'abc 141 qw 34325252d'</span>)Out[<span class="hljs-number">51</span>]: [<span class="hljs-string">'141'</span>, <span class="hljs-string">'34325252'</span>]In [<span class="hljs-number">52</span>]: re.findall(<span class="hljs-string">r'\d&#123;1,&#125;'</span>,<span class="hljs-string">r'abc 141 qw 34325252d'</span>)Out[<span class="hljs-number">52</span>]: [<span class="hljs-string">'141'</span>, <span class="hljs-string">'34325252'</span>]</code></pre><blockquote><p>匹配前面的子表达式一次或多次，等价于{1，} </p></blockquote><h4 id="-7"><a href="#-7" class="headerlink" title="?"></a>?</h4><pre><code class="hljs python">In [<span class="hljs-number">57</span>]: re.findall(<span class="hljs-string">r'\d&#123;0,1&#125;'</span>,<span class="hljs-string">r'ab5252d'</span>)Out[<span class="hljs-number">57</span>]: [<span class="hljs-string">''</span>, <span class="hljs-string">''</span>, <span class="hljs-string">'5'</span>, <span class="hljs-string">'2'</span>, <span class="hljs-string">'5'</span>, <span class="hljs-string">'2'</span>, <span class="hljs-string">''</span>, <span class="hljs-string">''</span>]In [<span class="hljs-number">58</span>]: re.findall(<span class="hljs-string">r'\d?'</span>,<span class="hljs-string">r'ab5252d'</span>)Out[<span class="hljs-number">58</span>]: [<span class="hljs-string">''</span>, <span class="hljs-string">''</span>, <span class="hljs-string">'5'</span>, <span class="hljs-string">'2'</span>, <span class="hljs-string">'5'</span>, <span class="hljs-string">'2'</span>, <span class="hljs-string">''</span>, <span class="hljs-string">''</span>]</code></pre><blockquote><p>匹配前面的子表达式零次或一次，等价于{0,1}</p></blockquote><h4 id="贪婪与非贪婪"><a href="#贪婪与非贪婪" class="headerlink" title="贪婪与非贪婪"></a>贪婪与非贪婪</h4><h5 id="、-？"><a href="#、-？" class="headerlink" title="*?、+？"></a>*?、+？</h5><pre><code class="hljs python">In [<span class="hljs-number">61</span>]: re.findall(<span class="hljs-string">r'\d*?'</span>,<span class="hljs-string">r'ab5252d'</span>)Out[<span class="hljs-number">61</span>]: [<span class="hljs-string">''</span>, <span class="hljs-string">''</span>, <span class="hljs-string">''</span>, <span class="hljs-string">''</span>, <span class="hljs-string">''</span>, <span class="hljs-string">''</span>, <span class="hljs-string">''</span>, <span class="hljs-string">''</span>]In [<span class="hljs-number">62</span>]: re.findall(<span class="hljs-string">r'\d+?'</span>,<span class="hljs-string">r'ab5252d'</span>)Out[<span class="hljs-number">62</span>]: [<span class="hljs-string">'5'</span>, <span class="hljs-string">'2'</span>, <span class="hljs-string">'5'</span>, <span class="hljs-string">'2'</span>]</code></pre><pre><code class="hljs python">In [<span class="hljs-number">65</span>]: st =<span class="hljs-string">"&lt;html&gt;aaaa&lt;/html&gt;&lt;td&gt;bbbb&lt;/td&gt;"</span>In [<span class="hljs-number">66</span>]: re.findall(<span class="hljs-string">r'&lt;.*&gt;'</span>,st)Out[<span class="hljs-number">66</span>]: [<span class="hljs-string">'&lt;html&gt;aaaa&lt;/html&gt;&lt;td&gt;bbbb&lt;/td&gt;'</span>]In [<span class="hljs-number">67</span>]: re.findall(<span class="hljs-string">r'&lt;.*?&gt;'</span>,st)Out[<span class="hljs-number">67</span>]: [<span class="hljs-string">'&lt;html&gt;'</span>, <span class="hljs-string">'&lt;/html&gt;'</span>, <span class="hljs-string">'&lt;td&gt;'</span>, <span class="hljs-string">'&lt;/td&gt;'</span>]</code></pre><blockquote><p>在非贪婪模式下，始终找最短匹配</p></blockquote><h4 id="字符集合"><a href="#字符集合" class="headerlink" title="[]字符集合"></a>[]字符集合</h4><blockquote><p>[]   字符类，将要匹配的一类字符集放在[]里面</p></blockquote><pre><code class="hljs python">In [<span class="hljs-number">68</span>]: re.findall(<span class="hljs-string">r'[\d]'</span>,<span class="hljs-string">r'abc 141 qw 34325252d'</span>)Out[<span class="hljs-number">68</span>]: [<span class="hljs-string">'1'</span>, <span class="hljs-string">'4'</span>, <span class="hljs-string">'1'</span>, <span class="hljs-string">'3'</span>, <span class="hljs-string">'4'</span>, <span class="hljs-string">'3'</span>, <span class="hljs-string">'2'</span>, <span class="hljs-string">'5'</span>, <span class="hljs-string">'2'</span>, <span class="hljs-string">'5'</span>, <span class="hljs-string">'2'</span>]In [<span class="hljs-number">69</span>]: re.findall(<span class="hljs-string">r'[0-9]'</span>,<span class="hljs-string">r'abc 141 qw 34325252d'</span>)Out[<span class="hljs-number">69</span>]: [<span class="hljs-string">'1'</span>, <span class="hljs-string">'4'</span>, <span class="hljs-string">'1'</span>, <span class="hljs-string">'3'</span>, <span class="hljs-string">'4'</span>, <span class="hljs-string">'3'</span>, <span class="hljs-string">'2'</span>, <span class="hljs-string">'5'</span>, <span class="hljs-string">'2'</span>, <span class="hljs-string">'5'</span>, <span class="hljs-string">'2'</span>]In [<span class="hljs-number">70</span>]: re.findall(<span class="hljs-string">r'[a-z]'</span>,<span class="hljs-string">r'abc 141 qw 34325252d'</span>)Out[<span class="hljs-number">70</span>]: [<span class="hljs-string">'a'</span>, <span class="hljs-string">'b'</span>, <span class="hljs-string">'c'</span>, <span class="hljs-string">'q'</span>, <span class="hljs-string">'w'</span>, <span class="hljs-string">'d'</span>]</code></pre><blockquote><p>[ . ? * ( ) {} ]          匹配里面的这些符号</p><p>[0-9]                    匹配0到9的数字相当于\d</p><p>[^\d]                    匹配除数字以外的字符，相当于\D取反的意思</p><p>[a-z]                    匹配所有的小写字母</p><p>[^a-z]                   匹配非小写字母</p><p> |                          相当于或（or）分支条件</p></blockquote><h4 id="分组匹配"><a href="#分组匹配" class="headerlink" title="()分组匹配"></a>()分组匹配</h4><blockquote><p>() 分组，将要匹配的一类字符集放在()组成一个小组</p></blockquote><pre><code class="hljs python">In [<span class="hljs-number">75</span>]: re.findall(<span class="hljs-string">r'(32)'</span>,<span class="hljs-string">r'abc 141 qw 34325252d'</span>)Out[<span class="hljs-number">75</span>]: [<span class="hljs-string">'32'</span>]In [<span class="hljs-number">76</span>]: re.findall(<span class="hljs-string">r'a(3|2)'</span>,<span class="hljs-string">r'a3 a2 a23 '</span>)Out[<span class="hljs-number">76</span>]: [<span class="hljs-string">'3'</span>, <span class="hljs-string">'2'</span>, <span class="hljs-string">'2'</span>]In [<span class="hljs-number">77</span>]: re.findall(<span class="hljs-string">r'a([32])'</span>,<span class="hljs-string">r'a3 a2 a23 '</span>)Out[<span class="hljs-number">77</span>]: [<span class="hljs-string">'3'</span>, <span class="hljs-string">'2'</span>, <span class="hljs-string">'2'</span>]</code></pre><blockquote><p>分组匹配匹配() 内的字符串组合</p></blockquote><h3 id="re模块"><a href="#re模块" class="headerlink" title="re模块"></a>re模块</h3><h4 id="compile"><a href="#compile" class="headerlink" title="compile"></a>compile</h4><blockquote><p>编译正则表达式为模式对象</p><p>当正则表达式多次使用，可以采用这种方式</p></blockquote><pre><code class="hljs python">In [<span class="hljs-number">78</span>]: r = re.compile(<span class="hljs-string">r'\das'</span>)In [<span class="hljs-number">79</span>]: r.findall(<span class="hljs-string">'1as234'</span>)Out[<span class="hljs-number">79</span>]: [<span class="hljs-string">'1as'</span>]</code></pre><h4 id="sub"><a href="#sub" class="headerlink" title="sub"></a>sub</h4><blockquote><p>字符串替换</p></blockquote><pre><code class="hljs python">In [<span class="hljs-number">80</span>]: re.sub(<span class="hljs-string">'i'</span>,<span class="hljs-string">'o'</span>,<span class="hljs-string">'pythin***pythin'</span>,<span class="hljs-number">1</span>)Out[<span class="hljs-number">80</span>]: <span class="hljs-string">'python***pythin'</span>In [<span class="hljs-number">81</span>]: re.sub(<span class="hljs-string">'i'</span>,<span class="hljs-string">'o'</span>,<span class="hljs-string">'pythin***pythin'</span>,<span class="hljs-number">2</span>)Out[<span class="hljs-number">81</span>]: <span class="hljs-string">'python***python'</span>In [<span class="hljs-number">82</span>]: re.sub(<span class="hljs-string">'i'</span>,<span class="hljs-string">'o'</span>,<span class="hljs-string">'pythin***pythin'</span>)Out[<span class="hljs-number">82</span>]: <span class="hljs-string">'python***python'</span></code></pre><h4 id="match-1"><a href="#match-1" class="headerlink" title="match"></a>match</h4><blockquote><p>从字符串开始位置匹配</p></blockquote><pre><code class="hljs python">In [<span class="hljs-number">83</span>]: re.match(<span class="hljs-string">r'\d'</span>,<span class="hljs-string">'123ad'</span>)Out[<span class="hljs-number">83</span>]: &lt;_sre.SRE_Match object; span=(<span class="hljs-number">0</span>, <span class="hljs-number">1</span>), match=<span class="hljs-string">'1'</span>&gt;In [<span class="hljs-number">84</span>]: re.match(<span class="hljs-string">r'\d'</span>,<span class="hljs-string">'a123ad'</span>)</code></pre><h4 id="group"><a href="#group" class="headerlink" title="group"></a>group</h4><blockquote><p>得到匹配到的元素</p></blockquote><pre><code class="hljs python">In [<span class="hljs-number">94</span>]: re.search(<span class="hljs-string">r'\d&#123;1,3&#125;'</span>,<span class="hljs-string">r'abc 141 qw 342d'</span>)Out[<span class="hljs-number">94</span>]: &lt;_sre.SRE_Match object; span=(<span class="hljs-number">4</span>, <span class="hljs-number">7</span>), match=<span class="hljs-string">'141'</span>&gt;In [<span class="hljs-number">95</span>]: li = re.search(<span class="hljs-string">r'\d&#123;1,3&#125;'</span>,<span class="hljs-string">r'abc 141 qw 342d'</span>)In [<span class="hljs-number">96</span>]: li.group()Out[<span class="hljs-number">96</span>]: <span class="hljs-string">'141'</span></code></pre><h4 id="start"><a href="#start" class="headerlink" title="start"></a>start</h4><blockquote><p>得到开始位置</p></blockquote><pre><code class="hljs python">In [<span class="hljs-number">97</span>]: li.start()Out[<span class="hljs-number">97</span>]: <span class="hljs-number">4</span></code></pre><h4 id="end"><a href="#end" class="headerlink" title="end"></a>end</h4><blockquote><p>得到结束位置</p></blockquote><pre><code class="hljs python">In [<span class="hljs-number">98</span>]: li.end()Out[<span class="hljs-number">98</span>]: <span class="hljs-number">7</span></code></pre><h4 id="span"><a href="#span" class="headerlink" title="span"></a>span</h4><blockquote><p>得到位置范围</p></blockquote><pre><code class="hljs python">In [<span class="hljs-number">99</span>]: li.span()Out[<span class="hljs-number">99</span>]: (<span class="hljs-number">4</span>, <span class="hljs-number">7</span>)</code></pre><h4 id="注意："><a href="#注意：" class="headerlink" title="注意："></a>注意：</h4><blockquote><p>这几个方法在search中也存在</p></blockquote><h4 id="扩展"><a href="#扩展" class="headerlink" title="扩展"></a>扩展</h4><blockquote><p>在re中也有和字符串一样的split方法</p></blockquote><pre><code class="hljs python">In [<span class="hljs-number">85</span>]: re.split(<span class="hljs-string">r'\s'</span>,<span class="hljs-string">' cee'</span>)Out[<span class="hljs-number">85</span>]: [<span class="hljs-string">''</span>, <span class="hljs-string">'cee'</span>]In [<span class="hljs-number">86</span>]: re.split(<span class="hljs-string">r'\s'</span>,<span class="hljs-string">'aa bb cc dd'</span>)Out[<span class="hljs-number">86</span>]: [<span class="hljs-string">'aa'</span>, <span class="hljs-string">'bb'</span>, <span class="hljs-string">'cc'</span>, <span class="hljs-string">'dd'</span>]In [<span class="hljs-number">87</span>]: re.split(<span class="hljs-string">r'[\s|c]'</span>,<span class="hljs-string">'aa bb c dd ee'</span>)Out[<span class="hljs-number">87</span>]: [<span class="hljs-string">'aa'</span>, <span class="hljs-string">'bb'</span>, <span class="hljs-string">''</span>, <span class="hljs-string">''</span>, <span class="hljs-string">'dd'</span>, <span class="hljs-string">'ee'</span>]</code></pre>]]></content>
    
    
    <categories>
      
      <category>python</category>
      
    </categories>
    
    
    <tags>
      
      <tag>正则</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>python第十三话之列表推导式、迭代器生成器，模块和包</title>
    <link href="/2018/12/python-model/"/>
    <url>/2018/12/python-model/</url>
    
    <content type="html"><![CDATA[<h3 id="推导表达式"><a href="#推导表达式" class="headerlink" title="推导表达式"></a>推导表达式</h3><blockquote><p>得到一个元素为1到10的列表，可以怎么做？</p></blockquote><h4 id="列表"><a href="#列表" class="headerlink" title="列表"></a>列表</h4><h5 id="循环添加"><a href="#循环添加" class="headerlink" title="循环添加"></a><strong>循环添加</strong></h5><p><strong>方法一：</strong></p><pre><code class="hljs python">x = list(range(<span class="hljs-number">1</span>,<span class="hljs-number">11</span>))</code></pre><pre><code class="hljs python">xOut[<span class="hljs-number">3</span>]: [<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>, <span class="hljs-number">4</span>, <span class="hljs-number">5</span>, <span class="hljs-number">6</span>, <span class="hljs-number">7</span>, <span class="hljs-number">8</span>, <span class="hljs-number">9</span>, <span class="hljs-number">10</span>]</code></pre><p><strong>方法二：</strong></p><pre><code class="hljs python">li = []<span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> range(<span class="hljs-number">1</span>,<span class="hljs-number">11</span>):    li.append(i)</code></pre><pre><code class="hljs python">liOut[<span class="hljs-number">3</span>]: [<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>, <span class="hljs-number">4</span>, <span class="hljs-number">5</span>, <span class="hljs-number">6</span>, <span class="hljs-number">7</span>, <span class="hljs-number">8</span>, <span class="hljs-number">9</span>, <span class="hljs-number">10</span>]</code></pre><blockquote><p>这是我们前面所总结过的生成一个列表的方法。</p></blockquote><h5 id="列表推导"><a href="#列表推导" class="headerlink" title="列表推导"></a><strong>列表推导</strong></h5><pre><code class="hljs python">li = [i <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> range(<span class="hljs-number">1</span>,<span class="hljs-number">11</span>)]</code></pre><pre><code class="hljs python">liOut[<span class="hljs-number">3</span>]: [<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>, <span class="hljs-number">4</span>, <span class="hljs-number">5</span>, <span class="hljs-number">6</span>, <span class="hljs-number">7</span>, <span class="hljs-number">8</span>, <span class="hljs-number">9</span>, <span class="hljs-number">10</span>]</code></pre><blockquote><p>通过列表推导，精简了代码。</p></blockquote><h5 id="运行时间对比"><a href="#运行时间对比" class="headerlink" title="运行时间对比"></a>运行时间对比</h5><pre><code class="hljs python"><span class="hljs-keyword">import</span> times = time.time()<span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> range(<span class="hljs-number">1000000</span>):    x = list(range(<span class="hljs-number">1</span>,<span class="hljs-number">11</span>))e = time.time()print(<span class="hljs-string">'list: '</span>, e-s)f = time.time()<span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> range(<span class="hljs-number">1000000</span>):    li = []    <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> range(<span class="hljs-number">1</span>,<span class="hljs-number">11</span>):        li.append(i)end = time.time()print(<span class="hljs-string">'append: '</span>, end-f)ff = time.time()<span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> range(<span class="hljs-number">1000000</span>):    li = [i <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> range(<span class="hljs-number">1</span>,<span class="hljs-number">11</span>)]end1 = time.time()print(<span class="hljs-string">'[]: '</span>, end1-ff)</code></pre><pre><code class="hljs python">list:  <span class="hljs-number">0.7910020351409912</span>append:  <span class="hljs-number">2.110013246536255</span>[]:  <span class="hljs-number">1.0232622623443604</span></code></pre><blockquote><p>可以看出列表推导的方式是比append方法更高效的，而list强制转换的是可能出现问题，所以总的来说就是推荐使用列表推导的方法。</p></blockquote><h4 id="列表推导-条件判断"><a href="#列表推导-条件判断" class="headerlink" title="列表推导+条件判断"></a><strong>列表推导+条件判断</strong></h4><pre><code class="hljs python">l2 = [i <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> range(<span class="hljs-number">1</span>,<span class="hljs-number">11</span>) <span class="hljs-keyword">if</span> i % <span class="hljs-number">2</span> == <span class="hljs-number">0</span>]</code></pre><pre><code class="hljs python">l2Out[<span class="hljs-number">4</span>]: [<span class="hljs-number">2</span>, <span class="hljs-number">4</span>, <span class="hljs-number">6</span>, <span class="hljs-number">8</span>, <span class="hljs-number">10</span>]</code></pre><blockquote><p>单独的if条件判断只能放在后面</p></blockquote><h4 id="列表推导-三目运算"><a href="#列表推导-三目运算" class="headerlink" title="列表推导+三目运算"></a><strong>列表推导+三目运算</strong></h4><pre><code class="hljs python">l2 = [i <span class="hljs-keyword">if</span> i % <span class="hljs-number">2</span> == <span class="hljs-number">0</span> <span class="hljs-keyword">else</span> <span class="hljs-number">0</span> <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> range(<span class="hljs-number">1</span>,<span class="hljs-number">11</span>)]</code></pre><pre><code class="hljs python">l2Out[<span class="hljs-number">3</span>]: [<span class="hljs-number">0</span>, <span class="hljs-number">2</span>, <span class="hljs-number">0</span>, <span class="hljs-number">4</span>, <span class="hljs-number">0</span>, <span class="hljs-number">6</span>, <span class="hljs-number">0</span>, <span class="hljs-number">8</span>, <span class="hljs-number">0</span>, <span class="hljs-number">10</span>]</code></pre><blockquote><p>如果是if+else的判断条件，就需要放前面。</p></blockquote><h4 id="集合"><a href="#集合" class="headerlink" title="集合"></a>集合</h4><pre><code class="hljs python">se = &#123;i <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> range(<span class="hljs-number">1</span>,<span class="hljs-number">11</span>)&#125;</code></pre><pre><code class="hljs python">seOut[<span class="hljs-number">4</span>]: &#123;<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>, <span class="hljs-number">4</span>, <span class="hljs-number">5</span>, <span class="hljs-number">6</span>, <span class="hljs-number">7</span>, <span class="hljs-number">8</span>, <span class="hljs-number">9</span>, <span class="hljs-number">10</span>&#125;</code></pre><h4 id="字典"><a href="#字典" class="headerlink" title="字典"></a>字典</h4><pre><code class="hljs python">li = list(range(<span class="hljs-number">1</span>,<span class="hljs-number">11</span>))di = &#123;i:j <span class="hljs-keyword">for</span> i,j <span class="hljs-keyword">in</span> enumerate(li)&#125;</code></pre><pre><code class="hljs python">diOut[<span class="hljs-number">3</span>]: &#123;<span class="hljs-number">0</span>: <span class="hljs-number">1</span>, <span class="hljs-number">1</span>: <span class="hljs-number">2</span>, <span class="hljs-number">2</span>: <span class="hljs-number">3</span>, <span class="hljs-number">3</span>: <span class="hljs-number">4</span>, <span class="hljs-number">4</span>: <span class="hljs-number">5</span>, <span class="hljs-number">5</span>: <span class="hljs-number">6</span>, <span class="hljs-number">6</span>: <span class="hljs-number">7</span>, <span class="hljs-number">7</span>: <span class="hljs-number">8</span>, <span class="hljs-number">8</span>: <span class="hljs-number">9</span>, <span class="hljs-number">9</span>: <span class="hljs-number">10</span>&#125;</code></pre><h4 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h4><blockquote><p>推导表达式相对于for循环来处理数据，要更加的方便。</p><p>列表推导表达式使用更加的广泛。</p></blockquote><h3 id="迭代器和生成器"><a href="#迭代器和生成器" class="headerlink" title="迭代器和生成器"></a>迭代器和生成器</h3><h4 id="迭代器"><a href="#迭代器" class="headerlink" title="迭代器"></a>迭代器</h4><p>列表推导是往列表中一个一个地放入数据，那如果一个一个地取出数据呢？</p><pre><code class="hljs python">li = list(range(<span class="hljs-number">1</span>, <span class="hljs-number">11</span>))it = iter(li)it2 = li.__iter__()</code></pre><pre><code class="hljs python">itOut[<span class="hljs-number">3</span>]: &lt;list_iterator at <span class="hljs-number">0x14c69f4b208</span>&gt;it2Out[<span class="hljs-number">4</span>]: &lt;list_iterator at <span class="hljs-number">0x14c69f4b6a0</span>&gt;</code></pre><blockquote><p>生成迭代器</p></blockquote><h5 id="迭代器对象"><a href="#迭代器对象" class="headerlink" title="迭代器对象"></a>迭代器对象</h5><pre><code class="hljs python">dir(list)sir(str)dir(tuple)</code></pre><blockquote><p>迭代器对象本身需要支持以下两种方法，它们一起构成迭代器协议：</p></blockquote><pre><code class="hljs python">iterator.__iter__()iterator.__next__()</code></pre><h5 id="取值"><a href="#取值" class="headerlink" title="取值"></a>取值</h5><pre><code class="hljs python">next(it)Out[<span class="hljs-number">12</span>]: <span class="hljs-number">1</span>next(it)Out[<span class="hljs-number">13</span>]: <span class="hljs-number">2</span>next(it)Out[<span class="hljs-number">14</span>]: <span class="hljs-number">3</span>it.__next__()Out[<span class="hljs-number">15</span>]: <span class="hljs-number">4</span>it.__next__()Out[<span class="hljs-number">16</span>]: <span class="hljs-number">5</span>it.__next__()Out[<span class="hljs-number">17</span>]: <span class="hljs-number">6</span>next(it)Traceback (most recent call last):  File <span class="hljs-string">"C:\Program Files\Python36\lib\site-packages\IPython\core\interactiveshell.py"</span>, line <span class="hljs-number">3265</span>, <span class="hljs-keyword">in</span> run_code    exec(code_obj, self.user_global_ns, self.user_ns)  File <span class="hljs-string">"&lt;ipython-input-13-bc1ab118995a&gt;"</span>, line <span class="hljs-number">1</span>, <span class="hljs-keyword">in</span> &lt;module&gt;    next(it)StopIteration</code></pre><blockquote><p>通过</p><p>next(iterator)</p><p>iterator.<strong>next</strong>()</p><p>来进行取值</p><p>注意：如果迭代器值取完之后，会返回 StopIteration 错误</p></blockquote><h5 id="自定义迭代器"><a href="#自定义迭代器" class="headerlink" title="自定义迭代器"></a>自定义迭代器</h5><pre><code class="hljs python"><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">TupleIter</span>:</span>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span><span class="hljs-params">(self, li)</span>:</span>        self.li = li        self._index = <span class="hljs-number">0</span>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__iter__</span><span class="hljs-params">(self)</span>:</span>        <span class="hljs-keyword">return</span> self    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__next__</span><span class="hljs-params">(self)</span>:</span>        <span class="hljs-keyword">if</span> self._index &lt; len(self.li):            index = self.li[self._index]            self._index += <span class="hljs-number">1</span>            <span class="hljs-keyword">return</span> index        <span class="hljs-keyword">else</span>:            <span class="hljs-keyword">raise</span> StopIterationt1 = (<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>, <span class="hljs-number">4</span>, <span class="hljs-number">5</span>)a = TupleIter(t1)</code></pre><pre><code class="hljs python">aOut[<span class="hljs-number">3</span>]: &lt;__main__.TupleIter at <span class="hljs-number">0x1376ba4b358</span>&gt;next(a)Out[<span class="hljs-number">4</span>]: <span class="hljs-number">1</span>a.__next__()Out[<span class="hljs-number">5</span>]: <span class="hljs-number">2</span></code></pre><blockquote><p>可以自己定义iter和next方法来自定义迭代器。</p></blockquote><h4 id="生成器"><a href="#生成器" class="headerlink" title="生成器"></a>生成器</h4><pre><code class="hljs python"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">func</span><span class="hljs-params">(n)</span>:</span>    i = <span class="hljs-number">0</span>    <span class="hljs-keyword">while</span> i &lt; n:        <span class="hljs-keyword">yield</span> i        i += <span class="hljs-number">1</span>s = func(<span class="hljs-number">10</span>)</code></pre><pre><code class="hljs python">sOut[<span class="hljs-number">3</span>]: &lt;generator object func at <span class="hljs-number">0x000001442E4FE990</span>&gt;[i <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> s]Out[<span class="hljs-number">4</span>]: [<span class="hljs-number">0</span>, <span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>, <span class="hljs-number">4</span>, <span class="hljs-number">5</span>, <span class="hljs-number">6</span>, <span class="hljs-number">7</span>, <span class="hljs-number">8</span>, <span class="hljs-number">9</span>]</code></pre><blockquote><p>迭代器提供了一个实现迭代器协议的简便方法</p><p>yield 表达式只能在函数中使用,在函数体中使用 yield 表达式可以使函数成为一个生成器</p></blockquote><pre><code class="hljs python"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">func</span><span class="hljs-params">(end)</span>:</span>    n, a, b = <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">1</span>    <span class="hljs-keyword">while</span> n &lt; end:        <span class="hljs-keyword">yield</span> b        a, b = b, a + b        n += <span class="hljs-number">1</span>g = func(<span class="hljs-number">10</span>)</code></pre><pre><code class="hljs python">gOut[<span class="hljs-number">3</span>]: &lt;generator object func at <span class="hljs-number">0x00000238904BD990</span>&gt;[i <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> g]Out[<span class="hljs-number">4</span>]: [<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>, <span class="hljs-number">5</span>, <span class="hljs-number">8</span>, <span class="hljs-number">13</span>, <span class="hljs-number">21</span>, <span class="hljs-number">34</span>, <span class="hljs-number">55</span>]</code></pre><blockquote><p>通过生成器就生成了一个迭代器，通过dir(g)就能查出iter和next的方法。就比我们自定义迭代器更简单。</p></blockquote><pre><code class="hljs python"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">func</span><span class="hljs-params">(end)</span>:</span>    n, a, b = <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">1</span>    <span class="hljs-keyword">while</span> n &lt; end:        print(<span class="hljs-string">'*'</span>*<span class="hljs-number">5</span>, a, b)        <span class="hljs-keyword">yield</span> b        print(<span class="hljs-string">'-'</span> * <span class="hljs-number">5</span>, a, b)        a, b = b, a + b        n += <span class="hljs-number">1</span>g = func(<span class="hljs-number">10</span>)</code></pre><pre><code class="hljs python">next(g)***** <span class="hljs-number">0</span> <span class="hljs-number">1</span>Out[<span class="hljs-number">3</span>]: <span class="hljs-number">1</span>next(g)----- <span class="hljs-number">0</span> <span class="hljs-number">1</span>***** <span class="hljs-number">1</span> <span class="hljs-number">1</span>Out[<span class="hljs-number">4</span>]: <span class="hljs-number">1</span>next(g)----- <span class="hljs-number">1</span> <span class="hljs-number">1</span>***** <span class="hljs-number">1</span> <span class="hljs-number">2</span>Out[<span class="hljs-number">5</span>]: <span class="hljs-number">2</span></code></pre><blockquote><p>yield 可以返回表达式结果，并且暂定函数执行</p></blockquote><blockquote><p>通过迭代器去生成斐波拉契数列要比直接得到更加节省内存。</p></blockquote><blockquote><p>   总结：yield只能在函数里面使用。</p></blockquote><h3 id="模块"><a href="#模块" class="headerlink" title="模块"></a>模块</h3><p>在另一个py文件中的对象如何导入到当前的py文件中呢？</p><h4 id="模块-1"><a href="#模块-1" class="headerlink" title="模块"></a>模块</h4><blockquote><p>在python中，模块就是一个py文件，可以使用下面两种方法导入</p></blockquote><pre><code class="hljs python"><span class="hljs-keyword">import</span> <span class="hljs-keyword">from</span> <span class="hljs-keyword">import</span> <span class="hljs-keyword">from</span> <span class="hljs-keyword">import</span> <span class="hljs-keyword">as</span></code></pre><pre><code class="hljs python"><span class="hljs-keyword">import</span> datetime<span class="hljs-keyword">from</span> datetime <span class="hljs-keyword">import</span> datetime (<span class="hljs-keyword">as</span> this_datetime)</code></pre><blockquote><p>在同一目录下，可直接使用上面两种方法去导入；在不同目录下，需要使用  sys.path  添加路径</p><p>sys.path.append(‘path’)</p></blockquote><p><img src="http://eveseven.oss-cn-shanghai.aliyuncs.com/18-12-21/19249391.jpg" srcset="/img/loading.gif" alt="路径"></p><pre><code class="hljs python"><span class="hljs-keyword">from</span> pages <span class="hljs-keyword">import</span> aa.demo()</code></pre><blockquote><p>不同的文件夹导入模块可以用这个两种方式。</p></blockquote><blockquote><p>在python3中导入后，会在当前路径下生成一个__pycache__  文件夹</p></blockquote><h4 id="sys模块"><a href="#sys模块" class="headerlink" title="sys模块"></a>sys模块</h4><blockquote><p>sys 模块提供了与python解释器交互的函数，在python中它是始终可以导入使用的.</p></blockquote><h5 id="sys-argv"><a href="#sys-argv" class="headerlink" title="sys.argv"></a><a href="https://docs.python.org/3/library/sys.html" target="_blank" rel="noopener">sys.argv</a></h5><pre><code class="hljs python"><span class="hljs-keyword">import</span> sysprint(sys.argv)</code></pre><pre><code class="hljs python">$ python b.py <span class="hljs-number">123</span> <span class="hljs-number">456</span>[<span class="hljs-string">'b.py'</span>, <span class="hljs-string">'123'</span>, <span class="hljs-string">'456'</span>]</code></pre><blockquote><p>获取终端命令行输入</p></blockquote><h5 id="sys-path"><a href="#sys-path" class="headerlink" title="sys.path"></a><a href="https://docs.python.org/3/library/sys.html" target="_blank" rel="noopener">sys.path</a></h5><pre><code class="hljs python"><span class="hljs-keyword">import</span> sysprint(sys.path)</code></pre><blockquote><p> 解释器模块导入查找路径</p></blockquote><h4 id="if-name-‘-main-‘"><a href="#if-name-‘-main-‘" class="headerlink" title="if__name__ == ‘__main__‘:"></a>if__name__ == ‘__main__‘:</h4><h5 id="name"><a href="#name" class="headerlink" title="__name__"></a><a href="https://docs.python.org/3/reference/import.html?highlight=__name__" target="_blank" rel="noopener">__name__</a></h5><blockquote><p>python会自动的给模块加上这个属性</p><p>如果模块是被直接调用的，则 <strong>name</strong> 的值是 __main__否则就是该模块的模块名 </p></blockquote><pre><code class="hljs python">pages.a a__main__ b</code></pre><h5 id="注意"><a href="#注意" class="headerlink" title="注意"></a>注意</h5><blockquote><p>if __name__ == ‘__main__‘: 该语句可以控制代码在被其他模块导入时不被执行</p></blockquote><pre><code class="hljs python">__main__ b</code></pre><h3 id="包和包管理"><a href="#包和包管理" class="headerlink" title="包和包管理"></a>包和包管理</h3><p>如果模块太多了，怎么方便的去管理呢？</p><h4 id="包概念"><a href="#包概念" class="headerlink" title="包概念"></a>包概念</h4><blockquote><p>把很多模块放到一个文件夹里面，就可以形成一个包.</p></blockquote><h4 id="包管理"><a href="#包管理" class="headerlink" title="包管理"></a>包管理</h4><blockquote><p>当把很多模块放在文件中时，为了方便引用包中的模块，引入了包管理</p></blockquote><h4 id="init-py"><a href="#init-py" class="headerlink" title="__init__.py"></a>__init__.py</h4><blockquote><p>在包管理中，加入此模块，则包名可以直接通过属性访问的方式，访问此模块内的对象，此模块不加上可能不会报错，但是规范是要加上，文件内容可以为空</p></blockquote><h4 id="相对路径导入"><a href="#相对路径导入" class="headerlink" title="相对路径导入"></a>相对路径导入</h4><blockquote><p>在包管理中，可以通过. (一个点) 和 .. (两个点)分别来导入同层和上一层的模块</p></blockquote><p><img src="http://eveseven.oss-cn-shanghai.aliyuncs.com/18-12-21/51214573.jpg" srcset="/img/loading.gif" alt="包管理"></p><h5 id="引入作用"><a href="#引入作用" class="headerlink" title="引入作用"></a>引入作用</h5><blockquote><p>在包中，如果包中模块要导入同一包中的其他模块，就必须使用此方法导入.</p></blockquote><h5 id="使用方法"><a href="#使用方法" class="headerlink" title="使用方法"></a>使用方法</h5><pre><code class="hljs python"><span class="hljs-keyword">from</span>  .module(..module)  <span class="hljs-keyword">import</span> obj   (<span class="hljs-keyword">as</span>  new_name)</code></pre><h5 id="引入之后的影响"><a href="#引入之后的影响" class="headerlink" title="引入之后的影响"></a>引入之后的影响</h5><blockquote><p>当一个模块中出现此导入方式，则该模块不能被直接运行，只能被导入</p></blockquote>]]></content>
    
    
    <categories>
      
      <category>python</category>
      
    </categories>
    
    
    <tags>
      
      <tag>列表推导式</tag>
      
      <tag>迭代器生成器</tag>
      
      <tag>模块</tag>
      
      <tag>包</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>python第十二话之异常</title>
    <link href="/2018/12/python-error/"/>
    <url>/2018/12/python-error/</url>
    
    <content type="html"><![CDATA[<h3 id="异常"><a href="#异常" class="headerlink" title="异常"></a>异常</h3><blockquote><p>异常是什么？</p><p>程序运行过程中出现异常，程序还能正常运行吗？</p><p>如果出现异常该如何让程序正常运行下去呢？</p></blockquote><p>异常即是一个事件，该事件会在程序执行过程中发生，影响了程序的正常执行。</p><p>一般情况下，在Python无法正常处理程序时就会发生一个异常。</p><p>异常是Python对象，表示一个错误。</p><p>当Python脚本发生异常时我们需要捕获处理它，否则程序会终止执行。</p><h4 id="语法规则"><a href="#语法规则" class="headerlink" title="语法规则"></a>语法规则</h4><p><img src="http://eveseven.oss-cn-shanghai.aliyuncs.com/18-12-19/20180386.jpg" srcset="/img/loading.gif" alt="异常处理"></p><h4 id="try-except"><a href="#try-except" class="headerlink" title="try-except"></a>try-except</h4><blockquote><p>异常是我们敲代码的过程中遇到最多的，那么我们有什么办法来捕获异常呢？</p></blockquote><pre><code class="hljs python">a = <span class="hljs-number">1</span>bprint(<span class="hljs-string">'ok'</span>)  <span class="hljs-comment">#ok能够打印出来吗？</span></code></pre><pre><code class="hljs python">NameError: name <span class="hljs-string">'b'</span> <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> defined</code></pre><blockquote><p>捕获异常 ，让代码正常执行</p></blockquote><pre><code class="hljs python"><span class="hljs-keyword">try</span>:    a = <span class="hljs-number">1</span>    b<span class="hljs-keyword">except</span> NameError <span class="hljs-keyword">as</span> e:    print(e)print(<span class="hljs-string">'ok'</span>)  <span class="hljs-comment">#ok能够打印出来吗？</span></code></pre><pre><code class="hljs python">name <span class="hljs-string">'b'</span> <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> definedok</code></pre><blockquote><p>通过捕获异常，代码不仅把错误打印了，后面的代码也正常执行了。</p></blockquote><blockquote><p>那么其他异常怎么办呢？是一样的吗？</p></blockquote><pre><code class="hljs python"><span class="hljs-keyword">try</span>:    a = <span class="hljs-number">1</span>    file = open(<span class="hljs-string">'test.txt'</span>, <span class="hljs-string">'r'</span>)<span class="hljs-keyword">except</span> NameError <span class="hljs-keyword">as</span> e:    print(e)print(<span class="hljs-string">'ok'</span>)  <span class="hljs-comment">#ok能够打印出来吗？</span></code></pre><pre><code class="hljs python">FileNotFoundError: [Errno <span class="hljs-number">2</span>] No such file <span class="hljs-keyword">or</span> directory: <span class="hljs-string">'test.txt'</span></code></pre><blockquote><p>不同类型的异常，就要用不同的状态去捕获</p></blockquote><pre><code class="hljs python"><span class="hljs-keyword">try</span>:    a = <span class="hljs-number">1</span>    file = open(<span class="hljs-string">'test.txt'</span>, <span class="hljs-string">'r'</span>)<span class="hljs-keyword">except</span> NameError <span class="hljs-keyword">as</span> e:    print(e)<span class="hljs-keyword">except</span> FileNotFoundError <span class="hljs-keyword">as</span> e:    print(e)print(<span class="hljs-string">'ok'</span>)  <span class="hljs-comment">#ok能够打印出来吗？</span></code></pre><pre><code class="hljs python">[Errno <span class="hljs-number">2</span>] No such file <span class="hljs-keyword">or</span> directory: <span class="hljs-string">'test.txt'</span>ok</code></pre><blockquote><p>异常那么多，我们需要每一个都写吗？</p></blockquote><pre><code class="hljs python"><span class="hljs-keyword">try</span>:    a = <span class="hljs-number">1</span>    c = <span class="hljs-number">1</span> + <span class="hljs-string">'a'</span>    file = open(<span class="hljs-string">'test.txt'</span>, <span class="hljs-string">'r'</span>)<span class="hljs-keyword">except</span> NameError <span class="hljs-keyword">as</span> e:    print(e)<span class="hljs-keyword">except</span> FileNotFoundError <span class="hljs-keyword">as</span> e:    print(e)<span class="hljs-keyword">except</span> Exception <span class="hljs-keyword">as</span> e:    print(e)print(<span class="hljs-string">'ok'</span>)  <span class="hljs-comment">#ok能够打印出来吗？</span></code></pre><pre><code class="hljs python">unsupported operand type(s) <span class="hljs-keyword">for</span> +: <span class="hljs-string">'int'</span> <span class="hljs-keyword">and</span> <span class="hljs-string">'str'</span>ok</code></pre><blockquote><p>同时出现两个异常，会都捕获吗？</p></blockquote><pre><code class="hljs python"><span class="hljs-keyword">try</span>:    a = <span class="hljs-number">1</span>    c    file = open(<span class="hljs-string">'test.txt'</span>, <span class="hljs-string">'r'</span>)<span class="hljs-keyword">except</span> NameError <span class="hljs-keyword">as</span> e:    print(e)<span class="hljs-keyword">except</span> FileNotFoundError <span class="hljs-keyword">as</span> e:    print(e)print(<span class="hljs-string">'ok'</span>)  <span class="hljs-comment">#ok能够打印出来吗？</span></code></pre><pre><code class="hljs python">name <span class="hljs-string">'c'</span> <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> definedok</code></pre><blockquote><p>出现异常后，报异常后的代码就不会执行了，就会跳到except去执行。</p></blockquote><h4 id="try-except-else"><a href="#try-except-else" class="headerlink" title="try-except-else"></a>try-except-else</h4><pre><code class="hljs python"><span class="hljs-keyword">try</span>:    a = <span class="hljs-number">1</span>    c = <span class="hljs-number">1</span>    file = open(<span class="hljs-string">'tests.txt'</span>, <span class="hljs-string">'r'</span>)<span class="hljs-keyword">except</span> NameError <span class="hljs-keyword">as</span> e:    print(e)<span class="hljs-keyword">except</span> FileNotFoundError <span class="hljs-keyword">as</span> e:    print(e)<span class="hljs-keyword">except</span> Exception <span class="hljs-keyword">as</span> e:    print(e)<span class="hljs-keyword">else</span>:    file.close()print(<span class="hljs-string">'ok'</span>)  <span class="hljs-comment">#ok能够打印出来吗？</span></code></pre><pre><code class="hljs python">[Errno <span class="hljs-number">2</span>] No such file <span class="hljs-keyword">or</span> directory: <span class="hljs-string">'tests.txt'</span>okfileTraceback (most recent call last):  File <span class="hljs-string">"C:\Program Files\Python36\lib\site-packages\IPython\core\interactiveshell.py"</span>, line <span class="hljs-number">3265</span>, <span class="hljs-keyword">in</span> run_code    exec(code_obj, self.user_global_ns, self.user_ns)  File <span class="hljs-string">"&lt;ipython-input-3-046c168df224&gt;"</span>, line <span class="hljs-number">1</span>, <span class="hljs-keyword">in</span> &lt;module&gt;    fileNameError: name <span class="hljs-string">'file'</span> <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> defined</code></pre><blockquote><p>这是有抛出异常的情况。</p></blockquote><pre><code class="hljs python"><span class="hljs-keyword">try</span>:    a = <span class="hljs-number">1</span>    c = <span class="hljs-number">1</span>    file = open(<span class="hljs-string">'test.txt'</span>, <span class="hljs-string">'x'</span>)<span class="hljs-keyword">except</span> NameError <span class="hljs-keyword">as</span> e:    print(e)<span class="hljs-keyword">except</span> FileNotFoundError <span class="hljs-keyword">as</span> e:    print(e)<span class="hljs-keyword">except</span> Exception <span class="hljs-keyword">as</span> e:    print(e)<span class="hljs-keyword">else</span>:    file.close()    print(<span class="hljs-string">'ok'</span>)  <span class="hljs-comment">#ok能够打印出来吗？</span></code></pre><pre><code class="hljs python">okfileOut[<span class="hljs-number">3</span>]: &lt;_io.TextIOWrapper name=<span class="hljs-string">'test.txt'</span> mode=<span class="hljs-string">'w'</span> encoding=<span class="hljs-string">'cp936'</span>&gt;file.closedOut[<span class="hljs-number">4</span>]: <span class="hljs-literal">True</span></code></pre><blockquote><p>这是没有异常的情况</p></blockquote><blockquote><p>else是在不抛出异常的情况下执行。</p></blockquote><h4 id="try-except-finally"><a href="#try-except-finally" class="headerlink" title="try-except-finally"></a>try-except-finally</h4><pre><code class="hljs python"><span class="hljs-keyword">try</span>:    a = <span class="hljs-number">1</span>    c = <span class="hljs-number">1</span>    file = open(<span class="hljs-string">'tests.txt'</span>, <span class="hljs-string">'r'</span>)<span class="hljs-keyword">except</span> NameError <span class="hljs-keyword">as</span> e:    print(e)<span class="hljs-keyword">except</span> FileNotFoundError <span class="hljs-keyword">as</span> e:    print(e)<span class="hljs-keyword">except</span> Exception <span class="hljs-keyword">as</span> e:    print(e)<span class="hljs-keyword">else</span>:    file.close()<span class="hljs-keyword">finally</span>:    print(<span class="hljs-string">'end'</span>)print(<span class="hljs-string">'ok'</span>)  <span class="hljs-comment">#ok能够打印出来吗？</span></code></pre><pre><code class="hljs python">[Errno <span class="hljs-number">2</span>] No such file <span class="hljs-keyword">or</span> directory: <span class="hljs-string">'tests.txt'</span>endok</code></pre><blockquote><p>这是抛出异常的情况</p></blockquote><pre><code class="hljs python"><span class="hljs-keyword">try</span>:    a = <span class="hljs-number">1</span>    c = <span class="hljs-number">1</span>    file = open(<span class="hljs-string">'test.txt'</span>, <span class="hljs-string">'r'</span>)<span class="hljs-keyword">except</span> NameError <span class="hljs-keyword">as</span> e:    print(e)<span class="hljs-keyword">except</span> FileNotFoundError <span class="hljs-keyword">as</span> e:    print(e)<span class="hljs-keyword">except</span> Exception <span class="hljs-keyword">as</span> e:    print(e)<span class="hljs-keyword">else</span>:    file.close()<span class="hljs-keyword">finally</span>:    print(<span class="hljs-string">'end'</span>)print(<span class="hljs-string">'ok'</span>)  <span class="hljs-comment">#ok能够打印出来吗？</span></code></pre><pre><code class="hljs python">endok</code></pre><blockquote><p>这是不抛出异常的情况</p></blockquote><blockquote><p>不管会不会抛出异常，finally都会在最后执行。</p></blockquote><h4 id="返回错误"><a href="#返回错误" class="headerlink" title="返回错误"></a>返回错误</h4><pre><code class="hljs python"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">func</span><span class="hljs-params">()</span>:</span>    <span class="hljs-keyword">try</span>:        c = <span class="hljs-number">1</span>        file = open(<span class="hljs-string">'tx.txt'</span>, <span class="hljs-string">'r'</span>)        print(<span class="hljs-string">'aaaaaa'</span>)    <span class="hljs-keyword">except</span> FileNotFoundError <span class="hljs-keyword">as</span> e:        <span class="hljs-keyword">return</span> e    <span class="hljs-keyword">except</span> NameError <span class="hljs-keyword">as</span> e:        print(e)x = func()print(x)</code></pre><pre><code class="hljs python">[Errno <span class="hljs-number">2</span>] No such file <span class="hljs-keyword">or</span> directory: <span class="hljs-string">'tx.txt'</span></code></pre><h4 id="直接抛出异常"><a href="#直接抛出异常" class="headerlink" title="直接抛出异常"></a>直接抛出异常</h4><pre><code class="hljs python"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">func</span><span class="hljs-params">()</span>:</span>    <span class="hljs-keyword">try</span>:        c = <span class="hljs-number">1</span>        file = open(<span class="hljs-string">'tx.txt'</span>, <span class="hljs-string">'r'</span>)        print(<span class="hljs-string">'aaaaaa'</span>)    <span class="hljs-keyword">except</span> FileNotFoundError <span class="hljs-keyword">as</span> e:        <span class="hljs-keyword">raise</span> e    <span class="hljs-keyword">except</span> NameError <span class="hljs-keyword">as</span> e:        print(e)x = func()print(x)</code></pre><pre><code class="hljs python">FileNotFoundError: [Errno <span class="hljs-number">2</span>] No such file <span class="hljs-keyword">or</span> directory: <span class="hljs-string">'tx.txt'</span></code></pre><blockquote><p>raise是直接抛出异常–和不使用try是一样的。</p></blockquote><h4 id="自定义类"><a href="#自定义类" class="headerlink" title="自定义类"></a>自定义类</h4><pre><code class="hljs python"><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">MyError</span><span class="hljs-params">(Exception)</span>:</span>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span><span class="hljs-params">(self, value)</span>:</span>        self.value = value    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__str__</span><span class="hljs-params">(self)</span>:</span>        <span class="hljs-keyword">return</span> repr(self.value)<span class="hljs-keyword">try</span>:    <span class="hljs-keyword">raise</span> MyError(<span class="hljs-number">2</span> * <span class="hljs-number">2</span>)<span class="hljs-keyword">except</span> MyError <span class="hljs-keyword">as</span> e:    print(<span class="hljs-string">'My exception occurred, value:'</span>, e.value)    print(e)</code></pre><pre><code class="hljs python">My exception occurred, value: <span class="hljs-number">4</span><span class="hljs-number">4</span></code></pre><h4 id="自定义异常错误"><a href="#自定义异常错误" class="headerlink" title="自定义异常错误"></a>自定义异常错误</h4><pre><code class="hljs python"><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">MyError</span><span class="hljs-params">(ValueError)</span>:</span>    ERROR = (<span class="hljs-string">"-1"</span>, <span class="hljs-string">"没有该用户！"</span>)<span class="hljs-comment"># 抛出异常测试函数</span><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">raiseTest</span><span class="hljs-params">()</span>:</span>    <span class="hljs-comment"># 抛出异常</span>    <span class="hljs-keyword">raise</span> MyError(MyError.ERROR[<span class="hljs-number">0</span>],  <span class="hljs-comment"># 异常错误参数1</span>                  MyError.ERROR[<span class="hljs-number">1</span>])  <span class="hljs-comment"># 异常错误参数2</span><span class="hljs-comment"># 主函数</span><span class="hljs-keyword">if</span> __name__ == <span class="hljs-string">'__main__'</span>:    <span class="hljs-keyword">try</span>:        raiseTest()    <span class="hljs-keyword">except</span> MyError <span class="hljs-keyword">as</span> msg:        print(<span class="hljs-string">"errCode:"</span>, msg.args[<span class="hljs-number">0</span>])  <span class="hljs-comment"># 获取异常错误参数1</span>        print(<span class="hljs-string">"errMsg:"</span>, msg.args[<span class="hljs-number">1</span>])  <span class="hljs-comment"># 获取异常错误参数2</span></code></pre><pre><code class="hljs python">errCode: <span class="hljs-number">-1</span>errMsg: 没有该用户！</code></pre><h4 id="注意"><a href="#注意" class="headerlink" title="注意"></a>注意</h4><blockquote><p>注意事项：</p><ol><li><p>try 后面必须跟上 except</p></li><li><p>except 只有在函数中才能使用 return</p></li><li><p>finally 不管是否发生异常，始终都会执行</p></li></ol></blockquote><h4 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h4><blockquote><p>try: 将可能会发生异常的代码放在try中，就可以得到异常，并做相应处理</p><p>except: except用来接受异常，并且可以抛出或者返回异常</p><p>else和finally:  else在没有异常的时候会执行；finally不管是否有异常，都会执行</p></blockquote><h3 id="异常处理"><a href="#异常处理" class="headerlink" title="异常处理"></a>异常处理</h3><blockquote><p>python中有哪些异常？</p><p>怎样查看所有的异常？</p><p>如何通过程序的报错来找到有问题的代码</p></blockquote><h4 id="异常层次结构"><a href="#异常层次结构" class="headerlink" title="异常层次结构"></a><a href="https://docs.python.org/3/library/exceptions.html" target="_blank" rel="noopener">异常层次结构</a></h4><blockquote><p>在 Python 中所有的异常都是继承 BaseException</p><p>代码中会出现的异常都是 Exception 的子类， 因此在 except 中只需要在最后加上 Exception 即可</p><p>在抛出异常的过程中，会从上倒下依次对比异常，找到之后就不会再往后查找</p></blockquote><p><img src="http://eveseven.oss-cn-shanghai.aliyuncs.com/18-12-19/719558.jpg" srcset="/img/loading.gif" alt="异常"></p><h4 id="错误回溯"><a href="#错误回溯" class="headerlink" title="错误回溯"></a>错误回溯</h4><p><img src="http://eveseven.oss-cn-shanghai.aliyuncs.com/18-12-19/1559957.jpg" srcset="/img/loading.gif" alt="错误"></p><h4 id="总结-1"><a href="#总结-1" class="headerlink" title="总结"></a>总结</h4><blockquote><p>在今后的学习和工作过程中，会遇到大量的报错，每个开发人员都必须掌握查找和解决报错的能力</p><p>在自己无法解决需要寻求帮助时，也要掌握如何描述问题，把问题描述清楚的能力，这会大大节省双方的时间和精力</p></blockquote><h3 id="断言"><a href="#断言" class="headerlink" title="断言"></a>断言</h3><p>在调试代码过程中，对于不知道的值可以使用print输出查看一下，但是有些时候，我们清楚某个值应该是怎样的，比如应该是int类型的数据，这个时候需要在类型不对的情况下终止代码，再来调试代码，该怎么做呢？</p><pre><code class="hljs python"><span class="hljs-keyword">assert</span>  <span class="hljs-number">1</span>==<span class="hljs-number">1</span> <span class="hljs-keyword">assert</span>  <span class="hljs-number">1</span>==<span class="hljs-number">2</span>  <span class="hljs-comment">#报错</span><span class="hljs-keyword">assert</span> len(in_s) == <span class="hljs-number">4</span>, <span class="hljs-string">'input size rank 4 required!'</span><span class="hljs-keyword">assert</span> len(f_s) == <span class="hljs-number">4</span>, <span class="hljs-string">'filter size rank 4 required!'</span><span class="hljs-keyword">assert</span> f_s[<span class="hljs-number">2</span>] == in_s[<span class="hljs-number">3</span>], <span class="hljs-string">'intput channels not match filter channels.'</span><span class="hljs-keyword">assert</span> f_s[<span class="hljs-number">0</span>] &gt;= stride <span class="hljs-keyword">and</span> f_s[<span class="hljs-number">1</span>] &gt;= stride, <span class="hljs-string">'filter should not be less than stride!'</span><span class="hljs-keyword">assert</span> padding <span class="hljs-keyword">in</span> [<span class="hljs-string">'SAME'</span>, <span class="hljs-string">'VALID'</span>], <span class="hljs-string">'padding value[&#123;0&#125;] not allowded!!'</span>.format(padding)</code></pre><blockquote><p>断言语句是将调试断言插入程序的一种便捷方式</p><p>assert 的语法规则是：</p><p>表达式返回 True  不报错</p><p>表达式返回 False  报错  报 AssertionError</p></blockquote>]]></content>
    
    
    <categories>
      
      <category>python</category>
      
    </categories>
    
    
    <tags>
      
      <tag>异常</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>python第十一话之文件</title>
    <link href="/2018/12/python-file/"/>
    <url>/2018/12/python-file/</url>
    
    <content type="html"><![CDATA[<h3 id="私有属性和私有方法"><a href="#私有属性和私有方法" class="headerlink" title="私有属性和私有方法"></a>私有属性和私有方法</h3><p>我们python3中有没有私有属性这种说法？如果有的话有是怎么使用的？</p><h4 id="”私有“变量、方法"><a href="#”私有“变量、方法" class="headerlink" title="”私有“变量、方法"></a>”私有“变量、方法</h4><blockquote><p>1、封装类的实例上面的“私有”数据，但是Python语言并没有访问控制。 </p><p>2、Python程序员不去依赖语言特性去封装数据，而是通过遵循一定的属性和方法命名规约来达到这个效果。</p></blockquote><h4 id="单下滑线"><a href="#单下滑线" class="headerlink" title="单下滑线(_)"></a>单下滑线(_)</h4><blockquote><p>第一个约定是任何以单下划线_开头的名字都应该是内部实现。</p></blockquote><pre><code class="hljs python"><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">A</span>:</span>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span><span class="hljs-params">(self)</span>:</span>        self._internal = <span class="hljs-number">0</span> <span class="hljs-comment"># An internal attribute</span>        self.public = <span class="hljs-number">1</span> <span class="hljs-comment"># A public attribute</span>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">public_method</span><span class="hljs-params">(self)</span>:</span>        <span class="hljs-string">'''</span><span class="hljs-string">        A public method</span><span class="hljs-string">        '''</span>        <span class="hljs-keyword">pass</span>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">_internal_method</span><span class="hljs-params">(self)</span>:</span>        print(<span class="hljs-string">'_internal_method'</span>)</code></pre><blockquote><p>Python并不会真的阻止别人访问内部名称。但是如果你这么做肯定是不好的，可能会导致脆弱的代码。 同时还要注意到，使用下划线开头的约定同样适用于模块名和模块级别函数。 </p></blockquote><pre><code class="hljs python">a = A()a._internal_method()a._internal</code></pre><pre><code class="hljs python">_internal_method<span class="hljs-number">0</span></code></pre><h4 id="双下滑线（-）"><a href="#双下滑线（-）" class="headerlink" title="双下滑线（__）"></a>双下滑线（__）</h4><p>你还可能会遇到在类定义中使用两个下划线(__)开头的命名。</p><pre><code class="hljs python"><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">B</span>:</span>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span><span class="hljs-params">(self)</span>:</span>        self.__private = <span class="hljs-number">0</span>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__private_method</span><span class="hljs-params">(self)</span>:</span>        print(<span class="hljs-string">'_B__private_method'</span>)    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">public_method</span><span class="hljs-params">(self)</span>:</span>        <span class="hljs-keyword">pass</span>        self.__private_method()</code></pre><blockquote><p>使用双下划线开始会导致访问名称变成其他形式。 比如，在前面的类B中，私有属性会被分别重命名为 <code>_B__private</code> 和 <code>_B__private_method</code> 。 这时候你可能会问这样重命名的目的是什么，答案就是继承——这种属性通过继承是无法被覆盖的。 </p></blockquote><pre><code class="hljs python">b = B()b._B__privateb._B__private_method()b.public_method()</code></pre><pre><code class="hljs python">_B__private_method_B__private_method</code></pre><blockquote><p>私有名称 <code>__private</code> 和 <code>__private_method</code> 被重命名为 <code>_C__private</code> 和 <code>_C__private_method</code> ，这个跟父类B中的名称是完全不同的。 </p></blockquote><pre><code class="hljs python"><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">C</span><span class="hljs-params">(B)</span>:</span>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span><span class="hljs-params">(self)</span>:</span>        super().__init__()        self.__private = <span class="hljs-number">1</span> <span class="hljs-comment"># Does not override B.__private</span>    <span class="hljs-comment"># Does not override B.__private_method()</span>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__private_method</span><span class="hljs-params">(self)</span>:</span>        print(<span class="hljs-string">'_C__private_method'</span>)</code></pre><pre><code class="hljs python">_B__private_method_B__private_method_C__private_method</code></pre><h3 id="文件基本操作"><a href="#文件基本操作" class="headerlink" title="文件基本操作"></a>文件基本操作</h3><p>我们的程序都是运行在内存中的，内存是不可持久化存储的，那怎样才能持久存储呢？</p><h4 id="打开文件"><a href="#打开文件" class="headerlink" title="打开文件"></a>打开文件</h4><pre><code class="hljs python">path = <span class="hljs-string">'text.txt'</span>  <span class="hljs-comment"># 相对路径</span>path = <span class="hljs-string">'home/seven/text.txt'</span>  <span class="hljs-comment"># 绝对路径</span>file = open(path, mode=<span class="hljs-string">'w+'</span>)</code></pre><blockquote><p>以w+模式打开文件，是为写入和读取的模式，没有文件会新建文件，有文件会清空文件。</p></blockquote><h4 id="文件打开模式"><a href="#文件打开模式" class="headerlink" title="文件打开模式"></a>文件打开模式</h4><p><img src="http://eveseven.oss-cn-shanghai.aliyuncs.com/18-12-19/31221654.jpg" srcset="/img/loading.gif" alt="文件打开模式"></p><blockquote><p>不同的文件打开模式，对文件的操作有不同，大家一定要注意。</p></blockquote><h4 id="写入文件"><a href="#写入文件" class="headerlink" title="写入文件"></a>写入文件</h4><pre><code class="hljs python">file.write(<span class="hljs-string">'python'</span>)Out[<span class="hljs-number">4</span>]: <span class="hljs-number">6</span>file.write(<span class="hljs-string">'python2'</span>)Out[<span class="hljs-number">5</span>]: <span class="hljs-number">7</span></code></pre><blockquote><p>写单个字符串</p></blockquote><pre><code class="hljs python">file.writelines([<span class="hljs-string">'1'</span>, <span class="hljs-string">'2'</span>, <span class="hljs-string">'3'</span>])</code></pre><blockquote><p>写一行数据</p></blockquote><pre><code class="hljs python">file.flush()</code></pre><blockquote><p>本来写入的数据是存在内存里的，使用flush方法，把数据保存到硬盘中。</p></blockquote><h4 id="读取与关闭"><a href="#读取与关闭" class="headerlink" title="读取与关闭"></a>读取与关闭</h4><pre><code class="hljs python">file.seek(<span class="hljs-number">0</span>) <span class="hljs-comment"># 把光标移到首位</span>file.read()Out[<span class="hljs-number">18</span>]: <span class="hljs-string">'python\n\npython3\n\nc++\n\nc\n\njava\n\nmachine learning\n\ndeep learning\n'</span></code></pre><blockquote><p>读取全部数据</p></blockquote><pre><code class="hljs python">file.readline()Out[<span class="hljs-number">21</span>]: <span class="hljs-string">'python\n'</span>file.readline()Out[<span class="hljs-number">22</span>]: <span class="hljs-string">'\n'</span>file.readline()Out[<span class="hljs-number">23</span>]: <span class="hljs-string">'python3\n'</span>file.readline()Out[<span class="hljs-number">24</span>]: <span class="hljs-string">'\n'</span>file.readline()Out[<span class="hljs-number">25</span>]: <span class="hljs-string">'c++\n'</span></code></pre><blockquote><p>一行一行的读取数据</p></blockquote><pre><code class="hljs python">file.readlines()Out[<span class="hljs-number">26</span>]: [<span class="hljs-string">'\n'</span>, <span class="hljs-string">'c\n'</span>, <span class="hljs-string">'\n'</span>, <span class="hljs-string">'java\n'</span>, <span class="hljs-string">'\n'</span>, <span class="hljs-string">'machine learning\n'</span>, <span class="hljs-string">'\n'</span>, <span class="hljs-string">'deep learning\n'</span>]</code></pre><blockquote><p>读取所有行并以列表形式返回</p></blockquote><pre><code class="hljs python">file.flush() <span class="hljs-comment"># 把内存中的数据保存到硬盘中</span>file.close() <span class="hljs-comment"># 关闭并保存文件</span>file.closed  <span class="hljs-comment"># 判断文件是否关闭</span></code></pre><pre><code class="hljs python">file.close()file.closedOut[<span class="hljs-number">28</span>]: <span class="hljs-literal">True</span></code></pre><h4 id="查看与移动指针"><a href="#查看与移动指针" class="headerlink" title="查看与移动指针"></a>查看与移动指针</h4><pre><code class="hljs python">file.tell()Out[<span class="hljs-number">9</span>]: <span class="hljs-number">50</span>file.seek(<span class="hljs-number">0</span>, <span class="hljs-number">0</span>)  <span class="hljs-comment">#0代表从文件开头开始算起，1代表从当前位置开始算起，2代表从文件末尾算起。</span>Out[<span class="hljs-number">10</span>]: <span class="hljs-number">0</span>file.tell()Out[<span class="hljs-number">11</span>]: <span class="hljs-number">0</span></code></pre><blockquote><p>tell 查看光标位置，seek移动光标的位置。</p></blockquote><h4 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h4><blockquote><p>持久存储：保存内存中数据都是易丢失的，只有保存在硬盘中才能持久的存储，保存在硬盘中的基本方法就是把数据写入文件中。</p><p>打开与关闭：在python中文件的打开与关闭变得十分简单快捷，文件在关闭的时候就会自动保存</p><p>写入与读取：文件的写入和读取是必须要十分熟练的内容</p></blockquote><h3 id="上下文管理"><a href="#上下文管理" class="headerlink" title="上下文管理"></a>上下文管理</h3><p>文件能够自动关闭吗？</p><pre><code class="hljs python"><span class="hljs-keyword">with</span> open(<span class="hljs-string">'test.txt'</span>,<span class="hljs-string">'r'</span>) <span class="hljs-keyword">as</span> file:    st = file.read()    print(st)</code></pre><pre><code class="hljs python">cjavamachine learningdeep learningfile.closedOut[<span class="hljs-number">3</span>]: <span class="hljs-literal">True</span></code></pre><blockquote><p>with能够自动关闭文件，不需要执行close方法</p></blockquote><pre><code class="hljs python"><span class="hljs-keyword">import</span> time<span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">RunTime</span>:</span>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__enter__</span><span class="hljs-params">(self)</span>:</span>        self.start_time = time.time()        <span class="hljs-keyword">return</span> self.start_time    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__exit__</span><span class="hljs-params">(self, exc_type, exc_val, exc_tb)</span>:</span>        self.end_time = time.time()        self.run_time = self.end_time - self.start_time        print(<span class="hljs-string">'Time consuming %s '</span> % self.run_time)<span class="hljs-keyword">with</span> RunTime():    <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> range(<span class="hljs-number">100000</span>):        <span class="hljs-keyword">pass</span></code></pre><pre><code class="hljs python">Time consuming <span class="hljs-number">0.005983591079711914</span></code></pre><blockquote><p>通过这两个方法可以方便的实现上下文管理</p><p>with会把 <strong>enter</strong> 的返回值赋值给 as 后的变量</p></blockquote><h4 id="总结-1"><a href="#总结-1" class="headerlink" title="总结"></a>总结</h4><blockquote><p>with: 使用with打开文件，则文件不需要自己关闭，会自动的关闭</p><p><strong>enter</strong>: 进入时需要执行的代码，相当于准备工作</p><p><strong>exit</strong> : 退出时需要执行的代码，相当于收尾工作</p></blockquote><h3 id="IO流"><a href="#IO流" class="headerlink" title="IO流"></a>IO流</h3><p>文件可以持久存储，但是现在类似于临时的一些文件，不需要持久存储，如一些临时的二维码等，这个不需要持久存储，但是却需要短时间内大量读取，这是时候还是只能保存在文件里面吗？</p><h4 id="StringIO"><a href="#StringIO" class="headerlink" title="StringIO"></a><a href="https://docs.python.org/3/library/io.html" target="_blank" rel="noopener">StringIO</a></h4><pre><code class="hljs python">In [<span class="hljs-number">4</span>]: <span class="hljs-keyword">import</span> ioIn [<span class="hljs-number">5</span>]: sio = io.StringIO()  <span class="hljs-comment"># 创建io</span>In [<span class="hljs-number">6</span>]: sio.write(<span class="hljs-string">'abc'</span>)  <span class="hljs-comment"># 写入数据</span>Out[<span class="hljs-number">6</span>]: <span class="hljs-number">3</span>In [<span class="hljs-number">7</span>]: sioOut[<span class="hljs-number">7</span>]: &lt;_io.StringIO at <span class="hljs-number">0x7f0b775ddaf8</span>&gt;In [<span class="hljs-number">8</span>]: sio.read()Out[<span class="hljs-number">8</span>]: <span class="hljs-string">''</span>In [<span class="hljs-number">9</span>]: sio.seek(<span class="hljs-number">0</span>)Out[<span class="hljs-number">9</span>]: <span class="hljs-number">0</span>In [<span class="hljs-number">10</span>]: sio.read()Out[<span class="hljs-number">10</span>]: <span class="hljs-string">'abc'</span>In [<span class="hljs-number">11</span>]: sio.getvalue()  <span class="hljs-comment"># 读取数据，全部的，不管光标位置</span>Out[<span class="hljs-number">11</span>]: <span class="hljs-string">'abc'</span>In [<span class="hljs-number">12</span>]: sio.close()In [<span class="hljs-number">13</span>]: sioOut[<span class="hljs-number">13</span>]: &lt;_io.StringIO at <span class="hljs-number">0x7f0b775ddaf8</span>&gt;In [<span class="hljs-number">14</span>]: sio.getvalue()---------------------------------------------------------------------------ValueError                                Traceback (most recent call last)&lt;ipython-input<span class="hljs-number">-14</span><span class="hljs-number">-2</span>c8cd5e6194b&gt; <span class="hljs-keyword">in</span> &lt;module&gt;()----&gt; 1 sio.getvalue()ValueError: I/O operation on closed file</code></pre><blockquote><p>StringIO在内存中如同打开文件一样操作字符串，因此也有文件的很多方法</p><p>当创建的StringIO调用 close() 方法时，在内存中的数据会被丢失 </p></blockquote><h4 id="BytesIO"><a href="#BytesIO" class="headerlink" title="BytesIO"></a><a href="https://docs.python.org/3/library/io.html" target="_blank" rel="noopener">BytesIO</a></h4><pre><code class="hljs python">In [<span class="hljs-number">17</span>]: bio = io.BytesIO()  <span class="hljs-comment"># 创建IO</span>In [<span class="hljs-number">18</span>]: bioOut[<span class="hljs-number">18</span>]: &lt;_io.BytesIO at <span class="hljs-number">0x7f0b775b9150</span>&gt;In [<span class="hljs-number">19</span>]: bio.write(<span class="hljs-string">b'abc'</span>)  <span class="hljs-comment"># 写入数据</span>Out[<span class="hljs-number">19</span>]: <span class="hljs-number">3</span>In [<span class="hljs-number">20</span>]: bio.read()Out[<span class="hljs-number">20</span>]: <span class="hljs-string">b''</span>In [<span class="hljs-number">21</span>]: bio.seek(<span class="hljs-number">0</span>)Out[<span class="hljs-number">21</span>]: <span class="hljs-number">0</span>In [<span class="hljs-number">22</span>]: bio.read()Out[<span class="hljs-number">22</span>]: <span class="hljs-string">b'abc'</span>In [<span class="hljs-number">23</span>]: bio.getvalue()  <span class="hljs-comment"># 读取数据 </span>Out[<span class="hljs-number">23</span>]: <span class="hljs-string">b'abc'</span>In [<span class="hljs-number">24</span>]: bio.close()In [<span class="hljs-number">25</span>]: bio.read()---------------------------------------------------------------------------ValueError                                Traceback (most recent call last)&lt;ipython-input<span class="hljs-number">-25</span>-dca3ff2736f4&gt; <span class="hljs-keyword">in</span> &lt;module&gt;()----&gt; 1 bio.read()ValueError: I/O operation on closed file.</code></pre><blockquote><p>BytesIO和 StringIO 类似，但是BytesIO操作的是 Bytes数据</p></blockquote><h3 id="使用工具"><a href="#使用工具" class="headerlink" title="使用工具"></a>使用工具</h3><p>文件可以直接新建，但是现在如果需要创建文件夹和移动文件夹怎么办呢？</p><h4 id="os-操作系统交互"><a href="#os-操作系统交互" class="headerlink" title="os   操作系统交互"></a><a href="https://docs.python.org/3/library/os.html" target="_blank" rel="noopener">os</a>   操作系统交互</h4><blockquote><p> os模块提供python和操作系统交互的接口</p></blockquote><h5 id="直接调用吸引命令"><a href="#直接调用吸引命令" class="headerlink" title="直接调用吸引命令"></a>直接调用吸引命令</h5><pre><code class="hljs python">In [<span class="hljs-number">1</span>]: <span class="hljs-keyword">import</span> osIn [<span class="hljs-number">2</span>]: os.system(<span class="hljs-string">'ls'</span>)Data  PythonClassEnv  ReadMe.mdOut[<span class="hljs-number">2</span>]: <span class="hljs-number">0</span></code></pre><h5 id="通用路径操作"><a href="#通用路径操作" class="headerlink" title="通用路径操作"></a>通用路径操作</h5><pre><code class="hljs python">In [<span class="hljs-number">5</span>]: os.pathOut[<span class="hljs-number">5</span>]: &lt;module <span class="hljs-string">'posixpath'</span> <span class="hljs-keyword">from</span> <span class="hljs-string">'/usr/lib/python3.5/posixpath.py'</span>&gt;In [<span class="hljs-number">6</span>]: os.path.join(<span class="hljs-string">r'Data'</span>, <span class="hljs-string">r'a'</span>)Out[<span class="hljs-number">6</span>]: <span class="hljs-string">'Data/a'</span></code></pre><h5 id="文件目录操作"><a href="#文件目录操作" class="headerlink" title="文件目录操作"></a>文件目录操作</h5><pre><code class="hljs python">In [<span class="hljs-number">7</span>]: os.mkdir(<span class="hljs-string">'text'</span>)In [<span class="hljs-number">8</span>]: os.system(<span class="hljs-string">'ls'</span>)Data  PythonClassEnv  ReadMe.md  textOut[<span class="hljs-number">8</span>]: <span class="hljs-number">0</span>    In [<span class="hljs-number">9</span>]: os.rename(<span class="hljs-string">'text'</span>, <span class="hljs-string">'text1'</span>)In [<span class="hljs-number">10</span>]: os.system(<span class="hljs-string">'ls'</span>)Data  PythonClassEnv  ReadMe.md  text1Out[<span class="hljs-number">10</span>]: <span class="hljs-number">0</span></code></pre><blockquote><p>os 提供了Python和操作系统交互方式，只要是和操作系统相关，就可以尝试在os模块中找方法</p></blockquote><h4 id="shutil-高级文件操作"><a href="#shutil-高级文件操作" class="headerlink" title="shutil   高级文件操作"></a><a href="https://docs.python.org/3/library/shutil.html" target="_blank" rel="noopener">shutil</a>   高级文件操作</h4><blockquote><p>shutil 模块提供了许多关于文件和文件集合的高级操作</p></blockquote><h5 id="移动文件"><a href="#移动文件" class="headerlink" title="移动文件"></a>移动文件</h5><pre><code class="hljs python">In [<span class="hljs-number">11</span>]: <span class="hljs-keyword">import</span> shutilIn [<span class="hljs-number">12</span>]: shutil.move(<span class="hljs-string">'text1'</span>, <span class="hljs-string">'text'</span>)Out[<span class="hljs-number">12</span>]: <span class="hljs-string">'text'</span>In [<span class="hljs-number">13</span>]: os.system(<span class="hljs-string">'ls'</span>)Data  PythonClassEnv  ReadMe.md  textOut[<span class="hljs-number">13</span>]: <span class="hljs-number">0</span></code></pre><h5 id="复制文件夹"><a href="#复制文件夹" class="headerlink" title="复制文件夹"></a>复制文件夹</h5><pre><code class="hljs python">In [<span class="hljs-number">11</span>]: <span class="hljs-keyword">import</span> shutilIn [<span class="hljs-number">12</span>]: shutil.move(<span class="hljs-string">'text1'</span>, <span class="hljs-string">'text'</span>)Out[<span class="hljs-number">12</span>]: <span class="hljs-string">'text'</span>In [<span class="hljs-number">13</span>]: os.system(<span class="hljs-string">'ls'</span>)Data  PythonClassEnv  ReadMe.md  textOut[<span class="hljs-number">13</span>]: <span class="hljs-number">0</span></code></pre><h5 id="删除文件"><a href="#删除文件" class="headerlink" title="删除文件"></a>删除文件</h5><pre><code class="hljs python">In [<span class="hljs-number">11</span>]: <span class="hljs-keyword">import</span> shutilIn [<span class="hljs-number">12</span>]: shutil.move(<span class="hljs-string">'text1'</span>, <span class="hljs-string">'text'</span>)Out[<span class="hljs-number">12</span>]: <span class="hljs-string">'text'</span>In [<span class="hljs-number">13</span>]: os.system(<span class="hljs-string">'ls'</span>)Data  PythonClassEnv  ReadMe.md  textOut[<span class="hljs-number">13</span>]: <span class="hljs-number">0</span></code></pre>]]></content>
    
    
    <categories>
      
      <category>python</category>
      
    </categories>
    
    
    <tags>
      
      <tag>文件</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>python第十话之描述器和装饰器</title>
    <link href="/2018/12/python-@/"/>
    <url>/2018/12/python-@/</url>
    
    <content type="html"><![CDATA[<h3 id="单例模式"><a href="#单例模式" class="headerlink" title="单例模式"></a>单例模式</h3><p>类每次实例化的时候都会创建一个新的对象，如果要求类只能被实例化一次该怎么做呢？</p><pre><code class="hljs python"><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">Earth</span>:</span>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span><span class="hljs-params">(self)</span>:</span>        self.name = <span class="hljs-string">'earth'</span>e = Earth()print(e, id(e))a = Earth()print(a, id(a))</code></pre><blockquote><p>按照我们上一节讲的，类可以多个实例化</p></blockquote><pre><code class="hljs python">&lt;__main__.Earth object at <span class="hljs-number">0x000001D54B28A978</span>&gt; <span class="hljs-number">2015600617848</span>&lt;__main__.Earth object at <span class="hljs-number">0x000001D54B293EF0</span>&gt; <span class="hljs-number">2015600656112</span></code></pre><blockquote><p>我们可以看出，多个实例化，每个实例化的地址都不相同。</p></blockquote><pre><code class="hljs python"><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">Earth</span>:</span>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__new__</span><span class="hljs-params">(cls, *args, **kwargs)</span>:</span>        <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> hasattr(cls, <span class="hljs-string">'instance'</span>):            cls.instance = super().__new__(cls)        <span class="hljs-keyword">return</span> cls.instance    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span><span class="hljs-params">(self)</span>:</span>        self.name = <span class="hljs-string">'earth'</span>e = Earth()print(e, id(e))a = Earth()print(a, id(a))</code></pre><blockquote><p>类的实例化的时候，会在init前调用new方法。</p></blockquote><pre><code class="hljs python">&lt;__main__.Earth object at <span class="hljs-number">0x0000024EF2DBA940</span>&gt; <span class="hljs-number">2538105186624</span>&lt;__main__.Earth object at <span class="hljs-number">0x0000024EF2DBA940</span>&gt; <span class="hljs-number">2538105186624</span></code></pre><blockquote><p>可以看出两次创建对象，结果返回的是同一个对象实例</p></blockquote><h4 id="变量共享"><a href="#变量共享" class="headerlink" title="变量共享"></a>变量共享</h4><pre><code class="hljs python"><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">Earth</span>:</span>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__new__</span><span class="hljs-params">(cls, *args, **kwargs)</span>:</span>        <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> hasattr(cls, <span class="hljs-string">'instance'</span>):            cls.instance = super().__new__(cls)        <span class="hljs-keyword">return</span> cls.instance    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span><span class="hljs-params">(self, name)</span>:</span>        self.name = namee = Earth(<span class="hljs-string">'china'</span>)print(e, id(e))a = Earth(<span class="hljs-string">'others'</span>)print(a, id(a))print(e.name)print(a.name)</code></pre><p><strong>输出</strong></p><pre><code class="hljs python">&lt;__main__.Earth object at <span class="hljs-number">0x000002077AA33E80</span>&gt; <span class="hljs-number">2231145545344</span>&lt;__main__.Earth object at <span class="hljs-number">0x000002077AA33E80</span>&gt; <span class="hljs-number">2231145545344</span>othersothers</code></pre><h4 id="应用"><a href="#应用" class="headerlink" title="应用"></a><strong>应用</strong></h4><blockquote><ul><li>Python的logger就是一个单例模式，用以日志记录</li><li>Windows的资源管理器是一个单例模式</li><li>线程池，数据库连接池等资源池一般也用单例模式</li><li>网站计数器</li></ul></blockquote><h4 id="使用情况"><a href="#使用情况" class="headerlink" title="使用情况"></a>使用情况</h4><blockquote><p>当每个实例都会占用资源，而且实例初始化会影响性能，这个时候就可以考虑使用单例模式，它给我们带来的好处是只有一个实例占用资源，并且只需初始化一次；</p><p>当有同步需要的时候，可以通过一个实例来进行同步控制，比如对某个共享文件（如日志文件）的控制，对计数器的同步控制等，这种情况下由于只有一个实例，所以不用担心同步问题。</p></blockquote><h4 id="总结"><a href="#总结" class="headerlink" title="总结"></a><strong>总结</strong></h4><blockquote><p>初始化函数之前：<strong>new</strong>方法会在初始化函数<strong>init</strong>方法之前执行。</p><p>单例模式：利用这个<strong>new</strong>方法可以很方便的实现类的单例模式。</p><p>合理利用：<strong>new</strong> 方法合理利用可以带来方便，常应用在类的单例模式。</p></blockquote><h3 id="定制属性访问"><a href="#定制属性访问" class="headerlink" title="定制属性访问"></a>定制属性访问</h3><blockquote><p>如何判断一个实例里面有某个属性呢？</p><p>怎样删除实例属性呢？</p><p>同样的怎样删除变量呢？</p></blockquote><pre><code class="hljs python"><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">Rectangle</span>:</span>    <span class="hljs-comment"># 传入长和宽</span>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span><span class="hljs-params">(self, length, width)</span>:</span>        self.length = length        self.width = width    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">area</span><span class="hljs-params">(self)</span>:</span>        area = self.width * self.length        <span class="hljs-keyword">return</span> areab = Rectangle(<span class="hljs-number">3</span>,<span class="hljs-number">4</span>)</code></pre><blockquote><p>接下来我们来对类的属性进行定制化</p></blockquote><h4 id="增加属性"><a href="#增加属性" class="headerlink" title="增加属性"></a>增加属性</h4><h5 id="setattr"><a href="#setattr" class="headerlink" title="setattr"></a><code>setattr</code></h5><pre><code class="hljs python">setattr(b, <span class="hljs-string">'s'</span>, <span class="hljs-number">12</span>)b.sOut[<span class="hljs-number">4</span>]: <span class="hljs-number">12</span>setattr(b, <span class="hljs-string">'h'</span>, <span class="hljs-number">6</span>)b.hOut[<span class="hljs-number">6</span>]: <span class="hljs-number">6</span></code></pre><blockquote><p>往类的属性里面添加方法并赋值。</p></blockquote><pre><code class="hljs python">b.__setattr__(<span class="hljs-string">'s'</span>, <span class="hljs-number">5</span>)b.__setattr__(<span class="hljs-string">'h'</span>, <span class="hljs-number">15</span>)b.sOut[<span class="hljs-number">12</span>]: <span class="hljs-number">5</span>b.hOut[<span class="hljs-number">13</span>]: <span class="hljs-number">15</span></code></pre><blockquote><p>等价于类的对应魔术方法</p></blockquote><h4 id="删除属性"><a href="#删除属性" class="headerlink" title="删除属性"></a>删除属性</h4><h5 id="delattr"><a href="#delattr" class="headerlink" title="delattr"></a><code>delattr</code></h5><pre><code class="hljs python">delattr(b, <span class="hljs-string">'s'</span>)delattr(b, <span class="hljs-string">'h'</span>)</code></pre><blockquote><p>删除属性</p></blockquote><pre><code class="hljs python">b.__delattr__(<span class="hljs-string">'s'</span>)b.__delattr__(<span class="hljs-string">'h'</span>)</code></pre><blockquote><p>等价于类的对应魔术方法</p></blockquote><h4 id="修改属性"><a href="#修改属性" class="headerlink" title="修改属性"></a>修改属性</h4><h5 id="setattr-1"><a href="#setattr-1" class="headerlink" title="setattr"></a><code>setattr</code></h5><pre><code class="hljs python">b.sOut[<span class="hljs-number">5</span>]: <span class="hljs-number">5</span>b.hOut[<span class="hljs-number">6</span>]: <span class="hljs-number">15</span>setattr(b, <span class="hljs-string">'s'</span>, <span class="hljs-number">20</span>)setattr(b, <span class="hljs-string">'h'</span>, <span class="hljs-number">20</span>)b.sOut[<span class="hljs-number">9</span>]: <span class="hljs-number">20</span>b.hOut[<span class="hljs-number">10</span>]: <span class="hljs-number">20</span></code></pre><blockquote><p>同样是使用setattr来修改属性</p></blockquote><pre><code class="hljs python">b.__setattr__(<span class="hljs-string">'s'</span>, <span class="hljs-number">20</span>)b.__setattr__(<span class="hljs-string">'h'</span>, <span class="hljs-number">20</span>)b.sOut[<span class="hljs-number">12</span>]: <span class="hljs-number">20</span>b.hOut[<span class="hljs-number">13</span>]: <span class="hljs-number">20</span></code></pre><blockquote><p>等价于类的对应魔术方法</p></blockquote><h4 id="查找属性"><a href="#查找属性" class="headerlink" title="查找属性"></a>查找属性</h4><h5 id="hasattr"><a href="#hasattr" class="headerlink" title="hasattr"></a><code>hasattr</code></h5><pre><code class="hljs python">hasattr(b, <span class="hljs-string">'s'</span>)Out[<span class="hljs-number">11</span>]: <span class="hljs-literal">True</span>hasattr(b, <span class="hljs-string">'h'</span>)Out[<span class="hljs-number">12</span>]: <span class="hljs-literal">True</span>hasattr(b, <span class="hljs-string">'x'</span>)Out[<span class="hljs-number">13</span>]: <span class="hljs-literal">False</span></code></pre><blockquote><p>有对应属性就返回True，否则就返回Flase</p></blockquote><h5 id="getattr"><a href="#getattr" class="headerlink" title="getattr"></a><code>getattr</code></h5><pre><code class="hljs python">getattr(b, <span class="hljs-string">'s'</span>)Out[<span class="hljs-number">14</span>]: <span class="hljs-number">20</span>getattr(b, <span class="hljs-string">'h'</span>)Out[<span class="hljs-number">15</span>]: <span class="hljs-number">20</span>getattr(b, <span class="hljs-string">'x'</span>)Traceback (most recent call last):  File <span class="hljs-string">"C:\Program Files\Python36\lib\site-packages\IPython\core\interactiveshell.py"</span>, line <span class="hljs-number">3265</span>, <span class="hljs-keyword">in</span> run_code    exec(code_obj, self.user_global_ns, self.user_ns)  File <span class="hljs-string">"&lt;ipython-input-16-ae1b4378a11a&gt;"</span>, line <span class="hljs-number">1</span>, <span class="hljs-keyword">in</span> &lt;module&gt;    getattr(b, <span class="hljs-string">'x'</span>)AttributeError: <span class="hljs-string">'Rectangle'</span> object has no attribute <span class="hljs-string">'x'</span></code></pre><blockquote><p>有就返回属性值，没有就报错。</p></blockquote><pre><code class="hljs python">b.__getattribute__(<span class="hljs-string">'s'</span>)Out[<span class="hljs-number">17</span>]: <span class="hljs-number">20</span>b.__getattribute__(<span class="hljs-string">'x'</span>)Traceback (most recent call last):  File <span class="hljs-string">"C:\Program Files\Python36\lib\site-packages\IPython\core\interactiveshell.py"</span>, line <span class="hljs-number">3265</span>, <span class="hljs-keyword">in</span> run_code    exec(code_obj, self.user_global_ns, self.user_ns)  File <span class="hljs-string">"&lt;ipython-input-18-f65163239ef2&gt;"</span>, line <span class="hljs-number">1</span>, <span class="hljs-keyword">in</span> &lt;module&gt;    b.__getattribute__(<span class="hljs-string">'x'</span>)AttributeError: <span class="hljs-string">'Rectangle'</span> object has no attribute <span class="hljs-string">'x'</span></code></pre><blockquote><p>等价于类的对应魔术方法</p></blockquote><h4 id="扩展"><a href="#扩展" class="headerlink" title="扩展"></a>扩展</h4><blockquote><p>我们在查询属性的时候，使用getattr，如果没有属性值，又不想报错怎么办呢？</p></blockquote><pre><code class="hljs python"><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">Rectangle</span>:</span>    <span class="hljs-comment"># 传入长和宽</span>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span><span class="hljs-params">(self, length, width)</span>:</span>        self.length = length        self.width = width    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">area</span><span class="hljs-params">(self)</span>:</span>        area = self.width * self.length        <span class="hljs-keyword">return</span> area    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__getattribute__</span><span class="hljs-params">(self, item)</span>:</span>        print(<span class="hljs-string">"没有这个属性！"</span>)b = Rectangle(<span class="hljs-number">3</span>,<span class="hljs-number">4</span>)</code></pre><pre><code class="hljs python">getattr(b, <span class="hljs-string">'s'</span>)没有这个属性！</code></pre><blockquote><p>当属性不存在时，如果定义了此方法，则调用方法</p></blockquote><h4 id="总结-1"><a href="#总结-1" class="headerlink" title="总结"></a>总结</h4><p><img src="http://eveseven.oss-cn-shanghai.aliyuncs.com/18-12-19/104326.jpg" srcset="/img/loading.gif" alt="属性访问"></p><blockquote><p>hasattr:  判断是否存在属性，如果属性存在则进行下一步操作。</p><p>getattr: 得到属性值。</p><p>setattr：设置属性。</p></blockquote><h3 id="描述符"><a href="#描述符" class="headerlink" title="描述符"></a>描述符</h3><p>如果在一个类中实例化另一个类，对这个属性进行访问的时候怎么做的？</p><pre><code class="hljs python"><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">MyAtrribute</span>:</span>    <span class="hljs-keyword">pass</span><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">MyClass</span>:</span>    m = MyAtrribute()c = MyClass() c.m</code></pre><pre><code class="hljs python">&lt;__main__.MyAtrribute object at <span class="hljs-number">0x000001F4922CF2E8</span>&gt;</code></pre><blockquote><p>返回的是对象</p></blockquote><pre><code class="hljs python"><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">MyAtrribute</span>:</span>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__get__</span><span class="hljs-params">(self, instance, owner)</span>:</span>        print(<span class="hljs-string">'get'</span>)        print(instance)        print(owner)<span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">MyClass</span>:</span>    m = MyAtrribute()c = MyClass()print(c.m)</code></pre><pre><code class="hljs python">get&lt;__main__.MyClass object at <span class="hljs-number">0x00000258C5703E80</span>&gt;&lt;<span class="hljs-class"><span class="hljs-keyword">class</span> '<span class="hljs-title">__main__</span>.<span class="hljs-title">MyClass</span>'&gt;</span><span class="hljs-class"><span class="hljs-title">None</span></span></code></pre><blockquote><p>直接访问时，调用get方法</p></blockquote><pre><code class="hljs python"><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">MyAtrribute</span>:</span>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__get__</span><span class="hljs-params">(self, instance, owner)</span>:</span>        print(<span class="hljs-string">'get'</span>)        print(instance)        print(owner)    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__set__</span><span class="hljs-params">(self, instance, value)</span>:</span>        print(instance)        print(value)        print(<span class="hljs-string">'set'</span>)    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__delete__</span><span class="hljs-params">(self, instance)</span>:</span>        print(instance)        print(<span class="hljs-string">'delete'</span>)<span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">MyClass</span>:</span>    m = MyAtrribute()    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__del__</span><span class="hljs-params">(self)</span>:</span>        print(<span class="hljs-string">'__del__'</span>)c = MyClass()<span class="hljs-comment"># 调用 __set__</span>c.m = <span class="hljs-number">1</span><span class="hljs-comment"># 调用 __deletel__</span><span class="hljs-keyword">del</span> c.mdelattr(c, <span class="hljs-string">'m'</span>)</code></pre><pre><code class="hljs python">&lt;__main__.MyClass object at <span class="hljs-number">0x000002312DAEBF28</span>&gt;<span class="hljs-number">1</span>set&lt;__main__.MyClass object at <span class="hljs-number">0x000002312DAEBF28</span>&gt;delete&lt;__main__.MyClass object at <span class="hljs-number">0x000002312DAEBF28</span>&gt;delete__del__</code></pre><blockquote><p>根据访问时带使用不同的方式，调用不用的属性。</p></blockquote><h4 id="总结-2"><a href="#总结-2" class="headerlink" title="总结"></a>总结</h4><blockquote><p>描述符大家了解即可</p><p>魔术方法的作用其实是让开发人员能够更加灵活的控制类的表现形式</p></blockquote><h3 id="装饰器"><a href="#装饰器" class="headerlink" title="装饰器"></a>装饰器</h3><p>之前我们讲了闭包，闭包中可以传入一个函数吗？</p><pre><code class="hljs python"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">fx</span><span class="hljs-params">(x)</span>:</span>    x += <span class="hljs-number">1</span>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">fy</span><span class="hljs-params">(y)</span>:</span>        <span class="hljs-keyword">return</span> x + y    <span class="hljs-keyword">return</span> fya = fx(<span class="hljs-number">1</span>)print(a(<span class="hljs-number">12</span>))</code></pre><pre><code class="hljs python"><span class="hljs-number">12</span></code></pre><blockquote><p>这是我们前面所见过的闭包</p></blockquote><pre><code class="hljs python"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">f1</span><span class="hljs-params">(func)</span>:</span>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">f2</span><span class="hljs-params">(y)</span>:</span>        print(<span class="hljs-string">'f2 running'</span>)        <span class="hljs-keyword">return</span> func(y) + <span class="hljs-number">1</span>    <span class="hljs-keyword">return</span> f2<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">f3</span><span class="hljs-params">(m)</span>:</span>    print(<span class="hljs-string">'f3 running'</span>)    <span class="hljs-keyword">return</span> m * ma = f1(f3)print(a)print(a(<span class="hljs-number">3</span>))</code></pre><pre><code class="hljs python">&lt;function f1.&lt;locals&gt;.f2 at <span class="hljs-number">0x0000026D2F4BA488</span>&gt;f2 runningf3 running<span class="hljs-number">10</span></code></pre><blockquote><p>闭包传入函数</p></blockquote><h4 id="语法糖"><a href="#语法糖" class="headerlink" title="语法糖"></a>语法糖</h4><pre><code class="hljs python"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">f1</span><span class="hljs-params">(func)</span>:</span>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">f2</span><span class="hljs-params">(y)</span>:</span>        print(<span class="hljs-string">'f2 running'</span>)        <span class="hljs-keyword">return</span> func(y) + <span class="hljs-number">1</span>    <span class="hljs-keyword">return</span> f2<span class="hljs-meta">@f1  # 语法糖</span><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">f3</span><span class="hljs-params">(m)</span>:</span>    print(<span class="hljs-string">'f3 running'</span>)    <span class="hljs-keyword">return</span> m * mprint(f3(<span class="hljs-number">3</span>))  <span class="hljs-comment"># f3 = f1(f3)(3)</span></code></pre><pre><code class="hljs python">f2 runningf3 running<span class="hljs-number">10</span></code></pre><blockquote><p>在Python中直接用语法糖，f3(3) = f1(f3)(3)</p></blockquote><h4 id="内置装饰器"><a href="#内置装饰器" class="headerlink" title="内置装饰器"></a>内置装饰器</h4><h5 id="property"><a href="#property" class="headerlink" title="@property"></a><code>@property</code></h5><pre><code class="hljs python"><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">Rectangle</span>:</span>    <span class="hljs-comment"># 传入长和宽</span>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span><span class="hljs-params">(self, length, width)</span>:</span>        self.length = length        self.width = width<span class="hljs-meta">    @property</span>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">area</span><span class="hljs-params">(self)</span>:</span>        area = self.width * self.length        <span class="hljs-keyword">return</span> area    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">area1</span><span class="hljs-params">(self)</span>:</span>        area = self.width * self.length        <span class="hljs-keyword">return</span> area    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__getattr__</span><span class="hljs-params">(self, item)</span>:</span>        print(<span class="hljs-string">'no attribute'</span>)b = Rectangle(<span class="hljs-number">3</span>,<span class="hljs-number">4</span>)print(b.area1())print(b.area)</code></pre><pre><code class="hljs python"><span class="hljs-number">12</span><span class="hljs-number">12</span></code></pre><blockquote><p>访问函数时，就像访问属性一样</p></blockquote><h5 id="staticmethod"><a href="#staticmethod" class="headerlink" title="@staticmethod"></a><code>@staticmethod</code></h5><pre><code class="hljs python"><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">Rectangle</span>:</span>    <span class="hljs-comment"># 传入长和宽</span>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span><span class="hljs-params">(self, length, width)</span>:</span>        self.length = length        self.width = width<span class="hljs-meta">    @property</span>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">area</span><span class="hljs-params">(self)</span>:</span>        area = self.width * self.length        <span class="hljs-keyword">return</span> area<span class="hljs-meta">    @staticmethod</span>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">func</span><span class="hljs-params">()</span>:</span>        print(<span class="hljs-string">'func'</span>)    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__getattr__</span><span class="hljs-params">(self, item)</span>:</span>        print(<span class="hljs-string">'no attribute'</span>)b = Rectangle(<span class="hljs-number">3</span>,<span class="hljs-number">4</span>)Rectangle.func()b.func()</code></pre><pre><code class="hljs python">funcfunc</code></pre><blockquote><p>静态方法</p></blockquote><h5 id="classmethod"><a href="#classmethod" class="headerlink" title="@classmethod"></a><code>@classmethod</code></h5><pre><code class="hljs python"><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">Rectangle</span>:</span>    <span class="hljs-comment"># 传入长和宽</span>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span><span class="hljs-params">(self, length, width)</span>:</span>        self.length = length        self.width = width<span class="hljs-meta">    @property</span>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">area</span><span class="hljs-params">(self)</span>:</span>        area = self.width * self.length        <span class="hljs-keyword">return</span> area<span class="hljs-meta">    @staticmethod</span>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">func</span><span class="hljs-params">()</span>:</span>        print(<span class="hljs-string">'func'</span>)<span class="hljs-meta">    @classmethod</span>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">show</span><span class="hljs-params">(cls)</span>:</span>        print(cls)        print(<span class="hljs-string">'show'</span>)    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">fun2</span><span class="hljs-params">(self)</span>:</span>        print(self)        print(<span class="hljs-string">'fun2'</span>)    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__getattr__</span><span class="hljs-params">(self, item)</span>:</span>        print(<span class="hljs-string">'no attribute'</span>)b = Rectangle(<span class="hljs-number">3</span>,<span class="hljs-number">4</span>)b.show()b.fun2()Rectangle.show()Rectangle.fun2(b)</code></pre><pre><code class="hljs python">&lt;<span class="hljs-class"><span class="hljs-keyword">class</span> '<span class="hljs-title">__main__</span>.<span class="hljs-title">Rectangle</span>'&gt;</span><span class="hljs-class"><span class="hljs-title">show</span></span><span class="hljs-class">&lt;<span class="hljs-title">__main__</span>.<span class="hljs-title">Rectangle</span> <span class="hljs-title">object</span> <span class="hljs-title">at</span> 0<span class="hljs-title">x000001897E3519B0</span>&gt;</span><span class="hljs-class"><span class="hljs-title">fun2</span></span><span class="hljs-class">&lt;<span class="hljs-title">class</span> '<span class="hljs-title">__main__</span>.<span class="hljs-title">Rectangle</span>'&gt;</span><span class="hljs-class"><span class="hljs-title">show</span></span><span class="hljs-class">&lt;<span class="hljs-title">__main__</span>.<span class="hljs-title">Rectangle</span> <span class="hljs-title">object</span> <span class="hljs-title">at</span> 0<span class="hljs-title">x000001897E3519B0</span>&gt;</span><span class="hljs-class"><span class="hljs-title">fun2</span></span></code></pre><blockquote><p>类方法：cls代表类本身，如果加上self，在调用时就要把实例传入。</p></blockquote><h4 id="类装饰器"><a href="#类装饰器" class="headerlink" title="类装饰器"></a>类装饰器</h4><pre><code class="hljs python"><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">Test_Class</span>:</span>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span><span class="hljs-params">(self, func)</span>:</span>        self.func = func    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__call__</span><span class="hljs-params">(self)</span>:</span>        print(<span class="hljs-string">'类'</span>)        print(self.func())        <span class="hljs-keyword">return</span> self.func<span class="hljs-meta">@Test_Class</span><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">fun_test</span><span class="hljs-params">()</span>:</span>    print(<span class="hljs-string">'这是个测试函数'</span>)</code></pre><pre><code class="hljs python">类这是个测试函数<span class="hljs-literal">None</span>&lt;function fun_test at <span class="hljs-number">0x000001E024B28730</span>&gt;</code></pre><blockquote><p>类也可以做装饰器，但是需要定义<strong>call</strong> 方法</p></blockquote><h4 id="扩展-1"><a href="#扩展-1" class="headerlink" title="扩展"></a>扩展</h4><blockquote><p>查看函数运行时间的装饰器</p></blockquote><pre><code class="hljs python"><span class="hljs-keyword">import</span> time<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">run_time</span><span class="hljs-params">(func)</span>:</span>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">new_fun</span><span class="hljs-params">(*args,**kwargs)</span>:</span>        t0 = time.time()        print(<span class="hljs-string">'star time: %s'</span>%(time.strftime(<span class="hljs-string">'%x'</span>,time.localtime())) )        back = func(*args,**kwargs)        print(<span class="hljs-string">'end time: %s'</span>%(time.strftime(<span class="hljs-string">'%x'</span>,time.localtime())) )        print(<span class="hljs-string">'run time: %s'</span>%(time.time() - t0))        <span class="hljs-keyword">return</span> back    <span class="hljs-keyword">return</span> new_fun<span class="hljs-meta">@run_time</span><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">demo</span><span class="hljs-params">()</span>:</span>    print(<span class="hljs-string">'1213'</span>)demo()</code></pre><pre><code class="hljs python">star time: <span class="hljs-number">12</span>/<span class="hljs-number">19</span>/<span class="hljs-number">18</span><span class="hljs-number">1213</span>end time: <span class="hljs-number">12</span>/<span class="hljs-number">19</span>/<span class="hljs-number">18</span>run time: <span class="hljs-number">0.0</span></code></pre><h4 id="总结-3"><a href="#总结-3" class="headerlink" title="总结"></a><strong>总结</strong></h4><blockquote><h5 id="装饰器本质是闭包，在不影响原函数使用的情况下，增加原函数功能。"><a href="#装饰器本质是闭包，在不影响原函数使用的情况下，增加原函数功能。" class="headerlink" title="装饰器本质是闭包，在不影响原函数使用的情况下，增加原函数功能。"></a>装饰器本质是闭包，在不影响原函数使用的情况下，增加原函数功能。</h5><p>内置装饰器：三个内置装饰器是需要掌握的，在项目中会经常使用。</p></blockquote>]]></content>
    
    
    <categories>
      
      <category>python</category>
      
    </categories>
    
    
    <tags>
      
      <tag>描述器</tag>
      
      <tag>装饰器承</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>python第九话之多继承和魔法方法</title>
    <link href="/2018/12/python-magic/"/>
    <url>/2018/12/python-magic/</url>
    
    <content type="html"><![CDATA[<h3 id="多继承和魔法方法"><a href="#多继承和魔法方法" class="headerlink" title="多继承和魔法方法"></a>多继承和魔法方法</h3><p>在上一节中，我们讲了类的定义，属性和方法，那么我们这个节课来看看<code>多继承</code>和<code>魔法方法</code>。</p><h3 id="多继承"><a href="#多继承" class="headerlink" title="多继承"></a>多继承</h3><p>在上节我们讲到了继承，一个类可以继承一个类，继承之后可以把父类所有的方法和属性都直接继承过来，那一个类可以继承多个类呢？</p><p>如果可以继承多个类的话，那如果两个父类中有一样的方法的情况下，子类继承哪一个呢？</p><h4 id="演示"><a href="#演示" class="headerlink" title="演示"></a>演示</h4><pre><code class="hljs python"><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">Base</span>:</span>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">play</span><span class="hljs-params">(self)</span>:</span>        <span class="hljs-keyword">return</span> <span class="hljs-string">'this is Base'</span><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">A</span><span class="hljs-params">(Base)</span>:</span>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">play</span><span class="hljs-params">(self)</span>:</span>        <span class="hljs-keyword">return</span> <span class="hljs-string">'this is A'</span><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">B</span><span class="hljs-params">(Base)</span>:</span>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">play</span><span class="hljs-params">(self)</span>:</span>        <span class="hljs-keyword">return</span> <span class="hljs-string">'this is B'</span> <span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">C</span><span class="hljs-params">(A,B)</span>:</span>    <span class="hljs-keyword">pass</span>c = C()print(c.play())</code></pre><p><strong>输出</strong></p><pre><code class="hljs python">this <span class="hljs-keyword">is</span> A</code></pre><blockquote><p>首先类是可以多继承的</p><p>优先使用第一个类里面的方法。</p></blockquote><p><strong>总结</strong></p><p><img src="http://eveseven.oss-cn-shanghai.aliyuncs.com/18-12-13/99683112.jpg" srcset="/img/loading.gif" alt="1"></p><blockquote><p>通过C类实例的方法调用来看</p><p>当继承多个父类时，如果父类中有相同的方法，那么子类会优先使用最先被继承的方法.</p></blockquote><h4 id="重构"><a href="#重构" class="headerlink" title="重构"></a>重构</h4><p>在上面的例子中，如果不想继承父类的方法怎么办呢？</p><pre><code class="hljs python"><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">Base</span>:</span>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">play</span><span class="hljs-params">(self)</span>:</span>        <span class="hljs-keyword">return</span> <span class="hljs-string">'this is Base'</span><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">A</span><span class="hljs-params">(Base)</span>:</span>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">play</span><span class="hljs-params">(self)</span>:</span>        <span class="hljs-keyword">return</span> <span class="hljs-string">'this is A'</span><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">B</span><span class="hljs-params">(Base)</span>:</span>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">play</span><span class="hljs-params">(self)</span>:</span>        <span class="hljs-keyword">return</span> <span class="hljs-string">'this is B'</span><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">C</span><span class="hljs-params">(A, B)</span>:</span>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">play</span><span class="hljs-params">(self)</span>:</span>        <span class="hljs-keyword">return</span> <span class="hljs-string">'this is C'</span>c = C()print(c.play())</code></pre><p><strong>输出</strong></p><pre><code class="hljs python">this <span class="hljs-keyword">is</span> C</code></pre><blockquote><p>当子类继承父类之后，如果子类不想使用父类的方法，可以通过重写来覆盖父类的方法.</p></blockquote><h4 id="扩展"><a href="#扩展" class="headerlink" title="扩展"></a>扩展</h4><p>重写父类方法之后，如果又需要使用父类的方法呢？</p><h5 id="方法一"><a href="#方法一" class="headerlink" title="方法一"></a>方法一</h5><pre><code class="hljs python"><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">Base</span>:</span>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">play</span><span class="hljs-params">(self)</span>:</span>        print(<span class="hljs-string">'this is Base'</span>)<span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">A</span><span class="hljs-params">(Base)</span>:</span>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">play</span><span class="hljs-params">(self)</span>:</span>        print(<span class="hljs-string">'this is A'</span>)<span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">B</span><span class="hljs-params">(Base)</span>:</span>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">play</span><span class="hljs-params">(self)</span>:</span>        print(<span class="hljs-string">'this is B'</span>)<span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">C</span><span class="hljs-params">(A, B)</span>:</span>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">play</span><span class="hljs-params">(self)</span>:</span>        A.play(self)        print(<span class="hljs-string">'这是C'</span>)demo = C()demo.play()</code></pre><p><strong>输出</strong></p><pre><code class="hljs python">this <span class="hljs-keyword">is</span> A这是C</code></pre><h5 id="方法二"><a href="#方法二" class="headerlink" title="方法二"></a>方法二</h5><pre><code class="hljs python"><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">Base</span>:</span>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">play</span><span class="hljs-params">(self)</span>:</span>        print(<span class="hljs-string">'this is Base'</span>)<span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">A</span><span class="hljs-params">(Base)</span>:</span>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">play</span><span class="hljs-params">(self)</span>:</span>        print(<span class="hljs-string">'this is A'</span>)<span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">B</span><span class="hljs-params">(Base)</span>:</span>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">play</span><span class="hljs-params">(self)</span>:</span>        print(<span class="hljs-string">'this is B'</span>)<span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">C</span><span class="hljs-params">(A, B)</span>:</span>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">play</span><span class="hljs-params">(self)</span>:</span>        super().play()        print(<span class="hljs-string">'这是C'</span>)demo = C()demo.play()</code></pre><p><strong>输出</strong></p><pre><code class="hljs python">this <span class="hljs-keyword">is</span> A这是C</code></pre><h4 id="super"><a href="#super" class="headerlink" title="super"></a>super</h4><blockquote><p> super函数可以调用父类的方法</p></blockquote><pre><code class="hljs python"><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">Base</span>:</span>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">play</span><span class="hljs-params">(self)</span>:</span>        print(<span class="hljs-string">'this is Base'</span>)<span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">A</span><span class="hljs-params">(Base)</span>:</span>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">play</span><span class="hljs-params">(self)</span>:</span>        super().play()        print(<span class="hljs-string">'this is A'</span>)<span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">B</span><span class="hljs-params">(Base)</span>:</span>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">play</span><span class="hljs-params">(self)</span>:</span>        super().play()        print(<span class="hljs-string">'this is B'</span>)<span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">C</span><span class="hljs-params">(A, B)</span>:</span>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">play</span><span class="hljs-params">(self)</span>:</span>        super().play()        print(<span class="hljs-string">'这是C'</span>)demo = C()demo.play()</code></pre><p><strong>输出</strong></p><pre><code class="hljs python">this <span class="hljs-keyword">is</span> Basethis <span class="hljs-keyword">is</span> Bthis <span class="hljs-keyword">is</span> A这是C</code></pre><p>那为什么是这个顺序输出呢？</p><pre><code class="hljs python">print(C.mro())</code></pre><p><strong>输出</strong></p><pre><code class="hljs python">[&lt;class '__main__.C'&gt;, &lt;class '__main__.A'&gt;, &lt;class '__main__.B'&gt;, &lt;class '__main__.Base'&gt;, &lt;class 'object'&gt;]</code></pre><h4 id="继承顺序："><a href="#继承顺序：" class="headerlink" title="继承顺序："></a>继承顺序：</h4><p><img src="http://eveseven.oss-cn-shanghai.aliyuncs.com/18-12-13/86231356.jpg" srcset="/img/loading.gif" alt="2"></p><blockquote><p>在python3中，类被创建时会自动创建方法解析顺序mro</p><p>object是所有类的父类</p></blockquote><h4 id="Mixin开发模式"><a href="#Mixin开发模式" class="headerlink" title="Mixin开发模式"></a>Mixin开发模式</h4><p><img src="http://eveseven.oss-cn-shanghai.aliyuncs.com/18-12-13/85222178.jpg" srcset="/img/loading.gif" alt="3"></p><blockquote><p>Mixin是一种开发模式，一般规范上，Mixin类是继承的终点，即不再被继承</p><p>Mixin的优点就是不需要过多考虑继承关系，不会出现各父类之间有相同方法的情况</p></blockquote><h4 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h4><blockquote><p>mro: 类在生成时会自动生成方法解析顺序，可以通过  类名.mro()来查看</p><p>super: super函数可以来调用父类的方法，使用super的好处在于即使父类改变了，那么也不需要更改类中的代码</p><p>Mixin: Mixin是一种开发模式，给大家在今后的开发中提供一种思路.</p></blockquote><h3 id="魔法方法"><a href="#魔法方法" class="headerlink" title="魔法方法"></a>魔法方法</h3><p>在讲字符串拼接的时候，字符串可以直接相加，那我们自定义的类可以实现吗？</p><pre><code class="hljs python"><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">Rectangle</span>:</span>    <span class="hljs-comment"># 传入长和宽</span>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span><span class="hljs-params">(self, length, width)</span>:</span>        self.length = length        self.width = width    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">area</span><span class="hljs-params">(self)</span>:</span>        area = self.width * self.length        <span class="hljs-keyword">return</span> area    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__add__</span><span class="hljs-params">(self, other)</span>:</span>        add_length = self.length + other.length        add_width = self.width + other.width        <span class="hljs-keyword">return</span> add_length,add_widtha = Rectangle(<span class="hljs-number">3</span>,<span class="hljs-number">4</span>)b = Rectangle(<span class="hljs-number">5</span>,<span class="hljs-number">6</span>)print(a+b)</code></pre><p><strong>输出</strong></p><blockquote><p>(8, 10)</p></blockquote><h4 id="运算方法"><a href="#运算方法" class="headerlink" title="运算方法"></a>运算方法</h4><p><img src="http://eveseven.oss-cn-shanghai.aliyuncs.com/18-12-13/78302936.jpg" srcset="/img/loading.gif" alt="4"></p><blockquote><p>运算方法大家了解下就行，在实际运用中用不并不多。</p></blockquote><h5 id="add和radd原理"><a href="#add和radd原理" class="headerlink" title="add和radd原理"></a>add和radd原理</h5><pre><code class="hljs python"><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">A</span>:</span>    <span class="hljs-keyword">pass</span><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">B</span>:</span>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__add__</span><span class="hljs-params">(self, other)</span>:</span>        print(<span class="hljs-string">'__add__'</span>)    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__radd__</span><span class="hljs-params">(self, other)</span>:</span>        print(<span class="hljs-string">'__radd__'</span>)a = A()b = B()a+b</code></pre><p><strong>输出</strong></p><pre><code class="hljs python">__radd__</code></pre><blockquote><p>优先在两类里找add方法，没有就自动调用radd方法。</p></blockquote><p><strong>演示</strong></p><pre><code class="hljs python"><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">A</span>:</span>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span><span class="hljs-params">(self,name,age)</span>:</span>        self.name =  name        self.age = age<span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">B</span>:</span>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span><span class="hljs-params">(self,age)</span>:</span>        self.age = age    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__add__</span><span class="hljs-params">(self, other)</span>:</span>        print(<span class="hljs-string">'__add__'</span>)    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__radd__</span><span class="hljs-params">(self, other)</span>:</span>        <span class="hljs-keyword">return</span> other.age + self.agea = A(<span class="hljs-string">'age'</span>,<span class="hljs-number">123</span>)b = B(<span class="hljs-number">123</span>)print(a+b)</code></pre><p><strong>输出</strong></p><pre><code class="hljs python"><span class="hljs-number">246</span></code></pre><h4 id="str和repr原理"><a href="#str和repr原理" class="headerlink" title="str和repr原理"></a>str和repr原理</h4><pre><code class="hljs python"><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">Rectangle</span>:</span>    <span class="hljs-comment"># 传入长和宽</span>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span><span class="hljs-params">(self, length, width)</span>:</span>        self.length = length        self.width = width    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">area</span><span class="hljs-params">(self)</span>:</span>        area = self.width * self.length        <span class="hljs-keyword">return</span> area    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__add__</span><span class="hljs-params">(self, other)</span>:</span>        add_length = self.length + other.length        add_width = self.width + other.width        <span class="hljs-keyword">return</span> add_length, add_width    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__radd__</span><span class="hljs-params">(self, other)</span>:</span>        <span class="hljs-keyword">return</span> <span class="hljs-string">"Rectangle radd"</span>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__str__</span><span class="hljs-params">(self)</span>:</span>        <span class="hljs-keyword">return</span> <span class="hljs-string">'length is %s, width is %s '</span> % (self.length, self.width)    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__repr__</span><span class="hljs-params">(self)</span>:</span>        <span class="hljs-keyword">return</span> <span class="hljs-string">'area  is %s'</span> % self.area()a = Rectangle(<span class="hljs-number">3</span>,<span class="hljs-number">4</span>)b = Rectangle(<span class="hljs-number">5</span>,<span class="hljs-number">6</span>)print(a+b)print(a.__add__(b))print(a)</code></pre><p><strong>输出</strong>：</p><pre><code class="hljs python"><span class="hljs-comment"># 有str</span>(<span class="hljs-number">8</span>, <span class="hljs-number">10</span>)(<span class="hljs-number">8</span>, <span class="hljs-number">10</span>)length <span class="hljs-keyword">is</span> <span class="hljs-number">3</span>, width <span class="hljs-keyword">is</span> <span class="hljs-number">4</span> <span class="hljs-comment"># 无str</span>(<span class="hljs-number">8</span>, <span class="hljs-number">10</span>)(<span class="hljs-number">8</span>, <span class="hljs-number">10</span>)area  <span class="hljs-keyword">is</span> <span class="hljs-number">12</span></code></pre><blockquote><p>优先在两类里找str方法，没有就自动调用repr方法。</p><p>在python中，str和repr方法在处理对象的时候，分别调用的是对象的<strong>str</strong>和<strong>repr</strong>方法</p><p>print也是如此，调用str函数来处理输出的对象，如果对象没有定义<strong>str</strong>方法，则调用repr处理</p></blockquote><h4 id="call方法"><a href="#call方法" class="headerlink" title="call方法"></a>call方法</h4><pre><code class="hljs python"><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">Rectangle</span>:</span>    <span class="hljs-comment"># 传入长和宽</span>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span><span class="hljs-params">(self, length, width)</span>:</span>        self.length = length        self.width = width    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">area</span><span class="hljs-params">(self)</span>:</span>        area = self.width * self.length        <span class="hljs-keyword">return</span> area    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__add__</span><span class="hljs-params">(self, other)</span>:</span>        add_length = self.length + other.length        add_width = self.width + other.width        <span class="hljs-keyword">return</span> add_length, add_width    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__radd__</span><span class="hljs-params">(self, other)</span>:</span>        <span class="hljs-keyword">return</span> <span class="hljs-string">"Rectangle radd"</span>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__str__</span><span class="hljs-params">(self)</span>:</span>        <span class="hljs-keyword">return</span> <span class="hljs-string">'length is %s, width is %s '</span> % (self.length, self.width)    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__repr__</span><span class="hljs-params">(self)</span>:</span>        <span class="hljs-keyword">return</span> <span class="hljs-string">'area  is %s'</span> % self.area()    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__call__</span><span class="hljs-params">(self)</span>:</span>        <span class="hljs-keyword">return</span> <span class="hljs-string">'Rectangle called'</span>a = Rectangle(<span class="hljs-number">3</span>,<span class="hljs-number">4</span>)b = Rectangle(<span class="hljs-number">5</span>,<span class="hljs-number">6</span>)print(a())</code></pre><p><strong>输出</strong></p><pre><code class="hljs python">Rectangle called</code></pre><blockquote><p>正常情况下，实例是不能像函数一样被调用的，要想实例能够被调用，就需要定义 <strong>call</strong>  方法</p></blockquote><h4 id="其他魔法方法"><a href="#其他魔法方法" class="headerlink" title="其他魔法方法"></a>其他魔法方法</h4><p><img src="http://eveseven.oss-cn-shanghai.aliyuncs.com/18-12-13/42006401.jpg" srcset="/img/loading.gif" alt="5"></p><p><strong>演示</strong>：</p><pre><code class="hljs python"><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">Rectangle</span>:</span>    <span class="hljs-comment"># 传入长和宽</span>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span><span class="hljs-params">(self, length, width)</span>:</span>        self.length = length        self.width = width    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">area</span><span class="hljs-params">(self)</span>:</span>        area = self.width * self.length        <span class="hljs-keyword">return</span> area    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__add__</span><span class="hljs-params">(self, other)</span>:</span>        add_length = self.length + other.length        add_width = self.width + other.width        <span class="hljs-keyword">return</span> add_length, add_width    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__radd__</span><span class="hljs-params">(self, other)</span>:</span>        <span class="hljs-keyword">return</span> <span class="hljs-string">"Rectangle radd"</span>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__str__</span><span class="hljs-params">(self)</span>:</span>        <span class="hljs-keyword">return</span> <span class="hljs-string">'length is %s, width is %s '</span> % (self.length, self.width)    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__repr__</span><span class="hljs-params">(self)</span>:</span>        <span class="hljs-keyword">return</span> <span class="hljs-string">'area  is %s'</span> % self.area()    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__call__</span><span class="hljs-params">(self)</span>:</span>        <span class="hljs-keyword">return</span> <span class="hljs-string">'Rectangle called'</span>a = Rectangle(<span class="hljs-number">3</span>,<span class="hljs-number">4</span>)print(a.__class__)print(a.__class__.__base__)print(a.__class__.__bases__)print(a.__dict__)   <span class="hljs-comment"># 所有属性，键值对返回</span>print(a.__doc__)print(a.__dir__())</code></pre><p><strong>输出</strong></p><pre><code class="hljs python">&lt;<span class="hljs-class"><span class="hljs-keyword">class</span> '<span class="hljs-title">__main__</span>.<span class="hljs-title">Rectangle</span>'&gt;</span><span class="hljs-class">&lt;<span class="hljs-title">class</span> '<span class="hljs-title">object</span>'&gt;</span><span class="hljs-class"><span class="hljs-params">(&lt;class <span class="hljs-string">'object'</span>&gt;,)</span></span>&#123;'length': 3, 'width': 4&#125;<span class="hljs-literal">None</span>[<span class="hljs-string">'length'</span>, <span class="hljs-string">'width'</span>, <span class="hljs-string">'__module__'</span>, <span class="hljs-string">'__init__'</span>, <span class="hljs-string">'area'</span>, <span class="hljs-string">'__add__'</span>, <span class="hljs-string">'__radd__'</span>, <span class="hljs-string">'__str__'</span>, <span class="hljs-string">'__repr__'</span>, <span class="hljs-string">'__call__'</span>, <span class="hljs-string">'__dict__'</span>, <span class="hljs-string">'__weakref__'</span>, <span class="hljs-string">'__doc__'</span>, <span class="hljs-string">'__hash__'</span>, <span class="hljs-string">'__getattribute__'</span>, <span class="hljs-string">'__setattr__'</span>, <span class="hljs-string">'__delattr__'</span>, <span class="hljs-string">'__lt__'</span>, <span class="hljs-string">'__le__'</span>, <span class="hljs-string">'__eq__'</span>, <span class="hljs-string">'__ne__'</span>, <span class="hljs-string">'__gt__'</span>, <span class="hljs-string">'__ge__'</span>, <span class="hljs-string">'__new__'</span>, <span class="hljs-string">'__reduce_ex__'</span>, <span class="hljs-string">'__reduce__'</span>, <span class="hljs-string">'__subclasshook__'</span>, <span class="hljs-string">'__init_subclass__'</span>, <span class="hljs-string">'__format__'</span>, <span class="hljs-string">'__sizeof__'</span>, <span class="hljs-string">'__dir__'</span>, <span class="hljs-string">'__class__'</span>]</code></pre><blockquote><p>简单了解。</p></blockquote><h4 id="魔法方法应用场景"><a href="#魔法方法应用场景" class="headerlink" title="魔法方法应用场景"></a><strong>魔法方法应用场景</strong></h4><blockquote><p><strong>str</strong>和<strong>repr</strong>: str和repr都是分别调用这两个魔术方法来实现的</p><p>原理：在类中，很多事情其实调用的魔术方法来实现的</p><p>作用：通过合理的利用魔术方法，可以让我们更加方便的展示我们的数据</p></blockquote>]]></content>
    
    
    <categories>
      
      <category>python</category>
      
    </categories>
    
    
    <tags>
      
      <tag>多继承</tag>
      
      <tag>魔法方法承</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>python第八话之类定义、属性和继承</title>
    <link href="/2018/12/python-class/"/>
    <url>/2018/12/python-class/</url>
    
    <content type="html"><![CDATA[<h3 id="类定义、属性和继承"><a href="#类定义、属性和继承" class="headerlink" title="类定义、属性和继承"></a>类定义、属性和继承</h3><p>面向对象是一种编程思想，所以这一章内容会比较抽象，大家可以先学会怎么去写，后面用的多了写的多了自然就理解了。在第一章中大概阐述了python中的类与类型，前面学过的基本数据类型就是类，这里就来自定义类。</p><h3 id="类定义"><a href="#类定义" class="headerlink" title="类定义"></a>类定义</h3><p>之前我们在数据类型里面学习到了列表的方法，那是怎么做的可以让列表里面放下这么多方法呢？</p><pre><code class="hljs python"><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">Abc</span>:</span>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">fun1</span><span class="hljs-params">(self)</span>:</span>        print(<span class="hljs-string">'this is fun1'</span>)    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">fun2</span><span class="hljs-params">(self)</span>:</span>        print(<span class="hljs-string">'this is fun2'</span>)a=Abc()print(a)print(a.fun1())print(a.fun2())</code></pre><p><strong>输出</strong></p><pre><code class="hljs python">&lt;__main__.Abc object at <span class="hljs-number">0x0000024ED872A908</span>&gt;this <span class="hljs-keyword">is</span> fun1<span class="hljs-literal">None</span>this <span class="hljs-keyword">is</span> fun2<span class="hljs-literal">None</span></code></pre><blockquote><p>cla  = ClassName()</p><p>cla.fun1()</p><p>cla.fun2()</p><p>实例化之后，可以实现类似于列表中方法的定义形式</p></blockquote><h4 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h4><blockquote><p>定义：累得定义使用关键字 class</p><p>封装：类可以把各种对象组织在一起，通过.(点)运算符来调用类中封装好的对象。</p><p>概念：类就像是我们平时说的名词，一个称呼，但是却不是一个具体的实例，比如说：我们都是人，但是人这个名词，不能具体指代你我，我们会用一个人的名字去指代一个具体的人，这个过程就类似于实例化。</p></blockquote><h3 id="属性"><a href="#属性" class="headerlink" title="属性"></a>属性</h3><p>1.类数据属性。类属性是可以直接通过“类名.属性名”来访问和修改。类属性是这个类的所有实例对象所共有的属性，任意一个实例对象都可以访问并修改这个属性（私有隐藏除外）。</p><p>2.实例数据属性。在属性前面加了self标识的属性为实例的属性，在定义的时候用的self加属性名字的形式，在查看实例的属性时就是通过实例的名称+‘.’+属性名来访问实例属性。</p><p>3.方法属性。定义属性方法的内容是函数，函数的第一个参数是self，代表实例本身。</p><h4 id="举个栗子"><a href="#举个栗子" class="headerlink" title="举个栗子"></a><strong>举个栗子</strong></h4><pre><code class="hljs python"><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">Animal</span>:</span>    eye = <span class="hljs-number">2</span>  <span class="hljs-comment"># 类属性</span>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span><span class="hljs-params">(self, name, food)</span>:</span>        self.name = name  <span class="hljs-comment"># 实例属性</span>        self.food = food    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">play</span><span class="hljs-params">(self)</span>:</span>        print(<span class="hljs-string">'hahaha'</span>)</code></pre><h4 id="实例化"><a href="#实例化" class="headerlink" title="实例化"></a>实例化</h4><pre><code class="hljs python">cat = Animal(<span class="hljs-string">'cat'</span>,<span class="hljs-string">'fish'</span>) <span class="hljs-comment">#先不传值</span>cat.play()</code></pre><p><strong>输出</strong></p><pre><code class="hljs python">hahaha</code></pre><blockquote><p>类的实例化，实例化后会自动执行<strong>init</strong>这个初始化函数。</p></blockquote><h4 id="实例属性访问"><a href="#实例属性访问" class="headerlink" title="实例属性访问"></a>实例属性访问</h4><pre><code class="hljs python">print(cat.name)print(cat.food)</code></pre><p><strong>输出</strong></p><pre><code class="hljs python">hahahacatfish</code></pre><blockquote><p>实例的属性，实例自己可以访问，定义时有加self，不可以 ClassName. attribute（类名.属性）</p></blockquote><h4 id="类属性"><a href="#类属性" class="headerlink" title="类属性"></a>类属性</h4><pre><code class="hljs python">print(Animal.eye)print(cat.eye)</code></pre><p><strong>输出</strong></p><pre><code class="hljs python"><span class="hljs-number">2</span><span class="hljs-number">2</span></code></pre><blockquote><p>直接定义在类中，类和实例都可以访问，没有加self</p><p>​       可以  ClassName. attribute（类名.属性）</p></blockquote><h4 id="扩展"><a href="#扩展" class="headerlink" title="扩展"></a>扩展</h4><pre><code class="hljs python">print(Animal.name)</code></pre><p><strong>输出</strong></p><pre><code class="hljs python">AttributeError: type object <span class="hljs-string">'Animal'</span> has no attribute <span class="hljs-string">'name'</span></code></pre><blockquote><p>类只能访问类属性，不能访问实例属性。</p></blockquote><h4 id="总结-1"><a href="#总结-1" class="headerlink" title="总结"></a>总结</h4><blockquote><p>类属性：类的属性，类名和实例都可以调用，相当于类和实例公用的变量</p><p>实例属性：实例自己的属性，类不能访问，其他的实例也不能访问</p><p>属性调用： 通过属性调用可以直接得到属性的属性值</p></blockquote><h3 id="方法"><a href="#方法" class="headerlink" title="方法"></a>方法</h3><p>类中的方法，就是函数，但是被称之为方法，在类中的方法，在被实例调用的时候会自动传入实例本身，因此，在一般情况下，需要在参数中加入self。</p><pre><code class="hljs python"><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">Animal</span>:</span>    eye = <span class="hljs-number">2</span>  <span class="hljs-comment"># 类属性</span>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span><span class="hljs-params">(self, name, food)</span>:</span>        self.name = name  <span class="hljs-comment"># 实例属性</span>        self.food = food    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">play</span><span class="hljs-params">(self)</span>:</span>        print(<span class="hljs-string">'hahaha'</span>)</code></pre><h4 id="方法调用"><a href="#方法调用" class="headerlink" title="方法调用"></a>方法调用</h4><pre><code class="hljs python">cat = Animal(<span class="hljs-string">'cat'</span>,<span class="hljs-string">'fish'</span>) cat.play()</code></pre><p><strong>输出</strong></p><pre><code class="hljs python">hahaha</code></pre><blockquote><p>类中的self指代的就是实例本身</p></blockquote><h4 id="扩展-1"><a href="#扩展-1" class="headerlink" title="扩展"></a>扩展</h4><pre><code class="hljs python">cat = Animal(<span class="hljs-string">'cat'</span>,<span class="hljs-string">'fish'</span>) Animal.play(cat)</code></pre><p><strong>输出</strong></p><pre><code class="hljs python">hahaha</code></pre><h3 id="继承"><a href="#继承" class="headerlink" title="继承"></a>继承</h3><p>如果在B类中定义一个方法，但是这个方法已经在A类中被定义过了，那怎样在B类中使用A类中的方法呢？</p><pre><code class="hljs python"><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">Animal</span>:</span>    eye = <span class="hljs-number">2</span>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span><span class="hljs-params">(self, name, food)</span>:</span>        self.name = name        self.food = food    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">play</span><span class="hljs-params">(self)</span>:</span>        print(<span class="hljs-string">'hahaha'</span>)<span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">Dog</span><span class="hljs-params">(Animal)</span>:</span>     <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">wangwang</span><span class="hljs-params">(self)</span>:</span>         print(<span class="hljs-string">'汪汪汪！！%s'</span> %self.name)demo = Dog(<span class="hljs-string">'旺财'</span>, <span class="hljs-string">'骨头'</span>)print(demo.name)print(demo.food)print(demo.wangwang())</code></pre><p><strong>输出</strong></p><pre><code class="hljs python">旺财骨头汪汪汪！！旺财</code></pre><h4 id="语法规则"><a href="#语法规则" class="headerlink" title="语法规则"></a>语法规则</h4><blockquote><p>class    A:</p><p>  def  play(slef):</p><p>  print(‘hahaha ‘)</p><p>class  B(A):</p><p>  pass</p></blockquote><h4 id="总结-2"><a href="#总结-2" class="headerlink" title="总结"></a>总结</h4><blockquote><p>​      类的继承可以让子类将父类的全部方法和属性继承过来.</p><p>​      在python3中，默认继承object类</p></blockquote>]]></content>
    
    
    <categories>
      
      <category>python</category>
      
    </categories>
    
    
    <tags>
      
      <tag>类</tag>
      
      <tag>属性</tag>
      
      <tag>继承</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>python第七话之函数作用域和匿名函数</title>
    <link href="/2018/12/python-function/"/>
    <url>/2018/12/python-function/</url>
    
    <content type="html"><![CDATA[<h3 id="函数作用域和匿名函数"><a href="#函数作用域和匿名函数" class="headerlink" title="函数作用域和匿名函数"></a>函数作用域和匿名函数</h3><p>本节知识点：<code>匿名函数</code>、<code>闭包</code>、<code>函数作用域</code>、<code>递归函数</code>。</p><h3 id="匿名函数"><a href="#匿名函数" class="headerlink" title="匿名函数"></a>匿名函数</h3><p>上节我们讲过的filter函数，可以过滤出列表中大于10的数据，但是使用都需要提前定义一个函数，有没有更加简便的方式呢？</p><p>我们先来复习下<code>filter</code>函数：</p><pre><code class="hljs python">In [<span class="hljs-number">1</span>]: s = [<span class="hljs-number">3</span>, <span class="hljs-number">20</span>, <span class="hljs-number">4</span>, <span class="hljs-number">6</span>, <span class="hljs-number">5</span>, <span class="hljs-number">9</span>]In [<span class="hljs-number">2</span>]: sOut[<span class="hljs-number">2</span>]: [<span class="hljs-number">3</span>, <span class="hljs-number">20</span>, <span class="hljs-number">4</span>, <span class="hljs-number">6</span>, <span class="hljs-number">5</span>, <span class="hljs-number">9</span>]In [<span class="hljs-number">3</span>]: <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">fun1</span><span class="hljs-params">(x)</span>:</span>   ...:     <span class="hljs-keyword">return</span> x&gt;<span class="hljs-number">5</span>In [<span class="hljs-number">4</span>]: filter(fun1, s)Out[<span class="hljs-number">4</span>]: &lt;filter at <span class="hljs-number">0x7f4a2038f080</span>&gt;In [<span class="hljs-number">5</span>]: list(filter(fun1, s))Out[<span class="hljs-number">5</span>]: [<span class="hljs-number">20</span>, <span class="hljs-number">6</span>, <span class="hljs-number">9</span>]</code></pre><blockquote><p>那如果有大量的操作是不是很麻烦，那有没有更简单的方法呢？</p></blockquote><h4 id="lambda"><a href="#lambda" class="headerlink" title="lambda"></a>lambda</h4><pre><code class="hljs python">In [<span class="hljs-number">6</span>]: sOut[<span class="hljs-number">6</span>]: [<span class="hljs-number">3</span>, <span class="hljs-number">20</span>, <span class="hljs-number">4</span>, <span class="hljs-number">6</span>, <span class="hljs-number">5</span>, <span class="hljs-number">9</span>]In [<span class="hljs-number">7</span>]: list(filter(<span class="hljs-keyword">lambda</span> x: x&gt;<span class="hljs-number">5</span>, s))Out[<span class="hljs-number">7</span>]: [<span class="hljs-number">20</span>, <span class="hljs-number">6</span>, <span class="hljs-number">9</span>]</code></pre><blockquote><p>Python中，lambda函数也叫匿名函数，及即没有具体名称的函数，它允许快速定义单行函数，类似于C语言的宏，可以用在任何需要函数的地方。这区别于def定义的函数。 </p></blockquote><h4 id="语法规则"><a href="#语法规则" class="headerlink" title="语法规则"></a>语法规则</h4><blockquote><p>lambda  参数： 表达式</p></blockquote><h3 id="应用场景"><a href="#应用场景" class="headerlink" title="应用场景"></a>应用场景</h3><blockquote><p>简单函数： 简单的函数，可以不用使用def定义一个函数，使用匿名函数。</p><p>函数调用：类似于<code>filter</code>、<code>map</code>等函数里面，可以使用匿名函数来处理</p><p>提高开发效率：匿名函数的合理使用能够代码更加简洁。</p></blockquote><pre><code class="hljs python">li = [<span class="hljs-number">3</span>, <span class="hljs-number">4</span>, <span class="hljs-number">5</span>, <span class="hljs-number">6</span>, <span class="hljs-number">98</span>, <span class="hljs-string">'a'</span>, <span class="hljs-number">5.2</span>]print(sorted(li, key=<span class="hljs-keyword">lambda</span> x: x <span class="hljs-keyword">if</span> isinstance(x, (int, float)) <span class="hljs-keyword">else</span> ord(str(x))))</code></pre><p><strong>输出</strong></p><pre><code class="hljs python">[<span class="hljs-number">3</span>, <span class="hljs-number">4</span>, <span class="hljs-number">5</span>, <span class="hljs-number">5.2</span>, <span class="hljs-number">6</span>, <span class="hljs-string">'a'</span>, <span class="hljs-number">98</span>]</code></pre><h4 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h4><blockquote><p>lambda与def的区别：<br>1）def创建的方法是有名称的，而lambda没有。<br>2）lambda会返回一个函数对象，但这个对象不会赋给一个标识符，而def则会把函数对象赋值给一个变量（函数名）。<br>3）lambda只是一个表达式，而def则是一个语句。<br>4）lambda表达式” : “后面，只能有一个表达式，def则可以有多个。<br>5）像if（三元运算符可以）或for（列表推导式可以）或print等语句不能用于lambda中，def可以。<br>6）lambda一般用来定义简单的函数，而def可以定义复杂的函数。<br>6）lambda函数不能共享给别的程序调用，def可以。 </p></blockquote><h3 id="函数作用域"><a href="#函数作用域" class="headerlink" title="函数作用域"></a>函数作用域</h3><p>在函数里面也有可以定义变量，那函数里面的变量名如果和函数外面的变量名重名，会相互影响吗？</p><h4 id="外部不能访问函数内部变量"><a href="#外部不能访问函数内部变量" class="headerlink" title="外部不能访问函数内部变量"></a>外部不能访问函数内部变量</h4><pre><code class="hljs python"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">fun1</span><span class="hljs-params">()</span>:</span>x = <span class="hljs-number">1</span><span class="hljs-keyword">return</span> xprint(x) <span class="hljs-comment">#不能访问函数里面</span></code></pre><p><strong>输出</strong></p><pre><code class="hljs python">NameError: name <span class="hljs-string">'x'</span> <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> defined</code></pre><h4 id="函数内部能够访问函数外部变量"><a href="#函数内部能够访问函数外部变量" class="headerlink" title="函数内部能够访问函数外部变量"></a>函数内部能够访问函数外部变量</h4><pre><code class="hljs python">x = <span class="hljs-number">123</span><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">fun2</span><span class="hljs-params">()</span>:</span>print(x)<span class="hljs-keyword">return</span> x + <span class="hljs-number">1</span>print(fun2())    <span class="hljs-comment">#函数里面可以使用全局变量</span></code></pre><p><strong>输出</strong></p><pre><code class="hljs python"><span class="hljs-number">123</span><span class="hljs-number">124</span></code></pre><h4 id="函数里面不能修改函数外部变量"><a href="#函数里面不能修改函数外部变量" class="headerlink" title="函数里面不能修改函数外部变量"></a>函数里面不能修改函数外部变量</h4><pre><code class="hljs python">x = <span class="hljs-number">123</span><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">fun3</span><span class="hljs-params">()</span>:</span>x = x + <span class="hljs-number">1</span>   <span class="hljs-comment">#直接报错，可以访问但是不能修改 </span>print(x)<span class="hljs-keyword">return</span> x + <span class="hljs-number">1</span>print(fun3())</code></pre><p><strong>输出</strong></p><pre><code class="hljs python">UnboundLocalError: local variable <span class="hljs-string">'x'</span> referenced before assignment</code></pre><h4 id="函数里面和函数外部变量名相同"><a href="#函数里面和函数外部变量名相同" class="headerlink" title="函数里面和函数外部变量名相同"></a>函数里面和函数外部变量名相同</h4><pre><code class="hljs python">x = <span class="hljs-number">123</span>print(x, id(x))<span class="hljs-function"><span class="hljs-keyword">def</span>  <span class="hljs-title">fun4</span><span class="hljs-params">()</span>:</span>x = <span class="hljs-number">456</span>print(x, id(x))x += <span class="hljs-number">1</span><span class="hljs-keyword">return</span> xprint(fun4())</code></pre><p><strong>输出</strong></p><pre><code class="hljs python"><span class="hljs-number">123</span> <span class="hljs-number">1722514256</span><span class="hljs-number">456</span> <span class="hljs-number">1982594811888</span><span class="hljs-number">457</span></code></pre><h4 id="扩展"><a href="#扩展" class="headerlink" title="扩展"></a>扩展</h4><h5 id="global"><a href="#global" class="headerlink" title="global"></a><code>global</code></h5><pre><code class="hljs python">x = <span class="hljs-number">123</span><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">fun5</span><span class="hljs-params">()</span>:</span><span class="hljs-keyword">global</span> x   <span class="hljs-comment">#如果非要改变全局的变量</span>x += <span class="hljs-number">1</span><span class="hljs-keyword">return</span> xprint(fun5())</code></pre><p><strong>输出</strong></p><pre><code class="hljs python"><span class="hljs-number">124</span></code></pre><blockquote><p>函数内部如果需要修改全局变量，就需要使用global修饰变量。</p></blockquote><h5 id="nonlocal"><a href="#nonlocal" class="headerlink" title="nonlocal"></a><code>nonlocal</code></h5><pre><code class="hljs python"><span class="hljs-function"><span class="hljs-keyword">def</span>  <span class="hljs-title">fun6</span><span class="hljs-params">()</span>:</span>x = <span class="hljs-number">123</span><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">fun7</span><span class="hljs-params">()</span>:</span><span class="hljs-keyword">nonlocal</span> x  <span class="hljs-comment">#python2没有，python3独有的</span>x += <span class="hljs-number">1</span><span class="hljs-keyword">return</span>  x<span class="hljs-keyword">return</span> fun7()print(fun6())</code></pre><p><strong>输出</strong></p><pre><code class="hljs python"><span class="hljs-number">124</span></code></pre><blockquote><p>在函数嵌套的情况下，同样也有函数作用域的问题，但是在Python3中提供了方便，只需要使用nonlocal就可以在里层函数内部修改外部函数的变量。</p></blockquote><h4 id="总结-1"><a href="#总结-1" class="headerlink" title="总结"></a>总结</h4><p><img src="http://eveseven.oss-cn-shanghai.aliyuncs.com/18-12-13/95671711.jpg" srcset="/img/loading.gif" alt="1"></p><blockquote><p>函数内部： 函数内部的变量，作业域只在函数内部，函数内部不可以直接更改函数外部的变量。</p><p>global：函数内部如果需要修改全局变量，就需要使用global修饰变量。</p><p>nonlocal:  在函数嵌套的情况下，同样也有函数作用域的问题，但是在Python3中提供了方便，只需要使用nonlocal就可以在里层函数内部修改外部函数的变量。</p></blockquote><h3 id="闭包"><a href="#闭包" class="headerlink" title="闭包"></a>闭包</h3><p>函数里面可以再定义函数，那函数里面定义的函数可以在外面调用吗？</p><h4 id="函数调用"><a href="#函数调用" class="headerlink" title="函数调用"></a>函数调用</h4><pre><code class="hljs python">g = <span class="hljs-keyword">lambda</span> x: x&gt;<span class="hljs-number">10</span>gOut[<span class="hljs-number">3</span>]: &lt;function __main__.&lt;<span class="hljs-keyword">lambda</span>&gt;(x)&gt;g(<span class="hljs-number">100</span>)Out[<span class="hljs-number">4</span>]: <span class="hljs-literal">True</span>h = ghOut[<span class="hljs-number">6</span>]: &lt;function __main__.&lt;<span class="hljs-keyword">lambda</span>&gt;(x)&gt;h(<span class="hljs-number">111</span>)Out[<span class="hljs-number">7</span>]: <span class="hljs-literal">True</span></code></pre><blockquote><p>在进行函数调用的时候需要加上<code>()</code>，才能进行使用。</p></blockquote><h4 id="内嵌函数"><a href="#内嵌函数" class="headerlink" title="内嵌函数"></a>内嵌函数</h4><pre><code class="hljs python"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">fun6</span><span class="hljs-params">()</span>:</span>    x = <span class="hljs-number">123</span>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">fun7</span><span class="hljs-params">()</span>:</span>        <span class="hljs-keyword">nonlocal</span> x        x += <span class="hljs-number">1</span>        <span class="hljs-keyword">return</span>  x    <span class="hljs-keyword">return</span> fun7f7=fun6()print(f7())</code></pre><p><strong>输出</strong></p><pre><code class="hljs python"><span class="hljs-number">124</span></code></pre><h4 id="总结-2"><a href="#总结-2" class="headerlink" title="总结"></a>总结</h4><blockquote><p>闭包是函数里面嵌套函数，外层函数返回里层函数，这种情况称之为闭包。</p><p>闭包是概念，不是某种函数类型，和递归的概念类似，就是种特殊的函数调用。</p><p>闭包可以得到外层函数的局部变量，是函数内部和函数外部沟通的桥梁。</p></blockquote><h3 id="递归函数"><a href="#递归函数" class="headerlink" title="递归函数"></a>递归函数</h3><p>函数里面可以自身调用自身吗</p><pre><code class="hljs python"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">fun8</span><span class="hljs-params">(n)</span>:</span>    <span class="hljs-keyword">if</span> n == <span class="hljs-number">1</span>:        <span class="hljs-keyword">return</span> <span class="hljs-number">1</span>    <span class="hljs-keyword">return</span> fun8(n - <span class="hljs-number">1</span>)* nprint(fun8(<span class="hljs-number">8</span>))</code></pre><p><strong>输出</strong></p><pre><code class="hljs python"><span class="hljs-number">40320</span></code></pre><blockquote><p>递归中可以函数自身调用自身，但是使用时类似于条件循环一样，要有递归的终止条件</p></blockquote><h4 id="总结-3"><a href="#总结-3" class="headerlink" title="总结"></a>总结</h4><blockquote><p>使用递归时，常常可以让代码更加简洁</p><p>递归会占用比较多的内存，当递归次数比较多时，性能就会降低，因此不建议多使用递归</p></blockquote>]]></content>
    
    
    <categories>
      
      <category>python</category>
      
    </categories>
    
    
    <tags>
      
      <tag>函数作用域</tag>
      
      <tag>匿名函数</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>python第六话之函数基础和函数参数</title>
    <link href="/2018/12/python-main/"/>
    <url>/2018/12/python-main/</url>
    
    <content type="html"><![CDATA[<p>[TOC]</p><h3 id="函数基础和函数参数"><a href="#函数基础和函数参数" class="headerlink" title="函数基础和函数参数"></a>函数基础和函数参数</h3><p>函数是组织好的，可重复使用的，用来实现单一，或相关联功能的代码段。</p><p>函数能提高应用的模块性，和代码的重复利用率。你已经知道Python提供了许多内建函数，比如print()。但你也可以自己创建函数，这被叫做用户自定义函数。</p><h3 id="函数基础"><a href="#函数基础" class="headerlink" title="函数基础"></a>函数基础</h3><h4 id="定义一个函数"><a href="#定义一个函数" class="headerlink" title="定义一个函数"></a>定义一个函数</h4><blockquote><p>你可以定义一个由自己想要功能的函数，以下是简单的规则：</p><ul><li>函数代码块以 <strong>def</strong> 关键词开头，后接函数标识符名称和圆括号 <strong>()</strong>。</li><li>任何传入参数和自变量必须放在圆括号中间，圆括号之间可以用于定义参数。</li><li>函数的第一行语句可以选择性地使用文档字符串—用于存放函数说明。</li><li>函数内容以冒号起始，并且缩进。</li><li><strong>return [表达式]</strong> 结束函数，选择性地返回一个值给调用方。不带表达式的return相当于返回 None。</li></ul></blockquote><p><strong>演示</strong>：</p><blockquote><p>我们上节课实现了打印列表，如果我们打印几个列表呢？</p></blockquote><pre><code class="hljs python">li = [<span class="hljs-number">1</span>, <span class="hljs-number">0</span>, <span class="hljs-number">5</span>, <span class="hljs-number">7</span>, <span class="hljs-number">9</span>]<span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> li:    print(i)print(<span class="hljs-string">'---------'</span>)li = [<span class="hljs-number">1</span>, <span class="hljs-string">'A'</span>, <span class="hljs-number">5</span>, <span class="hljs-number">7</span>, <span class="hljs-number">9</span>]<span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> li:    print(i)print(<span class="hljs-string">'---------'</span>)li = [<span class="hljs-number">1</span>, <span class="hljs-number">3</span>, <span class="hljs-string">'s'</span>, <span class="hljs-number">7</span>, <span class="hljs-number">9</span>]<span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> li:    print(i)</code></pre><p><strong>输出结果：</strong></p><pre><code class="hljs python"><span class="hljs-number">1</span><span class="hljs-number">0</span><span class="hljs-number">5</span><span class="hljs-number">7</span><span class="hljs-number">9</span>---------<span class="hljs-number">1</span>A<span class="hljs-number">5</span><span class="hljs-number">7</span><span class="hljs-number">9</span>---------<span class="hljs-number">1</span><span class="hljs-number">3</span>s<span class="hljs-number">7</span><span class="hljs-number">9</span></code></pre><blockquote><p>以我们上节所讲的知识点，如果要打印三个列表的话，就是上述这种方法，那还有没有更简单的呢？</p></blockquote><p><strong>演示</strong>：</p><pre><code class="hljs angelscript">l1 = [<span class="hljs-number">1</span>, <span class="hljs-number">0</span>, <span class="hljs-number">5</span>, <span class="hljs-number">7</span>, <span class="hljs-number">9</span>]l2 = [<span class="hljs-number">1</span>, <span class="hljs-string">'A'</span>, <span class="hljs-number">5</span>, <span class="hljs-number">7</span>, <span class="hljs-number">9</span>]l3 = [<span class="hljs-number">1</span>, <span class="hljs-number">0</span>, <span class="hljs-string">'S'</span>, <span class="hljs-number">7</span>, <span class="hljs-number">9</span>]def demo(li):<span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> li:print(i)demo(l1)print(<span class="hljs-string">'---------'</span>)demo(l2)print(<span class="hljs-string">'---------'</span>)demo(l3)</code></pre><p><strong>输出结果</strong></p><pre><code class="hljs python"><span class="hljs-number">1</span><span class="hljs-number">0</span><span class="hljs-number">5</span><span class="hljs-number">7</span><span class="hljs-number">9</span>---------<span class="hljs-number">1</span>A<span class="hljs-number">5</span><span class="hljs-number">7</span><span class="hljs-number">9</span>---------<span class="hljs-number">1</span><span class="hljs-number">0</span>S<span class="hljs-number">7</span><span class="hljs-number">9</span></code></pre><blockquote><p>上述就是使用函数的形式来实现多个列表的打印，是不是比前面的更简单。</p></blockquote><h4 id="函数的定义"><a href="#函数的定义" class="headerlink" title="函数的定义"></a>函数的定义</h4><blockquote><p>def  函数名(参数)：</p><p>​        pass</p><p>​        return    表达式</p><p>函数名命名规则： 字母、数字和下划线组成，和变量命名规则一致</p><p>return 后面可以返回任意表达式，但不能是赋值语句</p><p>注意：函数名定义和变量名的定义是一样的，只能使用字母、数字和下划线定义，不能以数字开头。</p></blockquote><h4 id="关键字"><a href="#关键字" class="headerlink" title="关键字"></a>关键字</h4><blockquote><p>关键字是不能拿来做变量定义的。</p></blockquote><p><strong>演示：</strong></p><pre><code class="hljs python">In [<span class="hljs-number">3</span>]: a---------------------------------------------------------------------------NameError                                 Traceback (most recent call last)&lt;ipython-input<span class="hljs-number">-3</span><span class="hljs-number">-3</span>f786850e387&gt; <span class="hljs-keyword">in</span> &lt;module&gt;()----&gt; 1 aNameError: name <span class="hljs-string">'a'</span> <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> definedIn [<span class="hljs-number">4</span>]: <span class="hljs-function"><span class="hljs-keyword">def</span></span>  File "&lt;ipython-input-4-7b18d017f89f&gt;", line 1    <span class="hljs-function"><span class="hljs-keyword">def</span></span><span class="hljs-function">       ^</span><span class="hljs-function"><span class="hljs-title">SyntaxError</span>:</span> invalid syntax</code></pre><blockquote><p>如果把关键字拿来定义，是会报语法错误的。</p></blockquote><pre><code class="hljs python">In [<span class="hljs-number">1</span>]: <span class="hljs-keyword">import</span> keywordIn [<span class="hljs-number">2</span>]: print(keyword.kwlist)[<span class="hljs-string">'False'</span>, <span class="hljs-string">'None'</span>, <span class="hljs-string">'True'</span>, <span class="hljs-string">'and'</span>, <span class="hljs-string">'as'</span>, <span class="hljs-string">'assert'</span>, <span class="hljs-string">'break'</span>, <span class="hljs-string">'class'</span>, <span class="hljs-string">'continue'</span>, <span class="hljs-string">'def'</span>, <span class="hljs-string">'del'</span>, <span class="hljs-string">'elif'</span>, <span class="hljs-string">'else'</span>, <span class="hljs-string">'except'</span>, <span class="hljs-string">'finally'</span>, <span class="hljs-string">'for'</span>, <span class="hljs-string">'from'</span>, <span class="hljs-string">'global'</span>, <span class="hljs-string">'if'</span>, <span class="hljs-string">'import'</span>, <span class="hljs-string">'in'</span>, <span class="hljs-string">'is'</span>, <span class="hljs-string">'lambda'</span>, <span class="hljs-string">'nonlocal'</span>, <span class="hljs-string">'not'</span>, <span class="hljs-string">'or'</span>, <span class="hljs-string">'pass'</span>, <span class="hljs-string">'raise'</span>, <span class="hljs-string">'return'</span>, <span class="hljs-string">'try'</span>, <span class="hljs-string">'while'</span>, <span class="hljs-string">'with'</span>, <span class="hljs-string">'yield'</span>]</code></pre><blockquote><p>上述就是整个Python编程语言的全部关键字，在基础阶段都会提到的。</p></blockquote><h4 id="函数调用"><a href="#函数调用" class="headerlink" title="函数调用"></a>函数调用</h4><pre><code class="hljs python">l1 = [<span class="hljs-number">1</span>, <span class="hljs-number">0</span>, <span class="hljs-string">'S'</span>, <span class="hljs-number">7</span>, <span class="hljs-number">9</span>]<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">demo</span><span class="hljs-params">(li)</span>:</span>    <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> li:        print(i)demo(l1)</code></pre><blockquote><p>调用方式：函数名（参数）</p></blockquote><h4 id="函数返回"><a href="#函数返回" class="headerlink" title="函数返回"></a>函数返回</h4><pre><code class="hljs python">l1 = [<span class="hljs-number">1</span>, <span class="hljs-number">0</span>, <span class="hljs-string">'S'</span>, <span class="hljs-number">7</span>, <span class="hljs-number">9</span>]<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">demo</span><span class="hljs-params">(li)</span>:</span>    <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> li:        print(i)    <span class="hljs-keyword">return</span> <span class="hljs-string">'ok'</span>print(demo(l1))</code></pre><p><strong>输出：</strong></p><pre><code class="hljs python"><span class="hljs-number">1</span><span class="hljs-number">0</span>S<span class="hljs-number">7</span><span class="hljs-number">9</span>ok</code></pre><blockquote><p>return：</p><p>注意 return 和 print 的区别，return是函数的返回值，返回值可以赋值给变量，而print只是打印出来</p></blockquote><h3 id="函数参数"><a href="#函数参数" class="headerlink" title="函数参数"></a>函数参数</h3><blockquote><p>刚才讲到了函数的定义，那函数里面可以传入哪些对象呢？</p></blockquote><pre><code class="hljs python"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">demo</span><span class="hljs-params">(x)</span>:</span>    print(x)demo(<span class="hljs-string">'demo'</span>)</code></pre><p><strong>输出</strong></p><pre><code class="hljs python">demo</code></pre><blockquote><p>如果我们不传值呢？</p></blockquote><pre><code class="hljs python"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">demo</span><span class="hljs-params">(x)</span>:</span>    print(x)demo()</code></pre><p><strong>输出</strong></p><pre><code class="hljs python">TypeError: demo() missing <span class="hljs-number">1</span> required positional argument: <span class="hljs-string">'x'</span></code></pre><blockquote><p>TypeError：demo()缺少一个必需的位置参数：’x’。</p></blockquote><blockquote><p>传入几个参数呢？</p></blockquote><h4 id="必备参数"><a href="#必备参数" class="headerlink" title="必备参数"></a>必备参数</h4><blockquote><p>def  func(x):</p><p>  pass</p></blockquote><pre><code class="hljs python"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">demo</span><span class="hljs-params">(x)</span>:</span>    print(x)demo(<span class="hljs-number">1</span>, <span class="hljs-number">2</span>)</code></pre><p><strong>输出</strong></p><pre><code class="hljs python">TypeError: demo() takes <span class="hljs-number">1</span> positional argument but <span class="hljs-number">2</span> were given</code></pre><blockquote><p>一个参数对应一个数值</p></blockquote><h4 id="默认参数"><a href="#默认参数" class="headerlink" title="默认参数"></a>默认参数</h4><blockquote><p>def  func(x, y=None):</p><p>  pass</p></blockquote><pre><code class="hljs python"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">demo</span><span class="hljs-params">(x, y=<span class="hljs-number">1</span>)</span>:</span>    print(x, y)demo(<span class="hljs-number">1</span>, <span class="hljs-number">2</span>)demo(<span class="hljs-number">3</span>)</code></pre><p><strong>输出</strong></p><pre><code class="hljs python"><span class="hljs-number">1</span> <span class="hljs-number">2</span><span class="hljs-number">3</span> <span class="hljs-number">1</span></code></pre><blockquote><p>y=1.就是默认参数，没有传入新参数的时候，就使用默认参数。</p></blockquote><h4 id="关键字参数"><a href="#关键字参数" class="headerlink" title="关键字参数"></a>关键字参数</h4><pre><code class="hljs python"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">demo</span><span class="hljs-params">(x, y=<span class="hljs-number">1</span>)</span>:</span>    print(x, y)demo(<span class="hljs-number">1</span>, <span class="hljs-number">2</span>)demo(y=<span class="hljs-string">"q"</span>, x=<span class="hljs-string">'s'</span>)</code></pre><p><strong>输出</strong></p><pre><code class="hljs python"><span class="hljs-number">1</span> <span class="hljs-number">2</span>s q</code></pre><blockquote><p>关键字参数，调用的时候带上参数名。</p></blockquote><h4 id="不定长参数"><a href="#不定长参数" class="headerlink" title="不定长参数"></a>不定长参数</h4><blockquote><p>def  func(<em>args, *</em>kwargs):</p><p>  pass</p><p>注意：*+参数名</p></blockquote><pre><code class="hljs python"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">demo</span><span class="hljs-params">(*args)</span>:</span>    print(args)demo(<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>, <span class="hljs-number">4</span>)demo(<span class="hljs-number">1</span>)</code></pre><p><strong>输出</strong></p><pre><code class="hljs python">(<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>, <span class="hljs-number">4</span>)(<span class="hljs-number">1</span>,)</code></pre><blockquote><p>参数名前面加<code>*号</code>是不定长参数，输出是一个元组。</p></blockquote><pre><code class="hljs python"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">demo</span><span class="hljs-params">(*a)</span>:</span>    print(*a)  <span class="hljs-comment"># 加*：去除括号</span>    print(a)demo(<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>, <span class="hljs-number">4</span>)print(<span class="hljs-string">'-------'</span>)demo((<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>, <span class="hljs-number">4</span>))print(<span class="hljs-string">'-------'</span>)demo(*(<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>, <span class="hljs-number">4</span>))</code></pre><p><strong>输出</strong></p><pre><code class="hljs python"><span class="hljs-number">1</span> <span class="hljs-number">2</span> <span class="hljs-number">3</span> <span class="hljs-number">4</span>(<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>, <span class="hljs-number">4</span>)-------(<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>, <span class="hljs-number">4</span>)((<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>, <span class="hljs-number">4</span>),)-------<span class="hljs-number">1</span> <span class="hljs-number">2</span> <span class="hljs-number">3</span> <span class="hljs-number">4</span>(<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>, <span class="hljs-number">4</span>)</code></pre><blockquote><p>加*：去除括号</p></blockquote><pre><code class="hljs python"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">demo</span><span class="hljs-params">(**a)</span>:</span>    print(a)demo(x=<span class="hljs-number">1</span>, y=<span class="hljs-number">2</span>, s=<span class="hljs-number">2</span>)</code></pre><p><strong>输出</strong></p><pre><code class="hljs python">&#123;<span class="hljs-string">'x'</span>: <span class="hljs-number">1</span>, <span class="hljs-string">'y'</span>: <span class="hljs-number">2</span>, <span class="hljs-string">'s'</span>: <span class="hljs-number">2</span>&#125;</code></pre><blockquote><p>参数名前面加<code>**号</code>是不定长参数，输出是一个字典。</p><p>注意：传入的参数是键值对。</p></blockquote><h4 id="演示："><a href="#演示：" class="headerlink" title="演示："></a><strong>演示：</strong></h4><pre><code class="hljs python"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">demo</span><span class="hljs-params">(*args, **kwargs)</span>:</span>    print(args)    print(kwargs)demo(<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>, x=<span class="hljs-number">1</span>, y=<span class="hljs-number">2</span>, s=<span class="hljs-number">2</span>)</code></pre><p><strong>输出</strong></p><pre><code class="hljs python">(<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>)&#123;<span class="hljs-string">'x'</span>: <span class="hljs-number">1</span>, <span class="hljs-string">'y'</span>: <span class="hljs-number">2</span>, <span class="hljs-string">'s'</span>: <span class="hljs-number">2</span>&#125;</code></pre><blockquote><p>传入的键值对，只能放在最后。</p></blockquote><h4 id="总结："><a href="#总结：" class="headerlink" title="总结："></a>总结：</h4><blockquote><p>必备参数：在函数调用的时候，必备参数必须要传入</p><p>默认参数： 在函数调用的时候，默认参数可以不传入值，不传入值时，会使用默认参数</p><p>不定长参数：在函数调用的时候，不定长参数可以不传入，也可以传入任意长度。其中定义时，元组形式可以放到参数最前面，字典形式只能放到最后面</p></blockquote><h3 id="常见的内置函数"><a href="#常见的内置函数" class="headerlink" title="常见的内置函数"></a>常见的内置函数</h3><p>  常见内置函数提供了一些处理的数据的方法，可以帮助我们提高开发速度</p><h4 id="常见函数"><a href="#常见函数" class="headerlink" title="常见函数"></a>常见函数</h4><h5 id="len"><a href="#len" class="headerlink" title="len"></a><code>len</code></h5><blockquote><p>求长度</p></blockquote><pre><code class="hljs python">li = [<span class="hljs-number">2</span>,<span class="hljs-number">8</span>,<span class="hljs-number">5</span>]In [<span class="hljs-number">6</span>]: len(li)Out[<span class="hljs-number">6</span>]: <span class="hljs-number">3</span></code></pre><h5 id="min"><a href="#min" class="headerlink" title="min"></a><code>min</code></h5><blockquote><p>求最小值</p></blockquote><pre><code class="hljs python">li = [<span class="hljs-number">2</span>,<span class="hljs-number">8</span>,<span class="hljs-number">5</span>]In [<span class="hljs-number">6</span>]: len(li)Out[<span class="hljs-number">6</span>]: <span class="hljs-number">3</span></code></pre><h5 id="max"><a href="#max" class="headerlink" title="max"></a><code>max</code></h5><blockquote><p>求最大值</p></blockquote><pre><code class="hljs python">li = [<span class="hljs-number">2</span>,<span class="hljs-number">8</span>,<span class="hljs-number">5</span>]In [<span class="hljs-number">8</span>]: max(li)Out[<span class="hljs-number">8</span>]: <span class="hljs-number">8</span></code></pre><h5 id="sorted"><a href="#sorted" class="headerlink" title="sorted"></a><code>sorted</code></h5><blockquote><p>排序</p></blockquote><pre><code class="hljs python">li = [<span class="hljs-number">2</span>,<span class="hljs-number">8</span>,<span class="hljs-number">5</span>]In [<span class="hljs-number">9</span>]: sorted(li)Out[<span class="hljs-number">9</span>]: [<span class="hljs-number">2</span>, <span class="hljs-number">5</span>, <span class="hljs-number">8</span>]</code></pre><h5 id="reversed"><a href="#reversed" class="headerlink" title="reversed"></a><code>reversed</code></h5><blockquote><p>反向</p></blockquote><pre><code class="hljs python">li = [<span class="hljs-number">2</span>,<span class="hljs-number">8</span>,<span class="hljs-number">5</span>]In [<span class="hljs-number">10</span>]: reversed(li)Out[<span class="hljs-number">10</span>]: &lt;list_reverseiterator at <span class="hljs-number">0x7f68aa81af98</span>&gt;In [<span class="hljs-number">11</span>]: list(reversed(li))Out[<span class="hljs-number">11</span>]: [<span class="hljs-number">5</span>, <span class="hljs-number">8</span>, <span class="hljs-number">2</span>]</code></pre><h5 id="sum"><a href="#sum" class="headerlink" title="sum"></a><code>sum</code></h5><blockquote><p>求和</p></blockquote><pre><code class="hljs python">li = [<span class="hljs-number">2</span>,<span class="hljs-number">8</span>,<span class="hljs-number">5</span>]In [<span class="hljs-number">12</span>]: sum(li)Out[<span class="hljs-number">12</span>]: <span class="hljs-number">15</span></code></pre><h4 id="进制转换函数"><a href="#进制转换函数" class="headerlink" title="进制转换函数"></a>进制转换函数</h4><h5 id="bin"><a href="#bin" class="headerlink" title="bin"></a><code>bin</code></h5><blockquote><p>二进制</p></blockquote><pre><code class="hljs python">In [<span class="hljs-number">13</span>]: bin(<span class="hljs-number">12</span>)Out[<span class="hljs-number">13</span>]: <span class="hljs-string">'0b1100'</span></code></pre><h5 id="oct"><a href="#oct" class="headerlink" title="oct"></a><code>oct</code></h5><blockquote><p>八进制</p></blockquote><pre><code class="hljs python">In [<span class="hljs-number">16</span>]: oct(<span class="hljs-number">18</span>)Out[<span class="hljs-number">16</span>]: <span class="hljs-string">'0o22</span></code></pre><h5 id="hex"><a href="#hex" class="headerlink" title="hex"></a><code>hex</code></h5><blockquote><p>十六进制</p></blockquote><pre><code class="hljs python">In [<span class="hljs-number">17</span>]: hex(<span class="hljs-number">12</span>)Out[<span class="hljs-number">17</span>]: <span class="hljs-string">'0xc'</span></code></pre><h5 id="ord"><a href="#ord" class="headerlink" title="ord"></a><code>ord</code></h5><blockquote><p>字符转ASCII码</p></blockquote><pre><code class="hljs python">In [<span class="hljs-number">19</span>]: ord(<span class="hljs-string">'a'</span>)Out[<span class="hljs-number">19</span>]: <span class="hljs-number">97</span></code></pre><h5 id="chr"><a href="#chr" class="headerlink" title="chr"></a><code>chr</code></h5><blockquote><p>ASCII码转字符</p></blockquote><pre><code class="hljs python">In [<span class="hljs-number">20</span>]: chr(<span class="hljs-number">97</span>)Out[<span class="hljs-number">20</span>]: <span class="hljs-string">'a'</span></code></pre><h4 id="扩展"><a href="#扩展" class="headerlink" title="扩展"></a>扩展</h4><h5 id="enumerate"><a href="#enumerate" class="headerlink" title="enumerate"></a><code>enumerate</code></h5><blockquote><p>返回一个可以枚举的对象</p></blockquote><pre><code class="hljs python">In [<span class="hljs-number">21</span>]: li = [<span class="hljs-string">'a'</span>,<span class="hljs-string">'b'</span>,<span class="hljs-string">'c'</span>,<span class="hljs-string">'d'</span>]In [<span class="hljs-number">22</span>]: enumerate(li)Out[<span class="hljs-number">22</span>]: &lt;enumerate at <span class="hljs-number">0x7f68aa877d80</span>&gt;In [<span class="hljs-number">23</span>]: list(enumerate(li))Out[<span class="hljs-number">23</span>]: [(<span class="hljs-number">0</span>, <span class="hljs-string">'a'</span>), (<span class="hljs-number">1</span>, <span class="hljs-string">'b'</span>), (<span class="hljs-number">2</span>, <span class="hljs-string">'c'</span>), (<span class="hljs-number">3</span>, <span class="hljs-string">'d'</span>)]In [<span class="hljs-number">24</span>]: dict(enumerate(li))Out[<span class="hljs-number">24</span>]: &#123;<span class="hljs-number">0</span>: <span class="hljs-string">'a'</span>, <span class="hljs-number">1</span>: <span class="hljs-string">'b'</span>, <span class="hljs-number">2</span>: <span class="hljs-string">'c'</span>, <span class="hljs-number">3</span>: <span class="hljs-string">'d'</span>&#125;</code></pre><h5 id="eval"><a href="#eval" class="headerlink" title="eval"></a><code>eval</code></h5><blockquote><p>取出字符串中内容</p><p>将字符串str当成有效的表达式来求值并返回计算结果</p></blockquote><pre><code class="hljs python">In [<span class="hljs-number">25</span>]: a = <span class="hljs-string">"&#123;'a':1&#125;"</span>In [<span class="hljs-number">26</span>]: eval(a)Out[<span class="hljs-number">26</span>]: &#123;<span class="hljs-string">'a'</span>: <span class="hljs-number">1</span>&#125;In [<span class="hljs-number">27</span>]: b = <span class="hljs-string">'1 + 2 + 3'</span>In [<span class="hljs-number">28</span>]: eval(b)Out[<span class="hljs-number">28</span>]: <span class="hljs-number">6</span></code></pre><h5 id="exec"><a href="#exec" class="headerlink" title="exec"></a><code>exec</code></h5><blockquote><p> 执行字符串或complie方法编译过的字符串，没有返回值</p></blockquote><pre><code class="hljs python">In [<span class="hljs-number">29</span>]: s = <span class="hljs-string">'''</span><span class="hljs-string">    ...: z = 10</span><span class="hljs-string">    ...: su = x + y + z</span><span class="hljs-string">    ...: print(su)</span><span class="hljs-string">    ...: print('OK')</span><span class="hljs-string">    ...: '''</span>In [<span class="hljs-number">30</span>]: x = <span class="hljs-number">1</span>In [<span class="hljs-number">31</span>]: y = <span class="hljs-number">2</span>In [<span class="hljs-number">32</span>]: exec(s)<span class="hljs-number">13</span>OKIn [<span class="hljs-number">33</span>]: exec(s,&#123;<span class="hljs-string">'x'</span>:<span class="hljs-number">0</span>,<span class="hljs-string">'y'</span>:<span class="hljs-number">0</span>&#125;)<span class="hljs-number">10</span>OKIn [<span class="hljs-number">34</span>]: exec(s,&#123;<span class="hljs-string">'x'</span>:<span class="hljs-number">0</span>,<span class="hljs-string">'y'</span>:<span class="hljs-number">0</span>&#125;,&#123;<span class="hljs-string">'y'</span>:<span class="hljs-number">10</span>,<span class="hljs-string">'z'</span>:<span class="hljs-number">0</span>&#125;)  <span class="hljs-comment">#以字符串为主,以最后的为主</span><span class="hljs-number">20</span>OK</code></pre><blockquote><p> 注意：eval 和 exec 是炸弹 能不能就不用，就好像你从不知道这东西一样，除非你足够的熟悉</p></blockquote><h5 id="filter"><a href="#filter" class="headerlink" title="filter"></a><code>filter</code></h5><blockquote><p>过滤器</p></blockquote><pre><code class="hljs python">In [<span class="hljs-number">38</span>]: <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">test1</span><span class="hljs-params">(x)</span>:</span>    ...:     <span class="hljs-keyword">return</span> x&gt;<span class="hljs-number">10</span>    ...: l1 = [<span class="hljs-number">10</span>,<span class="hljs-number">2</span>,<span class="hljs-number">20</span>,<span class="hljs-number">13</span>,<span class="hljs-number">5</span>]In [<span class="hljs-number">39</span>]: filter(test1, l1)Out[<span class="hljs-number">39</span>]: &lt;filter at <span class="hljs-number">0x7f68aa7ecb70</span>&gt;In [<span class="hljs-number">40</span>]: list(filter(test1, l1))Out[<span class="hljs-number">40</span>]: [<span class="hljs-number">20</span>, <span class="hljs-number">13</span>]</code></pre><h5 id="map"><a href="#map" class="headerlink" title="map"></a><code>map</code></h5><blockquote><p>对于参数iterable中的每个元素都应用fuction函数，并将结果作为列表返回</p></blockquote><pre><code class="hljs python">In [<span class="hljs-number">41</span>]: l2 = [<span class="hljs-number">1</span>,<span class="hljs-number">2</span>,<span class="hljs-number">3</span>]In [<span class="hljs-number">42</span>]: map(str,l2)Out[<span class="hljs-number">42</span>]: &lt;map at <span class="hljs-number">0x7f68aa7ecba8</span>&gt;In [<span class="hljs-number">43</span>]: list(map(str,l2))Out[<span class="hljs-number">43</span>]: [<span class="hljs-string">'1'</span>, <span class="hljs-string">'2'</span>, <span class="hljs-string">'3'</span>]</code></pre><h5 id="zip"><a href="#zip" class="headerlink" title="zip"></a><code>zip</code></h5><blockquote><p>将对象逐一配对</p></blockquote><pre><code class="hljs python">In [<span class="hljs-number">44</span>]: l3 = [<span class="hljs-number">1</span>,<span class="hljs-number">2</span>,<span class="hljs-number">3</span>]In [<span class="hljs-number">45</span>]: t1 = (<span class="hljs-string">'a'</span>,<span class="hljs-string">'b'</span>,<span class="hljs-string">'c'</span>)In [<span class="hljs-number">46</span>]: zip(t1,l3)Out[<span class="hljs-number">46</span>]: &lt;zip at <span class="hljs-number">0x7f68abb3ec48</span>&gt;In [<span class="hljs-number">47</span>]: list(zip(t1,l3))Out[<span class="hljs-number">47</span>]: [(<span class="hljs-string">'a'</span>, <span class="hljs-number">1</span>), (<span class="hljs-string">'b'</span>, <span class="hljs-number">2</span>), (<span class="hljs-string">'c'</span>, <span class="hljs-number">3</span>)]In [<span class="hljs-number">48</span>]: dict(zip(t1,l3))Out[<span class="hljs-number">48</span>]: &#123;<span class="hljs-string">'a'</span>: <span class="hljs-number">1</span>, <span class="hljs-string">'b'</span>: <span class="hljs-number">2</span>, <span class="hljs-string">'c'</span>: <span class="hljs-number">3</span>&#125;</code></pre>]]></content>
    
    
    <categories>
      
      <category>python</category>
      
    </categories>
    
    
    <tags>
      
      <tag>函数基础</tag>
      
      <tag>函数参数</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>python第五话之控制流程</title>
    <link href="/2018/12/python-if-while/"/>
    <url>/2018/12/python-if-while/</url>
    
    <content type="html"><![CDATA[<p>[TOC]</p><h3 id="控制流程"><a href="#控制流程" class="headerlink" title="控制流程"></a>控制流程</h3><p>逻辑值包含了两个值：<br><code>True</code>：表示非空的量(比如：string,tuple.list.set,dictonary等) ，所有非零数 。<br><code>False</code>：表示0,None,空的量等<br><code>作用</code>：主要用于判断语句中，用来判断</p><ul><li>一个字符串是否为空</li><li>一个运算结果是否为零</li><li>一个表达式是否可用</li></ul><h3 id="条件判断"><a href="#条件判断" class="headerlink" title="条件判断"></a>条件判断</h3><p>条件语句是根据条件来设置程序接下来的走向。</p><p>条件语句的关键字有<code>if，elif，else</code>。</p><h4 id="基本形式："><a href="#基本形式：" class="headerlink" title="基本形式："></a>基本形式：</h4><blockquote><p>if 判断条件:</p><p>执行语句</p><p>else:</p><p>执行语句</p></blockquote><p>判断条件后面和else这个关键字后面都必须加冒号，冒号后面缩进的语句是子语句，多个子语句组成了语句块，如果是单个语句可以与条件写在同一行直接跟在冒号的后面，如果是语句块则一行一条语句，每一行都必须缩进。注意冒号和缩进都是语法的一部分，缩进一般为四个空格。</p><h4 id="单个条件"><a href="#单个条件" class="headerlink" title="单个条件"></a>单个条件</h4><p>这个是针对只有一个判断条件时的，条件满足时就执行缩进的子语句，else就是表示其余的情况，只要条件不满足则执行else后面子语句。判断语句一般是返回值为bool类型的表达式，值为True则是条件满足，值为False则是条件不满足。</p><h4 id="演示"><a href="#演示" class="headerlink" title="演示"></a><strong>演示</strong></h4><pre><code class="hljs python"><span class="hljs-meta">&gt;&gt;&gt; </span>a = <span class="hljs-string">'天晴'</span><span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">if</span> a==<span class="hljs-string">'天晴'</span>:print(<span class="hljs-string">'天气好，出去玩吧！'</span>)<span class="hljs-keyword">else</span>:print(<span class="hljs-string">'天气不好，呆在家吧。。'</span>)天气好，出去玩吧！</code></pre><pre><code class="hljs python"><span class="hljs-meta">&gt;&gt;&gt; </span>a = <span class="hljs-string">'下雨'</span><span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">if</span> a==<span class="hljs-string">'天晴'</span>:print(<span class="hljs-string">'天气好，出去玩吧！'</span>)<span class="hljs-keyword">else</span>:print(<span class="hljs-string">'天气不好，呆在家吧。。'</span>)天气不好，呆在家吧。。</code></pre><h4 id="多个条件"><a href="#多个条件" class="headerlink" title="多个条件"></a>多个条件</h4><p>如果判断需要多个条件需同时判断时，可以使用 or （或），表示两个条件有一个成立时判断条件成功；使用 and （与）时，表示只有两个条件同时成立的情况下，判断条件才成功。</p><h4 id="演示："><a href="#演示：" class="headerlink" title="演示："></a>演示：</h4><pre><code class="hljs python"><span class="hljs-meta">&gt;&gt;&gt; </span>a = <span class="hljs-string">"天晴"</span><span class="hljs-meta">&gt;&gt;&gt; </span>t = <span class="hljs-string">"有空"</span><span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">if</span> a==<span class="hljs-string">"天晴"</span> <span class="hljs-keyword">and</span> t==<span class="hljs-string">"有空"</span>:<span class="hljs-meta">... </span>    print(<span class="hljs-string">"天气真好，咱们出去玩！！"</span>)<span class="hljs-meta">... </span><span class="hljs-keyword">else</span>:<span class="hljs-meta">... </span>    print(<span class="hljs-string">"天气不好，呆在家吧！！"</span>)<span class="hljs-meta">... </span>天气真好，咱们出去玩！！</code></pre><p>对于多条件分支的判断使用elif关键字来用来条件分支的.</p><h4 id="基本形式"><a href="#基本形式" class="headerlink" title="基本形式"></a>基本形式</h4><blockquote><p>if 判断条件1:</p><p>执行语句1</p><p>elif 判断条件2:</p><p>执行语句2</p><p>elif 判断条件n:</p><p>执行语句n</p><p>else:</p><p>执行语句x</p></blockquote><p>写多条件分支时，同一个条件中只能有一个if一个else，对elif的个数没有限制但必须是写在if后面，else放在最后表示以上条件都不满足的情况。满足哪个判断条件就执行这个判断条件对应的执行语句，如果列出的条件都不满足则执行else的子语句，语句的执行顺序是从上到下，遇到满足的条件则直接进入它的子语句块，其他剩余判断条件和子语句将不再进行判断和执行。</p><h4 id="演示-1"><a href="#演示-1" class="headerlink" title="演示"></a>演示</h4><pre><code class="hljs python">print(<span class="hljs-string">'分数等级测试'</span>)score = input(<span class="hljs-string">'请输入你的分数'</span>)<span class="hljs-keyword">if</span> <span class="hljs-number">90</span>&lt;=int(score)&lt;=<span class="hljs-number">100</span>:    print(<span class="hljs-string">'你的等级是A'</span>)<span class="hljs-keyword">elif</span> <span class="hljs-number">75</span>&lt;=int(score)&lt;<span class="hljs-number">90</span>:    print(<span class="hljs-string">'你的等级是B'</span>)<span class="hljs-keyword">elif</span> <span class="hljs-number">60</span>&lt;=int(score)&lt;<span class="hljs-number">75</span>:    print(<span class="hljs-string">'你的等级是C'</span>)<span class="hljs-keyword">elif</span> <span class="hljs-number">0</span>&lt;=int(score)&lt;<span class="hljs-number">60</span>:    print(<span class="hljs-string">'你的等级是D'</span>)<span class="hljs-keyword">else</span>:    print(<span class="hljs-string">'输入有误!'</span>)</code></pre><pre><code class="hljs python">运行结果（python shell中显示）：分数等级测试请输入你的分数<span class="hljs-number">98</span>你的等级是A</code></pre><p>这里使用了内置的函数input()获取键盘的输入，这里会把键盘的输入以字符串的形式赋值给score这个名字，同类型的才可以进行比较，所以后面在进行条件判断时要把score转换成int类型再进行比较。</p><h3 id="三目运算"><a href="#三目运算" class="headerlink" title="三目运算"></a>三目运算</h3><h4 id="演示：-1"><a href="#演示：-1" class="headerlink" title="演示："></a>演示：</h4><pre><code class="hljs python"><span class="hljs-meta">&gt;&gt;&gt; </span>a = <span class="hljs-number">3</span> <span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">if</span> a&gt;<span class="hljs-number">5</span>:<span class="hljs-meta">... </span>    print(<span class="hljs-literal">True</span>)<span class="hljs-meta">... </span><span class="hljs-keyword">else</span>:<span class="hljs-meta">... </span>    print(<span class="hljs-literal">False</span>)<span class="hljs-meta">... </span><span class="hljs-literal">False</span></code></pre><p>更简单的写法呢？</p><pre><code class="hljs python"><span class="hljs-meta">&gt;&gt;&gt; </span>print(<span class="hljs-literal">True</span>) <span class="hljs-keyword">if</span> a&gt;<span class="hljs-number">5</span> <span class="hljs-keyword">else</span> print(<span class="hljs-literal">False</span>) <span class="hljs-literal">False</span></code></pre><h4 id=""><a href="#" class="headerlink" title=""></a></h4><h3 id="条件循环"><a href="#条件循环" class="headerlink" title="条件循环"></a>条件循环</h3><h4 id="while"><a href="#while" class="headerlink" title="while"></a><code>while</code></h4><blockquote><p>语法规则：</p><p> while  判断语句：</p><p>​    循环体</p><p>注意：注意缩进</p></blockquote><h4 id="演示：-2"><a href="#演示：-2" class="headerlink" title="演示："></a>演示：</h4><pre><code class="hljs python"><span class="hljs-meta">&gt;&gt;&gt; </span>li = [<span class="hljs-number">1</span>, <span class="hljs-number">3</span>, <span class="hljs-number">5</span>, <span class="hljs-number">7</span>, <span class="hljs-number">9</span>]<span class="hljs-meta">&gt;&gt;&gt; </span>i = <span class="hljs-number">0</span><span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">while</span> i &lt; len(li):<span class="hljs-meta">... </span>    print(<span class="hljs-literal">True</span>) <span class="hljs-keyword">if</span> li[i]&gt;<span class="hljs-number">5</span> <span class="hljs-keyword">else</span> print(<span class="hljs-literal">False</span>) <span class="hljs-meta">... </span>    i += <span class="hljs-number">1</span><span class="hljs-meta">... </span><span class="hljs-literal">False</span><span class="hljs-literal">False</span><span class="hljs-literal">False</span><span class="hljs-literal">True</span><span class="hljs-literal">True</span></code></pre><p>对于刚才值大于5的三目运算，如果是判断一个列表中数字该怎么做呢？</p><h4 id="break"><a href="#break" class="headerlink" title="break"></a>break</h4><blockquote><p>跳出循环</p></blockquote><p><strong>演示</strong></p><pre><code class="hljs python">li = [<span class="hljs-number">1</span>, <span class="hljs-number">3</span>, <span class="hljs-number">5</span>, <span class="hljs-number">7</span>, <span class="hljs-number">9</span>]i = <span class="hljs-number">0</span><span class="hljs-keyword">while</span> i &lt; len(li):<span class="hljs-keyword">if</span> li[i] &gt; <span class="hljs-number">5</span>:    <span class="hljs-keyword">break</span>print(li[i])i += <span class="hljs-number">1</span></code></pre><p><strong>输出</strong></p><pre><code class="hljs python"><span class="hljs-number">1</span><span class="hljs-number">3</span><span class="hljs-number">5</span></code></pre><h4 id="continue"><a href="#continue" class="headerlink" title="continue"></a>continue</h4><blockquote><p>跳过此次循环</p></blockquote><p><strong>演示</strong></p><pre><code class="hljs python">li = [<span class="hljs-number">1</span>, <span class="hljs-number">3</span>, <span class="hljs-number">5</span>, <span class="hljs-number">7</span>, <span class="hljs-number">9</span>]i = <span class="hljs-number">0</span><span class="hljs-keyword">while</span> i &lt; len(li):    print(li[i])<span class="hljs-keyword">if</span> li[i] == <span class="hljs-number">5</span>:    <span class="hljs-keyword">continue</span>i += <span class="hljs-number">1</span></code></pre><p>大家猜测下执行结果。。。。。</p><p>解决上面的问题</p><pre><code class="hljs python">li = [<span class="hljs-number">1</span>, <span class="hljs-number">3</span>, <span class="hljs-number">5</span>, <span class="hljs-number">7</span>, <span class="hljs-number">9</span>]i = <span class="hljs-number">-1</span><span class="hljs-keyword">while</span> i &lt; len(li)<span class="hljs-number">-1</span>:    print(li[i])    i += <span class="hljs-number">1</span><span class="hljs-keyword">if</span> li[i] == <span class="hljs-number">5</span>:    <span class="hljs-keyword">continue</span></code></pre><p><strong>输出</strong></p><pre><code class="hljs python"><span class="hljs-number">1</span><span class="hljs-number">3</span><span class="hljs-number">7</span><span class="hljs-number">9</span></code></pre><h4 id="else"><a href="#else" class="headerlink" title="else"></a>else</h4><blockquote><p>当while的条件不满足时，运行。</p><p>注意：break时，不运行</p></blockquote><pre><code class="hljs python">li = [<span class="hljs-number">1</span>, <span class="hljs-number">3</span>, <span class="hljs-number">5</span>, <span class="hljs-number">7</span>, <span class="hljs-number">9</span>]i = <span class="hljs-number">-1</span><span class="hljs-keyword">while</span> i &lt; len(li)<span class="hljs-number">-1</span>:i += <span class="hljs-number">1</span><span class="hljs-keyword">if</span> li[i] == <span class="hljs-number">5</span>:    <span class="hljs-keyword">continue</span>print(li[i])<span class="hljs-keyword">else</span>:    print(<span class="hljs-string">'ok'</span>)</code></pre><p><strong>输出</strong></p><pre><code class="hljs python"><span class="hljs-number">1</span><span class="hljs-number">3</span><span class="hljs-number">7</span><span class="hljs-number">9</span>ok</code></pre><p><strong>演示</strong></p><pre><code class="hljs python">li = [<span class="hljs-number">1</span>, <span class="hljs-number">3</span>, <span class="hljs-number">5</span>, <span class="hljs-number">7</span>, <span class="hljs-number">9</span>]i = <span class="hljs-number">-1</span><span class="hljs-keyword">while</span> i &lt; len(li)<span class="hljs-number">-1</span>:i += <span class="hljs-number">1</span><span class="hljs-keyword">if</span> li[i] == <span class="hljs-number">5</span>:        <span class="hljs-comment"># continue</span><span class="hljs-keyword">break</span>print(li[i])<span class="hljs-keyword">else</span>:    print(<span class="hljs-string">'ok'</span>)</code></pre><p><strong>输出</strong></p><pre><code class="hljs python"><span class="hljs-number">1</span><span class="hljs-number">3</span></code></pre><h4 id="总结"><a href="#总结" class="headerlink" title="总结"></a><strong>总结</strong></h4><blockquote><p>循环可以被终止：</p><ul><li>判断语句可以返回  False</li><li>通过break终止循环 </li></ul><p>else的执行条件：</p><p>只有在循环不是被break终止的情况下才会执行else中的内容</p></blockquote><h3 id="迭代循环"><a href="#迭代循环" class="headerlink" title="迭代循环"></a>迭代循环</h3><h4 id="for"><a href="#for" class="headerlink" title="for"></a><strong>for</strong></h4><blockquote><p>只要是可迭代对象，都可以使用for循环遍历。</p></blockquote><h4 id="语法规则"><a href="#语法规则" class="headerlink" title="语法规则"></a>语法规则</h4><blockquote><p>for  i  in  obj：</p><p>​     循环体</p><p>注意：注意缩进</p></blockquote><p><strong>演示</strong></p><pre><code class="hljs python">li = [<span class="hljs-number">1</span>, <span class="hljs-number">3</span>, <span class="hljs-number">5</span>, <span class="hljs-number">7</span>, <span class="hljs-number">9</span>]<span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> li:    print(i)</code></pre><p><strong>输出</strong></p><pre><code class="hljs python"><span class="hljs-number">1</span><span class="hljs-number">3</span><span class="hljs-number">5</span><span class="hljs-number">7</span><span class="hljs-number">9</span></code></pre><h4 id="range"><a href="#range" class="headerlink" title="range"></a><strong>range</strong></h4><p><strong>演示</strong></p><pre><code class="hljs python"><span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> range(<span class="hljs-number">1</span>, <span class="hljs-number">21</span>):print(i)</code></pre><p><strong>输出</strong></p><pre><code class="hljs python"><span class="hljs-number">1</span><span class="hljs-number">2</span><span class="hljs-number">3</span><span class="hljs-number">4</span><span class="hljs-number">5</span><span class="hljs-number">6</span><span class="hljs-number">7</span><span class="hljs-number">8</span><span class="hljs-number">9</span><span class="hljs-number">10</span><span class="hljs-number">11</span><span class="hljs-number">12</span><span class="hljs-number">13</span><span class="hljs-number">14</span><span class="hljs-number">15</span><span class="hljs-number">16</span><span class="hljs-number">17</span><span class="hljs-number">18</span><span class="hljs-number">19</span><span class="hljs-number">20</span></code></pre><blockquote><p>内置函数，表示一个范围，不包含结尾值。</p></blockquote><pre><code class="hljs python">In [<span class="hljs-number">3</span>]: list(range(<span class="hljs-number">21</span>))Out[<span class="hljs-number">3</span>]: [<span class="hljs-number">0</span>, <span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>, <span class="hljs-number">4</span>, <span class="hljs-number">5</span>, <span class="hljs-number">6</span>, <span class="hljs-number">7</span>, <span class="hljs-number">8</span>, <span class="hljs-number">9</span>, <span class="hljs-number">10</span>, <span class="hljs-number">11</span>, <span class="hljs-number">12</span>, <span class="hljs-number">13</span>, <span class="hljs-number">14</span>, <span class="hljs-number">15</span>, <span class="hljs-number">16</span>, <span class="hljs-number">17</span>, <span class="hljs-number">18</span>, <span class="hljs-number">19</span>, <span class="hljs-number">20</span>]    In [<span class="hljs-number">4</span>]: list(range(<span class="hljs-number">2</span>, <span class="hljs-number">21</span>))Out[<span class="hljs-number">4</span>]: [<span class="hljs-number">2</span>, <span class="hljs-number">3</span>, <span class="hljs-number">4</span>, <span class="hljs-number">5</span>, <span class="hljs-number">6</span>, <span class="hljs-number">7</span>, <span class="hljs-number">8</span>, <span class="hljs-number">9</span>, <span class="hljs-number">10</span>, <span class="hljs-number">11</span>, <span class="hljs-number">12</span>, <span class="hljs-number">13</span>, <span class="hljs-number">14</span>, <span class="hljs-number">15</span>, <span class="hljs-number">16</span>, <span class="hljs-number">17</span>, <span class="hljs-number">18</span>, <span class="hljs-number">19</span>, <span class="hljs-number">20</span>]In [<span class="hljs-number">5</span>]: list(range(<span class="hljs-number">1</span>, <span class="hljs-number">21</span>, <span class="hljs-number">2</span>))Out[<span class="hljs-number">5</span>]: [<span class="hljs-number">1</span>, <span class="hljs-number">3</span>, <span class="hljs-number">5</span>, <span class="hljs-number">7</span>, <span class="hljs-number">9</span>, <span class="hljs-number">11</span>, <span class="hljs-number">13</span>, <span class="hljs-number">15</span>, <span class="hljs-number">17</span>, <span class="hljs-number">19</span>]</code></pre><h4 id="continue-1"><a href="#continue-1" class="headerlink" title="continue"></a><strong>continue</strong></h4><blockquote><p>跳出当前循环</p></blockquote><p><strong>演示</strong></p><pre><code class="hljs python"><span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> range(<span class="hljs-number">1</span>, <span class="hljs-number">21</span>):<span class="hljs-keyword">if</span> i%<span class="hljs-number">5</span> == <span class="hljs-number">0</span>:    <span class="hljs-keyword">continue</span>print(i)</code></pre><p><strong>输出</strong></p><pre><code class="hljs python"><span class="hljs-number">1</span><span class="hljs-number">2</span><span class="hljs-number">3</span><span class="hljs-number">4</span><span class="hljs-number">6</span><span class="hljs-number">7</span><span class="hljs-number">8</span><span class="hljs-number">9</span><span class="hljs-number">11</span><span class="hljs-number">12</span><span class="hljs-number">13</span><span class="hljs-number">14</span><span class="hljs-number">16</span><span class="hljs-number">17</span><span class="hljs-number">18</span><span class="hljs-number">19</span></code></pre><h4 id="break-1"><a href="#break-1" class="headerlink" title="break"></a>break</h4><blockquote><p>跳出循环</p></blockquote><p><strong>演示</strong></p><pre><code class="hljs python"><span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> range(<span class="hljs-number">1</span>, <span class="hljs-number">21</span>):<span class="hljs-keyword">if</span> i%<span class="hljs-number">5</span> == <span class="hljs-number">0</span>:    <span class="hljs-keyword">break</span>print(i)</code></pre><p><strong>输出</strong></p><pre><code class="hljs python"><span class="hljs-number">1</span><span class="hljs-number">2</span><span class="hljs-number">3</span><span class="hljs-number">4</span></code></pre><h4 id="else-1"><a href="#else-1" class="headerlink" title="else"></a><strong>else</strong></h4><blockquote><p>当for循环结束时，运行。</p><p>注意：break时，不运行</p></blockquote><p><strong>演示</strong>：</p><pre><code class="hljs python"><span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> range(<span class="hljs-number">1</span>, <span class="hljs-number">21</span>):<span class="hljs-keyword">if</span> i%<span class="hljs-number">5</span> == <span class="hljs-number">0</span>:    <span class="hljs-keyword">continue</span>print(i)<span class="hljs-keyword">else</span>:    print(<span class="hljs-string">'end...'</span>)</code></pre><p><strong>输出</strong></p><pre><code class="hljs python"><span class="hljs-number">1</span><span class="hljs-number">2</span><span class="hljs-number">3</span><span class="hljs-number">4</span><span class="hljs-number">6</span><span class="hljs-number">7</span><span class="hljs-number">8</span><span class="hljs-number">9</span><span class="hljs-number">11</span><span class="hljs-number">12</span><span class="hljs-number">13</span><span class="hljs-number">14</span><span class="hljs-number">16</span><span class="hljs-number">17</span><span class="hljs-number">18</span><span class="hljs-number">19</span>end...</code></pre><h4 id="总结-1"><a href="#总结-1" class="headerlink" title="总结"></a><strong>总结</strong></h4><blockquote><p>for 后面需要接上可迭代对象</p><p>for会依次取出可迭代对象中的元素</p><p>continue的用法：</p><p>continue和break类似，但是continue不会终止循环，而是结束本次循环，跳到下次循环</p></blockquote>]]></content>
    
    
    <categories>
      
      <category>python</category>
      
    </categories>
    
    
    <tags>
      
      <tag>控制流程</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>python第四话之散列类型、运算优先级和逻辑运算</title>
    <link href="/2018/12/python-dict-set/"/>
    <url>/2018/12/python-dict-set/</url>
    
    <content type="html"><![CDATA[<p>[TOC]</p><h3 id="散列类型、运算优先级和逻辑运算"><a href="#散列类型、运算优先级和逻辑运算" class="headerlink" title="散列类型、运算优先级和逻辑运算"></a>散列类型、运算优先级和逻辑运算</h3><p><code>散列类型</code>也就是我们所熟知的<code>字典</code>和<code>集合</code>，我们今天来看看散列类型的相关逻辑运算。</p><h3 id="集合（set）"><a href="#集合（set）" class="headerlink" title="集合（set）"></a>集合（set）</h3><h4 id="集合的特点："><a href="#集合的特点：" class="headerlink" title="集合的特点："></a>集合的特点：</h4><p>无序、元素是唯一的。</p><h4 id="集合的创建："><a href="#集合的创建：" class="headerlink" title="集合的创建："></a>集合的创建：</h4><p>用大括号“{}”，各元素之间用逗号隔开；也可以通过类型转换的方式使用set()内置函数将列表或元祖转换为集合类型。在创建的过程中会自动过滤掉重复的元素，保证元素的唯一性。</p><h4 id="演示"><a href="#演示" class="headerlink" title="演示"></a>演示</h4><pre><code class="hljs python">In [<span class="hljs-number">1</span>]: s = [<span class="hljs-number">1</span>,<span class="hljs-number">2</span>,<span class="hljs-number">3</span>,<span class="hljs-number">4</span>,<span class="hljs-number">1</span>,<span class="hljs-number">2</span>]                                              In [<span class="hljs-number">2</span>]: s                                                              Out[<span class="hljs-number">2</span>]: [<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>, <span class="hljs-number">4</span>, <span class="hljs-number">1</span>, <span class="hljs-number">2</span>]In [<span class="hljs-number">3</span>]: se = set(s)                                                    In [<span class="hljs-number">4</span>]: se                                                             Out[<span class="hljs-number">4</span>]: &#123;<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>, <span class="hljs-number">4</span>&#125;In [<span class="hljs-number">5</span>]: type(se)                                                       Out[<span class="hljs-number">5</span>]: set    In [<span class="hljs-number">6</span>]: &#123;<span class="hljs-number">1</span>,<span class="hljs-number">2</span>,<span class="hljs-number">3</span>,<span class="hljs-number">1</span>,<span class="hljs-number">2</span>,<span class="hljs-number">3</span>&#125;                                                  Out[<span class="hljs-number">6</span>]: &#123;<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>&#125;</code></pre><blockquote><p>注意：列表是允许元素重复的，但是当我们把列表转成集合后，里面重复的元素就去掉了。</p></blockquote><h4 id="集合的运算"><a href="#集合的运算" class="headerlink" title="集合的运算"></a>集合的运算</h4><p>交集：&amp;</p><p>并集：| </p><p>差集：-</p><p><img src="http://eveseven.oss-cn-shanghai.aliyuncs.com/18-12-11/79094336.jpg" srcset="/img/loading.gif" alt=""></p><h5 id="交集"><a href="#交集" class="headerlink" title="交集"></a><code>交集</code></h5><p>两个集合(s 和t)的差补或相对补集是指一个集合C，该集合中的元素，只属于集合s，而不属于集合t。</p><pre><code class="hljs python">In [<span class="hljs-number">7</span>]: s1 = &#123;<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>, <span class="hljs-number">4</span>, <span class="hljs-string">'a'</span>, <span class="hljs-string">'b'</span>&#125;                                    In [<span class="hljs-number">8</span>]: s2 = &#123;<span class="hljs-number">0</span>,<span class="hljs-number">2</span>,<span class="hljs-number">3</span>,<span class="hljs-string">'b'</span>,<span class="hljs-string">'c'</span>&#125;                                           In [<span class="hljs-number">9</span>]: s1 &amp; s2                                                        Out[<span class="hljs-number">9</span>]: &#123;<span class="hljs-number">2</span>, <span class="hljs-number">3</span>, <span class="hljs-string">'b'</span>&#125;</code></pre><blockquote><p>两个集合取交集，最后输出的元素是属于两个集合所共有的元素。</p></blockquote><h5 id="并集"><a href="#并集" class="headerlink" title="并集"></a><code>并集</code></h5><p>联合(union)操作和集合的OR(又称可兼析取(inclusive disjunction))其实是等价的，两个集合的联合是一个新集合，该集合中的每个元素都至少是其中一个集合的成员，即：属于两个集合其中之一的成员。</p><pre><code class="hljs python">In [<span class="hljs-number">10</span>]: s1 = &#123;<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>, <span class="hljs-number">4</span>, <span class="hljs-string">'a'</span>, <span class="hljs-string">'b'</span>&#125;                                   In [<span class="hljs-number">11</span>]: s2 = &#123;<span class="hljs-number">0</span>,<span class="hljs-number">2</span>,<span class="hljs-number">3</span>,<span class="hljs-string">'b'</span>,<span class="hljs-string">'c'</span>&#125;                                          In [<span class="hljs-number">12</span>]: s1 | s2                                                       Out[<span class="hljs-number">12</span>]: &#123;<span class="hljs-number">0</span>, <span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>, <span class="hljs-number">4</span>, <span class="hljs-string">'a'</span>, <span class="hljs-string">'b'</span>, <span class="hljs-string">'c'</span>&#125;</code></pre><blockquote><p>并集指的是两个集合的元素进行一个整合，最后生成的元素都是属于原来两个集合之中的某一个。</p></blockquote><h5 id="差集"><a href="#差集" class="headerlink" title="差集"></a><code>差集</code></h5><p>和其他的布尔集合操作相似，对称差分是集合的XOR(又称”异 或” (exclusive disjunction)).两个集合(s 和t)的对称差分是指另外一个集合C,该集合中的元素，只能是属于集合s 或者集合t的成员，不能同时属于两个集合。</p><pre><code class="hljs python">In [<span class="hljs-number">13</span>]: s1 = &#123;<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>, <span class="hljs-number">4</span>, <span class="hljs-string">'a'</span>, <span class="hljs-string">'b'</span>&#125;                                   In [<span class="hljs-number">14</span>]: s2 = &#123;<span class="hljs-number">0</span>,<span class="hljs-number">2</span>,<span class="hljs-number">3</span>,<span class="hljs-string">'b'</span>,<span class="hljs-string">'c'</span>&#125;                                          In [<span class="hljs-number">15</span>]: s1 - s2                                                       Out[<span class="hljs-number">15</span>]: &#123;<span class="hljs-number">1</span>, <span class="hljs-number">4</span>, <span class="hljs-string">'a'</span>&#125;</code></pre><blockquote><p>差集也叫被减集合的补集。</p></blockquote><h5 id="扩展"><a href="#扩展" class="headerlink" title="扩展"></a><code>扩展</code></h5><p><strong>add</strong></p><pre><code class="hljs python">In [<span class="hljs-number">16</span>]: s1 = &#123;<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>, <span class="hljs-number">4</span>, <span class="hljs-string">'a'</span>, <span class="hljs-string">'b'</span>&#125;                                   In [<span class="hljs-number">17</span>]: s1.add(<span class="hljs-number">8</span>)                                                     In [<span class="hljs-number">18</span>]: s1                                                            Out[<span class="hljs-number">18</span>]: &#123;<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>, <span class="hljs-number">4</span>, <span class="hljs-number">8</span>, <span class="hljs-string">'a'</span>, <span class="hljs-string">'b'</span>&#125;In [<span class="hljs-number">19</span>]: s1.add(<span class="hljs-string">'w'</span>)                                                   In [<span class="hljs-number">20</span>]: s1                                                            Out[<span class="hljs-number">20</span>]: &#123;<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>, <span class="hljs-number">4</span>, <span class="hljs-number">8</span>, <span class="hljs-string">'a'</span>, <span class="hljs-string">'b'</span>, <span class="hljs-string">'w'</span>&#125;</code></pre><blockquote><p>往集合里添加元素。</p></blockquote><p><strong>pop</strong></p><pre><code class="hljs python">In [<span class="hljs-number">23</span>]: s1                                                            Out[<span class="hljs-number">23</span>]: &#123;<span class="hljs-number">0</span>, <span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>, <span class="hljs-number">4</span>, <span class="hljs-number">8</span>, <span class="hljs-string">'a'</span>, <span class="hljs-string">'b'</span>, <span class="hljs-string">'w'</span>&#125;In [<span class="hljs-number">24</span>]: s1.pop()                                                      Out[<span class="hljs-number">24</span>]: <span class="hljs-number">0</span>In [<span class="hljs-number">25</span>]: s1.pop()                                                      Out[<span class="hljs-number">25</span>]: <span class="hljs-number">1</span>In [<span class="hljs-number">26</span>]: s1.pop()                                                      Out[<span class="hljs-number">26</span>]: <span class="hljs-number">2</span>In [<span class="hljs-number">27</span>]: s1.pop()                                                      Out[<span class="hljs-number">27</span>]: <span class="hljs-number">3</span>In [<span class="hljs-number">28</span>]: s1.pop()                                                      Out[<span class="hljs-number">28</span>]: <span class="hljs-number">4</span></code></pre><blockquote><p>pop方法是没有参数的，因为集合是无序的，所以在移除的时候是随机移除的。</p></blockquote><p><strong>remove</strong></p><pre><code class="hljs python">In [<span class="hljs-number">30</span>]: s1 = &#123;<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>, <span class="hljs-number">4</span>, <span class="hljs-string">'a'</span>, <span class="hljs-string">'b'</span>&#125;                                   In [<span class="hljs-number">31</span>]: s1.remove(<span class="hljs-number">1</span>)                                                  In [<span class="hljs-number">32</span>]: s1                                                            Out[<span class="hljs-number">32</span>]: &#123;<span class="hljs-number">2</span>, <span class="hljs-number">3</span>, <span class="hljs-number">4</span>, <span class="hljs-string">'a'</span>, <span class="hljs-string">'b'</span>&#125;In [<span class="hljs-number">33</span>]: s1.remove(<span class="hljs-string">'a'</span>)                                                In [<span class="hljs-number">34</span>]: s1                                                            Out[<span class="hljs-number">34</span>]: &#123;<span class="hljs-number">2</span>, <span class="hljs-number">3</span>, <span class="hljs-number">4</span>, <span class="hljs-string">'b'</span>&#125;</code></pre><blockquote><p>remove方法是指定元素进行删除。</p></blockquote><p><strong>update</strong></p><pre><code class="hljs python">In [<span class="hljs-number">35</span>]: s1                                                            Out[<span class="hljs-number">35</span>]: &#123;<span class="hljs-number">2</span>, <span class="hljs-number">3</span>, <span class="hljs-number">4</span>, <span class="hljs-string">'b'</span>&#125;In [<span class="hljs-number">36</span>]: s1.update(&#123;<span class="hljs-string">'w'</span>, <span class="hljs-string">'c'</span>&#125;)                                         In [<span class="hljs-number">37</span>]: s1                                                            Out[<span class="hljs-number">37</span>]: &#123;<span class="hljs-number">2</span>, <span class="hljs-number">3</span>, <span class="hljs-number">4</span>, <span class="hljs-string">'b'</span>, <span class="hljs-string">'c'</span>, <span class="hljs-string">'w'</span>&#125;</code></pre><blockquote><p>update方法是往集合里面添加集合。</p></blockquote><p><strong>isdisjoint</strong></p><pre><code class="hljs python">In [<span class="hljs-number">38</span>]: s1 = &#123;<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>, <span class="hljs-number">4</span>, <span class="hljs-string">'a'</span>, <span class="hljs-string">'b'</span>&#125;                                   In [<span class="hljs-number">39</span>]: s2 = &#123;<span class="hljs-number">0</span>,<span class="hljs-number">2</span>,<span class="hljs-number">3</span>,<span class="hljs-string">'b'</span>,<span class="hljs-string">'c'</span>&#125;                                          In [<span class="hljs-number">40</span>]: s1.isdisjoint(s2)                                             Out[<span class="hljs-number">40</span>]: <span class="hljs-literal">False</span>In [<span class="hljs-number">41</span>]: s1.isdisjoint(&#123;<span class="hljs-number">6</span>, <span class="hljs-number">8</span>, <span class="hljs-number">7</span>&#125;)                                      Out[<span class="hljs-number">41</span>]: <span class="hljs-literal">True</span></code></pre><blockquote><p>isdisjoint方法是判断两个集合有没有交集，有返回False，没有则返回True</p></blockquote><p><strong>issubset</strong></p><pre><code class="hljs python">In [<span class="hljs-number">42</span>]: s1 = &#123;<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>, <span class="hljs-number">4</span>, <span class="hljs-string">'a'</span>, <span class="hljs-string">'b'</span>&#125;                                   In [<span class="hljs-number">43</span>]: s2 = &#123;<span class="hljs-number">0</span>,<span class="hljs-number">2</span>,<span class="hljs-number">3</span>,<span class="hljs-string">'b'</span>,<span class="hljs-string">'c'</span>&#125;                                          In [<span class="hljs-number">44</span>]: s1.issubset(s2)                                               Out[<span class="hljs-number">44</span>]: <span class="hljs-literal">False</span>In [<span class="hljs-number">45</span>]: s2.issubset(s1)                                               Out[<span class="hljs-number">45</span>]: <span class="hljs-literal">False</span>In [<span class="hljs-number">46</span>]: s1.issubset(s1)                                               Out[<span class="hljs-number">46</span>]: <span class="hljs-literal">True</span>In [<span class="hljs-number">47</span>]: &#123;<span class="hljs-number">1</span>, <span class="hljs-number">2</span>&#125;.issubset(s1)                                           Out[<span class="hljs-number">47</span>]: <span class="hljs-literal">True</span>In [<span class="hljs-number">48</span>]: s1                                                            Out[<span class="hljs-number">48</span>]: &#123;<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>, <span class="hljs-number">4</span>, <span class="hljs-string">'a'</span>, <span class="hljs-string">'b'</span>&#125;</code></pre><blockquote><p>判断前面的集合是不是后面的集合的子集。</p></blockquote><p><strong>issuperset</strong></p><pre><code class="hljs python">In [<span class="hljs-number">51</span>]: s1 = &#123;<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>, <span class="hljs-number">4</span>, <span class="hljs-string">'a'</span>, <span class="hljs-string">'b'</span>&#125;                                   In [<span class="hljs-number">52</span>]: s2 = &#123;<span class="hljs-number">2</span>,<span class="hljs-number">3</span>,<span class="hljs-string">'b'</span>&#125;                                                In [<span class="hljs-number">53</span>]: s2.issubset(s1)                                               Out[<span class="hljs-number">53</span>]: <span class="hljs-literal">True</span>In [<span class="hljs-number">54</span>]: s1.issuperset(s2)                                             Out[<span class="hljs-number">54</span>]: <span class="hljs-literal">True</span></code></pre><blockquote><p>判断后面的集合是前面集合的子集。</p></blockquote><h4 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h4><blockquote><ul><li>集合唯一性：集合中的元素具有唯一性，不存在两个相同的元素。</li><li>集合可变性：集合中的元素是可变的，集合是可变对象。</li><li>集合无序性：集合中的元素是无序的，所以没有存在索引。</li></ul></blockquote><h3 id="字典（dict）"><a href="#字典（dict）" class="headerlink" title="字典（dict）"></a>字典（dict）</h3><p>字典是除了列表外的另一种<code>可变类型</code>，字典的元素是以键值对的形式存在，字典的键必须是唯一，可以是数字、字符串或者是元组，键可以为任何不可变类型，列表和集合不能作为字典的键。</p><h4 id="字典的创建"><a href="#字典的创建" class="headerlink" title="字典的创建"></a>字典的创建</h4><p>第一种 { key :value } ，字典里的键和值用“：”隔开，一对键和值组成一个项，项和项之间用“，”隔开。</p><p>第二种使用内置函数dict(key=value)，要注意的是这里使用的是“=”赋值的方式，键是以名字的形式所以这种方法的键就必须符合名字的要求，且不能使用关键字作为键。</p><p>如果你要使用关键字作为键名那么就只能用第一种方法，关键字以字符串的形式来创建。</p><p>通过字典的键可以访问这个键所对应的值，字典是可变类型，所以可以直接对字典的项进行修改，使用dictname[key] = value，如果这个键存在于字典中，则是修改这个键所对应的值，如果这个键不存在则是往字典中添加这个项。</p><h4 id="演示-1"><a href="#演示-1" class="headerlink" title="演示"></a><strong>演示</strong></h4><pre><code class="hljs python">In [<span class="hljs-number">56</span>]: &#123;<span class="hljs-string">'a'</span>:<span class="hljs-number">1</span>, <span class="hljs-string">'b'</span>:<span class="hljs-number">2</span>&#125;                                                Out[<span class="hljs-number">56</span>]: &#123;<span class="hljs-string">'a'</span>: <span class="hljs-number">1</span>, <span class="hljs-string">'b'</span>: <span class="hljs-number">2</span>&#125;In [<span class="hljs-number">57</span>]: s = &#123;<span class="hljs-string">'a'</span>:<span class="hljs-number">1</span>, <span class="hljs-string">'b'</span>:<span class="hljs-number">2</span>&#125;                                            In [<span class="hljs-number">58</span>]: s,type(s)                                                     Out[<span class="hljs-number">58</span>]: (&#123;<span class="hljs-string">'a'</span>: <span class="hljs-number">1</span>, <span class="hljs-string">'b'</span>: <span class="hljs-number">2</span>&#125;, dict)In [<span class="hljs-number">56</span>]: &#123;<span class="hljs-string">'a'</span>:<span class="hljs-number">1</span>, <span class="hljs-string">'b'</span>:<span class="hljs-number">2</span>&#125;                                                Out[<span class="hljs-number">56</span>]: &#123;<span class="hljs-string">'a'</span>: <span class="hljs-number">1</span>, <span class="hljs-string">'b'</span>: <span class="hljs-number">2</span>&#125;In [<span class="hljs-number">57</span>]: s = &#123;<span class="hljs-string">'a'</span>:<span class="hljs-number">1</span>, <span class="hljs-string">'b'</span>:<span class="hljs-number">2</span>&#125;                                            In [<span class="hljs-number">58</span>]: s,type(s)                                                     Out[<span class="hljs-number">58</span>]: (&#123;<span class="hljs-string">'a'</span>: <span class="hljs-number">1</span>, <span class="hljs-string">'b'</span>: <span class="hljs-number">2</span>&#125;, dict)</code></pre><blockquote><p>字典形式：{key:value}</p></blockquote><h4 id="字典的运用"><a href="#字典的运用" class="headerlink" title="字典的运用"></a>字典的运用</h4><h5 id="查看"><a href="#查看" class="headerlink" title="查看"></a><code>查看</code></h5><pre><code class="hljs python">In [<span class="hljs-number">67</span>]: a = dict(a=<span class="hljs-number">1</span>, b=<span class="hljs-number">2</span>)                                            In [<span class="hljs-number">68</span>]: a                                                             Out[<span class="hljs-number">68</span>]: &#123;<span class="hljs-string">'a'</span>: <span class="hljs-number">1</span>, <span class="hljs-string">'b'</span>: <span class="hljs-number">2</span>&#125;In [<span class="hljs-number">69</span>]: a[<span class="hljs-string">'a'</span>]                                                        Out[<span class="hljs-number">69</span>]: <span class="hljs-number">1</span></code></pre><blockquote><p>由于字典也是无序的，所以我们在取值的时候，是根据key来取出对应的value的。</p></blockquote><h5 id="增加"><a href="#增加" class="headerlink" title="增加"></a><code>增加</code></h5><pre><code class="hljs python">In [<span class="hljs-number">70</span>]: a                                                             Out[<span class="hljs-number">70</span>]: &#123;<span class="hljs-string">'a'</span>: <span class="hljs-number">1</span>, <span class="hljs-string">'b'</span>: <span class="hljs-number">2</span>&#125;In [<span class="hljs-number">71</span>]: a[<span class="hljs-string">'c'</span>] = <span class="hljs-number">3</span>                                                    In [<span class="hljs-number">72</span>]: a                                                             Out[<span class="hljs-number">72</span>]: &#123;<span class="hljs-string">'a'</span>: <span class="hljs-number">1</span>, <span class="hljs-string">'b'</span>: <span class="hljs-number">2</span>, <span class="hljs-string">'c'</span>: <span class="hljs-number">3</span>&#125;</code></pre><blockquote><p>往字典里添加元素时，是key和value对应增加的。</p></blockquote><h5 id="修改"><a href="#修改" class="headerlink" title="修改"></a><code>修改</code></h5><pre><code class="hljs python">In [<span class="hljs-number">73</span>]: a                                                             Out[<span class="hljs-number">73</span>]: &#123;<span class="hljs-string">'a'</span>: <span class="hljs-number">1</span>, <span class="hljs-string">'b'</span>: <span class="hljs-number">2</span>, <span class="hljs-string">'c'</span>: <span class="hljs-number">3</span>&#125;In [<span class="hljs-number">74</span>]: a[<span class="hljs-string">'a'</span>] = <span class="hljs-string">'w'</span>                                                  In [<span class="hljs-number">75</span>]: a                                                             Out[<span class="hljs-number">75</span>]: &#123;<span class="hljs-string">'a'</span>: <span class="hljs-string">'w'</span>, <span class="hljs-string">'b'</span>: <span class="hljs-number">2</span>, <span class="hljs-string">'c'</span>: <span class="hljs-number">3</span>&#125;</code></pre><blockquote><p>修改字典是通过key取出value，然后对应的去重新赋值。</p></blockquote><h4 id="字典的增删改查"><a href="#字典的增删改查" class="headerlink" title="字典的增删改查"></a>字典的增删改查</h4><h5 id="增加-1"><a href="#增加-1" class="headerlink" title="增加"></a>增加</h5><p><code>copy</code></p><pre><code class="hljs python">In [<span class="hljs-number">76</span>]: a                                                             Out[<span class="hljs-number">76</span>]: &#123;<span class="hljs-string">'a'</span>: <span class="hljs-string">'w'</span>, <span class="hljs-string">'b'</span>: <span class="hljs-number">2</span>, <span class="hljs-string">'c'</span>: <span class="hljs-number">3</span>&#125;In [<span class="hljs-number">77</span>]: b = a.copy()                                                  In [<span class="hljs-number">78</span>]: b                                                             Out[<span class="hljs-number">78</span>]: &#123;<span class="hljs-string">'a'</span>: <span class="hljs-string">'w'</span>, <span class="hljs-string">'b'</span>: <span class="hljs-number">2</span>, <span class="hljs-string">'c'</span>: <span class="hljs-number">3</span>&#125;</code></pre><blockquote><p>复制成一个新字典。</p></blockquote><p><code>fromkeys</code></p><p>查看fromkeys的使用方法</p><pre><code class="hljs python">In [<span class="hljs-number">79</span>]: help(a.fromkeys)</code></pre><blockquote><p>fromkeys(iterable, value=None, /) method of builtins.type instance  Returns a new dict with keys from iterable and values equal to value.</p><p>注意： 返回一个新的dict，其中包含来自iterable的键，值等于value。</p></blockquote><pre><code class="hljs python">In [<span class="hljs-number">93</span>]: a                                                             Out[<span class="hljs-number">93</span>]: &#123;<span class="hljs-string">'a'</span>: <span class="hljs-number">1</span>, <span class="hljs-string">'b'</span>: <span class="hljs-number">2</span>&#125;In [<span class="hljs-number">94</span>]: s = a.fromkeys([<span class="hljs-string">'c'</span>, <span class="hljs-string">'d'</span>])                                    In [<span class="hljs-number">95</span>]: s                                                             Out[<span class="hljs-number">95</span>]: &#123;<span class="hljs-string">'c'</span>: <span class="hljs-literal">None</span>, <span class="hljs-string">'d'</span>: <span class="hljs-literal">None</span>&#125;In [<span class="hljs-number">96</span>]: s = a.fromkeys([<span class="hljs-string">'c'</span>, <span class="hljs-string">'d'</span>], <span class="hljs-number">7</span>)                                 In [<span class="hljs-number">97</span>]: s                                                             Out[<span class="hljs-number">97</span>]: &#123;<span class="hljs-string">'c'</span>: <span class="hljs-number">7</span>, <span class="hljs-string">'d'</span>: <span class="hljs-number">7</span>&#125;</code></pre><blockquote><p>使用fromkey方法的时候，原字典是不变的，会返回一个新的字典。</p></blockquote><p><code>setfefault</code></p><pre><code class="hljs python">In [<span class="hljs-number">103</span>]: a                                                            Out[<span class="hljs-number">103</span>]: &#123;<span class="hljs-string">'a'</span>: <span class="hljs-string">'w'</span>, <span class="hljs-string">'b'</span>: <span class="hljs-number">2</span>&#125;In [<span class="hljs-number">104</span>]: a.setdefault(<span class="hljs-string">'a'</span>)                                            Out[<span class="hljs-number">104</span>]: <span class="hljs-string">'w'</span>In [<span class="hljs-number">105</span>]: a.setdefault(<span class="hljs-string">'b'</span>)                                            Out[<span class="hljs-number">105</span>]: <span class="hljs-number">2</span>In [<span class="hljs-number">106</span>]: a.setdefault(<span class="hljs-string">'c'</span>)                                            In [<span class="hljs-number">107</span>]: a                                                            Out[<span class="hljs-number">107</span>]: &#123;<span class="hljs-string">'a'</span>: <span class="hljs-string">'w'</span>, <span class="hljs-string">'b'</span>: <span class="hljs-number">2</span>, <span class="hljs-string">'c'</span>: <span class="hljs-literal">None</span>&#125;In [<span class="hljs-number">108</span>]: a.setdefault(<span class="hljs-string">'d'</span>, <span class="hljs-number">4</span>)                                         Out[<span class="hljs-number">108</span>]: <span class="hljs-number">4</span>In [<span class="hljs-number">109</span>]: a                                                            Out[<span class="hljs-number">109</span>]: &#123;<span class="hljs-string">'a'</span>: <span class="hljs-string">'w'</span>, <span class="hljs-string">'b'</span>: <span class="hljs-number">2</span>, <span class="hljs-string">'c'</span>: <span class="hljs-literal">None</span>, <span class="hljs-string">'d'</span>: <span class="hljs-number">4</span>&#125;</code></pre><blockquote><p>查询并返回key所对应的值，如果没有这个key,则会新建。有则查，无则增。</p></blockquote><h5 id="删除"><a href="#删除" class="headerlink" title="删除"></a>删除</h5><p><code>clear</code></p><pre><code class="hljs python">In [<span class="hljs-number">110</span>]: a                                                            Out[<span class="hljs-number">110</span>]: &#123;<span class="hljs-string">'a'</span>: <span class="hljs-string">'w'</span>, <span class="hljs-string">'b'</span>: <span class="hljs-number">2</span>, <span class="hljs-string">'c'</span>: <span class="hljs-literal">None</span>, <span class="hljs-string">'d'</span>: <span class="hljs-number">4</span>&#125;In [<span class="hljs-number">111</span>]: a.clear()                                                    In [<span class="hljs-number">112</span>]: a                                                            Out[<span class="hljs-number">112</span>]: &#123;&#125;</code></pre><blockquote><p>删除所有键值对</p></blockquote><p><code>pop</code></p><p>查看pop 的方法</p><pre><code class="hljs python">In [<span class="hljs-number">121</span>]: help(a.pop)</code></pre><blockquote><p>pop(…) method of builtins.dict instance<br>​    D.pop(k[,d]) -&gt; v, remove specified key and return the corresponding value.<br>​    If key is not found, d is returned if given, otherwise KeyError is raised</p></blockquote><pre><code class="hljs python">In [<span class="hljs-number">116</span>]: a                                                            Out[<span class="hljs-number">116</span>]: &#123;<span class="hljs-string">'a'</span>: <span class="hljs-number">1</span>, <span class="hljs-string">'b'</span>: <span class="hljs-number">2</span>, <span class="hljs-string">'d'</span>: <span class="hljs-number">4</span>&#125;In [<span class="hljs-number">117</span>]: a.pop(<span class="hljs-string">'a'</span>)                                                   Out[<span class="hljs-number">117</span>]: <span class="hljs-number">1</span>In [<span class="hljs-number">118</span>]: a                                                            Out[<span class="hljs-number">118</span>]: &#123;<span class="hljs-string">'b'</span>: <span class="hljs-number">2</span>, <span class="hljs-string">'d'</span>: <span class="hljs-number">4</span>&#125;In [<span class="hljs-number">119</span>]: a.pop(<span class="hljs-string">'d'</span>)                                                   Out[<span class="hljs-number">119</span>]: <span class="hljs-number">4</span>In [<span class="hljs-number">120</span>]: a.pop()                                                      -----------------------------------------------------------------------TypeError                             Traceback (most recent call last)&lt;ipython-input<span class="hljs-number">-120</span><span class="hljs-number">-9</span>c070c907602&gt; <span class="hljs-keyword">in</span> &lt;module&gt;----&gt; 1 a.pop()TypeError: pop expected at least <span class="hljs-number">1</span> arguments, got <span class="hljs-number">0</span></code></pre><blockquote><p>pop方法是删除指定的键并返回相应的值。</p></blockquote><pre><code class="hljs python">In [<span class="hljs-number">134</span>]: a                                                            Out[<span class="hljs-number">134</span>]: &#123;<span class="hljs-string">'a'</span>: <span class="hljs-number">1</span>, <span class="hljs-string">'b'</span>: <span class="hljs-number">2</span>, <span class="hljs-string">'d'</span>: <span class="hljs-number">4</span>&#125;In [<span class="hljs-number">135</span>]: a.pop(<span class="hljs-string">'c'</span>, <span class="hljs-string">'b'</span>)                                              Out[<span class="hljs-number">135</span>]: <span class="hljs-string">'b'</span>In [<span class="hljs-number">136</span>]: a.pop(<span class="hljs-string">'a'</span>, <span class="hljs-string">'b'</span>)                                              Out[<span class="hljs-number">136</span>]: <span class="hljs-number">1</span>In [<span class="hljs-number">137</span>]: a                                                            Out[<span class="hljs-number">137</span>]: &#123;<span class="hljs-string">'b'</span>: <span class="hljs-number">2</span>, <span class="hljs-string">'d'</span>: <span class="hljs-number">4</span>&#125;</code></pre><blockquote><p>如果传入两个值，第一个是key，第二个是一个值，如果找到key, 就删除对应键值对，并返回该值，如果没有找到key,就返回你所传入的第二个值。</p></blockquote><p><code>popitem</code></p><pre><code class="hljs python">In [<span class="hljs-number">140</span>]: a                                                            Out[<span class="hljs-number">140</span>]: &#123;<span class="hljs-string">'b'</span>: <span class="hljs-number">2</span>, <span class="hljs-string">'d'</span>: <span class="hljs-number">4</span>, <span class="hljs-string">'s'</span>: <span class="hljs-number">6</span>&#125;In [<span class="hljs-number">141</span>]: a.popitem()                                                  Out[<span class="hljs-number">141</span>]: (<span class="hljs-string">'s'</span>, <span class="hljs-number">6</span>)In [<span class="hljs-number">142</span>]: a.popitem()                                                  Out[<span class="hljs-number">142</span>]: (<span class="hljs-string">'d'</span>, <span class="hljs-number">4</span>)In [<span class="hljs-number">143</span>]: a                                                            Out[<span class="hljs-number">143</span>]: &#123;<span class="hljs-string">'b'</span>: <span class="hljs-number">2</span>&#125;</code></pre><blockquote><p>由于字典也是无序的，多以popitem是随机删除一个键值对。</p></blockquote><h5 id="修改-1"><a href="#修改-1" class="headerlink" title="修改"></a>修改</h5><p><code>update</code></p><pre><code class="hljs python">In [<span class="hljs-number">145</span>]: a                                                            Out[<span class="hljs-number">145</span>]: &#123;<span class="hljs-string">'b'</span>: <span class="hljs-number">2</span>&#125;In [<span class="hljs-number">146</span>]: a.update(&#123;<span class="hljs-string">'a'</span>:<span class="hljs-number">1</span>, <span class="hljs-string">'c'</span>:<span class="hljs-number">4</span>, <span class="hljs-string">'s'</span>: <span class="hljs-number">9</span>&#125;)                             In [<span class="hljs-number">147</span>]: a                                                            Out[<span class="hljs-number">147</span>]: &#123;<span class="hljs-string">'a'</span>: <span class="hljs-number">1</span>, <span class="hljs-string">'b'</span>: <span class="hljs-number">2</span>, <span class="hljs-string">'c'</span>: <span class="hljs-number">4</span>, <span class="hljs-string">'s'</span>: <span class="hljs-number">9</span>&#125;In [<span class="hljs-number">148</span>]: a.update(&#123;<span class="hljs-string">'a'</span>:<span class="hljs-number">0</span>&#125;)                                            In [<span class="hljs-number">149</span>]: a                                                            Out[<span class="hljs-number">149</span>]: &#123;<span class="hljs-string">'a'</span>: <span class="hljs-number">0</span>, <span class="hljs-string">'b'</span>: <span class="hljs-number">2</span>, <span class="hljs-string">'c'</span>: <span class="hljs-number">4</span>, <span class="hljs-string">'s'</span>: <span class="hljs-number">9</span>&#125;</code></pre><blockquote><p>update方法，对于键值对的处理是，有则改，无则增。</p></blockquote><h5 id="查询"><a href="#查询" class="headerlink" title="查询"></a>查询</h5><p><code>get</code></p><p>查看get的使用方法</p><pre><code class="hljs python">In [<span class="hljs-number">155</span>]: help(a.get)</code></pre><blockquote><p>get(…) method of builtins.dict instance<br>​    D.get(k[,d]) -&gt; D[k] if k in D, else d.  d defaults to None.</p><p>注意：默认返回None</p></blockquote><pre><code class="hljs python">In [<span class="hljs-number">151</span>]: a                                                            Out[<span class="hljs-number">151</span>]: &#123;<span class="hljs-string">'a'</span>: <span class="hljs-number">0</span>, <span class="hljs-string">'b'</span>: <span class="hljs-number">2</span>, <span class="hljs-string">'c'</span>: <span class="hljs-number">4</span>, <span class="hljs-string">'s'</span>: <span class="hljs-number">9</span>&#125;In [<span class="hljs-number">152</span>]: a.get(<span class="hljs-string">'a'</span>)                                                   Out[<span class="hljs-number">152</span>]: <span class="hljs-number">0</span>In [<span class="hljs-number">153</span>]: a.get(<span class="hljs-string">'f'</span>)                                                   In [<span class="hljs-number">154</span>]: a.get(<span class="hljs-string">'f'</span>, <span class="hljs-string">"没有"</span>)                                           Out[<span class="hljs-number">154</span>]: <span class="hljs-string">'没有'</span></code></pre><blockquote><p>get方法是如果查询到key就返回对应的value，如果没有，就返回你给定的提示值。</p></blockquote><pre><code class="hljs python">In [<span class="hljs-number">156</span>]: c,d = a.get(<span class="hljs-string">'f'</span>, (<span class="hljs-number">2</span>,<span class="hljs-number">3</span>))                                      In [<span class="hljs-number">157</span>]: c                                                            Out[<span class="hljs-number">157</span>]: <span class="hljs-number">2</span>In [<span class="hljs-number">158</span>]: d                                                            Out[<span class="hljs-number">158</span>]: <span class="hljs-number">3</span></code></pre><blockquote><p>也可以通过这个功能，做些操作。</p></blockquote><p><code>keys</code></p><pre><code class="hljs python">In [<span class="hljs-number">159</span>]: a                                                            Out[<span class="hljs-number">159</span>]: &#123;<span class="hljs-string">'a'</span>: <span class="hljs-number">0</span>, <span class="hljs-string">'b'</span>: <span class="hljs-number">2</span>, <span class="hljs-string">'c'</span>: <span class="hljs-number">4</span>, <span class="hljs-string">'s'</span>: <span class="hljs-number">9</span>&#125;In [<span class="hljs-number">160</span>]: a.keys()                                                     Out[<span class="hljs-number">160</span>]: dict_keys([<span class="hljs-string">'s'</span>, <span class="hljs-string">'a'</span>, <span class="hljs-string">'c'</span>, <span class="hljs-string">'b'</span>])In [<span class="hljs-number">161</span>]: list(a.keys())                                               Out[<span class="hljs-number">161</span>]: [<span class="hljs-string">'s'</span>, <span class="hljs-string">'a'</span>, <span class="hljs-string">'c'</span>, <span class="hljs-string">'b'</span>]</code></pre><blockquote><p>获取字典里所有的key。</p></blockquote><p><code>value</code></p><pre><code class="hljs python">In [<span class="hljs-number">162</span>]: a                                                            Out[<span class="hljs-number">162</span>]: &#123;<span class="hljs-string">'a'</span>: <span class="hljs-number">0</span>, <span class="hljs-string">'b'</span>: <span class="hljs-number">2</span>, <span class="hljs-string">'c'</span>: <span class="hljs-number">4</span>, <span class="hljs-string">'s'</span>: <span class="hljs-number">9</span>&#125;In [<span class="hljs-number">163</span>]: a.values()                                                   Out[<span class="hljs-number">163</span>]: dict_values([<span class="hljs-number">9</span>, <span class="hljs-number">0</span>, <span class="hljs-number">4</span>, <span class="hljs-number">2</span>])In [<span class="hljs-number">164</span>]: list(a.values())                                             Out[<span class="hljs-number">164</span>]: [<span class="hljs-number">9</span>, <span class="hljs-number">0</span>, <span class="hljs-number">4</span>, <span class="hljs-number">2</span>]</code></pre><blockquote><p>获取所有的value。</p></blockquote><p><code>items</code></p><pre><code class="hljs python">In [<span class="hljs-number">165</span>]: a                                                            Out[<span class="hljs-number">165</span>]: &#123;<span class="hljs-string">'a'</span>: <span class="hljs-number">0</span>, <span class="hljs-string">'b'</span>: <span class="hljs-number">2</span>, <span class="hljs-string">'c'</span>: <span class="hljs-number">4</span>, <span class="hljs-string">'s'</span>: <span class="hljs-number">9</span>&#125;In [<span class="hljs-number">166</span>]: a.items()                                                    Out[<span class="hljs-number">166</span>]: dict_items([(<span class="hljs-string">'s'</span>, <span class="hljs-number">9</span>), (<span class="hljs-string">'a'</span>, <span class="hljs-number">0</span>), (<span class="hljs-string">'c'</span>, <span class="hljs-number">4</span>), (<span class="hljs-string">'b'</span>, <span class="hljs-number">2</span>)])In [<span class="hljs-number">167</span>]: list(a.items())                                              Out[<span class="hljs-number">167</span>]: [(<span class="hljs-string">'s'</span>, <span class="hljs-number">9</span>), (<span class="hljs-string">'a'</span>, <span class="hljs-number">0</span>), (<span class="hljs-string">'c'</span>, <span class="hljs-number">4</span>), (<span class="hljs-string">'b'</span>, <span class="hljs-number">2</span>)]</code></pre><blockquote><p>获取所有的键值对。</p></blockquote><h4 id="总结-1"><a href="#总结-1" class="headerlink" title="总结"></a>总结</h4><blockquote><ul><li>键（key）唯一性： 字典中的键（key）具有唯一性，不存在两个相同的键（key）</li><li>可变性： 字典是可变对象，但是自动减的键（key）必须是不可变对象</li><li>无序性：字典中的键也是无序的，所以不能通过索引取值。</li></ul></blockquote><h3 id="运算符及优先级"><a href="#运算符及优先级" class="headerlink" title="运算符及优先级"></a>运算符及优先级</h3><h4 id="Python中的运算符"><a href="#Python中的运算符" class="headerlink" title="Python中的运算符"></a>Python中的运算符</h4><p><img src="http://eveseven.oss-cn-shanghai.aliyuncs.com/18-12-11/41589190.jpg" srcset="/img/loading.gif" alt="1"></p><h4 id="演示："><a href="#演示：" class="headerlink" title="演示："></a>演示：</h4><pre><code class="hljs python">In [<span class="hljs-number">168</span>]: <span class="hljs-number">2</span> **<span class="hljs-number">3</span>                                                        Out[<span class="hljs-number">168</span>]: <span class="hljs-number">8</span>In [<span class="hljs-number">169</span>]: <span class="hljs-number">2</span>+<span class="hljs-number">2</span>                                                          Out[<span class="hljs-number">169</span>]: <span class="hljs-number">4</span>In [<span class="hljs-number">170</span>]: <span class="hljs-number">2</span><span class="hljs-number">-1</span>                                                          Out[<span class="hljs-number">170</span>]: <span class="hljs-number">1</span>In [<span class="hljs-number">171</span>]: <span class="hljs-number">2</span>&lt;<span class="hljs-number">2</span>                                                          Out[<span class="hljs-number">171</span>]: <span class="hljs-literal">False</span>In [<span class="hljs-number">172</span>]: <span class="hljs-number">2</span>&gt;<span class="hljs-number">20</span>                                                         Out[<span class="hljs-number">172</span>]: <span class="hljs-literal">False</span>In [<span class="hljs-number">173</span>]: <span class="hljs-number">3</span>&lt;=(<span class="hljs-number">1</span>+<span class="hljs-number">2</span>)                                                     Out[<span class="hljs-number">173</span>]: <span class="hljs-literal">True</span>In [<span class="hljs-number">174</span>]: <span class="hljs-number">5</span>&gt;=<span class="hljs-number">1</span>                                                         Out[<span class="hljs-number">174</span>]: <span class="hljs-literal">True</span>In [<span class="hljs-number">175</span>]: <span class="hljs-number">2</span>==<span class="hljs-number">2</span>                                                         Out[<span class="hljs-number">175</span>]: <span class="hljs-literal">True</span>In [<span class="hljs-number">176</span>]: <span class="hljs-number">2</span>!=<span class="hljs-number">2</span>                                                         Out[<span class="hljs-number">176</span>]: <span class="hljs-literal">False</span>In [<span class="hljs-number">177</span>]: a =<span class="hljs-number">1</span> In [<span class="hljs-number">181</span>]: <span class="hljs-number">8</span> %<span class="hljs-number">2</span>                                                         Out[<span class="hljs-number">181</span>]: <span class="hljs-number">0</span>In [<span class="hljs-number">182</span>]: a                                                            Out[<span class="hljs-number">182</span>]: <span class="hljs-number">1</span>In [<span class="hljs-number">183</span>]: a += <span class="hljs-number">1</span>                                                       In [<span class="hljs-number">184</span>]: a                                                            Out[<span class="hljs-number">184</span>]: <span class="hljs-number">2</span>In [<span class="hljs-number">185</span>]: a /= <span class="hljs-number">1.2</span>                                                     In [<span class="hljs-number">186</span>]: a                                                            Out[<span class="hljs-number">186</span>]: <span class="hljs-number">1.6666666666666667</span>In [<span class="hljs-number">187</span>]: a %= <span class="hljs-number">1</span>                                                       In [<span class="hljs-number">188</span>]: a                                                            Out[<span class="hljs-number">188</span>]: <span class="hljs-number">0.6666666666666667</span>    In [<span class="hljs-number">189</span>]: a = <span class="hljs-number">1</span>                                                        In [<span class="hljs-number">190</span>]: b = a                                                        In [<span class="hljs-number">191</span>]: a <span class="hljs-keyword">is</span> b  <span class="hljs-comment"># 判断是否是id一致                                                      </span>Out[<span class="hljs-number">191</span>]: <span class="hljs-literal">True</span>    In [<span class="hljs-number">192</span>]: <span class="hljs-number">1</span> <span class="hljs-keyword">in</span> [<span class="hljs-number">1</span>, <span class="hljs-number">2</span>]                                                  Out[<span class="hljs-number">192</span>]: <span class="hljs-literal">True</span>In [<span class="hljs-number">193</span>]: <span class="hljs-number">1</span> <span class="hljs-keyword">not</span> <span class="hljs-keyword">in</span> [<span class="hljs-number">1</span>, <span class="hljs-number">2</span>]                                              Out[<span class="hljs-number">193</span>]: <span class="hljs-literal">False</span></code></pre><h4 id="逻辑运算符"><a href="#逻辑运算符" class="headerlink" title="逻辑运算符"></a>逻辑运算符</h4><h5 id="查看对象类型"><a href="#查看对象类型" class="headerlink" title="查看对象类型"></a>查看对象类型</h5><p><code>type</code></p><pre><code class="hljs python">In [<span class="hljs-number">204</span>]: a = <span class="hljs-number">1</span>                                                        In [<span class="hljs-number">205</span>]: b = <span class="hljs-string">'s'</span>                                                      In [<span class="hljs-number">206</span>]: c = [<span class="hljs-number">1</span>, <span class="hljs-number">2</span>]                                                   In [<span class="hljs-number">207</span>]: type(a), type(b), type(c)                                    Out[<span class="hljs-number">207</span>]: (int, str, list)</code></pre><blockquote><p>直接返回对象的类型</p></blockquote><p><code>isinstance</code></p><pre><code class="hljs python">In [<span class="hljs-number">208</span>]: a                                                            Out[<span class="hljs-number">208</span>]: <span class="hljs-number">1</span>In [<span class="hljs-number">209</span>]: b                                                            Out[<span class="hljs-number">209</span>]: <span class="hljs-string">'s'</span>In [<span class="hljs-number">210</span>]: c                                                            Out[<span class="hljs-number">210</span>]: [<span class="hljs-number">1</span>, <span class="hljs-number">2</span>]In [<span class="hljs-number">211</span>]: isinstance(a, int)                                           Out[<span class="hljs-number">211</span>]: <span class="hljs-literal">True</span>In [<span class="hljs-number">212</span>]: isinstance(a, str)                                           Out[<span class="hljs-number">212</span>]: <span class="hljs-literal">False</span>In [<span class="hljs-number">213</span>]: isinstance(b, str)                                           Out[<span class="hljs-number">213</span>]: <span class="hljs-literal">True</span></code></pre><blockquote><p>判断对象的类型</p></blockquote><h5 id="比较运算符"><a href="#比较运算符" class="headerlink" title="比较运算符"></a>比较运算符</h5><pre><code class="hljs python">In [<span class="hljs-number">216</span>]: a = <span class="hljs-number">1</span>                                                        In [<span class="hljs-number">217</span>]: b = <span class="hljs-number">2</span>                                                        In [<span class="hljs-number">218</span>]: c = <span class="hljs-number">1</span>                                                        In [<span class="hljs-number">219</span>]: a == b                                                       Out[<span class="hljs-number">219</span>]: <span class="hljs-literal">False</span>In [<span class="hljs-number">220</span>]: a == c                                                       Out[<span class="hljs-number">220</span>]: <span class="hljs-literal">True</span>In [<span class="hljs-number">221</span>]: b != c                                                       Out[<span class="hljs-number">221</span>]: <span class="hljs-literal">True</span></code></pre><h5 id="如果有多个条件"><a href="#如果有多个条件" class="headerlink" title="如果有多个条件"></a>如果有多个条件</h5><blockquote><ul><li>判断语句1    and    判断语句2</li><li>判断语句1    or       判断语句2</li><li>not    判断语句1</li></ul></blockquote><pre><code class="hljs python">In [<span class="hljs-number">227</span>]: a==b <span class="hljs-keyword">and</span> b!=c                                                Out[<span class="hljs-number">227</span>]: <span class="hljs-literal">False</span>In [<span class="hljs-number">228</span>]: a==b <span class="hljs-keyword">or</span> b!=c                                                 Out[<span class="hljs-number">228</span>]: <span class="hljs-literal">True</span>In [<span class="hljs-number">229</span>]: <span class="hljs-keyword">not</span> a==b                                                     Out[<span class="hljs-number">229</span>]: <span class="hljs-literal">True</span></code></pre>]]></content>
    
    
    <categories>
      
      <category>python</category>
      
    </categories>
    
    
    <tags>
      
      <tag>散列类型</tag>
      
      <tag>运算优先级</tag>
      
      <tag>逻辑运算</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>python第三话之格式化输出和深浅复制</title>
    <link href="/2018/12/python-output/"/>
    <url>/2018/12/python-output/</url>
    
    <content type="html"><![CDATA[<p>[TOC]</p><h3 id="格式化输出和深浅拷贝"><a href="#格式化输出和深浅拷贝" class="headerlink" title="格式化输出和深浅拷贝"></a>格式化输出和深浅拷贝</h3><p>前面我熟悉了<code>列表</code>、<code>字符串</code>、<code>元组</code>的常用操作以及对应的<code>增删改查</code>，今天我们来深入了解<code>格式化输出</code>和<code>深浅复制</code>相关的知识点。</p><h3 id="格式化输出"><a href="#格式化输出" class="headerlink" title="格式化输出"></a>格式化输出</h3><h4 id="传统方法"><a href="#传统方法" class="headerlink" title="传统方法"></a>传统方法</h4><h5 id="s"><a href="#s" class="headerlink" title="%s"></a><code>%s</code></h5><blockquote><p>%字符串</p></blockquote><p><strong>演示：</strong></p><pre><code class="hljs python">In [<span class="hljs-number">1</span>]: a = <span class="hljs-string">'hello'</span>                                                    In [<span class="hljs-number">2</span>]: b = <span class="hljs-string">'python '</span>                                                  In [<span class="hljs-number">3</span>]: <span class="hljs-string">"%s %s"</span> %(a, b)                                                Out[<span class="hljs-number">3</span>]: <span class="hljs-string">'hello python '</span>In [<span class="hljs-number">4</span>]: c = <span class="hljs-number">123</span>                                                        In [<span class="hljs-number">5</span>]: <span class="hljs-string">"%s %s"</span> %(a, c)                                                Out[<span class="hljs-number">5</span>]: <span class="hljs-string">'hello 123'</span></code></pre><h5 id="d"><a href="#d" class="headerlink" title="%d"></a><code>%d</code></h5><blockquote><p>%数字</p><p>注意：只能传入数字</p></blockquote><p><strong>演示：</strong></p><pre><code class="hljs python">In [<span class="hljs-number">6</span>]: a = <span class="hljs-string">'hello'</span>                                                    In [<span class="hljs-number">7</span>]: b = <span class="hljs-string">'python '</span>                                                  In [<span class="hljs-number">8</span>]: c = <span class="hljs-number">123</span>                                                        In [<span class="hljs-number">9</span>]: <span class="hljs-string">"%d %d"</span> %(a, c)                                                -----------------------------------------------------------------------TypeError                             Traceback (most recent call last)&lt;ipython-input<span class="hljs-number">-9</span><span class="hljs-number">-2</span>d6d5e198328&gt; <span class="hljs-keyword">in</span> &lt;module&gt;----&gt; 1 "%d %d" %(a, c)TypeError: %d format: a number <span class="hljs-keyword">is</span> required, <span class="hljs-keyword">not</span> strIn [<span class="hljs-number">10</span>]: <span class="hljs-string">"%d %d"</span> %(c, c)                                               Out[<span class="hljs-number">10</span>]: <span class="hljs-string">'123 123'</span>In [<span class="hljs-number">11</span>]: <span class="hljs-string">"%d"</span>%<span class="hljs-number">123.34</span>                                                   Out[<span class="hljs-number">11</span>]: <span class="hljs-string">'123'</span>In [<span class="hljs-number">12</span>]: <span class="hljs-string">"%d %d"</span> %(<span class="hljs-number">123</span>, <span class="hljs-number">123.34</span>)                                        Out[<span class="hljs-number">12</span>]: <span class="hljs-string">'123 123'</span></code></pre><h5 id="f"><a href="#f" class="headerlink" title="%f"></a><code>%f</code></h5><blockquote><p>%浮点数</p></blockquote><p><strong>演示：</strong></p><pre><code class="hljs python">In [<span class="hljs-number">14</span>]: <span class="hljs-string">'%f'</span>%<span class="hljs-number">2.3</span>                                                      Out[<span class="hljs-number">14</span>]: <span class="hljs-string">'2.300000'</span>In [<span class="hljs-number">15</span>]: <span class="hljs-string">'%.2f'</span>%<span class="hljs-number">2.3</span>                                                    Out[<span class="hljs-number">15</span>]: <span class="hljs-string">'2.30'</span>In [<span class="hljs-number">16</span>]: <span class="hljs-string">'%.2f'</span>%<span class="hljs-number">2.333434</span>                                               Out[<span class="hljs-number">16</span>]: <span class="hljs-string">'2.33'</span>In [<span class="hljs-number">17</span>]: <span class="hljs-string">'%.2f'</span>%<span class="hljs-number">2.35</span>                                                   Out[<span class="hljs-number">17</span>]: <span class="hljs-string">'2.35'</span>In [<span class="hljs-number">18</span>]: <span class="hljs-string">'%.2f'</span>%<span class="hljs-number">2.36</span>                                                   Out[<span class="hljs-number">18</span>]: <span class="hljs-string">'2.36'</span>In [<span class="hljs-number">19</span>]: <span class="hljs-string">'%.3f'</span>%<span class="hljs-number">2.36</span>                                                   Out[<span class="hljs-number">19</span>]: <span class="hljs-string">'2.360'</span>In [<span class="hljs-number">20</span>]: <span class="hljs-string">'%.4f'</span>%<span class="hljs-number">2.36</span>                                                   Out[<span class="hljs-number">20</span>]: <span class="hljs-string">'2.3600'</span></code></pre><h5 id="c"><a href="#c" class="headerlink" title="%c"></a><code>%c</code></h5><blockquote><p>%ASCII字符</p></blockquote><p><strong>演示：</strong></p><pre><code class="hljs python">In [<span class="hljs-number">21</span>]: <span class="hljs-string">'%c'</span>%<span class="hljs-number">97</span>                                                       Out[<span class="hljs-number">21</span>]: <span class="hljs-string">'a'</span>In [<span class="hljs-number">22</span>]: <span class="hljs-string">'%c'</span>%<span class="hljs-number">65</span>                                                       Out[<span class="hljs-number">22</span>]: <span class="hljs-string">'A'</span>In [<span class="hljs-number">23</span>]: <span class="hljs-string">'%c'</span>%<span class="hljs-number">61</span>                                                       Out[<span class="hljs-number">23</span>]: <span class="hljs-string">'='</span>In [<span class="hljs-number">24</span>]: <span class="hljs-string">'%c'</span>%<span class="hljs-number">60</span>                                                       Out[<span class="hljs-number">24</span>]: <span class="hljs-string">'&lt;'</span></code></pre><h5 id="o"><a href="#o" class="headerlink" title="%o"></a><code>%o</code></h5><blockquote><p>%8进制</p></blockquote><p><strong>演示：</strong></p><pre><code class="hljs python">In [<span class="hljs-number">25</span>]: <span class="hljs-string">'%o'</span>%<span class="hljs-number">9</span>                               Out[<span class="hljs-number">25</span>]: <span class="hljs-string">'11'</span>In [<span class="hljs-number">26</span>]: <span class="hljs-string">'%o'</span>%<span class="hljs-number">10</span>                              Out[<span class="hljs-number">26</span>]: <span class="hljs-string">'12'</span>In [<span class="hljs-number">27</span>]: <span class="hljs-string">'%o'</span>%<span class="hljs-number">107</span>                             Out[<span class="hljs-number">27</span>]: <span class="hljs-string">'153'</span>In [<span class="hljs-number">28</span>]: <span class="hljs-string">'%o'</span>%<span class="hljs-number">17</span>                              Out[<span class="hljs-number">28</span>]: <span class="hljs-string">'21'</span></code></pre><h5 id="x"><a href="#x" class="headerlink" title="%x"></a><code>%x</code></h5><blockquote><p>%16进制</p></blockquote><p><strong>演示：</strong></p><pre><code class="hljs python">In [<span class="hljs-number">29</span>]: <span class="hljs-string">'%x'</span>%<span class="hljs-number">16</span>                              Out[<span class="hljs-number">29</span>]: <span class="hljs-string">'10'</span>In [<span class="hljs-number">30</span>]: <span class="hljs-string">'%x'</span>%<span class="hljs-number">15</span>                              Out[<span class="hljs-number">30</span>]: <span class="hljs-string">'f'</span>In [<span class="hljs-number">31</span>]: <span class="hljs-string">'%x'</span>%<span class="hljs-number">14</span>                              Out[<span class="hljs-number">31</span>]: <span class="hljs-string">'e'</span>In [<span class="hljs-number">32</span>]: <span class="hljs-string">'%x'</span>%<span class="hljs-number">10</span>                              Out[<span class="hljs-number">32</span>]: <span class="hljs-string">'a'</span></code></pre><h5 id="e"><a href="#e" class="headerlink" title="%e"></a><code>%e</code></h5><blockquote><p>%科学计数法</p></blockquote><p><strong>演示：</strong></p><pre><code class="hljs python">In [<span class="hljs-number">33</span>]: <span class="hljs-string">'%e'</span>%<span class="hljs-number">0.01</span>                            Out[<span class="hljs-number">33</span>]: <span class="hljs-string">'1.000000e-02'</span>In [<span class="hljs-number">34</span>]: <span class="hljs-string">'%e'</span>%<span class="hljs-number">0.001</span>                           Out[<span class="hljs-number">34</span>]: <span class="hljs-string">'1.000000e-03'</span>In [<span class="hljs-number">35</span>]: <span class="hljs-string">'%e'</span>%<span class="hljs-number">10</span>                              Out[<span class="hljs-number">35</span>]: <span class="hljs-string">'1.000000e+01'</span>In [<span class="hljs-number">36</span>]: <span class="hljs-string">'%e'</span>%<span class="hljs-number">100</span>                             Out[<span class="hljs-number">36</span>]: <span class="hljs-string">'1.000000e+02'</span></code></pre><h5 id="扩展"><a href="#扩展" class="headerlink" title="扩展"></a><code>扩展</code></h5><p><strong>%r</strong></p><blockquote><p>原始化</p></blockquote><p>演示：</p><pre><code class="hljs python">In [<span class="hljs-number">43</span>]: print(<span class="hljs-string">'%s'</span>%<span class="hljs-string">'123'</span>)                    <span class="hljs-number">123</span>In [<span class="hljs-number">44</span>]: print(<span class="hljs-string">'%r'</span>%<span class="hljs-string">'123'</span>)                    <span class="hljs-string">'123'</span></code></pre><p><strong>%+6.5f</strong></p><blockquote><p>规定输出的字符串的个数和输出小数的位数</p><p>注意：%6.5f 指的是一个输出6个字符，其中5个小数</p><p>如果：前面带+就是表示输出符号</p><p>​        前面带-号表示左对齐</p></blockquote><p>演示：</p><pre><code class="hljs python">In [<span class="hljs-number">46</span>]: <span class="hljs-string">'%5.3f'</span>%<span class="hljs-number">1.2</span>                          Out[<span class="hljs-number">46</span>]: <span class="hljs-string">'1.200'</span>In [<span class="hljs-number">47</span>]: <span class="hljs-string">'%6.3f'</span>%<span class="hljs-number">1.2</span>                          Out[<span class="hljs-number">47</span>]: <span class="hljs-string">' 1.200'</span>In [<span class="hljs-number">48</span>]: <span class="hljs-string">'%6.3f'</span>%<span class="hljs-number">12.34567</span>                     Out[<span class="hljs-number">48</span>]: <span class="hljs-string">'12.346'</span>    In [<span class="hljs-number">49</span>]: <span class="hljs-string">'%+6.3f'</span>%<span class="hljs-number">12.34567</span>                    Out[<span class="hljs-number">49</span>]: <span class="hljs-string">'+12.346'</span>In [<span class="hljs-number">50</span>]: <span class="hljs-string">'%+6.3f'</span>%<span class="hljs-number">-12.34567</span>                   Out[<span class="hljs-number">50</span>]: <span class="hljs-string">'-12.346'</span>    In [<span class="hljs-number">60</span>]: <span class="hljs-string">'%8.2f'</span>%<span class="hljs-number">13.3333</span>                      Out[<span class="hljs-number">60</span>]: <span class="hljs-string">'   13.33'</span>In [<span class="hljs-number">61</span>]: <span class="hljs-string">'%-8.2f'</span>%<span class="hljs-number">13.3333</span>                     Out[<span class="hljs-number">61</span>]: <span class="hljs-string">'13.33   '</span>In [<span class="hljs-number">62</span>]: <span class="hljs-string">'%08.2f'</span>%<span class="hljs-number">13.3333</span>                     Out[<span class="hljs-number">62</span>]: <span class="hljs-string">'00013.33'</span></code></pre><h4 id="Python方法"><a href="#Python方法" class="headerlink" title="Python方法"></a><strong>Python方法</strong></h4><p>在Python中我们一般使用<code>format</code>来进行格式化输出</p><h5 id="format"><a href="#format" class="headerlink" title="format"></a><code>format</code></h5><pre><code class="hljs python">In [<span class="hljs-number">67</span>]: <span class="hljs-string">'&#123;:.2f&#125;'</span>.format(<span class="hljs-number">12.333</span>)              Out[<span class="hljs-number">67</span>]: <span class="hljs-string">'12.33'</span>In [<span class="hljs-number">68</span>]: <span class="hljs-string">'&#123;a:.2f&#125;'</span>.format(a=<span class="hljs-number">12.333</span>)           Out[<span class="hljs-number">68</span>]: <span class="hljs-string">'12.33'</span></code></pre><blockquote><p>保留两位小数</p></blockquote><pre><code class="hljs python">In [<span class="hljs-number">70</span>]: <span class="hljs-string">'&#123;:.2%&#125;'</span>.format(<span class="hljs-number">0.001</span>)               Out[<span class="hljs-number">70</span>]: <span class="hljs-string">'0.10%'</span>In [<span class="hljs-number">71</span>]: <span class="hljs-string">'&#123;:.2%&#125;'</span>.format(<span class="hljs-number">0.61</span>)                Out[<span class="hljs-number">71</span>]: <span class="hljs-string">'61.00%'</span></code></pre><blockquote><p>百分比格式</p></blockquote><pre><code class="hljs python">In [<span class="hljs-number">72</span>]: <span class="hljs-string">'&#123;0:x&#125;'</span>.format(<span class="hljs-number">20</span>)                   Out[<span class="hljs-number">72</span>]: <span class="hljs-string">'14'</span>In [<span class="hljs-number">73</span>]: <span class="hljs-string">'&#123;0:x&#125;'</span>.format(<span class="hljs-number">10</span>)                   Out[<span class="hljs-number">73</span>]: <span class="hljs-string">'a'</span></code></pre><blockquote><p>转换成十六进制</p></blockquote><pre><code class="hljs python">In [<span class="hljs-number">74</span>]: <span class="hljs-string">'&#123;0:o&#125;'</span>.format(<span class="hljs-number">20</span>)                   Out[<span class="hljs-number">74</span>]: <span class="hljs-string">'24'</span>In [<span class="hljs-number">75</span>]: <span class="hljs-string">'&#123;0:o&#125;'</span>.format(<span class="hljs-number">10</span>)                   Out[<span class="hljs-number">75</span>]: <span class="hljs-string">'12'</span></code></pre><blockquote><p>转换成8进制</p></blockquote><blockquote><p>注意：进制转换时使用{0:进制}这个格式</p></blockquote><pre><code class="hljs python">In [<span class="hljs-number">76</span>]: <span class="hljs-string">'&#123;a:&lt;10&#125;'</span>.format(a=<span class="hljs-number">12.3</span>,b=<span class="hljs-number">13.44</span>)     Out[<span class="hljs-number">76</span>]: <span class="hljs-string">'12.3      '</span></code></pre><blockquote><p>左对齐，长度为10</p></blockquote><pre><code class="hljs python">In [<span class="hljs-number">77</span>]: <span class="hljs-string">'&#123;a:0&lt;10&#125;'</span>.format(a=<span class="hljs-number">12.3</span>,b=<span class="hljs-number">13.44</span>)        ...:                                      Out[<span class="hljs-number">77</span>]: <span class="hljs-string">'12.3000000'</span></code></pre><blockquote><p>数字补x(填充右边，宽度为4)</p></blockquote><pre><code class="hljs python">In [<span class="hljs-number">78</span>]: <span class="hljs-string">'&#123;a:0&gt;10&#125;'</span>.format(a=<span class="hljs-number">12.3</span>,b=<span class="hljs-number">13.44</span>)    Out[<span class="hljs-number">78</span>]: <span class="hljs-string">'00000012.3'</span></code></pre><blockquote><p>右对齐，长度为10</p></blockquote><pre><code class="hljs python">In [<span class="hljs-number">79</span>]: <span class="hljs-string">'&#123;a:0^10&#125;'</span>.format(a=<span class="hljs-number">12.3</span>,b=<span class="hljs-number">13.44</span>)    Out[<span class="hljs-number">79</span>]: <span class="hljs-string">'00012.3000'</span></code></pre><blockquote><p>两边对齐， 长度为10</p></blockquote><h3 id="字符串转义"><a href="#字符串转义" class="headerlink" title="字符串转义"></a>字符串转义</h3><p>字符前面加上   \  ，字符就不再表示字符本身的意思，表示ASCII码中不能显示字符，常见有下：</p><h5 id="n"><a href="#n" class="headerlink" title="\n"></a><code>\n</code></h5><blockquote><p>  换行</p></blockquote><p>  <strong>演示：</strong></p><pre><code class="hljs python">In [<span class="hljs-number">83</span>]: print(<span class="hljs-string">'abc\nabc'</span>)                    abcabc</code></pre><h5 id="t"><a href="#t" class="headerlink" title="\t"></a><code>\t</code></h5><blockquote><p>水平制表符</p></blockquote><p><strong>演示</strong>：</p><pre><code class="hljs python">In [<span class="hljs-number">84</span>]: print(<span class="hljs-string">'abc\tabc'</span>)                    abcabc</code></pre><h5 id="b"><a href="#b" class="headerlink" title="\b"></a><code>\b</code></h5><blockquote><p> 退格</p></blockquote><p>  <strong>演示：</strong></p><pre><code class="hljs python">In [<span class="hljs-number">85</span>]: print(<span class="hljs-string">'abc\babc'</span>)                    ababc</code></pre><h5 id="r"><a href="#r" class="headerlink" title="\r"></a><code>\r</code></h5><blockquote><p> 回车，当前位置移到本行开头</p></blockquote><p><strong>演示：</strong></p><pre><code class="hljs python">In [<span class="hljs-number">87</span>]: print(<span class="hljs-string">'abc\rbc'</span>)                     bcc</code></pre><h5 id=""><a href="#" class="headerlink" title="\"></a><code>\</code></h5><blockquote><p>代表反斜杠  \</p></blockquote><p><strong>演示：</strong></p><pre><code class="hljs python">In [<span class="hljs-number">88</span>]: print(<span class="hljs-string">'abc\\bc'</span>)                     abc\bc</code></pre><h5 id="’"><a href="#’" class="headerlink" title="\’"></a><code>\’</code></h5><blockquote><p>代表一个单引号，同样的 “  ？ 等符号也可以这么输出</p></blockquote><p><strong>演示：</strong></p><pre><code class="hljs python">In [<span class="hljs-number">95</span>]: print(<span class="hljs-string">'abc\' \"b c'</span>)                 abc<span class="hljs-string">' "b c</span></code></pre><h5 id="0"><a href="#0" class="headerlink" title="\0"></a><code>\0</code></h5><blockquote><p> 代表一个空字符</p></blockquote><p><strong>演示：</strong></p><pre><code class="hljs python">In [<span class="hljs-number">96</span>]: print(<span class="hljs-string">'abc\0abc'</span>)                    abcabcIn [<span class="hljs-number">97</span>]: print(<span class="hljs-string">'abc\0\0abc'</span>)                  abcabc</code></pre><h5 id="a"><a href="#a" class="headerlink" title="\a"></a><code>\a</code></h5><blockquote><p>系统提示音(交互环境需使用print)</p></blockquote><h5 id="取消转义"><a href="#取消转义" class="headerlink" title="取消转义"></a><code>取消转义</code></h5><blockquote><p>在python中如果要去掉字符串的转义，只需要在字符串前面加上 r</p></blockquote><p><strong>演示：</strong></p><pre><code class="hljs python">In [<span class="hljs-number">104</span>]: print(<span class="hljs-string">r'abc\b\t\nabc'</span>)              abc\b\t\nabc</code></pre><h3 id="格式化和转义的应用"><a href="#格式化和转义的应用" class="headerlink" title="格式化和转义的应用"></a>格式化和转义的应用</h3><p>格式化得到的结果都是字符串，通过把位置预先留出来，后期再往其中填入内容可以让输出内容更加整洁美观，而又具有良好的可读性，同时让代码更简洁精练。</p><p>字符串的转义可以方便我们表示我们不太方便表示的字符，同时转义有些情况下又会带来麻烦，特别是在表示路径的时候，这种情况下可以在字符串前面加上  r  来去掉字符串的转义。</p><h3 id="字符串编码"><a href="#字符串编码" class="headerlink" title="字符串编码"></a>字符串编码</h3><p>对于编码这部分，我们先举个栗子来看看：</p><pre><code class="hljs python">In [<span class="hljs-number">105</span>]: <span class="hljs-string">'你好'</span>.encode(<span class="hljs-string">'utf-8'</span>)                  Out[<span class="hljs-number">105</span>]: <span class="hljs-string">b'\xe4\xbd\xa0\xe5\xa5\xbd'</span>In [<span class="hljs-number">106</span>]: <span class="hljs-string">'你好'</span>.encode(<span class="hljs-string">'gbk'</span>)                    Out[<span class="hljs-number">106</span>]: <span class="hljs-string">b'\xc4\xe3\xba\xc3'</span></code></pre><p>我们可以通过不同的编码方式来进行编码以便我们在不同情况下使用，接下来我们来看看编码相关的知识点</p><p><img src="http://eveseven.oss-cn-shanghai.aliyuncs.com/18-12-8/6179178.jpg" srcset="/img/loading.gif" alt="1"></p><h4 id="Python对于字符集的处理"><a href="#Python对于字符集的处理" class="headerlink" title="Python对于字符集的处理"></a>Python对于字符集的处理</h4><p><img src="http://eveseven.oss-cn-shanghai.aliyuncs.com/18-12-8/45812485.jpg" srcset="/img/loading.gif" alt=""></p><h4 id="字符编码的作用"><a href="#字符编码的作用" class="headerlink" title="字符编码的作用"></a>字符编码的作用</h4><p>Python统一了编码，这样Python在内部处理的时候不会因编码不同而出现程序不能正常执行的问题。</p><p>Python会自动根据系统环境选择编码，但是经常在文件传输的过程中，会遇到各种不同的编码，这个时候就需要我们去处理编码问题。</p><h3 id="深浅复制"><a href="#深浅复制" class="headerlink" title="深浅复制"></a>深浅复制</h3><p>举个栗子看看</p><pre><code class="hljs python">In [<span class="hljs-number">107</span>]: s = [<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>]                           In [<span class="hljs-number">108</span>]: s2 = [<span class="hljs-string">'a'</span>, s]                           In [<span class="hljs-number">109</span>]: s2                                      Out[<span class="hljs-number">109</span>]: [<span class="hljs-string">'a'</span>, [<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>]]In [<span class="hljs-number">111</span>]: id(s), id(s2), id(s2[<span class="hljs-number">1</span>])                                     Out[<span class="hljs-number">111</span>]: (<span class="hljs-number">139956880001800</span>, <span class="hljs-number">139956880021256</span>, <span class="hljs-number">139956880001800</span>)</code></pre><blockquote><p>我们可以看出来，s2只是引用了s的值，s2[1]的值会随着s的变化而变化。</p></blockquote><h4 id="浅复制"><a href="#浅复制" class="headerlink" title="浅复制"></a>浅复制</h4><p>举个栗子：</p><pre><code class="hljs python">In [<span class="hljs-number">136</span>]: s = [<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>]                                                In [<span class="hljs-number">137</span>]: s2 = [<span class="hljs-string">'a'</span>, s]                                                In [<span class="hljs-number">138</span>]: s3 = s2.copy()                                               In [<span class="hljs-number">139</span>]: s[<span class="hljs-number">0</span>]=<span class="hljs-string">'w'</span>                                                     In [<span class="hljs-number">140</span>]: s2                                                           Out[<span class="hljs-number">140</span>]: [<span class="hljs-string">'a'</span>, [<span class="hljs-string">'w'</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>]]In [<span class="hljs-number">141</span>]: s3                                                           Out[<span class="hljs-number">141</span>]: [<span class="hljs-string">'a'</span>, [<span class="hljs-string">'w'</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>]]In [<span class="hljs-number">149</span>]: id(s2[<span class="hljs-number">1</span>]), id(s3[<span class="hljs-number">1</span>]),id(s)                                   Out[<span class="hljs-number">149</span>]: (<span class="hljs-number">139956883394376</span>, <span class="hljs-number">139956883394376</span>, <span class="hljs-number">139956883394376</span>)</code></pre><blockquote><p>由上面的栗子可以看出来，虽然s3是copy的，但是s3还是会随着s的变化来变化的。那我们有什么办法让s3的值不变呢?那就是深拷贝。</p></blockquote><h4 id="深复制"><a href="#深复制" class="headerlink" title="深复制"></a>深复制</h4><p>举个栗子：</p><pre><code class="hljs python">In [<span class="hljs-number">150</span>]: s = [<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>]                                                In [<span class="hljs-number">151</span>]: s2 = [<span class="hljs-string">'a'</span>, s]                                                In [<span class="hljs-number">152</span>]: <span class="hljs-keyword">import</span> copy                                                  In [<span class="hljs-number">153</span>]: s3 = copy.deepcopy(s2)                                       In [<span class="hljs-number">154</span>]: s2                                                           Out[<span class="hljs-number">154</span>]: [<span class="hljs-string">'a'</span>, [<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>]]In [<span class="hljs-number">155</span>]: s3                                                           Out[<span class="hljs-number">155</span>]: [<span class="hljs-string">'a'</span>, [<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>]]In [<span class="hljs-number">156</span>]: s[<span class="hljs-number">0</span>]=<span class="hljs-string">'r'</span>                                                     In [<span class="hljs-number">157</span>]: s2                                                           Out[<span class="hljs-number">157</span>]: [<span class="hljs-string">'a'</span>, [<span class="hljs-string">'r'</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>]]In [<span class="hljs-number">158</span>]: s3                                                           Out[<span class="hljs-number">158</span>]: [<span class="hljs-string">'a'</span>, [<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>]]In [<span class="hljs-number">159</span>]: id(s),id(s2[<span class="hljs-number">1</span>]),id(s3[<span class="hljs-number">1</span>])                                    Out[<span class="hljs-number">159</span>]: (<span class="hljs-number">139956879794952</span>, <span class="hljs-number">139956879794952</span>, <span class="hljs-number">139956879278408</span>)</code></pre><blockquote><p>通过深拷贝，我们就实现了，原数据改变的时候，复制的数据不会随着改变。</p></blockquote><h4 id="深浅复制的应用"><a href="#深浅复制的应用" class="headerlink" title="深浅复制的应用"></a>深浅复制的应用</h4><p>深浅复制只有在<code>列表嵌套列表</code>的情况下讨论。</p><p>如果想保留修改之前的数据，就可以使用列表的复制，但要注意列表嵌套情况下的问题。</p><h3 id="bytes和bytearray"><a href="#bytes和bytearray" class="headerlink" title="bytes和bytearray"></a>bytes和bytearray</h3><h4 id="bytes二进制序列类型"><a href="#bytes二进制序列类型" class="headerlink" title="bytes二进制序列类型"></a>bytes二进制序列类型</h4><pre><code class="hljs python">In [<span class="hljs-number">169</span>]: a= bytes(<span class="hljs-number">3</span>)                                                  In [<span class="hljs-number">170</span>]: a[<span class="hljs-number">0</span>]                                                         Out[<span class="hljs-number">170</span>]: <span class="hljs-number">0</span>In [<span class="hljs-number">171</span>]: a[<span class="hljs-number">1</span>]                                                         Out[<span class="hljs-number">171</span>]: <span class="hljs-number">0</span></code></pre><blockquote><p>使用bytes(number)指定长度的零填充字节生成一个二进制的序列类型</p></blockquote><pre><code class="hljs python">In [<span class="hljs-number">172</span>]: bytes(<span class="hljs-string">b'abc'</span>)                                                Out[<span class="hljs-number">172</span>]: <span class="hljs-string">b'abc'</span></code></pre><blockquote><p>二进制字符串</p></blockquote><h4 id="bytearray二进制数组"><a href="#bytearray二进制数组" class="headerlink" title="bytearray二进制数组"></a>bytearray二进制数组</h4><pre><code class="hljs python">In [<span class="hljs-number">173</span>]: a = bytearray(<span class="hljs-number">3</span>)                                             In [<span class="hljs-number">174</span>]: a                                                            Out[<span class="hljs-number">174</span>]: bytearray(<span class="hljs-string">b'\x00\x00\x00'</span>)In [<span class="hljs-number">175</span>]: a[<span class="hljs-number">1</span>]                                                         Out[<span class="hljs-number">175</span>]: <span class="hljs-number">0</span></code></pre><blockquote><p>使用bytearray(number)指定长度的零填充字节生成一个二进制的数组</p></blockquote><pre><code class="hljs python">In [<span class="hljs-number">176</span>]: bytearray(<span class="hljs-string">b'abc'</span>)                                            Out[<span class="hljs-number">176</span>]: bytearray(<span class="hljs-string">b'abc'</span>)</code></pre><blockquote><p>二进制字符串</p></blockquote><h4 id="二进制序列类型的应用"><a href="#二进制序列类型的应用" class="headerlink" title="二进制序列类型的应用"></a>二进制序列类型的应用</h4><p>二进制序列类型的用法比较少见，是python中少用的一种序列类型。</p><p>对于二进制序列类型，大家基本了解即可。</p>]]></content>
    
    
    <categories>
      
      <category>python</category>
      
    </categories>
    
    
    <tags>
      
      <tag>格式化输出</tag>
      
      <tag>深浅复制</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>python第二话之序列类型方法</title>
    <link href="/2018/12/python-sequence/"/>
    <url>/2018/12/python-sequence/</url>
    
    <content type="html"><![CDATA[<p>[TOC]</p><h3 id="序列类型方法"><a href="#序列类型方法" class="headerlink" title="序列类型方法"></a>序列类型方法</h3><p>前面我们简单的介绍了什么是序列类型，并且简单的给大家提了下<code>字符串</code>、<code>元组</code>、<code>列表</code>的一些基本操作。接下来我们详细的来看下关于<code>字符串</code>、<code>元组</code>、<code>列表</code>的常用操作。</p><h4 id="列表常用方法"><a href="#列表常用方法" class="headerlink" title="列表常用方法"></a>列表常用方法</h4><p>我们先查看下<code>list</code>可以使用的一些方法：</p><blockquote><p>  append(…)<br>|      L.append(object) -&gt; None – append object to end<br>|<br>|  clear(…)<br>|      L.clear() -&gt; None – remove all items from L<br>|<br>|  copy(…)<br>|      L.copy() -&gt; list – a shallow copy of L<br>|<br>|  count(…)<br>|      L.count(value) -&gt; integer – return number of occurrences of value<br>|<br>|  extend(…)<br>|      L.extend(iterable) -&gt; None – extend list by appending elements from the iterable<br>|<br>|  index(…)<br>|      L.index(value, [start, [stop]]) -&gt; integer – return first index of value.<br>|      Raises ValueError if the value is not present.<br>|<br>|  insert(…)<br>|      L.insert(index, object) – insert object before index<br>|<br>|  pop(…)<br>|      L.pop([index]) -&gt; item – remove and return item at index (default last).<br>|      Raises IndexError if list is empty or index is out of range.<br>|<br>|  remove(…)<br>|      L.remove(value) -&gt; None – remove first occurrence of value.<br>|      Raises ValueError if the value is not present.<br>|<br>|  reverse(…)<br>|      L.reverse() – reverse <em>IN PLACE</em><br>|<br>|  sort(…)<br>|      L.sort(key=None, reverse=False) -&gt; None – stable sort *IN PLAC</p></blockquote><p>接下来我们给大家分类讲解一下这样操作方法。</p><h5 id="增加元素"><a href="#增加元素" class="headerlink" title="增加元素"></a><code>增加元素</code></h5><p><strong>append</strong></p><p>查看下<code>append</code>这个方法的用法：</p><pre><code class="hljs python">In [<span class="hljs-number">1</span>]: s = [<span class="hljs-number">1</span>,<span class="hljs-number">2</span>,<span class="hljs-number">3</span>,<span class="hljs-number">4</span>,<span class="hljs-number">5</span>,<span class="hljs-string">'a'</span>,<span class="hljs-string">'b'</span>,<span class="hljs-string">'c'</span>]                                    In [<span class="hljs-number">2</span>]: help(s.append)</code></pre><blockquote><p>append(…) method of builtins.list instance<br>​    L.append(object) -&gt; None – append object to end</p><p>使用方式：L.append(‘s’)</p><p>注意：append方法是在列表的最后添加元素</p></blockquote><p><strong>演示</strong>：</p><pre><code class="hljs python">In [<span class="hljs-number">3</span>]: s.append(<span class="hljs-string">'w'</span>)                                                  In [<span class="hljs-number">4</span>]: s                                                              Out[<span class="hljs-number">4</span>]: [<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>, <span class="hljs-number">4</span>, <span class="hljs-number">5</span>, <span class="hljs-string">'a'</span>, <span class="hljs-string">'b'</span>, <span class="hljs-string">'c'</span>, <span class="hljs-string">'w'</span>]</code></pre><p><strong>insert</strong></p><p>查看<code>insert</code>的使用方法：</p><pre><code class="hljs python">In [<span class="hljs-number">5</span>]: help(s.insert)</code></pre><blockquote><p>insert(…) method of builtins.list instance<br>​    L.insert(index, object) – insert object before index</p><p>使用方法： L.insert(index,’s’)</p><p>注意：index: 索引值，s为元素</p><p>insert方法是在指定的索引值处添加元素</p></blockquote><p><strong>演示：</strong></p><pre><code class="hljs python">In [<span class="hljs-number">6</span>]: s.insert(<span class="hljs-number">3</span>,<span class="hljs-string">'s'</span>)                                                In [<span class="hljs-number">7</span>]: s                                                              Out[<span class="hljs-number">7</span>]: [<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>, <span class="hljs-string">'s'</span>, <span class="hljs-number">4</span>, <span class="hljs-number">5</span>, <span class="hljs-string">'a'</span>, <span class="hljs-string">'b'</span>, <span class="hljs-string">'c'</span>, <span class="hljs-string">'w'</span>]</code></pre><p><strong>extend</strong></p><p>查看<code>extend</code>的使用方法</p><pre><code class="hljs python">In [<span class="hljs-number">8</span>]: help(s.extend)</code></pre><blockquote><p>extend(…) method of builtins.list instance<br>​    L.extend(iterable) -&gt; None – extend list by appending elements from the iterable</p><p>使用方法：L.extend([1,2,’a’]) </p><p>注意：extend方法是指在列表中追加可迭代器</p></blockquote><p><strong>演示：</strong></p><pre><code class="hljs python">In [<span class="hljs-number">9</span>]: a = [<span class="hljs-string">'w'</span>,<span class="hljs-string">'o'</span>,<span class="hljs-number">0</span>]                                                In [<span class="hljs-number">10</span>]: s.extend(a)                                                   In [<span class="hljs-number">11</span>]: s                                                             Out[<span class="hljs-number">11</span>]: [<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>, <span class="hljs-string">'s'</span>, <span class="hljs-number">4</span>, <span class="hljs-number">5</span>, <span class="hljs-string">'a'</span>, <span class="hljs-string">'b'</span>, <span class="hljs-string">'c'</span>, <span class="hljs-string">'w'</span>, <span class="hljs-string">'w'</span>, <span class="hljs-string">'o'</span>, <span class="hljs-number">0</span>]</code></pre><h5 id="删除元素"><a href="#删除元素" class="headerlink" title="删除元素"></a><code>删除元素</code></h5><p><strong>clear</strong></p><p>查看下<code>clear</code>的使用方法</p><pre><code class="hljs python">In [<span class="hljs-number">14</span>]: help(s.clear)</code></pre><blockquote><p>clear(…) method of builtins.list instance<br>​    L.clear() -&gt; None – remove all items from L</p><p>使用方法：L.clear()</p><p>注意：clear是删除所有的元素</p></blockquote><p><strong>演示</strong>：</p><pre><code class="hljs python">In [<span class="hljs-number">20</span>]: s = [<span class="hljs-number">1</span>,<span class="hljs-number">2</span>,<span class="hljs-number">3</span>,<span class="hljs-number">4</span>,<span class="hljs-number">5</span>,<span class="hljs-string">'a'</span>,<span class="hljs-string">'b'</span>,<span class="hljs-string">'c'</span>]                                   In [<span class="hljs-number">21</span>]: s.clear()                                                     In [<span class="hljs-number">22</span>]: s                                                             Out[<span class="hljs-number">22</span>]: []</code></pre><p><strong>pop</strong></p><p>查看<code>pop</code>的使用方法：</p><pre><code class="hljs python">In [<span class="hljs-number">23</span>]: help(s.pop)</code></pre><blockquote><p>pop(…) method of builtins.list instance<br>​    L.pop([index]) -&gt; item – remove and return item at index (default last).<br>​    Raises IndexError if list is empty or index is out of range.</p><p>使用方法：L.pop()、L.pop(index)</p><p>注意：L.pop()指的是删除最后一个元素</p><p>​        L.pop(index)指的是指定索引值删除元素</p></blockquote><p><strong>演示</strong>：</p><pre><code class="hljs python">In [<span class="hljs-number">24</span>]: s = [<span class="hljs-number">1</span>,<span class="hljs-number">2</span>,<span class="hljs-number">3</span>,<span class="hljs-number">4</span>,<span class="hljs-number">5</span>,<span class="hljs-string">'a'</span>,<span class="hljs-string">'b'</span>,<span class="hljs-string">'c'</span>]                                   In [<span class="hljs-number">25</span>]: s.pop()                                                       Out[<span class="hljs-number">25</span>]: <span class="hljs-string">'c'</span>In [<span class="hljs-number">26</span>]: s.pop()                                                       Out[<span class="hljs-number">26</span>]: <span class="hljs-string">'b'</span>In [<span class="hljs-number">27</span>]: s.pop()                                                       Out[<span class="hljs-number">27</span>]: <span class="hljs-string">'a'</span>In [<span class="hljs-number">28</span>]: s.pop()                                                       Out[<span class="hljs-number">28</span>]: <span class="hljs-number">5</span>In [<span class="hljs-number">29</span>]: s.pop(<span class="hljs-number">1</span>)                                                      Out[<span class="hljs-number">29</span>]: <span class="hljs-number">2</span>In [<span class="hljs-number">30</span>]: s                                                             Out[<span class="hljs-number">30</span>]: [<span class="hljs-number">1</span>, <span class="hljs-number">3</span>, <span class="hljs-number">4</span>]</code></pre><p><strong>remove</strong></p><p>查看<code>remove</code>的使用方法：</p><pre><code class="hljs python">In [<span class="hljs-number">35</span>]: help(s.remove)</code></pre><blockquote><p>remove(…) method of builtins.list instance<br>​    L.remove(value) -&gt; None – remove first occurrence of value.<br>​    Raises ValueError if the value is not present.</p><p>使用方法：L.remove(obj) </p><p>注意：移除指定元素从左边开始的第一个。</p></blockquote><p><strong>演示：</strong></p><pre><code class="hljs python">In [<span class="hljs-number">56</span>]: s = [<span class="hljs-number">1</span>,<span class="hljs-number">2</span>,<span class="hljs-number">3</span>,<span class="hljs-number">4</span>,<span class="hljs-number">5</span>,<span class="hljs-string">'a'</span>,<span class="hljs-string">'b'</span>,<span class="hljs-string">'c'</span>,<span class="hljs-number">1</span>,<span class="hljs-number">2</span>,<span class="hljs-number">3</span>]                             In [<span class="hljs-number">57</span>]: s.remove(<span class="hljs-number">2</span>)                                                   In [<span class="hljs-number">58</span>]: s                                                             Out[<span class="hljs-number">58</span>]: [<span class="hljs-number">1</span>, <span class="hljs-number">3</span>, <span class="hljs-number">4</span>, <span class="hljs-number">5</span>, <span class="hljs-string">'a'</span>, <span class="hljs-string">'b'</span>, <span class="hljs-string">'c'</span>, <span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>]In [<span class="hljs-number">59</span>]: s.remove(<span class="hljs-number">2</span>)                                                   In [<span class="hljs-number">60</span>]: s                                                             Out[<span class="hljs-number">60</span>]: [<span class="hljs-number">1</span>, <span class="hljs-number">3</span>, <span class="hljs-number">4</span>, <span class="hljs-number">5</span>, <span class="hljs-string">'a'</span>, <span class="hljs-string">'b'</span>, <span class="hljs-string">'c'</span>, <span class="hljs-number">1</span>, <span class="hljs-number">3</span>]</code></pre><h5 id="修改元素"><a href="#修改元素" class="headerlink" title="修改元素"></a><code>修改元素</code></h5><p>修改元素在列表中就非常简单了。</p><blockquote><p>使用方法：L[index] = obj</p><p>注意：修改元素是根据元素的索引来修改。</p></blockquote><p><strong>演示：</strong></p><pre><code class="hljs python">In [<span class="hljs-number">61</span>]: s = [<span class="hljs-number">1</span>,<span class="hljs-number">2</span>,<span class="hljs-number">3</span>,<span class="hljs-number">4</span>,<span class="hljs-number">5</span>]                                               In [<span class="hljs-number">62</span>]: s[<span class="hljs-number">3</span>]=<span class="hljs-string">'s'</span>                                                      In [<span class="hljs-number">63</span>]: s                                                             Out[<span class="hljs-number">63</span>]: [<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>, <span class="hljs-string">'s'</span>, <span class="hljs-number">5</span>]</code></pre><h5 id="查找元素"><a href="#查找元素" class="headerlink" title="查找元素"></a><code>查找元素</code></h5><p><strong>index</strong></p><p>查看index的使用方法</p><pre><code class="hljs python">In [<span class="hljs-number">64</span>]: help(s.index)</code></pre><blockquote><p>index(…) method of builtins.list instance<br>​    L.index(value, [start, [stop]]) -&gt; integer – return first index of value.<br>​    Raises ValueError if the value is not present.</p><p>使用方法：L.index(obj) , L.index(value, [start, [stop]])</p><p>注意：L.index(obj) 从列表中找某个值第一个匹配项的索引位置。</p><p>​        L.index(value, [start, [stop]])指定索引范围查找元素</p></blockquote><p><strong>演示</strong>：</p><pre><code class="hljs python">In [<span class="hljs-number">71</span>]: s = [<span class="hljs-number">1</span>,<span class="hljs-number">2</span>,<span class="hljs-number">3</span>,<span class="hljs-number">4</span>,<span class="hljs-number">5</span>]                                               In [<span class="hljs-number">72</span>]: s.index(<span class="hljs-number">1</span>)                                                    Out[<span class="hljs-number">72</span>]: <span class="hljs-number">0</span>In [<span class="hljs-number">73</span>]: s.index(<span class="hljs-number">3</span>)                                                    Out[<span class="hljs-number">73</span>]: <span class="hljs-number">2</span>In [<span class="hljs-number">98</span>]: s = [<span class="hljs-number">1</span>,<span class="hljs-number">2</span>,<span class="hljs-number">3</span>,<span class="hljs-number">4</span>,<span class="hljs-number">5</span>,<span class="hljs-string">'a'</span>,<span class="hljs-string">'b'</span>,<span class="hljs-string">'c'</span>,<span class="hljs-number">1</span>,<span class="hljs-number">2</span>,<span class="hljs-number">3</span>]                             In [<span class="hljs-number">99</span>]: s.index(<span class="hljs-number">2</span>,<span class="hljs-number">2</span>)                                                  Out[<span class="hljs-number">99</span>]: <span class="hljs-number">9</span>In [<span class="hljs-number">100</span>]: s.index(<span class="hljs-number">2</span>)                                                   Out[<span class="hljs-number">100</span>]: <span class="hljs-number">1</span></code></pre><p><strong>count</strong></p><p>查看<code>count</code>的使用方法</p><pre><code class="hljs python">In [<span class="hljs-number">74</span>]: help(s.count)</code></pre><blockquote><p>count(…) method of builtins.list instance<br>​    L.count(value) -&gt; integer – return number of occurrences of value</p><p>使用方法：L.count(obj)</p><p>注意：统计某个元素在列表中出现的次数。</p></blockquote><p><strong>演示：</strong></p><pre><code class="hljs python">In [<span class="hljs-number">75</span>]: s = [<span class="hljs-number">1</span>,<span class="hljs-number">2</span>,<span class="hljs-number">3</span>,<span class="hljs-number">4</span>,<span class="hljs-number">5</span>,<span class="hljs-number">3</span>,<span class="hljs-number">4</span>,<span class="hljs-number">2</span>]                                         In [<span class="hljs-number">76</span>]: s.count(<span class="hljs-number">2</span>)                                                    Out[<span class="hljs-number">76</span>]: <span class="hljs-number">2</span>In [<span class="hljs-number">77</span>]: s.count(<span class="hljs-number">3</span>)                                                    Out[<span class="hljs-number">77</span>]: <span class="hljs-number">2</span>In [<span class="hljs-number">78</span>]: s.count(<span class="hljs-number">1</span>)                                                    Out[<span class="hljs-number">78</span>]: <span class="hljs-number">1</span></code></pre><h5 id="扩展"><a href="#扩展" class="headerlink" title="扩展"></a><code>扩展</code></h5><p><strong>copy</strong></p><p>查看<code>copy</code>的使用方法</p><pre><code class="hljs python">In [<span class="hljs-number">79</span>]: help(s.copy)</code></pre><blockquote><p>copy(…) method of builtins.list instance<br>​    L.copy() -&gt; list – a shallow copy of L</p><p>使用方法： L.copy()</p><p>注意： 复制列表，和L[:]的复制方式一样属于浅复制。</p></blockquote><p><strong>演示：</strong></p><pre><code class="hljs python">In [<span class="hljs-number">101</span>]: s = [<span class="hljs-number">1</span>,<span class="hljs-number">2</span>,<span class="hljs-number">3</span>,<span class="hljs-number">4</span>,<span class="hljs-number">5</span>]                                              In [<span class="hljs-number">102</span>]: a = s.copy()                                                 In [<span class="hljs-number">103</span>]: a                                                            Out[<span class="hljs-number">103</span>]: [<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>, <span class="hljs-number">4</span>, <span class="hljs-number">5</span>]In [<span class="hljs-number">104</span>]: id(s)                                                        Out[<span class="hljs-number">104</span>]: <span class="hljs-number">140462445434696</span>In [<span class="hljs-number">105</span>]: id(a)                                                        Out[<span class="hljs-number">105</span>]: <span class="hljs-number">140462410623560</span></code></pre><p><strong>reverse</strong></p><p>查看<code>reverse</code>的使用方法</p><pre><code class="hljs python">In [<span class="hljs-number">83</span>]: help(s.reverse)</code></pre><blockquote><p>reverse(…) method of builtins.list instance<br>​    L.reverse() – reverse <em>IN PLACE</em></p><p>使用方法：L.reverse()</p><p>注意： 反向列表中元素。</p></blockquote><p><strong>演示：</strong></p><pre><code class="hljs python">In [<span class="hljs-number">84</span>]: s = [<span class="hljs-number">1</span>,<span class="hljs-number">2</span>,<span class="hljs-number">3</span>,<span class="hljs-number">4</span>,<span class="hljs-number">5</span>]                                               In [<span class="hljs-number">85</span>]: s.reverse()                                                   In [<span class="hljs-number">86</span>]: s                                                             Out[<span class="hljs-number">86</span>]: [<span class="hljs-number">5</span>, <span class="hljs-number">4</span>, <span class="hljs-number">3</span>, <span class="hljs-number">2</span>, <span class="hljs-number">1</span>]</code></pre><p><strong>sort</strong></p><p>查看<code>sort</code>的使用方法</p><pre><code class="hljs python">In [<span class="hljs-number">87</span>]: help(s.sort)</code></pre><blockquote><p>sort(…) method of builtins.list instance<br>​    L.sort(key=None, reverse=False) -&gt; None – stable sort <em>IN PLACE</em></p><p>使用方法：L.sort() </p><p>注意： 对原列表进行排序。列表中的元素要类型相同  (key = len int lambda)，不同元素需要改变元素类型，然后根据ASCII码进行排序。</p></blockquote><p><strong>演示</strong>：</p><pre><code class="hljs python">In [<span class="hljs-number">92</span>]: s                                                             Out[<span class="hljs-number">92</span>]: [<span class="hljs-number">5</span>, <span class="hljs-number">4</span>, <span class="hljs-number">3</span>, <span class="hljs-number">2</span>, <span class="hljs-number">1</span>]In [<span class="hljs-number">93</span>]: s.sort()                                                      In [<span class="hljs-number">94</span>]: s                                                             Out[<span class="hljs-number">94</span>]: [<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>, <span class="hljs-number">4</span>, <span class="hljs-number">5</span>]    In [<span class="hljs-number">110</span>]: s = [<span class="hljs-number">1</span>,<span class="hljs-number">2</span>,<span class="hljs-number">3</span>,<span class="hljs-number">4</span>,<span class="hljs-number">5</span>,<span class="hljs-string">'b'</span>,<span class="hljs-string">'a'</span>]                                      In [<span class="hljs-number">111</span>]: s.sort()                                                     -----------------------------------------------------------------------TypeError                             Traceback (most recent call last)&lt;ipython-input<span class="hljs-number">-111</span><span class="hljs-number">-474</span>c8408a842&gt; <span class="hljs-keyword">in</span> &lt;module&gt;----&gt; 1 s.sort()TypeError: unorderable types: str() &lt; int()In [<span class="hljs-number">112</span>]: s.sort(key=str)                                              In [<span class="hljs-number">113</span>]: s                                                            Out[<span class="hljs-number">113</span>]: [<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>, <span class="hljs-number">4</span>, <span class="hljs-number">5</span>, <span class="hljs-string">'a'</span>, <span class="hljs-string">'b'</span>]</code></pre><h4 id="字符串常用方法"><a href="#字符串常用方法" class="headerlink" title="字符串常用方法"></a>字符串常用方法</h4><h5 id="增加元素-1"><a href="#增加元素-1" class="headerlink" title="增加元素"></a><code>增加元素</code></h5><p><strong>使用 +</strong></p><p><strong>演示：</strong></p><pre><code class="hljs python">In [<span class="hljs-number">215</span>]: a = <span class="hljs-string">'hello'</span>                                                  In [<span class="hljs-number">216</span>]: b = <span class="hljs-string">'python'</span>                                                 In [<span class="hljs-number">217</span>]: c = <span class="hljs-string">'!'</span>                                                      In [<span class="hljs-number">218</span>]: a+b+c                                                        Out[<span class="hljs-number">218</span>]: <span class="hljs-string">'hellopython!'</span>In [<span class="hljs-number">219</span>]: a+<span class="hljs-string">' '</span>+b+<span class="hljs-string">' '</span>+c                                                Out[<span class="hljs-number">219</span>]: <span class="hljs-string">'hello python !'</span></code></pre><p><strong>格式化字符串</strong></p><p><strong>演示：</strong></p><pre><code class="hljs python">In [<span class="hljs-number">220</span>]: a = <span class="hljs-string">'hello'</span>                                                  In [<span class="hljs-number">221</span>]: b = <span class="hljs-string">'python'</span>                                                 In [<span class="hljs-number">222</span>]: c = <span class="hljs-string">'!'</span>                                                      In [<span class="hljs-number">223</span>]: <span class="hljs-string">'%s %s %s'</span>%(a,b,c)                                           Out[<span class="hljs-number">223</span>]: <span class="hljs-string">'hello python !'</span></code></pre><p><strong>使用join</strong></p><p><strong>演示：</strong></p><pre><code class="hljs python">In [<span class="hljs-number">229</span>]: a = <span class="hljs-string">'hello'</span>                                                  In [<span class="hljs-number">230</span>]: b = <span class="hljs-string">'python'</span>                                                 In [<span class="hljs-number">231</span>]: c = <span class="hljs-string">'!'</span>                                                      In [<span class="hljs-number">232</span>]: <span class="hljs-string">' '</span>.join([a,b,c])                                            Out[<span class="hljs-number">232</span>]: <span class="hljs-string">'hello python !'</span>In [<span class="hljs-number">233</span>]: <span class="hljs-string">'****'</span>.join(<span class="hljs-string">'abc'</span>)                                           Out[<span class="hljs-number">233</span>]: <span class="hljs-string">'a****b****c'</span></code></pre><p><strong>使用format</strong></p><p><strong>演示：</strong></p><pre><code class="hljs python">In [<span class="hljs-number">234</span>]: a = <span class="hljs-string">'hello'</span>                                                  In [<span class="hljs-number">235</span>]: b = <span class="hljs-string">'python'</span>                                                 In [<span class="hljs-number">236</span>]: c = <span class="hljs-string">'!'</span>                                                      In [<span class="hljs-number">237</span>]: <span class="hljs-string">'&#123;&#125; &#123;&#125; &#123;&#125;'</span>.format(a,b,c)                                     Out[<span class="hljs-number">237</span>]: <span class="hljs-string">'hello python !'</span>In [<span class="hljs-number">238</span>]: <span class="hljs-string">'&#123;0&#125; &#123;1&#125; &#123;2&#125;'</span>.format(a,b,c)                                  Out[<span class="hljs-number">238</span>]: <span class="hljs-string">'hello python !'</span>In [<span class="hljs-number">239</span>]: <span class="hljs-string">'&#123;2&#125; &#123;1&#125; &#123;0&#125;'</span>.format(a,b,c)                                  Out[<span class="hljs-number">239</span>]: <span class="hljs-string">'! python hello'</span>In [<span class="hljs-number">240</span>]: <span class="hljs-string">'&#123;1&#125; &#123;1&#125; &#123;1&#125;'</span>.format(a,b,c)                                  Out[<span class="hljs-number">240</span>]: <span class="hljs-string">'python python python'</span>In [<span class="hljs-number">241</span>]: <span class="hljs-string">'&#123;n1&#125; &#123;n2&#125; &#123;n3&#125;'</span>.format(n1=a, n2=b, n3=c)                    Out[<span class="hljs-number">241</span>]: <span class="hljs-string">'hello python !'</span></code></pre><h5 id="删除元素-1"><a href="#删除元素-1" class="headerlink" title="删除元素"></a><code>删除元素</code></h5><p><strong>replace</strong></p><p>查看<code>replace</code> 的使用方法</p><pre><code class="hljs python">In [<span class="hljs-number">115</span>]: help(s.replace)</code></pre><blockquote><p>replace(…) method of builtins.str instance<br>​    S.replace(old, new[, count]) -&gt; str<br>​    Return a copy of S with all occurrences of substring old replaced by new.  If the optional argument count is given, only the first count occurrences are replaced.</p><p>使用方法：s.replace (x,y) ：</p><p>注意： 子串替换,在字符串s中出现字符串x的任意位置都用y进行替换</p></blockquote><p><strong>演示：</strong></p><pre><code class="hljs python">In [<span class="hljs-number">205</span>]: s = <span class="hljs-string">'abc cnn dnn'</span>                                            In [<span class="hljs-number">206</span>]: s.replace(<span class="hljs-string">'a'</span>,<span class="hljs-string">'2'</span>)                                           Out[<span class="hljs-number">206</span>]: <span class="hljs-string">'2bc cnn dnn'</span>In [<span class="hljs-number">207</span>]: s.replace(<span class="hljs-string">'n'</span>,<span class="hljs-string">'w'</span>,<span class="hljs-number">1</span>)                                         Out[<span class="hljs-number">207</span>]: <span class="hljs-string">'abc cwn dnn'</span>In [<span class="hljs-number">208</span>]: s.replace(<span class="hljs-string">'n'</span>,<span class="hljs-string">'w'</span>,<span class="hljs-number">2</span>)                                         Out[<span class="hljs-number">208</span>]: <span class="hljs-string">'abc cww dnn'</span>In [<span class="hljs-number">209</span>]: s.replace(<span class="hljs-string">'n'</span>,<span class="hljs-string">'w'</span>)                                           Out[<span class="hljs-number">209</span>]: <span class="hljs-string">'abc cww dww'</span></code></pre><h5 id="修改元素-1"><a href="#修改元素-1" class="headerlink" title="修改元素"></a><code>修改元素</code></h5><p><strong>upper</strong></p><p>查看<code>upper</code> 的使用方法</p><pre><code class="hljs python">In [<span class="hljs-number">116</span>]: help(s.upper)</code></pre><blockquote><p>upper(…) method of builtins.str instance<br>​    S.upper() -&gt; str</p><p>Return a copy of S converted to uppercase.</p><p>使用方法： s.upper () </p><p>注意： 将字符串转为大写 </p></blockquote><p><strong>演示</strong>：</p><pre><code class="hljs python">In [<span class="hljs-number">189</span>]: s = <span class="hljs-string">'123456abc'</span>                                              In [<span class="hljs-number">190</span>]: s.upper()                                                    Out[<span class="hljs-number">190</span>]: <span class="hljs-string">'123456ABC'</span></code></pre><p><strong>lower</strong></p><p>查看<code>lower</code> 的使用方法</p><pre><code class="hljs python">In [<span class="hljs-number">116</span>]: help(s.lower)</code></pre><blockquote><p>lower(…) method of builtins.str instance<br>​    S.lower() -&gt; str<br>​    Return a copy of the string S converted to lowercase.</p><p>使用方法：s.lower () </p><p>注意：将字符串转为小写</p></blockquote><p><strong>演示：</strong></p><pre><code class="hljs python">In [<span class="hljs-number">193</span>]: s = <span class="hljs-string">'123456abcDF'</span>                                            In [<span class="hljs-number">194</span>]: s.lower()                                                    Out[<span class="hljs-number">194</span>]: <span class="hljs-string">'123456abcdf'</span></code></pre><p><strong>strip(lstrip、rstrip)</strong></p><p>查看<code>strip(lstrip、rstrip)</code>的使用方法</p><pre><code class="hljs python">In [<span class="hljs-number">116</span>]: help(s.strip) In [<span class="hljs-number">117</span>]: help(s.lstrip) In [<span class="hljs-number">118</span>]: help(s.rstrip)</code></pre><blockquote><p>strip(…) method of builtins.str instance<br>​    S.strip([chars]) -&gt; str<br>   Return a copy of the string S with leading and trailing whitespace removed. If chars is given and not None, remove characters in chars instead</p><p>使用方式：s.trip()</p><p>注意：去除两边的空格</p></blockquote><blockquote><p>lstrip(…) method of builtins.str instance<br>​    S.lstrip([chars]) -&gt; str    </p><p>Return a copy of the string S with leading whitespace removed.  If chars is given and not None, remove characters in chars instead</p><p>使用方式：s.lstrip()</p><p>注意:去除左边的空格</p></blockquote><blockquote><p>rstrip(…) method of builtins.str instance<br>​    S.rstrip([chars]) -&gt; str<br>   Return a copy of the string S with trailing whitespace removed.  If chars is given and not None, remove characters in chars instead</p><p>使用方式：s.rstrip()</p><p>注意：去除右边空格</p></blockquote><p><strong>演示：</strong></p><pre><code class="hljs python">In [<span class="hljs-number">195</span>]: s = <span class="hljs-string">'    abc    '</span>                                            In [<span class="hljs-number">196</span>]: s.strip()                                                    Out[<span class="hljs-number">196</span>]: <span class="hljs-string">'abc'</span>In [<span class="hljs-number">197</span>]: s.lstrip()                                                   Out[<span class="hljs-number">197</span>]: <span class="hljs-string">'abc    '</span>In [<span class="hljs-number">198</span>]: s.rstrip()                                                   Out[<span class="hljs-number">198</span>]: <span class="hljs-string">'    abc'</span></code></pre><p><strong>capitalize</strong></p><p>查看<code>capitalize</code>的使用方法</p><pre><code class="hljs python">In [<span class="hljs-number">115</span>]: help(s.capitalize)</code></pre><blockquote><p>capitalize(…) method of builtins.str instance<br>​    S.capitalize() -&gt; str<br>​    Return a capitalized version of S, i.e. make the first character  have upper case and the rest lower case.</p><p>使用方式： s.capitalize()</p><p>注意：首字母大写</p></blockquote><p><strong>演示：</strong></p><pre><code class="hljs prolog"><span class="hljs-symbol">In</span> [<span class="hljs-number">201</span>]: s = <span class="hljs-string">'abc cnn dnn'</span>                                            <span class="hljs-symbol">In</span> [<span class="hljs-number">202</span>]: s.capitalize()                                               <span class="hljs-symbol">Out</span>[<span class="hljs-number">202</span>]: <span class="hljs-string">'Abc cnn dnn'</span></code></pre><p><strong>title</strong></p><p>查看<code>title</code>的使用方法</p><pre><code class="hljs python">In [<span class="hljs-number">115</span>]: help(s.title)</code></pre><blockquote><p>title(…) method of builtins.str instance<br>​    S.title() -&gt; str<br>​    Return a titlecased version of S, i.e. words start with title case characters, all remaining cased characters have lower case.</p><p>使用方式： S.title() </p><p>注意：每个单词的首字母大写</p></blockquote><p><strong>演示：</strong></p><pre><code class="hljs python">In [<span class="hljs-number">203</span>]: s = <span class="hljs-string">'abc cnn dnn'</span>                                            In [<span class="hljs-number">204</span>]: s.title()                                                    Out[<span class="hljs-number">204</span>]: <span class="hljs-string">'Abc Cnn Dnn'</span></code></pre><p><strong>split</strong></p><p>查看<code>split</code>的使用方法</p><pre><code class="hljs python">In [<span class="hljs-number">115</span>]: help(s.split)</code></pre><blockquote><p>split(…) method of builtins.str instance<br>​    S.split(sep=None, maxsplit=-1) -&gt; list of strings<br>​    Return a list of the words in S, using sep as the delimiter string.  If maxsplit is given, at most maxsplit splits are done. If sep is not specified or is None, any whitespace string is a separator and empty strings are removed from the result.</p><p>使用方法：s.split()，s.split(a,b)</p><p>注意：s.split()指的是返回一系列用空格分割的字符串列表</p><p>​        s.split(a,b)指的是a,b为可选参数，a是将要分割的字符串，b是说明最多要分割几个</p></blockquote><p><strong>演示：</strong></p><pre><code class="hljs python">In [<span class="hljs-number">210</span>]: s = <span class="hljs-string">'abc123cba'</span>                                              In [<span class="hljs-number">211</span>]: s.split(<span class="hljs-string">'b'</span>)                                                 Out[<span class="hljs-number">211</span>]: [<span class="hljs-string">'a'</span>, <span class="hljs-string">'c123c'</span>, <span class="hljs-string">'a'</span>]In [<span class="hljs-number">213</span>]: s = <span class="hljs-string">'abc cnn dnn'</span>                                            In [<span class="hljs-number">214</span>]: s.split(<span class="hljs-string">' '</span>)                                                 Out[<span class="hljs-number">214</span>]: [<span class="hljs-string">'abc'</span>, <span class="hljs-string">'cnn'</span>, <span class="hljs-string">'dnn'</span>]</code></pre><h5 id="查找元素-1"><a href="#查找元素-1" class="headerlink" title="查找元素"></a><code>查找元素</code></h5><p><strong>count</strong></p><p>查看<code>count</code>的使用方法</p><pre><code class="hljs python">In [<span class="hljs-number">115</span>]: help(s.count)</code></pre><blockquote><p>count(…) method of builtins.str instance<br>​    S.count(sub[, start[, end]]) -&gt; int<br>​    Return the number of non-overlapping occurrences of substring sub in string S[start:end].  Optional arguments start and end are interpreted as in slice notation.</p><p>使用方法：s.count(x)</p><p>注意： 返回字符串x在s中出现的次数，带可选参数</p></blockquote><p><strong>演示：</strong></p><pre><code class="hljs python">In [<span class="hljs-number">138</span>]: s = <span class="hljs-string">'abc123abc321678'</span>                                        In [<span class="hljs-number">139</span>]: s.count(<span class="hljs-string">'a'</span>)                                                 Out[<span class="hljs-number">139</span>]: <span class="hljs-number">2</span>In [<span class="hljs-number">140</span>]: s.count(<span class="hljs-string">'8'</span>)                                                 Out[<span class="hljs-number">140</span>]: <span class="hljs-number">1</span></code></pre><p><strong>index</strong></p><p>查看<code>index</code>的使用方法</p><pre><code class="hljs python">In [<span class="hljs-number">115</span>]: help(s.index)</code></pre><blockquote><p>index(…) method of builtins.str instance<br>​    S.index(sub[, start[, end]]) -&gt; int<br>​    Like S.find() but raise ValueError when the substring is not found</p><p>使用方法：s.index(x)</p><p>注意：返回字符串中出现x的最左端的索引值，如果不在则抛出valueError异常</p></blockquote><p><strong>演示</strong>：</p><pre><code class="hljs python">In [<span class="hljs-number">141</span>]: s = <span class="hljs-string">'abc123abc321678'</span>                                        In [<span class="hljs-number">142</span>]: s.index(<span class="hljs-string">'a'</span>)                                                 Out[<span class="hljs-number">142</span>]: <span class="hljs-number">0</span>In [<span class="hljs-number">143</span>]: s.index(<span class="hljs-string">'a'</span>,<span class="hljs-number">2</span>)                                               Out[<span class="hljs-number">143</span>]: <span class="hljs-number">6</span></code></pre><p><strong>find</strong></p><p>查看<code>find</code>的使用方法</p><pre><code class="hljs python">In [<span class="hljs-number">115</span>]: help(s.find)</code></pre><blockquote><p>find(…) method of builtins.str instance<br>​    S.find(sub[, start[, end]]) -&gt; int<br>​    Return the lowest index in S where substring sub is found, such that sub is contained within S[start:end].Optional<br>arguments start and end are interpreted as in slice notation.</p><p>Return -1 on failure.</p><p>使用方法：s.find(x) </p><p>注意：返回字符串中出现x的最左端字符的索引值，如果不在则返回-1</p></blockquote><p><strong>演示</strong>：</p><pre><code class="hljs python">In [<span class="hljs-number">146</span>]: s = <span class="hljs-string">'abc123abc321678'</span>                                        In [<span class="hljs-number">147</span>]: s.index(<span class="hljs-string">'a'</span>,<span class="hljs-number">1</span>,<span class="hljs-number">4</span>)  <span class="hljs-comment"># index没有找到就报错                                            </span>-----------------------------------------------------------------------ValueError                            Traceback (most recent call last)&lt;ipython-input<span class="hljs-number">-147</span>-fbbfe36ae6a6&gt; <span class="hljs-keyword">in</span> &lt;module&gt;----&gt; 1 s.index('a',1,4)ValueError: substring <span class="hljs-keyword">not</span> foundIn [<span class="hljs-number">148</span>]: s.find(<span class="hljs-string">'a'</span>,<span class="hljs-number">1</span>,<span class="hljs-number">4</span>)  <span class="hljs-comment"># find 没有找到就提示-1                                             </span>Out[<span class="hljs-number">148</span>]: <span class="hljs-number">-1</span>In [<span class="hljs-number">149</span>]: s.find(<span class="hljs-string">'a'</span>,<span class="hljs-number">1</span>)                                                Out[<span class="hljs-number">149</span>]: <span class="hljs-number">6</span>In [<span class="hljs-number">150</span>]: s.find(<span class="hljs-string">'a'</span>)                                                  Out[<span class="hljs-number">150</span>]: <span class="hljs-number">0</span></code></pre><p><strong>isdigit</strong></p><p>查看<code>isdigit</code>的使用方法</p><pre><code class="hljs python">In [<span class="hljs-number">115</span>]: help(s.isdigit)</code></pre><blockquote><p>isdigit(…) method of builtins.str instance<br>​    S.isdigit() -&gt; bool<br>​    Return True if all characters in S are digits and there is at least one character in S, False otherwise.</p><p>使用方法：s.isdigit ()</p><p>注意 ：测试是否全是数字，都是数字则返回 True 否则返回 False.</p></blockquote><p><strong>演示</strong>：</p><pre><code class="hljs python">In [<span class="hljs-number">151</span>]: s = <span class="hljs-string">'123456'</span>                                                 In [<span class="hljs-number">152</span>]: s1 = <span class="hljs-string">'123abc'</span>                                                In [<span class="hljs-number">153</span>]: s2 = <span class="hljs-string">'abcdef'</span>                                                In [<span class="hljs-number">154</span>]: s.isdigit()                                                  Out[<span class="hljs-number">154</span>]: <span class="hljs-literal">True</span>In [<span class="hljs-number">155</span>]: s1.isdigit()                                                 Out[<span class="hljs-number">155</span>]: <span class="hljs-literal">False</span>In [<span class="hljs-number">156</span>]: s2.isdigit()                                                 Out[<span class="hljs-number">156</span>]: <span class="hljs-literal">False</span></code></pre><p>*<em>isalpha *</em></p><p>查看<code>isalpha</code>的使用方法</p><pre><code class="hljs python">In [<span class="hljs-number">115</span>]: help(s.isalpha )</code></pre><blockquote><p>isalpha(…) method of builtins.str instance<br>​    S.isalpha() -&gt; bool<br>​    Return True if all characters in S are alphabetic and there is at least one character in S, False otherwise.</p><p>使用方法：s.isalpha () </p><p>注意 ：测试是否全是字母，都是字母则返回 True,否则返回 False.</p></blockquote><p><strong>演示：</strong></p><pre><code class="hljs python">In [<span class="hljs-number">157</span>]: s = <span class="hljs-string">'123456'</span>                                                 In [<span class="hljs-number">158</span>]: s1 = <span class="hljs-string">'123abc'</span>                                                In [<span class="hljs-number">159</span>]: s2 = <span class="hljs-string">'abcdef'</span>                                                In [<span class="hljs-number">160</span>]: s.isalpha()                                                  Out[<span class="hljs-number">160</span>]: <span class="hljs-literal">False</span>In [<span class="hljs-number">161</span>]: s1.isalpha()                                                 Out[<span class="hljs-number">161</span>]: <span class="hljs-literal">False</span>In [<span class="hljs-number">162</span>]: s2.isalpha()                                                 Out[<span class="hljs-number">162</span>]: <span class="hljs-literal">True</span></code></pre><p><strong>endswith</strong></p><p>查看<code>endswith</code>的使用方法</p><pre><code class="hljs python">In [<span class="hljs-number">115</span>]: help(s.endswith )</code></pre><blockquote><p>endswith(…) method of builtins.str instance<br>​    S.endswith(suffix[, start[, end]]) -&gt; bool<br>​    Return True if S ends with the specified suffix, False otherwise. With optional start, test S beginning at that position. With optional end, stop comparing S at that position. suffix can also be a tuple of strings to try.</p><p>使用方法：s.endswith(x)</p><p>注意：如果字符串s以x结尾，返回True</p></blockquote><p><strong>演示：</strong></p><pre><code class="hljs python">In [<span class="hljs-number">163</span>]: s = <span class="hljs-string">'123456abc'</span>                                              In [<span class="hljs-number">164</span>]: s1 = <span class="hljs-string">'123abced'</span>                                              In [<span class="hljs-number">165</span>]: s2 = <span class="hljs-string">'abcdef'</span>                                                In [<span class="hljs-number">166</span>]: s.endswith(<span class="hljs-string">'c'</span>)                                              Out[<span class="hljs-number">166</span>]: <span class="hljs-literal">True</span>In [<span class="hljs-number">167</span>]: s.endswith(<span class="hljs-string">'2'</span>)                                              Out[<span class="hljs-number">167</span>]: <span class="hljs-literal">False</span>In [<span class="hljs-number">168</span>]: s1.endswith(<span class="hljs-string">'d'</span>)                                             Out[<span class="hljs-number">168</span>]: <span class="hljs-literal">True</span>In [<span class="hljs-number">169</span>]: s2.endswith(<span class="hljs-string">'f'</span>)                                             Out[<span class="hljs-number">169</span>]: <span class="hljs-literal">True</span></code></pre><blockquote><p>备注：方法s.startwith与s.endwith相反，前者是以什么开始，后者是以什么结尾。</p></blockquote><p><strong>islower</strong></p><p>查看<code>islower</code>的使用方法</p><pre><code class="hljs python">In [<span class="hljs-number">115</span>]: help(s.islower )</code></pre><blockquote><p>islower(…) method of builtins.str instance<br>​    S.islower() -&gt; bool<br>​    Return True if all cased characters in S are lowercase and there is at least one cased character in S, False otherwise.</p><p>使用方法：s.islower () </p><p>注意：测试是否全是小写</p></blockquote><p><strong>演示：</strong></p><pre><code class="hljs python">In [<span class="hljs-number">173</span>]: s = <span class="hljs-string">'123456abc'</span>                                              In [<span class="hljs-number">174</span>]: s1 = <span class="hljs-string">'abcDE'</span>                                                 In [<span class="hljs-number">175</span>]: s2 = <span class="hljs-string">'DE'</span>                                                    In [<span class="hljs-number">176</span>]: s.islower()                                                  Out[<span class="hljs-number">176</span>]: <span class="hljs-literal">True</span>In [<span class="hljs-number">177</span>]: s1.islower()                                                 Out[<span class="hljs-number">177</span>]: <span class="hljs-literal">False</span>In [<span class="hljs-number">178</span>]: s2.islower()                                                 Out[<span class="hljs-number">178</span>]: <span class="hljs-literal">False</span></code></pre><p><strong>isupper</strong></p><p>查看<code>isupper</code>的使用方法</p><pre><code class="hljs python">In [<span class="hljs-number">115</span>]: help(s.isupper )</code></pre><blockquote><p>isupper(…) method of builtins.str instance<br>​    S.isupper() -&gt; bool<br>​    Return True if all cased characters in S are uppercase and there is at least one cased character in S, False otherwise.</p><p>使用方式：s.isupper () </p><p>注意：测试是否全是大写</p></blockquote><p><strong>演示：</strong></p><pre><code class="hljs python">In [<span class="hljs-number">181</span>]: s = <span class="hljs-string">'123456abc'</span>                                              In [<span class="hljs-number">182</span>]: s1 = <span class="hljs-string">'abcDE'</span>                                                 In [<span class="hljs-number">183</span>]: s2 = <span class="hljs-string">'DE'</span>                                                    In [<span class="hljs-number">184</span>]: s3 = <span class="hljs-string">'123ADFAFA'</span>                                             In [<span class="hljs-number">185</span>]: s.isupper()                                                  Out[<span class="hljs-number">185</span>]: <span class="hljs-literal">False</span>In [<span class="hljs-number">186</span>]: s1.isupper()                                                 Out[<span class="hljs-number">186</span>]: <span class="hljs-literal">False</span>In [<span class="hljs-number">187</span>]: s2.isupper()                                                 Out[<span class="hljs-number">187</span>]: <span class="hljs-literal">True</span>In [<span class="hljs-number">188</span>]: s3.isupper()                                                 Out[<span class="hljs-number">188</span>]: <span class="hljs-literal">True</span></code></pre><h4 id="元组常用方法"><a href="#元组常用方法" class="headerlink" title="元组常用方法"></a>元组常用方法</h4><blockquote><p>元组为不可变序列，所以只有两种方法。</p></blockquote><h5 id="查找元素-2"><a href="#查找元素-2" class="headerlink" title="查找元素"></a><code>查找元素</code></h5><p><strong>count</strong></p><p>查看<code>count</code>的使用方法</p><pre><code class="hljs python">In [<span class="hljs-number">115</span>]: help(s.count )</code></pre><blockquote><p>count(…) method of builtins.tuple instance<br>​    T.count(value) -&gt; integer – return number of occurrences of value</p><p>使用方法：s.count(value)</p><p>注意：统计元素的个数</p></blockquote><p><strong>演示：</strong></p><pre><code class="hljs python">In [<span class="hljs-number">243</span>]: s = (<span class="hljs-number">1</span>,<span class="hljs-number">2</span>,<span class="hljs-number">3</span>,<span class="hljs-number">4</span>,<span class="hljs-number">3</span>,<span class="hljs-number">2</span>,<span class="hljs-number">1</span>)                                          In [<span class="hljs-number">244</span>]: s.count(<span class="hljs-number">1</span>)                                                   Out[<span class="hljs-number">244</span>]: <span class="hljs-number">2</span>In [<span class="hljs-number">245</span>]: s.count(<span class="hljs-number">4</span>)                                                   Out[<span class="hljs-number">245</span>]: <span class="hljs-number">1</span></code></pre><p><strong>index</strong></p><p>查看<code>index</code>的使用方法</p><pre><code class="hljs python">In [<span class="hljs-number">115</span>]: help(s.index )</code></pre><blockquote><p>index(…) method of builtins.tuple instance<br>​    T.index(value, [start, [stop]]) -&gt; integer – return first index of value.<br>​    Raises ValueError if the value is not present.</p><p>使用方式：  T.index(value, [start, [stop]])</p><p>注意：查看元素的索引</p></blockquote><pre><code class="hljs python">In [<span class="hljs-number">246</span>]: s = (<span class="hljs-number">1</span>,<span class="hljs-number">2</span>,<span class="hljs-number">3</span>,<span class="hljs-number">4</span>,<span class="hljs-number">3</span>,<span class="hljs-number">2</span>,<span class="hljs-number">1</span>)                                          In [<span class="hljs-number">247</span>]: s.index(<span class="hljs-number">1</span>)                                                   Out[<span class="hljs-number">247</span>]: <span class="hljs-number">0</span>In [<span class="hljs-number">248</span>]: s.index(<span class="hljs-number">1</span>,<span class="hljs-number">1</span>)                                                 Out[<span class="hljs-number">248</span>]: <span class="hljs-number">6</span>In [<span class="hljs-number">249</span>]: s.index(<span class="hljs-number">4</span>)                                                   Out[<span class="hljs-number">249</span>]: <span class="hljs-number">3</span></code></pre>]]></content>
    
    
    <categories>
      
      <category>python</category>
      
    </categories>
    
    
    <tags>
      
      <tag>序列类型</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>python第一话之数值类型和序列类型</title>
    <link href="/2018/12/python-numerical-sequence/"/>
    <url>/2018/12/python-numerical-sequence/</url>
    
    <content type="html"><![CDATA[<p>[TOC]</p><h3 id="Python的数值类型"><a href="#Python的数值类型" class="headerlink" title="Python的数值类型"></a>Python的数值类型</h3><p>Python中的基本数据类型有数值类型、字符串型、列表、元组、字典、集合等。本节介绍数值类型。数值类型包括整型、布尔型、浮点型和复数类型。</p><h4 id="基本整形四则运算"><a href="#基本整形四则运算" class="headerlink" title="基本整形四则运算"></a>基本整形四则运算</h4><p>用Python实现简单的加减乘除</p><p>我们先进入<code>Python的交互模式</code></p><p><img src="http://eveseven.oss-cn-shanghai.aliyuncs.com/18-12-1/36226595.jpg" srcset="/img/loading.gif" alt="python"></p><p>或者执行<code>ipython</code></p><p><img src="http://eveseven.oss-cn-shanghai.aliyuncs.com/18-12-1/65669846.jpg" srcset="/img/loading.gif" alt="ipython"></p><h5 id="加法"><a href="#加法" class="headerlink" title="加法"></a><code>加法</code></h5><pre><code class="hljs python">In [<span class="hljs-number">1</span>]: <span class="hljs-number">1</span>+<span class="hljs-number">1</span>Out[<span class="hljs-number">1</span>]: <span class="hljs-number">2</span>In [<span class="hljs-number">2</span>]: <span class="hljs-number">12</span>+<span class="hljs-number">2</span>Out[<span class="hljs-number">2</span>]: <span class="hljs-number">14</span></code></pre><h5 id="减法"><a href="#减法" class="headerlink" title="减法"></a><code>减法</code></h5><pre><code class="hljs python">In [<span class="hljs-number">3</span>]: <span class="hljs-number">21</span><span class="hljs-number">-1</span>Out[<span class="hljs-number">3</span>]: <span class="hljs-number">20</span>In [<span class="hljs-number">4</span>]: <span class="hljs-number">20</span><span class="hljs-number">-22</span>Out[<span class="hljs-number">4</span>]: <span class="hljs-number">-2</span></code></pre><h5 id="乘法"><a href="#乘法" class="headerlink" title="乘法"></a><code>乘法</code></h5><pre><code class="hljs python">In [<span class="hljs-number">5</span>]: <span class="hljs-number">2</span>*<span class="hljs-number">2</span>Out[<span class="hljs-number">5</span>]: <span class="hljs-number">4</span>In [<span class="hljs-number">6</span>]: <span class="hljs-number">10</span>*<span class="hljs-number">10</span>Out[<span class="hljs-number">6</span>]: <span class="hljs-number">100</span></code></pre><h5 id="除法"><a href="#除法" class="headerlink" title="除法"></a><code>除法</code></h5><pre><code class="hljs python">In [<span class="hljs-number">9</span>]: <span class="hljs-number">4</span>/<span class="hljs-number">2</span>Out[<span class="hljs-number">9</span>]: <span class="hljs-number">2.0</span>In [<span class="hljs-number">10</span>]: <span class="hljs-number">2</span>/<span class="hljs-number">2</span>Out[<span class="hljs-number">10</span>]: <span class="hljs-number">1.0</span></code></pre><blockquote><p>以上就是我们所遇到的一些基本的加减乘除运算，接下来我们再看看其他形式的扩展。</p></blockquote><h4 id="保存计算结果-基本的赋值运算"><a href="#保存计算结果-基本的赋值运算" class="headerlink" title="保存计算结果-基本的赋值运算"></a>保存计算结果-基本的赋值运算</h4><p>在我们后面的学习中，经常会遇到需要保存计算结果的情况，下面我们来看一下如何保存我们的计算结果。</p><pre><code class="hljs python">In [<span class="hljs-number">11</span>]: a = <span class="hljs-number">1</span>+<span class="hljs-number">1</span>In [<span class="hljs-number">12</span>]: <span class="hljs-number">1</span>+<span class="hljs-number">1</span>Out[<span class="hljs-number">12</span>]: <span class="hljs-number">2</span>In [<span class="hljs-number">13</span>]: aOut[<span class="hljs-number">13</span>]: <span class="hljs-number">2</span></code></pre><h4 id="基本整数与小数四则运算"><a href="#基本整数与小数四则运算" class="headerlink" title="基本整数与小数四则运算"></a>基本整数与小数四则运算</h4><p>计算机在计算的时候，除了整数运算，还有小数运算，还有小数和整数的混合运算。</p><h5 id="加法-1"><a href="#加法-1" class="headerlink" title="加法"></a><code>加法</code></h5><pre><code class="hljs python">In [<span class="hljs-number">14</span>]: <span class="hljs-number">2</span>+<span class="hljs-number">1.2</span>Out[<span class="hljs-number">14</span>]: <span class="hljs-number">3.2</span></code></pre><h5 id="减法-1"><a href="#减法-1" class="headerlink" title="减法"></a><code>减法</code></h5><pre><code class="hljs python">In [<span class="hljs-number">16</span>]: <span class="hljs-number">2.2</span><span class="hljs-number">-2</span>Out[<span class="hljs-number">16</span>]: <span class="hljs-number">0.20000000000000018</span></code></pre><h5 id="乘法-1"><a href="#乘法-1" class="headerlink" title="乘法"></a><code>乘法</code></h5><pre><code class="hljs python">In [<span class="hljs-number">17</span>]: <span class="hljs-number">3.3</span>*<span class="hljs-number">2</span>Out[<span class="hljs-number">17</span>]: <span class="hljs-number">6.6</span></code></pre><h5 id="除法-1"><a href="#除法-1" class="headerlink" title="除法"></a><code>除法</code></h5><pre><code class="hljs python">In [<span class="hljs-number">18</span>]: <span class="hljs-number">2.4</span>/<span class="hljs-number">4</span>Out[<span class="hljs-number">18</span>]: <span class="hljs-number">0.6</span></code></pre><blockquote><p>小数一般是<code>float</code>类型</p><pre><code class="hljs python">In [<span class="hljs-number">19</span>]: b = <span class="hljs-number">3</span>+<span class="hljs-number">2.2</span>In [<span class="hljs-number">20</span>]: type(b)Out[<span class="hljs-number">20</span>]: float</code></pre><p>但是<code>浮点数</code>不是我们真正看到的数，比如<code>1.2</code>实际是<code>1.1999999999</code></p><p>所以小数计算都是不精确的，那么我们如何进行精确计算呢？</p></blockquote><h4 id="精确计算-decimal的运算"><a href="#精确计算-decimal的运算" class="headerlink" title="精确计算-decimal的运算"></a>精确计算-decimal的运算</h4><p>在<code>Python</code>中如果我们要实现精确计算，我们是使用<code>decimal</code>这个库函数</p><pre><code class="hljs python">In [<span class="hljs-number">27</span>]: <span class="hljs-keyword">import</span> decimal In [<span class="hljs-number">28</span>]: a = decimal.Decimal(<span class="hljs-string">'2.2'</span>)In [<span class="hljs-number">29</span>]: b = decimal.Decimal(<span class="hljs-string">'2'</span>)In [<span class="hljs-number">30</span>]: a-bOut[<span class="hljs-number">30</span>]: Decimal(<span class="hljs-string">'0.2'</span>)</code></pre><h4 id="布尔型的计算"><a href="#布尔型的计算" class="headerlink" title="布尔型的计算"></a>布尔型的计算</h4><p><code>布尔型变量</code>只有<code>True</code>和<code>False</code>两种情况，<code>True</code>就是<code>1</code>，<code>False</code>就是<code>0</code>。</p><h5 id="基本情况"><a href="#基本情况" class="headerlink" title="基本情况"></a><code>基本情况</code></h5><pre><code class="hljs python">In [<span class="hljs-number">34</span>]: a = <span class="hljs-literal">True</span>In [<span class="hljs-number">35</span>]: b = <span class="hljs-literal">False</span>In [<span class="hljs-number">36</span>]: aOut[<span class="hljs-number">36</span>]: <span class="hljs-literal">True</span>In [<span class="hljs-number">37</span>]: bOut[<span class="hljs-number">37</span>]: <span class="hljs-literal">False</span>In [<span class="hljs-number">38</span>]: type(a)Out[<span class="hljs-number">38</span>]: bool</code></pre><h5 id="运算"><a href="#运算" class="headerlink" title="运算"></a><code>运算</code></h5><pre><code class="hljs python">In [<span class="hljs-number">39</span>]: a + <span class="hljs-number">1</span>Out[<span class="hljs-number">39</span>]: <span class="hljs-number">2</span>In [<span class="hljs-number">40</span>]: a + a Out[<span class="hljs-number">40</span>]: <span class="hljs-number">2</span>In [<span class="hljs-number">41</span>]: a + bOut[<span class="hljs-number">41</span>]: <span class="hljs-number">1</span></code></pre><h4 id="复数类型"><a href="#复数类型" class="headerlink" title="复数类型"></a>复数类型</h4><pre><code class="hljs python">In [<span class="hljs-number">42</span>]: <span class="hljs-number">1</span> + <span class="hljs-number">2j</span>Out[<span class="hljs-number">42</span>]: (<span class="hljs-number">1</span>+<span class="hljs-number">2j</span>)In [<span class="hljs-number">43</span>]: a = <span class="hljs-number">1</span> + <span class="hljs-number">2j</span>In [<span class="hljs-number">44</span>]: type(a)Out[<span class="hljs-number">44</span>]: complex</code></pre><h4 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h4><p>整型、布尔型、浮点型和复数类型</p><pre><code class="hljs python">In [<span class="hljs-number">45</span>]: type(<span class="hljs-number">2</span>)Out[<span class="hljs-number">45</span>]: intIn [<span class="hljs-number">46</span>]: type(<span class="hljs-literal">True</span>)Out[<span class="hljs-number">46</span>]: boolIn [<span class="hljs-number">47</span>]: type(<span class="hljs-number">1.2</span>)Out[<span class="hljs-number">47</span>]: floatIn [<span class="hljs-number">48</span>]: type(<span class="hljs-number">1</span>+<span class="hljs-number">2j</span>)Out[<span class="hljs-number">48</span>]: complex</code></pre><h4 id="扩展"><a href="#扩展" class="headerlink" title="扩展"></a>扩展</h4><h5 id="整除-向下取整"><a href="#整除-向下取整" class="headerlink" title="整除-向下取整"></a><code>整除-向下取整</code></h5><p><code>//</code>在<code>Python</code>中是指的<code>向下取整</code>：1 &lt; 1.n &lt; 2  ==&gt; 1</p><pre><code class="hljs python">In [<span class="hljs-number">49</span>]: <span class="hljs-number">2.3</span> / <span class="hljs-number">2</span>Out[<span class="hljs-number">49</span>]: <span class="hljs-number">1.15</span>In [<span class="hljs-number">50</span>]: <span class="hljs-number">2.3</span> // <span class="hljs-number">2</span>Out[<span class="hljs-number">50</span>]: <span class="hljs-number">1.0</span></code></pre><h5 id="整除-向上取整"><a href="#整除-向上取整" class="headerlink" title="整除-向上取整"></a><code>整除-向上取整</code></h5><p><code>向上取整</code>：使用<code>math</code>库函数实现</p><pre><code class="hljs python">In [<span class="hljs-number">52</span>]: <span class="hljs-keyword">import</span> mathIn [<span class="hljs-number">53</span>]: math.ceil(<span class="hljs-number">2.3</span>/<span class="hljs-number">2</span>)Out[<span class="hljs-number">53</span>]: <span class="hljs-number">2</span></code></pre><h5 id="幂运算"><a href="#幂运算" class="headerlink" title="幂运算"></a><code>幂运算</code></h5><pre><code class="hljs python">In [<span class="hljs-number">54</span>]: <span class="hljs-number">2</span> * <span class="hljs-number">2</span> *<span class="hljs-number">2</span>Out[<span class="hljs-number">54</span>]: <span class="hljs-number">8</span>In [<span class="hljs-number">55</span>]: <span class="hljs-number">2</span> **<span class="hljs-number">3</span>Out[<span class="hljs-number">55</span>]: <span class="hljs-number">8</span></code></pre><h5 id="取余"><a href="#取余" class="headerlink" title="取余"></a><code>取余</code></h5><pre><code class="hljs python">In [<span class="hljs-number">56</span>]: <span class="hljs-number">6</span> % <span class="hljs-number">4</span>Out[<span class="hljs-number">56</span>]: <span class="hljs-number">2</span></code></pre><p>这就是我们常见的数值类型的相关使用和运算。</p><h3 id="Python的序列类型"><a href="#Python的序列类型" class="headerlink" title="Python的序列类型"></a>Python的序列类型</h3><p>前面我们遇到的都是一些数值类型的使用与运算，但是如果我想在Python中表示字母怎么办呢？这就是我们接下来看一下序列类型。</p><h4 id="字符串-str"><a href="#字符串-str" class="headerlink" title="字符串-str"></a>字符串-<code>str</code></h4><pre><code class="hljs python">In [<span class="hljs-number">57</span>]: <span class="hljs-string">'abc'</span>Out[<span class="hljs-number">57</span>]: <span class="hljs-string">'abc'</span>In [<span class="hljs-number">58</span>]: type(<span class="hljs-string">'abc'</span>)Out[<span class="hljs-number">58</span>]: strIn [<span class="hljs-number">59</span>]: <span class="hljs-string">"I'm seven"</span>Out[<span class="hljs-number">59</span>]: <span class="hljs-string">"I'm seven"</span>In [<span class="hljs-number">60</span>]: <span class="hljs-string">"""abcde</span><span class="hljs-string">    ...: fghijk"""</span>Out[<span class="hljs-number">60</span>]: <span class="hljs-string">'abcde\nfghijk'</span></code></pre><h5 id="字符串的使用"><a href="#字符串的使用" class="headerlink" title="字符串的使用"></a><code>字符串的使用</code></h5><blockquote><p>字符串是通过下标索引值来获取对应值的。</p></blockquote><pre><code class="hljs python">In [<span class="hljs-number">74</span>]: a = <span class="hljs-string">'abc'</span>In [<span class="hljs-number">75</span>]: a[<span class="hljs-number">0</span>]Out[<span class="hljs-number">75</span>]: <span class="hljs-string">'a'</span>In [<span class="hljs-number">76</span>]: a[<span class="hljs-number">2</span>]Out[<span class="hljs-number">76</span>]: <span class="hljs-string">'c'</span>In [<span class="hljs-number">77</span>]: a[<span class="hljs-number">1</span>]Out[<span class="hljs-number">77</span>]: <span class="hljs-string">'b'</span></code></pre><h4 id="列表-list"><a href="#列表-list" class="headerlink" title="列表-list"></a>列表-<code>list</code></h4><blockquote><p>数值和字符串的混合使用</p></blockquote><pre><code class="hljs python">In [<span class="hljs-number">61</span>]: [<span class="hljs-string">'abc'</span>, <span class="hljs-number">123</span>, <span class="hljs-string">'dde'</span>]Out[<span class="hljs-number">61</span>]: [<span class="hljs-string">'abc'</span>, <span class="hljs-number">123</span>, <span class="hljs-string">'dde'</span>]In [<span class="hljs-number">62</span>]: type([<span class="hljs-string">'abc'</span>, <span class="hljs-number">123</span>, <span class="hljs-string">'dde'</span>])Out[<span class="hljs-number">62</span>]: list</code></pre><h5 id="列表的使用-简单取值"><a href="#列表的使用-简单取值" class="headerlink" title="列表的使用-简单取值"></a><code>列表的使用-简单取值</code></h5><blockquote><p>根据下标取出对应数据，下标从0开始计数：0,1,2,3…</p></blockquote><pre><code class="hljs python">In [<span class="hljs-number">65</span>]: a = [<span class="hljs-string">'abc'</span>, <span class="hljs-number">123</span>, <span class="hljs-string">'dde'</span>]In [<span class="hljs-number">66</span>]: a[<span class="hljs-number">0</span>]Out[<span class="hljs-number">66</span>]: <span class="hljs-string">'abc'</span>In [<span class="hljs-number">67</span>]: a[<span class="hljs-number">1</span>]Out[<span class="hljs-number">67</span>]: <span class="hljs-number">123</span>In [<span class="hljs-number">68</span>]: a[<span class="hljs-number">2</span>]Out[<span class="hljs-number">68</span>]: <span class="hljs-string">'dde'</span></code></pre><h5 id="列表的使用-切片"><a href="#列表的使用-切片" class="headerlink" title="列表的使用-切片"></a><code>列表的使用-切片</code></h5><blockquote><p>通过<code>list[start_index : end_index : stride ]</code>来进行<code>切片</code>，<code>切片</code>方式类似数学中的<code>左闭右开区间</code></p><p><code>start_index:</code> 开始的索引值</p><p><code>end_index:</code>  结束的索引值</p><p><code>stride:</code> 步长</p></blockquote><pre><code class="hljs python">In [<span class="hljs-number">78</span>]: a = [<span class="hljs-string">'abc'</span>, <span class="hljs-number">123</span>, <span class="hljs-string">'dde'</span>]In [<span class="hljs-number">79</span>]: a[:<span class="hljs-number">1</span>]Out[<span class="hljs-number">79</span>]: [<span class="hljs-string">'abc'</span>]In [<span class="hljs-number">80</span>]: a[:<span class="hljs-number">2</span>]Out[<span class="hljs-number">80</span>]: [<span class="hljs-string">'abc'</span>, <span class="hljs-number">123</span>]In [<span class="hljs-number">81</span>]: a[<span class="hljs-number">1</span>:]Out[<span class="hljs-number">81</span>]: [<span class="hljs-number">123</span>, <span class="hljs-string">'dde'</span>]In [<span class="hljs-number">82</span>]: a[<span class="hljs-number">1</span>:<span class="hljs-number">3</span>]Out[<span class="hljs-number">82</span>]: [<span class="hljs-number">123</span>, <span class="hljs-string">'dde'</span>]    In [<span class="hljs-number">84</span>]: a = [<span class="hljs-number">1</span>,<span class="hljs-number">2</span>,<span class="hljs-number">3</span>,<span class="hljs-number">4</span>,<span class="hljs-number">5</span>,<span class="hljs-number">6</span>,<span class="hljs-string">'a'</span>,<span class="hljs-string">'b'</span>,<span class="hljs-string">'c'</span>]In [<span class="hljs-number">85</span>]: a[<span class="hljs-number">1</span>:<span class="hljs-number">6</span>:<span class="hljs-number">2</span>]Out[<span class="hljs-number">85</span>]: [<span class="hljs-number">2</span>, <span class="hljs-number">4</span>, <span class="hljs-number">6</span>]</code></pre><p>其中：<code>[-1]:</code> 表示倒着计数</p><pre><code class="hljs python">In [<span class="hljs-number">92</span>]: a = [<span class="hljs-number">1</span>,<span class="hljs-number">2</span>,<span class="hljs-number">3</span>,<span class="hljs-number">4</span>,<span class="hljs-number">5</span>,<span class="hljs-number">6</span>,<span class="hljs-string">'a'</span>,<span class="hljs-string">'b'</span>,<span class="hljs-string">'c'</span>]In [<span class="hljs-number">93</span>]: a[<span class="hljs-number">1</span>:]Out[<span class="hljs-number">93</span>]: [<span class="hljs-number">2</span>, <span class="hljs-number">3</span>, <span class="hljs-number">4</span>, <span class="hljs-number">5</span>, <span class="hljs-number">6</span>, <span class="hljs-string">'a'</span>, <span class="hljs-string">'b'</span>, <span class="hljs-string">'c'</span>]In [<span class="hljs-number">94</span>]: a[<span class="hljs-number">-1</span>:]Out[<span class="hljs-number">94</span>]: [<span class="hljs-string">'c'</span>]In [<span class="hljs-number">95</span>]: a[<span class="hljs-number">2</span>:<span class="hljs-number">-1</span>]Out[<span class="hljs-number">95</span>]: [<span class="hljs-number">3</span>, <span class="hljs-number">4</span>, <span class="hljs-number">5</span>, <span class="hljs-number">6</span>, <span class="hljs-string">'a'</span>, <span class="hljs-string">'b'</span>]In [<span class="hljs-number">96</span>]: a[<span class="hljs-number">2</span>:<span class="hljs-number">8</span>]Out[<span class="hljs-number">96</span>]: [<span class="hljs-number">3</span>, <span class="hljs-number">4</span>, <span class="hljs-number">5</span>, <span class="hljs-number">6</span>, <span class="hljs-string">'a'</span>, <span class="hljs-string">'b'</span>]</code></pre><h4 id="元组-tuple"><a href="#元组-tuple" class="headerlink" title="元组-tuple"></a>元组-<code>tuple</code></h4><pre><code class="hljs python">In [<span class="hljs-number">63</span>]: (<span class="hljs-number">123</span>, <span class="hljs-string">'abc'</span>, <span class="hljs-string">'seven'</span>)Out[<span class="hljs-number">63</span>]: (<span class="hljs-number">123</span>, <span class="hljs-string">'abc'</span>, <span class="hljs-string">'seven'</span>)In [<span class="hljs-number">64</span>]: type((<span class="hljs-number">123</span>, <span class="hljs-string">'abc'</span>, <span class="hljs-string">'seven'</span>))Out[<span class="hljs-number">64</span>]: tuple</code></pre><h5 id="元组的使用"><a href="#元组的使用" class="headerlink" title="元组的使用"></a><code>元组的使用</code></h5><blockquote><p>元组的使用是和列表的类似的。</p></blockquote><pre><code class="hljs python">In [<span class="hljs-number">70</span>]: b = (<span class="hljs-number">123</span>, <span class="hljs-string">'abc'</span>, <span class="hljs-string">'seven'</span>)In [<span class="hljs-number">71</span>]: b[<span class="hljs-number">1</span>]Out[<span class="hljs-number">71</span>]: <span class="hljs-string">'abc'</span>In [<span class="hljs-number">72</span>]: b[<span class="hljs-number">0</span>]Out[<span class="hljs-number">72</span>]: <span class="hljs-number">123</span>In [<span class="hljs-number">73</span>]: b[<span class="hljs-number">2</span>]Out[<span class="hljs-number">73</span>]: <span class="hljs-string">'seven'</span></code></pre><h4 id="类型转换"><a href="#类型转换" class="headerlink" title="类型转换"></a>类型转换</h4><blockquote><p>我们经常会使用到这几种类型， 所以这几种类型间的转换又尤为关键</p></blockquote><h5 id="字符串转列表"><a href="#字符串转列表" class="headerlink" title="字符串转列表"></a><code>字符串转列表</code></h5><pre><code class="hljs python">In [<span class="hljs-number">111</span>]: a = <span class="hljs-string">'abcd'</span>In [<span class="hljs-number">112</span>]: b = list(a)In [<span class="hljs-number">113</span>]: type(a)Out[<span class="hljs-number">113</span>]: strIn [<span class="hljs-number">114</span>]: type(b)Out[<span class="hljs-number">114</span>]: listIn [<span class="hljs-number">115</span>]: bOut[<span class="hljs-number">115</span>]: [<span class="hljs-string">'a'</span>, <span class="hljs-string">'b'</span>, <span class="hljs-string">'c'</span>, <span class="hljs-string">'d'</span>]</code></pre><h5 id="列表转字符串"><a href="#列表转字符串" class="headerlink" title="列表转字符串"></a><code>列表转字符串</code></h5><blockquote><p>列表变成字符串会把列表里的中括号和空格也变成字符串</p></blockquote><pre><code class="hljs python">In [<span class="hljs-number">123</span>]: a = [<span class="hljs-string">'a'</span>, <span class="hljs-string">'b'</span>, <span class="hljs-string">'c'</span>, <span class="hljs-string">'d'</span>]In [<span class="hljs-number">124</span>]: b = str(a)In [<span class="hljs-number">125</span>]: type(a)Out[<span class="hljs-number">125</span>]: listIn [<span class="hljs-number">126</span>]: type(b)Out[<span class="hljs-number">126</span>]: strIn [<span class="hljs-number">127</span>]: bOut[<span class="hljs-number">127</span>]: <span class="hljs-string">"['a', 'b', 'c', 'd']"</span></code></pre><h5 id="字符串转元组"><a href="#字符串转元组" class="headerlink" title="字符串转元组"></a><code>字符串转元组</code></h5><pre><code class="hljs python">In [<span class="hljs-number">116</span>]: a = <span class="hljs-string">'abcd'</span>In [<span class="hljs-number">117</span>]: b = tuple(a)In [<span class="hljs-number">118</span>]: type(a)Out[<span class="hljs-number">118</span>]: strIn [<span class="hljs-number">119</span>]: type(b)Out[<span class="hljs-number">119</span>]: tupleIn [<span class="hljs-number">120</span>]: bOut[<span class="hljs-number">120</span>]: (<span class="hljs-string">'a'</span>, <span class="hljs-string">'b'</span>, <span class="hljs-string">'c'</span>, <span class="hljs-string">'d'</span>)</code></pre><h5 id="元组转字符串"><a href="#元组转字符串" class="headerlink" title="元组转字符串"></a><code>元组转字符串</code></h5><blockquote><p>列表变成字符串会把列表里的小括号和空格也变成字符串</p></blockquote><pre><code class="hljs python">In [<span class="hljs-number">128</span>]: a = (<span class="hljs-string">'a'</span>, <span class="hljs-string">'b'</span>, <span class="hljs-string">'c'</span>, <span class="hljs-string">'d'</span>)In [<span class="hljs-number">129</span>]: b = str(a)In [<span class="hljs-number">130</span>]: type(a)Out[<span class="hljs-number">130</span>]: tupleIn [<span class="hljs-number">131</span>]: type(b)Out[<span class="hljs-number">131</span>]: strIn [<span class="hljs-number">132</span>]: bOut[<span class="hljs-number">132</span>]: <span class="hljs-string">"('a', 'b', 'c', 'd')"</span></code></pre><h4 id="元组和列表的区别"><a href="#元组和列表的区别" class="headerlink" title="元组和列表的区别"></a>元组和列表的区别</h4><blockquote><p>在我们前面的接触过程中，列表和元组基本的功能是一样的，那列表和元组都存在，是为什么呢？</p></blockquote><pre><code class="hljs python">In [<span class="hljs-number">133</span>]: a = (<span class="hljs-string">'a'</span>, <span class="hljs-string">'b'</span>, <span class="hljs-string">'c'</span>, <span class="hljs-string">'d'</span>)In [<span class="hljs-number">134</span>]: b = [<span class="hljs-string">'a'</span>, <span class="hljs-string">'b'</span>, <span class="hljs-string">'c'</span>, <span class="hljs-string">'d'</span>]In [<span class="hljs-number">135</span>]: a[<span class="hljs-number">1</span>]Out[<span class="hljs-number">135</span>]: <span class="hljs-string">'b'</span>In [<span class="hljs-number">136</span>]: b[<span class="hljs-number">1</span>]Out[<span class="hljs-number">136</span>]: <span class="hljs-string">'b'</span>In [<span class="hljs-number">137</span>]: b[<span class="hljs-number">1</span>] = <span class="hljs-number">3</span>In [<span class="hljs-number">138</span>]: bOut[<span class="hljs-number">138</span>]: [<span class="hljs-string">'a'</span>, <span class="hljs-number">3</span>, <span class="hljs-string">'c'</span>, <span class="hljs-string">'d'</span>]In [<span class="hljs-number">139</span>]: a[<span class="hljs-number">1</span>] = <span class="hljs-number">3</span>---------------------------------------------------------------------------TypeError                                 Traceback (most recent call last)&lt;ipython-input<span class="hljs-number">-139</span><span class="hljs-number">-23</span>f2cf2bdf70&gt; <span class="hljs-keyword">in</span> &lt;module&gt;()----&gt; 1 a[1] = 3TypeError: <span class="hljs-string">'tuple'</span> object does <span class="hljs-keyword">not</span> support item assignment</code></pre><p><code>扩展：</code></p><pre><code class="hljs python">In [<span class="hljs-number">140</span>]: a = <span class="hljs-string">'abcdefg'</span>In [<span class="hljs-number">141</span>]: type(a)Out[<span class="hljs-number">141</span>]: strIn [<span class="hljs-number">142</span>]: a[<span class="hljs-number">1</span>]Out[<span class="hljs-number">142</span>]: <span class="hljs-string">'b'</span>In [<span class="hljs-number">143</span>]: a[<span class="hljs-number">1</span>] = <span class="hljs-string">'s'</span>---------------------------------------------------------------------------TypeError                                 Traceback (most recent call last)&lt;ipython-input<span class="hljs-number">-143</span><span class="hljs-number">-55e6</span>e4038777&gt; <span class="hljs-keyword">in</span> &lt;module&gt;()----&gt; 1 a[1] = 's'TypeError: <span class="hljs-string">'str'</span> object does <span class="hljs-keyword">not</span> support item assignment</code></pre><blockquote><p>总结：列表：可变</p><p>​        元组： 不可变</p><p>​             字符串： 不可变</p><p>所以，在序列类型中，只有列表才是可变的类型。</p></blockquote><h4 id="更改字符串和元组的元素"><a href="#更改字符串和元组的元素" class="headerlink" title="更改字符串和元组的元素"></a>更改字符串和元组的元素</h4><blockquote><p>前面我们讲了在序列类型中，只有列表才是可变的类型。那我们如何来更改不可变数据类型的元素呢？</p></blockquote><pre><code class="hljs python">In [<span class="hljs-number">151</span>]: a = <span class="hljs-string">'abcdefg'</span>In [<span class="hljs-number">152</span>]: a = a[<span class="hljs-number">1</span>:]In [<span class="hljs-number">153</span>]: aOut[<span class="hljs-number">153</span>]: <span class="hljs-string">'bcdefg'</span>In [<span class="hljs-number">154</span>]: a = (<span class="hljs-string">'a'</span>, <span class="hljs-string">'b'</span>, <span class="hljs-string">'c'</span>, <span class="hljs-string">'d'</span>)In [<span class="hljs-number">155</span>]: a = a[<span class="hljs-number">1</span>:<span class="hljs-number">3</span>]In [<span class="hljs-number">156</span>]: aOut[<span class="hljs-number">156</span>]: (<span class="hljs-string">'b'</span>, <span class="hljs-string">'c'</span>)</code></pre><h4 id="拆包"><a href="#拆包" class="headerlink" title="拆包"></a>拆包</h4><blockquote><p>元组拆包可以应用到任何迭代对象上， 唯一的要求是， 被可迭代对象中的元素数量必须要和这些元素的元组的空档数一致， 除非我们用* 来表示忽略多余的元素。</p></blockquote><pre><code class="hljs python">In [<span class="hljs-number">157</span>]: a = (<span class="hljs-string">'a'</span>, <span class="hljs-string">'b'</span>, <span class="hljs-string">'c'</span>, <span class="hljs-string">'d'</span>)In [<span class="hljs-number">158</span>]: x,*y,z = aIn [<span class="hljs-number">159</span>]: xOut[<span class="hljs-number">159</span>]: <span class="hljs-string">'a'</span>In [<span class="hljs-number">160</span>]: zOut[<span class="hljs-number">160</span>]: <span class="hljs-string">'d'</span>In [<span class="hljs-number">161</span>]: yOut[<span class="hljs-number">161</span>]: [<span class="hljs-string">'b'</span>, <span class="hljs-string">'c'</span>]In [<span class="hljs-number">162</span>]: x,y,z = a---------------------------------------------------------------------------ValueError                                Traceback (most recent call last)&lt;ipython-input<span class="hljs-number">-162</span><span class="hljs-number">-57</span>ae45ef0060&gt; <span class="hljs-keyword">in</span> &lt;module&gt;()----&gt; 1 x,y,z = aValueError: too many values to unpack (expected <span class="hljs-number">3</span>)</code></pre><blockquote><p><code>x</code> 接收第一个元素，<code>z</code> 接收最后一个元素，由于<code>y</code>前面有<code>*</code>号，所以剩余的元素由<code>y</code>接收</p><p>总结：有多少个元素就需要多少个变量来接收，除非有<code>*</code>号，不然就会报错。</p></blockquote><pre><code class="hljs python">In [<span class="hljs-number">163</span>]: x,y,z = a,a,aIn [<span class="hljs-number">164</span>]: xOut[<span class="hljs-number">164</span>]: (<span class="hljs-string">'a'</span>, <span class="hljs-string">'b'</span>, <span class="hljs-string">'c'</span>, <span class="hljs-string">'d'</span>)In [<span class="hljs-number">165</span>]: yOut[<span class="hljs-number">165</span>]: (<span class="hljs-string">'a'</span>, <span class="hljs-string">'b'</span>, <span class="hljs-string">'c'</span>, <span class="hljs-string">'d'</span>)In [<span class="hljs-number">166</span>]: zOut[<span class="hljs-number">166</span>]: (<span class="hljs-string">'a'</span>, <span class="hljs-string">'b'</span>, <span class="hljs-string">'c'</span>, <span class="hljs-string">'d'</span>)</code></pre><h4 id="变量的赋值"><a href="#变量的赋值" class="headerlink" title="变量的赋值"></a>变量的赋值</h4><blockquote><p>变量的保存都是保存在内存中</p><p>注意：<code>变量</code>是没有类型的，有类型的是他所<code>指向的数据</code></p></blockquote><pre><code class="hljs python">In [<span class="hljs-number">167</span>]: a = <span class="hljs-number">123</span>In [<span class="hljs-number">168</span>]: id(a)Out[<span class="hljs-number">168</span>]: <span class="hljs-number">10923232</span>In [<span class="hljs-number">169</span>]: b = <span class="hljs-string">'abc'</span>In [<span class="hljs-number">170</span>]: id(b)Out[<span class="hljs-number">170</span>]: <span class="hljs-number">140343125492152</span>In [<span class="hljs-number">171</span>]: a = <span class="hljs-string">'111'</span>In [<span class="hljs-number">172</span>]: id(a)Out[<span class="hljs-number">172</span>]: <span class="hljs-number">140342939172344</span></code></pre><blockquote><p><code>id()</code>：查看数据的地址</p><p>总结：赋值给变量是保存在内存中，重新赋值后，变量指向新的地址</p></blockquote><h4 id="变量的引用-成员运算"><a href="#变量的引用-成员运算" class="headerlink" title="变量的引用-成员运算"></a>变量的引用-成员运算</h4><blockquote><p>通过<code>in</code>或者<code>not in</code> 来进行成员运算</p></blockquote><pre><code class="hljs python">In [<span class="hljs-number">173</span>]: a = <span class="hljs-string">'abcd123'</span>In [<span class="hljs-number">174</span>]: <span class="hljs-string">'c'</span> <span class="hljs-keyword">in</span> aOut[<span class="hljs-number">174</span>]: <span class="hljs-literal">True</span>In [<span class="hljs-number">175</span>]: <span class="hljs-string">'8'</span> <span class="hljs-keyword">in</span> aOut[<span class="hljs-number">175</span>]: <span class="hljs-literal">False</span>In [<span class="hljs-number">176</span>]: <span class="hljs-string">'a'</span> <span class="hljs-keyword">not</span> <span class="hljs-keyword">in</span> aOut[<span class="hljs-number">176</span>]: <span class="hljs-literal">False</span>In [<span class="hljs-number">177</span>]: <span class="hljs-string">'q'</span> <span class="hljs-keyword">not</span> <span class="hljs-keyword">in</span> aOut[<span class="hljs-number">177</span>]: <span class="hljs-literal">True</span></code></pre>]]></content>
    
    
    <categories>
      
      <category>python</category>
      
    </categories>
    
    
    <tags>
      
      <tag>python</tag>
      
      <tag>数值类型</tag>
      
      <tag>序列类型</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Anaconda python3.6及pycharm安装和简单使用--Ubuntu16.04</title>
    <link href="/2018/11/anaconda-ubuntu/"/>
    <url>/2018/11/anaconda-ubuntu/</url>
    
    <content type="html"><![CDATA[<h3 id="Ubuntu-安装Anaconda基于python3-6"><a href="#Ubuntu-安装Anaconda基于python3-6" class="headerlink" title="Ubuntu 安装Anaconda基于python3.6"></a>Ubuntu 安装Anaconda基于python3.6</h3><ul><li><p>anaconda是python发行的包的管理工具，其中自带python的版本，还带很多python的包.安装它比安装python好。可以省掉再安装python包的时间。推荐使用Anaconda,用Anaconda安装python的包是非常便捷高效的，比如安装scrapy框架，如果用原生python的pip安装，要安装很多依赖的包，还经常报错，但是用Anaconda直接输入conda install scrapy就可以了，其他的都不用管了，就是这么方便。另外它自带了 180多个python常用的包，简直就是好得没朋友。</p></li><li><p>要利用 Python 进行科学计算，就需要一一安装所需的模块， 而这些模块可能又依赖于其它的软件包或库，安装和使用起来相对麻烦。Anaconda 就是将科学计算所需要的模块都编译好，然后打包以发行版的形式供用户使用的一个常用的科学计算环境。 </p></li></ul><h4 id="它包含了众多流行的科学、数学、工程、数据分析的-Python-包。"><a href="#它包含了众多流行的科学、数学、工程、数据分析的-Python-包。" class="headerlink" title="它包含了众多流行的科学、数学、工程、数据分析的 Python 包。"></a>它包含了众多流行的科学、数学、工程、数据分析的 Python 包。</h4><h3 id="下载地址"><a href="#下载地址" class="headerlink" title="下载地址"></a>下载地址</h3><h4 id="官网：https-www-anaconda-com"><a href="#官网：https-www-anaconda-com" class="headerlink" title="官网：https://www.anaconda.com/"></a><a href="https://www.anaconda.com/" target="_blank" rel="noopener">官网</a>：<a href="https://www.anaconda.com/" target="_blank" rel="noopener">https://www.anaconda.com/</a></h4><h4 id="最新版本下载地址：https-www-anaconda-com-download"><a href="#最新版本下载地址：https-www-anaconda-com-download" class="headerlink" title="最新版本下载地址：https://www.anaconda.com/download/"></a><a href="https://www.anaconda.com/download/" target="_blank" rel="noopener">最新版本下载地址</a>：<a href="https://www.anaconda.com/download/" target="_blank" rel="noopener">https://www.anaconda.com/download/</a></h4><h4 id="历史版本：https-repo-anaconda-com-archive"><a href="#历史版本：https-repo-anaconda-com-archive" class="headerlink" title="历史版本：https://repo.anaconda.com/archive/"></a><a href="https://repo.anaconda.com/archive/" target="_blank" rel="noopener">历史版本</a>：<a href="https://repo.anaconda.com/archive/" target="_blank" rel="noopener">https://repo.anaconda.com/archive/</a></h4><p>对应有python3.6和python2.7的版本，可自行选择。</p><p>我们这里下载的是<strong><a href="https://repo.anaconda.com/archive/Anaconda3-4.4.0-Linux-x86_64.sh" target="_blank" rel="noopener">Anaconda3-4.4.0-Linux-x86_64.sh</a>(对应是64位python3.6版本)</strong></p><h3 id="开始安装"><a href="#开始安装" class="headerlink" title="开始安装"></a>开始安装</h3><ul><li><h4 id="进入文件所在路径"><a href="#进入文件所在路径" class="headerlink" title="进入文件所在路径"></a>进入文件所在路径</h4><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20200601183254.png" srcset="/img/loading.gif" alt="1"></p></li><li><h4 id="开始安装-1"><a href="#开始安装-1" class="headerlink" title="开始安装"></a>开始安装</h4><ul><li><h5 id="出现欢迎信息，-阅读许可文件"><a href="#出现欢迎信息，-阅读许可文件" class="headerlink" title="出现欢迎信息， 阅读许可文件"></a>出现欢迎信息， 阅读许可文件</h5><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20200601183255.png" srcset="/img/loading.gif" alt="1"></p></li><li><h5 id="同意许可"><a href="#同意许可" class="headerlink" title="同意许可"></a>同意许可</h5><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20200601183256.png" srcset="/img/loading.gif" alt="1"></p></li><li><h5 id="选择安装目录-默认"><a href="#选择安装目录-默认" class="headerlink" title="选择安装目录-默认"></a>选择安装目录-默认</h5><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20200601183257.png" srcset="/img/loading.gif" alt="1"></p></li><li><h5 id="installing"><a href="#installing" class="headerlink" title="installing"></a>installing</h5><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20200601183258.png" srcset="/img/loading.gif" alt="1"></p></li><li><h5 id="添加系统环境"><a href="#添加系统环境" class="headerlink" title="添加系统环境"></a>添加系统环境</h5><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20200601183259.png" srcset="/img/loading.gif" alt="1"></p></li><li><h5 id="安装完成"><a href="#安装完成" class="headerlink" title="安装完成"></a>安装完成</h5><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20200601183300.png" srcset="/img/loading.gif" alt="1"></p></li><li><h5 id="使环境生效"><a href="#使环境生效" class="headerlink" title="使环境生效"></a>使环境生效</h5></li></ul><pre><code class="hljs shell">seven@ubuntu:~/Desktop$ source ~/.bashrc</code></pre></li></ul><h3 id="Conda的环境管理"><a href="#Conda的环境管理" class="headerlink" title="Conda的环境管理"></a>Conda的环境管理</h3><ul><li><p>创建一个名为python-seven的环境，指定Python版本是3.6（不用管是3.6.x，conda会为我们自动寻找3.6.x中的最新版本）</p><pre><code class="hljs shell">seven@ubuntu:~$ conda create --name python-seven python=3.6</code></pre></li><li><p>安装好后，使用activate激活某个环境</p><pre><code class="hljs shell">seven@ubuntu:~$ source activate python-seven(python-seven) seven@ubuntu:~$</code></pre></li><li><p>测试环境</p><pre><code class="hljs shell">(python-seven) seven@ubuntu:~$ python --versionPython 3.6.2 :: Continuum Analytics, Inc.(python-seven) seven@ubuntu:~$</code></pre></li><li><p>退出虚拟环境</p><pre><code class="hljs shell">(python-seven) seven@ubuntu:~$ source deactivate python-sevenseven@ubuntu:~$</code></pre></li><li><p>删除虚拟环境</p><pre><code class="hljs shell">seven@ubuntu:~$ conda remove --name python-seven --all</code></pre></li></ul><h3 id="ubuntu16-04-安装pycharm"><a href="#ubuntu16-04-安装pycharm" class="headerlink" title="ubuntu16.04 安装pycharm"></a>ubuntu16.04 安装pycharm</h3><p>PyCharm是一种Python IDE，其带有一整套可以帮助用户在使用Python语言开发时提高其效率的工具，比如， 调试、语法高亮、Project管理、代码跳转、智能提示、自动完成、单元测试、版本控制等等。此外，该IDE提供了一些高级功能，以用于支持Django框架下的专业Web开发。同时支持Google App Engine，更酷的是，PyCharm支持IronPython！这些功能在先进代码分析程序的支持下，使 PyCharm 成为 Python 专业开发人员和刚起步人员使用的有力工具。</p><h3 id="下载地址-1"><a href="#下载地址-1" class="headerlink" title="下载地址"></a>下载地址</h3><p><a href="https://www.jetbrains.com/pycharm/download/#section=windows" target="_blank" rel="noopener">官网</a>：<a href="https://www.jetbrains.com/pycharm/download/#section=windows" target="_blank" rel="noopener">https://www.jetbrains.com/pycharm/download/#section=windows</a></p><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20200601183301.png" srcset="/img/loading.gif" alt="1"></p><h3 id="开始安装-2"><a href="#开始安装-2" class="headerlink" title="开始安装"></a>开始安装</h3><ul><li><h4 id="进入文件所在路径-1"><a href="#进入文件所在路径-1" class="headerlink" title="进入文件所在路径"></a>进入文件所在路径</h4><pre><code class="hljs shell">seven@ubuntu:~$ cd Desktop/seven@ubuntu:~/Desktop$ lsAnaconda3-4.4.0-Linux-x86_64.sh  Mnist  pycharm-community-2018.2.4.tar.gz</code></pre></li><li><h4 id="解压文件"><a href="#解压文件" class="headerlink" title="解压文件"></a>解压文件</h4><pre><code class="hljs shell">seven@ubuntu:~/Desktop$ sudo tar -zxvf pycharm-community-2018.2.4.tar.gz -C /opt/</code></pre></li><li><h4 id="开始安装-3"><a href="#开始安装-3" class="headerlink" title="开始安装"></a>开始安装</h4><pre><code class="hljs shell">seven@ubuntu:~/Desktop$ cd /opt/pycharm-community-2018.2.4/bin/seven@ubuntu:/opt/pycharm-community-2018.2.4/bin$ ./pprintenv.py  pycharm.sh   seven@ubuntu:/opt/pycharm-community-2018.2.4/bin$ ./pycharm.sh</code></pre></li></ul><h3 id="简单使用"><a href="#简单使用" class="headerlink" title="简单使用"></a>简单使用</h3><ul><li><h4 id="是否导入设置"><a href="#是否导入设置" class="headerlink" title="是否导入设置"></a>是否导入设置</h4><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20200601183302.png" srcset="/img/loading.gif" alt="1"></p></li><li><h4 id="查看并接受协议"><a href="#查看并接受协议" class="headerlink" title="查看并接受协议"></a>查看并接受协议</h4><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20200601183303.png" srcset="/img/loading.gif" alt="1"></p></li><li><h4 id="选择主题"><a href="#选择主题" class="headerlink" title="选择主题"></a>选择主题</h4><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20200601183304.png" srcset="/img/loading.gif" alt="1"></p></li><li><h4 id="创建项目保存路径"><a href="#创建项目保存路径" class="headerlink" title="创建项目保存路径"></a>创建项目保存路径</h4><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20200601183305.png" srcset="/img/loading.gif" alt="1"></p></li><li><h4 id="选择是否下载插件"><a href="#选择是否下载插件" class="headerlink" title="选择是否下载插件"></a>选择是否下载插件</h4><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20200601183306.png" srcset="/img/loading.gif" alt="1"></p></li><li><h4 id="开始使用"><a href="#开始使用" class="headerlink" title="开始使用"></a>开始使用</h4><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20200601183307.png" srcset="/img/loading.gif" alt="1"></p></li><li><h4 id="创建项目或打开项目"><a href="#创建项目或打开项目" class="headerlink" title="创建项目或打开项目"></a>创建项目或打开项目</h4><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20200601183308.png" srcset="/img/loading.gif" alt="1"></p></li><li><h4 id="项目保存路径及解释器路径"><a href="#项目保存路径及解释器路径" class="headerlink" title="项目保存路径及解释器路径"></a>项目保存路径及解释器路径</h4><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20200601183309.png" srcset="/img/loading.gif" alt="1"></p><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20200601183310.png" srcset="/img/loading.gif" alt="1"></p><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20200601183311.png" srcset="/img/loading.gif" alt="1"></p><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20200601183312.png" srcset="/img/loading.gif" alt="1"></p></li><li><h4 id="创建代码-右键项目文件"><a href="#创建代码-右键项目文件" class="headerlink" title="创建代码-右键项目文件"></a>创建代码-右键项目文件</h4><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20200601183313.png" srcset="/img/loading.gif" alt="1"></p></li><li><h4 id="编写代码并执行"><a href="#编写代码并执行" class="headerlink" title="编写代码并执行"></a>编写代码并执行</h4><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20200601183314.png" srcset="/img/loading.gif" alt="1"></p></li><li><h4 id="修改字体–依次点击File-settings"><a href="#修改字体–依次点击File-settings" class="headerlink" title="修改字体–依次点击File-settings"></a>修改字体–依次点击File-settings</h4><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20200601183315.png" srcset="/img/loading.gif" alt="1"></p></li><li><h4 id="运行结果"><a href="#运行结果" class="headerlink" title="运行结果"></a>运行结果</h4><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20200601183316.png" srcset="/img/loading.gif" alt="1"></p></li><li><h4 id="切换python解释器"><a href="#切换python解释器" class="headerlink" title="切换python解释器"></a>切换python解释器</h4><ul><li><h5 id="依次点击File-settings"><a href="#依次点击File-settings" class="headerlink" title="依次点击File-settings"></a>依次点击File-settings</h5><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20200601183317.png" srcset="/img/loading.gif" alt="1"></p></li><li><h5 id="增加解释器-add"><a href="#增加解释器-add" class="headerlink" title="增加解释器-add"></a>增加解释器-add</h5><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20200601183318.png" srcset="/img/loading.gif" alt="1"></p></li><li><h5 id="找到我们刚刚用Anaconda-新建的python-seven的虚拟环境"><a href="#找到我们刚刚用Anaconda-新建的python-seven的虚拟环境" class="headerlink" title="找到我们刚刚用Anaconda 新建的python-seven的虚拟环境"></a>找到我们刚刚用Anaconda 新建的python-seven的虚拟环境</h5><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20200601183319.png" srcset="/img/loading.gif" alt="1"></p></li><li><h5 id="保存并退出"><a href="#保存并退出" class="headerlink" title="保存并退出"></a>保存并退出</h5><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20200601183320.png" srcset="/img/loading.gif" alt="1"></p></li></ul></li></ul><h3 id="总结："><a href="#总结：" class="headerlink" title="总结："></a>总结：</h3><p>上述方法是采用<strong>Anaconda</strong> 配置 的<strong>python</strong>的开发环境</p><p>如果你不用，自己配置，只需要把<strong>pycharm</strong>的解释器路径设置为你自己的<strong>python路径</strong>。</p>]]></content>
    
    
    <categories>
      
      <category>Anaconda</category>
      
    </categories>
    
    
    <tags>
      
      <tag>linux</tag>
      
      <tag>Anaconda</tag>
      
      <tag>pycharm</tag>
      
      <tag>ubuntu</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Linux-vim及python虚拟环境</title>
    <link href="/2018/11/linux-vim/"/>
    <url>/2018/11/linux-vim/</url>
    
    <content type="html"><![CDATA[<h2 id="linux常用命令"><a href="#linux常用命令" class="headerlink" title="linux常用命令"></a>linux常用命令</h2><h3 id="添加或删除用户："><a href="#添加或删除用户：" class="headerlink" title="添加或删除用户："></a><strong>添加或删除用户</strong>：</h3><h4 id="adduser"><a href="#adduser" class="headerlink" title="adduser:"></a><code>adduser</code>:</h4><pre><code class="hljs shell">adduser username # 添加一个叫username的用户</code></pre><h4 id="userdel"><a href="#userdel" class="headerlink" title="userdel:"></a><code>userdel</code>:</h4><pre><code class="hljs shell">userdel  username  # 删除用户  不删错home目录userdel -r  username # 删除用户 同时删除用户home目录</code></pre><h3 id="文件压缩解压"><a href="#文件压缩解压" class="headerlink" title="文件压缩解压"></a>文件压缩解压</h3><h4 id="tar格式"><a href="#tar格式" class="headerlink" title=".tar格式"></a><code>.tar格式</code></h4><p>在<code>Linux</code>中打包就是使用<code>tar</code>命令    命令有点特殊 前面的参数  可加可不加 <code>-</code></p><pre><code class="hljs bash"><span class="hljs-comment"># 打包</span> tar -cvf  文件名.tar   <span class="hljs-comment"># 要打包的文件</span><span class="hljs-comment"># 解包</span> tar  -xvf  文件名.tar  <span class="hljs-comment">#查看包里的内容</span>tar -tvf 包的文件名.tar</code></pre><h4 id="gz格式"><a href="#gz格式" class="headerlink" title=".gz格式"></a><code>.gz格式</code></h4><p>或者说配合 <code>tar</code> 则是</p><pre><code class="hljs shell">tar -zcvf  xxx.tar.gz  文件 # 压缩tar -zxvf  xxx.tar.gz 文件  # 解压<span class="hljs-meta">#</span><span class="bash"> 解压到指定目录 </span>tar -zxvf xxx.tar.gz -C dirname</code></pre><h4 id="bz2格式"><a href="#bz2格式" class="headerlink" title=".bz2格式"></a><code>.bz2格式</code></h4><pre><code class="hljs shell">tar -jcvf  xxx.tar.bz2  文件  # 压缩tar -jxvf  xxx.tar.bz2       # 解压</code></pre><h4 id="zip格式"><a href="#zip格式" class="headerlink" title=".zip格式"></a><code>.zip格式</code></h4><p>通过<code>zip</code> 压缩的话 不需要指定拓展名  默认拓展名为 <code>zip</code></p><pre><code class="hljs shell">sudo apt-get install zip</code></pre><pre><code class="hljs bash">压缩文件zip  压缩文件  源文件解压 unzip  压缩文件-d 解压到指定目录   如果目录不存在 会自动创建新目录 并压缩进去unzip test.zip -d filename</code></pre><h3 id="进程"><a href="#进程" class="headerlink" title="进程"></a>进程</h3><h3 id="ps-aux-查看进程"><a href="#ps-aux-查看进程" class="headerlink" title="ps   aux 查看进程"></a><code>ps</code>   aux 查看进程</h3><p>查看所有进程 ： <code>ps aux</code> </p><h3 id="top-动态查看进程"><a href="#top-动态查看进程" class="headerlink" title="top 动态查看进程"></a><code>top</code> 动态查看进程</h3><p>查看所有进程 ： </p><pre><code class="hljs shell">top</code></pre><p>top命令用来动态显示运行中的进程     top命令能够在运行后     在指定的时间间隔更新显示信息     可以在使用top命令时加上-d 来指定显示信息更新的时间间隔</p><pre><code class="hljs shell">jobs # 查看所有在后台的进程</code></pre><p><code>ctrl＋Z</code> 　 暂停当前进程　比如你正运行一个命令　突然觉得有点问题想暂停一下　就可以使用这个快捷键　暂停后　可以使用<code>fg</code> 恢复它</p><p>###<code>kill</code>      杀死一个进程</p><pre><code class="hljs shell">kill -9  id号  强制杀死</code></pre><pre><code class="hljs shell">ps -aux 查看正在内存中的程序 会配合 管道符 ps aux | grep ssh/python <span class="hljs-meta">#</span><span class="bash"><span class="hljs-comment">### top动态查看</span></span>默认3秒  -d 时间 <span class="hljs-meta">#</span><span class="bash"><span class="hljs-comment">### 结束进程</span></span>kill -9  id号  强制杀死ctrl+z 可以让你正在运行的东西 暂停 jobs 查看所有在后台的进程</code></pre><h4 id="ctrl-z-暂停-。-fg-继续"><a href="#ctrl-z-暂停-。-fg-继续" class="headerlink" title="ctrl  z   暂停 。   fg   继续"></a>ctrl  z   暂停 。   fg   继续</h4><h3 id="vim-编辑器"><a href="#vim-编辑器" class="headerlink" title="vim 编辑器"></a>vim <strong>编辑器</strong></h3><p>工作模式：命令模式、输入模式、末行模式</p><h4 id="模式之间切换"><a href="#模式之间切换" class="headerlink" title="模式之间切换"></a>模式之间切换</h4><pre><code class="hljs shell">当打开一个文件时处于命令模式在命令模式下，按 i 进入输入模式在输入模式，按ESC回到命令模式。在命令模式下，按shift+; ，末行出现:冒号，则进入末行模式</code></pre><h4 id="进入与退出"><a href="#进入与退出" class="headerlink" title="进入与退出"></a>进入与退出</h4><pre><code class="hljs shell">进入    vim   filename退出    :wq    末行模式，wq 保存退出    :q       末行模式，q 直接退出    :q!      末行模式，q! 强制退出，不保存</code></pre><h4 id="输入模式"><a href="#输入模式" class="headerlink" title="输入模式"></a><strong>输入模式</strong></h4><pre><code class="hljs shell">输入模式    i    从光标所在位置前面开始插入    I    在当前行首插入    a   从光标所在位置后面开始输入    A   在当前行尾插入    o   在光标所在行下方新增一行并进入输入模式    O  在当前上面一行插入进入输入模式后，在最后一行会出现--INSERT—的字样</code></pre><h4 id="移动光标"><a href="#移动光标" class="headerlink" title="移动光标"></a><strong>移动光标</strong></h4><pre><code class="hljs shell">移动光标    gg    到文件第一行    G      到文件最后一行   (Shift + g)    ^      非空格行首    0       行首(数字0)    $       行尾</code></pre><h4 id="复制和粘贴"><a href="#复制和粘贴" class="headerlink" title="复制和粘贴"></a><strong>复制和粘贴</strong></h4><pre><code class="hljs shell">复制和粘贴    yy    复制整行内容    3yy  复制3行内容    yw   复制当前光标到单词尾内容    p      粘贴</code></pre><h4 id="删除"><a href="#删除" class="headerlink" title="删除"></a><strong>删除</strong></h4><pre><code class="hljs shell">删除    dd  删除光标所在行    dw  删除一个单词    x     删除光标所在字符    u    撤销上一次操作    s     替换    ctrl + r    撤销   u</code></pre><h4 id="块操作"><a href="#块操作" class="headerlink" title="块操作"></a><strong>块操作</strong></h4><pre><code class="hljs shell">块操作    v  + 方向键进行块选择    ctrl + v +方向键进行列块选择</code></pre><h4 id="查找"><a href="#查找" class="headerlink" title="查找"></a><strong>查找</strong></h4><pre><code class="hljs shell">查找    /    命令模式下输入：/   向前搜索     不能空格    ?    命令模式下输入：?   向后搜索<span class="hljs-meta">#</span><span class="bash"> / 方式</span>    n    向下查找    N   向上查找<span class="hljs-meta">#</span><span class="bash"> ? 方式</span>    n    向上查找    N   向下查找</code></pre><h4 id="注意"><a href="#注意" class="headerlink" title="注意"></a>注意</h4><pre><code class="hljs shell">在vim下千万不要按Ctrl + s 。 不然就会卡死如果按了Ctrl + s，按Ctrl + q 可以退出</code></pre><h4 id="实例"><a href="#实例" class="headerlink" title="实例"></a><strong>实例</strong></h4><pre><code class="hljs shell">vim hello.py # 创建一个hello.py的python文件</code></pre><p>filename：hello.py</p><pre><code class="hljs python">print(<span class="hljs-string">'hello word'</span>) <span class="hljs-comment"># 打印hello word</span></code></pre><p>保存，退出</p><pre><code class="hljs shell">seven@seven:~$ python hello.py hello word</code></pre><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20200601183212.png" srcset="/img/loading.gif" alt="1542693813625"></p><h3 id="python虚拟环境的使用"><a href="#python虚拟环境的使用" class="headerlink" title="python虚拟环境的使用"></a>python<strong>虚拟环境的使用</strong></h3><h4 id="安装虚拟环境"><a href="#安装虚拟环境" class="headerlink" title="安装虚拟环境"></a><strong>安装虚拟环境</strong></h4><pre><code class="hljs shell"><span class="hljs-meta">$</span><span class="bash"> sudo pip install virtualenv</span></code></pre><h4 id="创建虚拟环境"><a href="#创建虚拟环境" class="headerlink" title="创建虚拟环境"></a><strong>创建虚拟环境</strong></h4><pre><code class="hljs shell"><span class="hljs-meta">$</span><span class="bash"> virtualenv envname <span class="hljs-comment"># envname 自定义的虚拟环境名字</span></span></code></pre><h4 id="激活虚拟环境"><a href="#激活虚拟环境" class="headerlink" title="激活虚拟环境"></a><strong>激活虚拟环境</strong></h4><pre><code class="hljs shell"><span class="hljs-meta">$</span><span class="bash"> <span class="hljs-built_in">source</span> envname/bin/activate</span></code></pre><h4 id="退出虚拟环境"><a href="#退出虚拟环境" class="headerlink" title="退出虚拟环境"></a><strong>退出虚拟环境</strong></h4><pre><code class="hljs shell"><span class="hljs-meta">(envname)$</span><span class="bash"> deactivate</span></code></pre><h4 id="删除虚拟环境"><a href="#删除虚拟环境" class="headerlink" title="删除虚拟环境"></a><strong>删除虚拟环境</strong></h4><pre><code class="hljs shell"><span class="hljs-meta">$</span><span class="bash"> sudo rm -rf envname <span class="hljs-comment"># 只需删除建立的文件夹就行</span></span></code></pre>]]></content>
    
    
    <categories>
      
      <category>linux</category>
      
    </categories>
    
    
    <tags>
      
      <tag>linux</tag>
      
      <tag>vim</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Linux简介及简单操作命令</title>
    <link href="/2018/11/linux/"/>
    <url>/2018/11/linux/</url>
    
    <content type="html"><![CDATA[<h3 id="Linux-简介"><a href="#Linux-简介" class="headerlink" title="Linux 简介"></a>Linux 简介</h3><p>Linux内核最初只是由芬兰人林纳斯·托瓦兹（Linus Torvalds）在赫尔辛基大学上学时出于个人爱好而编写的。</p><p>Linux是一套免费使用和自由传播的类Unix操作系统，是一个基于POSIX和UNIX的多用户、多任务、支持多线程和多CPU的操作系统。</p><p>Linux能运行主要的UNIX工具软件、应用程序和网络协议。它支持32位和64位硬件。Linux继承了Unix以网络为核心的设计思想，是一个性能稳定的多用户网络操作系统。</p><h3 id="Linux的发行版"><a href="#Linux的发行版" class="headerlink" title="Linux的发行版"></a>Linux的发行版</h3><p>Linux的发行版说简单点就是将Linux内核与应用软件做一个打包。</p><p>目前市面上较知名的发行版有：Ubuntu、RedHat、CentOS、Debian、Fedora、SuSE、OpenSUSE、Arch Linux、SolusOS 等。</p><p>Linux具有如下优点：</p><p>Ø  稳定、免费或者花费少</p><p>Ø  安全性高</p><p>Ø  多任务，多用户</p><p>Ø  耗资源少</p><p>Ø  由于内核小，所以它可以支持多种电子产品，如：Android手机</p><h3 id="Linux应用领域"><a href="#Linux应用领域" class="headerlink" title="Linux应用领域"></a>Linux应用领域</h3><p>今天各种场合都有使用各种Linux发行版，从嵌入式设备到超级计算机，并且在服务器领域确定了地位，通常服务器使用LAMP（Linux + Apache + MySQL + PHP）或LNMP（Linux + Nginx+ MySQL + PHP）组合。</p><p>目前Linux不仅在家庭与企业中使用，并且在政府中也很受欢迎。</p><ul><li>巴西联邦政府由于支持Linux而世界闻名。</li><li>有新闻报道俄罗斯军队自己制造的Linux发布版的，做为G.H.ost项目已经取得成果.</li><li>印度的Kerala联邦计划在向全联邦的高中推广使用Linux。</li><li>中华人民共和国为取得技术独立，在龙芯过程中排他性地使用Linux。</li><li>在西班牙的一些地区开发了自己的Linux发布版，并且在政府与教育领域广泛使用，如Extremadura地区的gnuLinEx和Andalusia地区的Guadalinex。</li><li>葡萄牙同样使用自己的Linux发布版Caixa Mágica，用于Magalh?es笔记本电脑和e-escola政府软件。</li><li>法国和德国同样开始逐步采用Linux。</li></ul><h3 id="linux电源管理"><a href="#linux电源管理" class="headerlink" title="linux电源管理"></a>linux电源管理</h3><h4 id="shutdown"><a href="#shutdown" class="headerlink" title="shutdown"></a><code>shutdown</code></h4><ul><li><code>-c</code> 取消关机命令</li><li><code>-h</code> 关机</li><li><code>-r</code> 重启</li></ul><pre><code class="hljs shell">shutdown now # 关机shutdown -h 30 # 30分钟后关机 shutdown -r now # 重启shutdown -r 05:30 # 于05:30重启shutdown -c # 取消关机</code></pre><h4 id="不保存资料直接关机"><a href="#不保存资料直接关机" class="headerlink" title="不保存资料直接关机"></a><code>不保存资料直接关机</code></h4><pre><code class="hljs shell"><span class="hljs-meta">#</span><span class="bash"> halt</span><span class="hljs-meta">#</span><span class="bash"> poweroff</span><span class="hljs-meta">#</span><span class="bash"> init 0</span></code></pre><h4 id="重启"><a href="#重启" class="headerlink" title="重启"></a><code>重启</code></h4><pre><code class="hljs shell">reboot # 重启电脑</code></pre><h4 id="init命令"><a href="#init命令" class="headerlink" title="init命令"></a><code>init命令</code></h4><pre><code class="hljs shell">init 0 # 关机init 1 # 救援模式init 3 # 进入文字模式init 5 # 进入GUI模式init 6 # 重启</code></pre><h4 id="退出登录"><a href="#退出登录" class="headerlink" title="退出登录"></a><code>退出登录</code></h4><pre><code class="hljs shell">logout # 注销</code></pre><h3 id="Linux目录结构"><a href="#Linux目录结构" class="headerlink" title="Linux目录结构"></a>Linux目录结构</h3><ul><li><p><strong>/bin</strong>：<br>bin是Binary的缩写, 这个目录存放着最经常使用的命令。</p></li><li><p><strong>/boot：</strong><br>这里存放的是启动Linux时使用的一些核心文件，包括一些连接文件以及镜像文件。</p></li><li><p><strong>/dev ：</strong><br>dev是Device(设备)的缩写, 该目录下存放的是Linux的外部设备，在Linux中访问设备的方式和访问文件的方式是相同的。</p></li><li><p><strong>/etc：</strong><br>这个目录用来存放所有的系统管理所需要的配置文件和子目录。</p></li><li><p><strong>/home</strong>：<br>用户的主目录，在Linux中，每个用户都有一个自己的目录，一般该目录名是以用户的账号命名的。</p></li><li><p><strong>/lib</strong>：<br>这个目录里存放着系统最基本的动态连接共享库，其作用类似于Windows里的DLL文件。几乎所有的应用程序都需要用到这些共享库。</p></li><li><p><strong>/lost+found</strong>：<br>这个目录一般情况下是空的，当系统非法关机后，这里就存放了一些文件。</p></li><li><p><strong>/media</strong>：<br>linux系统会自动识别一些设备，例如U盘、光驱等等，当识别后，linux会把识别的设备挂载到这个目录下。</p></li><li><p><strong>/mnt</strong>：<br>系统提供该目录是为了让用户临时挂载别的文件系统的，我们可以将光驱挂载在/mnt/上，然后进入该目录就可以查看光驱里的内容了。</p></li><li><p><strong>/opt</strong>：<br>这是给主机额外安装软件所摆放的目录。比如你安装一个ORACLE数据库则就可以放到这个目录下。默认是空的。</p></li><li><p><strong>/proc</strong>：<br>这个目录是一个虚拟的目录，它是系统内存的映射，我们可以通过直接访问这个目录来获取系统信息。<br>这个目录的内容不在硬盘上而是在内存里，我们也可以直接修改里面的某些文件，比如可以通过下面的命令来屏蔽主机的ping命令，使别人无法ping你的机器：</p><pre><code class="hljs awk">echo <span class="hljs-number">1</span> &gt; <span class="hljs-regexp">/proc/</span>sys<span class="hljs-regexp">/net/i</span>pv4<span class="hljs-regexp">/icmp_echo_ignore_all</span></code></pre></li><li><p><strong>/root</strong>：<br>该目录为系统管理员，也称作超级权限者的用户主目录。</p></li><li><p><strong>/sbin</strong>：<br>s就是Super User的意思，这里存放的是系统管理员使用的系统管理程序。</p></li><li><p><strong>/selinux</strong>：<br>这个目录是Redhat/CentOS所特有的目录，Selinux是一个安全机制，类似于windows的防火墙，但是这套机制比较复杂，这个目录就是存放selinux相关的文件的。</p></li><li><p><strong>/srv</strong>：<br>该目录存放一些服务启动之后需要提取的数据。</p></li><li><p><strong>/sys</strong>：<br>这是linux2.6内核的一个很大的变化。该目录下安装了2.6内核中新出现的一个文件系统 sysfs 。</p><p>sysfs文件系统集成了下面3种文件系统的信息：</p></li></ul><blockquote><p>针对进程信息的proc文件系统</p><p>针对设备的devfs文件系统</p><p>针对伪终端的devpts文件系统。</p></blockquote><p>  该文件系统是内核设备树的一个直观反映。</p><p>  当一个内核对象被创建的时候，对应的文件和目录也在内核对象子系统中被创建。</p><ul><li><strong>/tmp</strong>：<br>这个目录是用来存放一些临时文件的。</li><li><strong>/usr</strong>：<br>这是一个非常重要的目录，用户的很多应用程序和文件都放在这个目录下，类似于windows下的program files目录。</li><li><strong>/usr/bin：</strong><br>系统用户使用的应用程序。</li><li><strong>/usr/sbin：</strong><br>超级用户使用的比较高级的管理程序和系统守护程序。</li><li><strong>/usr/src：</strong>内核源代码默认的放置目录。</li><li><strong>/var</strong>：<br>这个目录中存放着在不断扩充着的东西，我们习惯将那些经常被修改的目录放在这个目录下。包括各种日志文件。</li></ul><p>在linux系统中，有几个目录是比较重要的，平时需要注意不要误删除或者随意更改内部文件。</p><p>/etc： 上边也提到了，这个是系统中的配置文件，如果你更改了该目录下的某个文件可能会导致系统不能启动。</p><p>/bin, /sbin, /usr/bin, /usr/sbin: 这是系统预设的执行文件的放置目录，比如 ls 就是在/bin/ls 目录下的。</p><p>值得提出的是，/bin, /usr/bin 是给系统用户使用的指令（除root外的通用户），而/sbin, /usr/sbin 则是给root使用的指令。</p><p>/var： 这是一个非常重要的目录，系统上跑了很多程序，那么每个程序都会有相应的日志产生，而这些日志就被记录到这个目录下，具体在/var/log 目录下，另外mail的预设放置也是在这里。</p><h3 id="shell"><a href="#shell" class="headerlink" title="shell"></a><strong>shell</strong></h3><p>Shell基本上是一个命令<a href="https://baike.baidu.com/item/%E8%A7%A3%E9%87%8A%E5%99%A8" target="_blank" rel="noopener">解释器</a></p><p>Shell俗称壳（用来区别于核）</p><p>它接收用户命令，然后调用相应的应用程序。</p><h3 id="ls"><a href="#ls" class="headerlink" title="ls:"></a><code>ls</code>:</h3><p>ls是英文单词list的简写，其功能为列出目录的内容。这是用户最常用的一个命令，因为用户需要不时地查看某个目录的内容。该命令类似于DOS下的dir命令。 对于每个目录，该命令将列出其中的所有子目录与文件。对于每个文件，ls将输出其文件名以及所要求的其<br>他信息。默认情况下，输出条目按字母顺序排序。当未给出目录名或是文件名时，就显示当<br>前目录的信息。<br>主要的OPTION有：</p><p>-a 列出隐藏文件，文件中以“.”开头的均为隐藏文件，如：~/.bashrc</p><p>-l 列出文件的详细信息</p><p>-R 连同子目录中的内容一起列出</p><p>用ls -l命令显示的信息中，开头是由10个字符构成的字符串，其中第一个字符表示文</p><p>件类型，它可以是下述类型之一：</p><ul><li><p>普通文件</p><p>d 目录</p><p>l 符号链接</p><p>b 块设备文件</p><p>c 字符设备文件</p><p>s socket文件，网络套接字</p><p>p 管道</p><p>后面的9个字符表示文件的访问权限，分为3组，每组3位。第一组表示文件属主的权限，第二组表示同组用户的权限，第三组表示其他用户的权限。每一组的三个字符分别表示对文件的读、写和执行权限。</p><p>各权限如下所示：</p><p>r 读</p><p>w 写</p><p>x 可执行。对于目录，表示进入权限。</p><p>s 当文件被执行时，把该文件的UID或GID赋予执行进程的UID（用户ID）或GID（组 ID）。</p><p>t 设置标志位（sticky bit）。如果是有sticky bit的目录，在该目录下任何用户只要有适当的权限即可创建件，但文件只能被超级用户、目录拥有者或文件属主删除。</p><p>如果是有sticky bit的可执行文件，在该文件执行后，指向其正文段的指针仍留在内存。这样再次执行它时，系统就能更快地装入该文件。</p></li><li><p>没有相应位置的权限。</p><p>访问权限后面的数字表示与该文件共享inode的文件总数，即硬链接数(参见下面ln命令)。</p></li></ul><p>-a：显示所有档案及目录（ls内定将档案名或目录名称为“.”的视为影藏，不会列出）；</p><p>-A：显示除影藏文件“.”和“..”以外的所有文件列表；</p><p>-C：多列显示输出结果。这是默认选项；</p><p>-l：与“-C”选项功能相反，所有输出信息用单列格式输出，不输出为多列；</p><p>-F：在每个输出项后追加文件的类型标识符，具体含义：“*”表示具有可执行权限的普通文件，“/”表示目录，“@”表示符号链接，“|”表示命令管道FIFO，“=”表示sockets套接字。当文件为普通文件时，不输出任何标识符；</p><p>-b：将文件中的不可输出的字符以反斜线“”加字符编码的方式输出；</p><p>-c：与“-lt”选项连用时，按照文件状态时间排序输出目录内容，排序的依据是文件的索引节点中的ctime字段。与“-l”选项连用时，则排序的一句是文件的状态改变时间；</p><p>-d：仅显示目录名，而不显示目录下的内容列表。显示符号链接文件本身，而不显示其所指向的目录列表；</p><p>-f：此参数的效果和同时指定“aU”参数相同，并关闭“lst”参数的效果；</p><p>-i：显示文件索引节点号（inode）。一个索引节点代表一个文件；</p><p>–file-type：与“-F”选项的功能相同，但是不显示“*”；</p><p>-k：以KB（千字节）为单位显示文件大小；</p><p>-l：以长格式显示目录下的内容列表。输出的信息从左到右依次包括文件名，文件类型、权限模式、硬连接数、所有者、组、文件大小和文件的最后修改时间等；</p><p>-m：用“,”号区隔每个文件和目录的名称；</p><p>-n：以用户识别码和群组识别码替代其名称；</p><p>-r：以文件名反序排列并输出目录内容列表；</p><p>-s：显示文件和目录的大小，以区块为单位；</p><p>-t：用文件和目录的更改时间排序；</p><p>-L：如果遇到性质为符号链接的文件或目录，直接列出该链接所指向的原始文件或目录；</p><p>-R：递归处理，将指定目录下的所有文件及子目录一并处理；</p><p>–full-time：列出完整的日期与时间；</p><p>–color[=WHEN]：使用不同的颜色高亮显示不同类型的。</p><h3 id="文件权限"><a href="#文件权限" class="headerlink" title="文件权限"></a><code>文件权限</code></h3><pre><code class="hljs shell">-rwx-rwx-rwx <span class="hljs-meta">#</span><span class="bash"> 第一个代表文件类型</span><span class="hljs-meta">#</span><span class="bash"> 代表所有者的权限 </span><span class="hljs-meta">#</span><span class="bash"> 代表所属组的权限</span><span class="hljs-meta">#</span><span class="bash"> 代表其他人的权限</span></code></pre><h3 id="改变权限-chmod"><a href="#改变权限-chmod" class="headerlink" title="改变权限 - chmod"></a><code>改变权限</code> - <code>chmod</code></h3><pre><code class="hljs shell">r 读取权限  如果没有r 就不能 ls 查看里面的内容   对应数字 4w 写权限    如果没有w 就不能在目录下创建新的文件  对应数字 2x 执行权限  如果没有x 就不能cd进入这个目录 对应数字 1- 没权限  对应数字 0chmod 777 filenamerwx-rwx-rwx</code></pre><h3 id="cd"><a href="#cd" class="headerlink" title="cd:"></a><code>cd</code>:</h3><p>切换文件夹</p><pre><code class="hljs shell">cd filename  # 进入文件-filename：文件名cd -  # 返回上一次进入的目录cd ~  # 进入根目录cd .. # 返回上级目录</code></pre><h3 id="pwd"><a href="#pwd" class="headerlink" title="pwd:"></a><code>pwd</code>:</h3><p>查看你的当前绝对路径</p><h3 id="mkdir"><a href="#mkdir" class="headerlink" title="mkdir:"></a><code>mkdir</code>:</h3><p>mkdir [OPTION] DIRECTORY…</p><p>创建目录DIRECTORY，可以一次创建多个。OPTION如果是-p，表示可以连同父目录一起创建。</p><pre><code class="hljs shell">mkdir filename # 创建一个filename的文件</code></pre><h3 id="rmdir"><a href="#rmdir" class="headerlink" title="rmdir:"></a><code>rmdir</code>:</h3><p>删除一个空的目录</p><pre><code class="hljs shell">rmdir filename # 删除一个空的filename的文件</code></pre><h3 id="cp"><a href="#cp" class="headerlink" title="cp:"></a><code>cp</code>:</h3><p><strong>复制文件或目录</strong></p><pre><code class="hljs avrasm"><span class="hljs-keyword">cp</span> file1 file2<span class="hljs-keyword">cp</span> file1 dir/<span class="hljs-keyword">cp</span> file1 ../</code></pre><p><strong>拷贝目录</strong>:</p><pre><code class="hljs shell">cp dir1 dir2 -rcp dir1 ~/ -r</code></pre><h3 id="rm"><a href="#rm" class="headerlink" title="rm:"></a><code>rm</code>:</h3><p>移除文件或目录</p><pre><code class="hljs shell">rm -r  # 递归删除文件rm -rf # 强制删除文件*****</code></pre><h3 id="cat"><a href="#cat" class="headerlink" title="cat:"></a><code>cat</code>:</h3><p> 由第一行开始显示文件内容 <code>-b</code>显示行号</p><pre><code class="hljs shell">cat file  # 一次查看所有的文件cat file1  file2   # 一次查看两个命令</code></pre><h3 id="tac"><a href="#tac" class="headerlink" title="tac :"></a><code>tac</code> :</h3><p> 从最后一行开始显示，可以看出 tac 是 cat 的倒著写！</p><pre><code class="hljs shell">tac filename</code></pre><h3 id="nl"><a href="#nl" class="headerlink" title="nl:"></a><code>nl</code>:</h3><p> 显示的时候，顺道输出行号！</p><pre><code class="hljs shell">nl filename</code></pre><h3 id="more"><a href="#more" class="headerlink" title="more"></a><code>more</code></h3><p> 一页一页的显示文件内容</p><p>按Space键：显示文本的下一屏内容。</p><p>按Enier键：只显示文本的下一行内容。</p><p>按斜线符/：接着输入一个模式，可以在文本中寻找下一个相匹配的模式。</p><p>按H键：显示帮助屏，该屏上有相关的帮助信息。</p><p>按B键：显示上一屏内容。</p><p>按Q键：退出more命令。</p><pre><code class="hljs shell">more filename</code></pre><h3 id="less"><a href="#less" class="headerlink" title="less"></a><code>less</code></h3><p>用PageUp键向上翻页，用PageDown键向下翻页</p><p>f 往后 b 往前q 退出</p><p>-e：文件内容显示完毕后，自动退出；</p><p>-f：强制显示文件；</p><p>-g：不加亮显示搜索到的所有关键词，仅显示当前显示的关键字，以提高显示速度；</p><p>-l：搜索时忽略大小写的差异；</p><p>-N：每一行行首显示行号；</p><p>-s：将连续多个空行压缩成一行显示；</p><p>-S：在单行显示较长的内容，而不换行显示；</p><p>-x&lt;数字&gt;：将TAB字符显示为指定个数的空格字符。</p><pre><code class="hljs shell">less -e filename</code></pre><h3 id="head"><a href="#head" class="headerlink" title="head:"></a><code>head</code>:</h3><p>-n&lt;数字&gt;：指定显示头部内容的行数；</p><p>-c&lt;字符数&gt;：指定显示头部内容的字符数；</p><p>-v：总是显示文件名的头信息；</p><p>-q：不显示文件名的头信息。</p><pre><code class="hljs shell">head filename</code></pre><h3 id="tail："><a href="#tail：" class="headerlink" title="tail："></a><code>tail</code>：</h3><p>只看尾巴几行</p><p>–retry：即是在tail命令启动时，文件不可访问或者文件稍后变得不可访问，都始终尝试打开文件。使用此选项时需要与选项“——follow=name”连用；</p><p>-c<N>或——bytes=<N>：输出文件尾部的N（N为整数）个字节内容；</p><p>-f&lt;name/descriptor&gt;或；–follow<nameldescript>：显示文件最新追加的内容。“name”表示以文件名的方式监视文件的变化。“-f”与“-fdescriptor”等效；</p><p>-F：与选项“-follow=name”和“–retry”连用时功能相同；</p><p>-n<N>或——line=<N>：输出文件的尾部N（N位数字）行内容。</p><p>–pid=&lt;进程号&gt;：与“-f”选项连用，当指定的进程号的进程终止后，自动退出tail命令；</p><p>-q或——quiet或——silent：当有多个文件参数时，不输出各个文件名；</p><p>-s&lt;秒数&gt;或——sleep-interal=&lt;秒数&gt;：与“-f”选项连用，指定监视文件变化时间隔的秒数；</p><p>-v或——verbose：当有多个文件参数时，总是输出各个文件名；</p><p>–help：显示指令的帮助信息；</p><p>–version：显示指令的版本信息。</p><pre><code class="hljs shell">tail file # （显示文件file的最后10行）tail -c 10 file # （显示文件file的最后10个字符）</code></pre><h3 id="which-whereis-查找命令"><a href="#which-whereis-查找命令" class="headerlink" title="which / whereis 查找命令"></a><code>which / whereis</code> 查找命令</h3><pre><code class="hljs shell">which command  # 查看    -二进制文件whereis 可执行文件    # 二进制文件 、man手册帮助文档：1.man手册  ，帮助文档    man  ls2.--help   , ls --help</code></pre><h3 id="find-查找文件"><a href="#find-查找文件" class="headerlink" title="find   查找文件"></a><code>find</code>   查找文件</h3><pre><code class="hljs shell">find 路径  参数  <span class="hljs-meta">#</span><span class="bash"> 常用参数 </span>-name   # 按照名字-size   # 按照大小find ./  -size +100k -size -10M  # 在当前目录下找大于100k 小于 10的文件</code></pre><h3 id="grep-文本搜索-筛选内容"><a href="#grep-文本搜索-筛选内容" class="headerlink" title="grep  文本搜索(筛选内容)"></a><code>grep</code>  文本搜索(筛选内容)</h3><pre><code class="hljs shell">grep  'content'  filename<span class="hljs-meta">#</span><span class="bash"> 常用参数</span>-v    显示不包含匹配文本的所有‘行’ (求反)-n    显示匹配行及行号-i    忽略大小写<span class="hljs-meta">#</span><span class="bash"> 内容参数</span>^wu  行首 搜索以wu开头的行  <span class="hljs-meta">wh$</span><span class="bash">  行尾 索以wh结束的行</span></code></pre><h3 id="管道"><a href="#管道" class="headerlink" title="| 管道"></a><code>|</code> 管道</h3><h5 id="一个命令的输出，可以通过管道符，做为另一个命令的输入"><a href="#一个命令的输出，可以通过管道符，做为另一个命令的输入" class="headerlink" title="一个命令的输出，可以通过管道符，做为另一个命令的输入"></a>一个命令的输出，可以通过管道符，做为另一个<code>命令</code>的输入</h5><pre><code class="hljs shell">命令输出  |  命令处理ls --help | less   # 将输出，放入翻页模式中ls --help | grep -n 'f'  #将输出，放入‘筛选’模式中ls --help | grep -n 'f' &gt;&gt; 3.txt  #将输出，放入‘筛选’模式中，然后将重定向到3.txt</code></pre><h3 id="ln-创建链接文件"><a href="#ln-创建链接文件" class="headerlink" title="ln   创建链接文件"></a><code>ln</code>   创建链接文件</h3><pre><code class="hljs shell">ln  file  hardlink # 硬链接ln -s  file softlink  # 软链接</code></pre><p>软链接:  相当于 <code>window</code>上的快捷方式  源文件删除则软链接失效</p><p>硬链接:   硬链接只能连接普通的文件   不能连接目录</p><p><strong>注意</strong>  如果软链接文件和源文件不在同一个目录    源文件要使用绝对路径    不能使用相对路径</p><h3 id="alias-创建别名"><a href="#alias-创建别名" class="headerlink" title="alias  创建别名"></a><code>alias</code>  创建别名</h3><pre><code class="hljs shell">alias     # 查看所有别名   alias c4='cat 4.txt'unalias   # 删除别名</code></pre><p><strong>注意</strong>  这种定义别名的方式 只在当前登录有效  如果要永久定义生效  可以通过修改<code>~/.bashrc</code>文件   这个修改要下次登录才能生效  想要立即生效  可以输入<code>source  ~/.bashrc</code></p>]]></content>
    
    
    <categories>
      
      <category>linux</category>
      
    </categories>
    
    
    <tags>
      
      <tag>linux</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Anaconda python3.6及pycharm安装和简单使用--Windows</title>
    <link href="/2018/11/anaconda-windows/"/>
    <url>/2018/11/anaconda-windows/</url>
    
    <content type="html"><![CDATA[<h3 id="Windows-安装Anaconda基于python3-6"><a href="#Windows-安装Anaconda基于python3-6" class="headerlink" title="Windows 安装Anaconda基于python3.6"></a>Windows 安装Anaconda基于python3.6</h3><ul><li><p>anaconda是python发行的包的管理工具，其中自带python的版本，还带很多python的包.安装它比安装python好。可以省掉再安装python包的时间。推荐使用Anaconda,用Anaconda安装python的包是非常便捷高效的，比如安装scrapy框架，如果用原生python的pip安装，要安装很多依赖的包，还经常报错，但是用Anaconda直接输入conda install scrapy就可以了，其他的都不用管了，就是这么方便。另外它自带了 180多个python常用的包，简直就是好得没朋友。</p></li><li><p>要利用 Python 进行科学计算，就需要一一安装所需的模块， 而这些模块可能又依赖于其它的软件包或库，安装和使用起来相对麻烦。Anaconda 就是将科学计算所需要的模块都编译好，然后打包以发行版的形式供用户使用的一个常用的科学计算环境。 </p></li></ul><h4 id="它包含了众多流行的科学、数学、工程、数据分析的-Python-包。"><a href="#它包含了众多流行的科学、数学、工程、数据分析的-Python-包。" class="headerlink" title="它包含了众多流行的科学、数学、工程、数据分析的 Python 包。"></a>它包含了众多流行的科学、数学、工程、数据分析的 Python 包。</h4><h3 id="下载地址"><a href="#下载地址" class="headerlink" title="下载地址"></a>下载地址</h3><h4 id="官网：https-www-anaconda-com"><a href="#官网：https-www-anaconda-com" class="headerlink" title="官网：https://www.anaconda.com/"></a><a href="https://www.anaconda.com/" target="_blank" rel="noopener">官网</a>：<a href="https://www.anaconda.com/" target="_blank" rel="noopener">https://www.anaconda.com/</a></h4><h4 id="最新版本下载地址：https-www-anaconda-com-download"><a href="#最新版本下载地址：https-www-anaconda-com-download" class="headerlink" title="最新版本下载地址：https://www.anaconda.com/download/"></a><a href="https://www.anaconda.com/download/" target="_blank" rel="noopener">最新版本下载地址</a>：<a href="https://www.anaconda.com/download/" target="_blank" rel="noopener">https://www.anaconda.com/download/</a></h4><h4 id="历史版本：https-repo-anaconda-com-archive"><a href="#历史版本：https-repo-anaconda-com-archive" class="headerlink" title="历史版本：https://repo.anaconda.com/archive/"></a><a href="https://repo.anaconda.com/archive/" target="_blank" rel="noopener">历史版本</a>：<a href="https://repo.anaconda.com/archive/" target="_blank" rel="noopener">https://repo.anaconda.com/archive/</a></h4><p>对应有python3.6和python2.7的版本，可自行选择。</p><p>我们这里下载的是<strong>Anaconda3-4.4.0-Windows-x86_64.exe(对应是64位python3.6版本)</strong></p><h3 id="开始安装"><a href="#开始安装" class="headerlink" title="开始安装"></a>开始安装</h3><ul><li><h4 id="运行可执行文件"><a href="#运行可执行文件" class="headerlink" title="运行可执行文件"></a>运行可执行文件</h4><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20200601183324.png" srcset="/img/loading.gif" alt="1"></p></li><li><h4 id="同意协议"><a href="#同意协议" class="headerlink" title="同意协议"></a>同意协议</h4><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20200601183325.png" srcset="/img/loading.gif" alt="2"></p></li><li><h4 id="选择使用者"><a href="#选择使用者" class="headerlink" title="选择使用者"></a>选择使用者</h4><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20200601183326.png" srcset="/img/loading.gif" alt="1"></p></li><li><h4 id="选择安装路径"><a href="#选择安装路径" class="headerlink" title="选择安装路径"></a>选择安装路径</h4><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20200601183327.png" srcset="/img/loading.gif" alt="1"></p></li><li><h4 id="选择是否添加环境变量和默认启动的是Anaconda-python"><a href="#选择是否添加环境变量和默认启动的是Anaconda-python" class="headerlink" title="选择是否添加环境变量和默认启动的是Anaconda python"></a>选择是否添加环境变量和默认启动的是Anaconda python</h4><blockquote><ul><li>增加Anaconda到系统环境变量</li><li>Anaconda python为默认解释器<ul><li>勾选后如本地还有其他python解释器，则不是默认使用</li></ul></li></ul></blockquote><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20200601183328.png" srcset="/img/loading.gif" alt="1"></p></li><li><h4 id="installing"><a href="#installing" class="headerlink" title="installing"></a>installing</h4><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20200601183329.png" srcset="/img/loading.gif" alt="1"></p></li><li><h4 id="安装完成"><a href="#安装完成" class="headerlink" title="安装完成"></a>安装完成</h4><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20200601183330.png" srcset="/img/loading.gif" alt="1"></p><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20200601183331.png" srcset="/img/loading.gif" alt="1"></p></li></ul><h3 id="简单使用"><a href="#简单使用" class="headerlink" title="简单使用"></a>简单使用</h3><ul><li><h4 id="点击打开"><a href="#点击打开" class="headerlink" title="点击打开"></a>点击打开</h4></li></ul><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20200601183332.png" srcset="/img/loading.gif" alt="1"></p><ul><li><h4 id="打开Anaconda-Navigator"><a href="#打开Anaconda-Navigator" class="headerlink" title="打开Anaconda Navigator"></a>打开Anaconda Navigator</h4><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20200601183333.png" srcset="/img/loading.gif" alt="1"></p></li><li><h4 id="主界面"><a href="#主界面" class="headerlink" title="主界面"></a>主界面</h4><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20200601183334.png" srcset="/img/loading.gif" alt="1"></p></li><li><h4 id="添加或删除python库"><a href="#添加或删除python库" class="headerlink" title="添加或删除python库"></a>添加或删除python库</h4><ul><li><h5 id="打开environment"><a href="#打开environment" class="headerlink" title="打开environment"></a>打开environment</h5><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20200601183335.png" srcset="/img/loading.gif" alt="1"></p></li><li><h5 id="搜索库文件"><a href="#搜索库文件" class="headerlink" title="搜索库文件"></a>搜索库文件</h5><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20200601183336.png" srcset="/img/loading.gif" alt="1"></p></li><li><h5 id="安装库文件"><a href="#安装库文件" class="headerlink" title="安装库文件"></a>安装库文件</h5><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20200601183337.png" srcset="/img/loading.gif" alt="1"></p></li><li><h5 id="删除或更新"><a href="#删除或更新" class="headerlink" title="删除或更新"></a>删除或更新</h5><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20200601183338.png" srcset="/img/loading.gif" alt="1"></p></li></ul></li><li><h4 id="创建虚拟环境"><a href="#创建虚拟环境" class="headerlink" title="创建虚拟环境"></a>创建虚拟环境</h4><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20200601183339.png" srcset="/img/loading.gif" alt="1"></p><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20200601183340.png" srcset="/img/loading.gif" alt="1"></p><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20200601183341.png" srcset="/img/loading.gif" alt="1"></p></li><li><h4 id="删除虚拟环境"><a href="#删除虚拟环境" class="headerlink" title="删除虚拟环境"></a>删除虚拟环境</h4></li></ul><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20200601183342.png" srcset="/img/loading.gif" alt="1"></p><h3 id="windows-安装pycharm"><a href="#windows-安装pycharm" class="headerlink" title="windows 安装pycharm"></a>windows 安装pycharm</h3><p>PyCharm是一种Python IDE，其带有一整套可以帮助用户在使用Python语言开发时提高其效率的工具，比如， 调试、语法高亮、Project管理、代码跳转、智能提示、自动完成、单元测试、版本控制等等。此外，该IDE提供了一些高级功能，以用于支持Django框架下的专业Web开发。同时支持Google App Engine，更酷的是，PyCharm支持IronPython！这些功能在先进代码分析程序的支持下，使 PyCharm 成为 Python 专业开发人员和刚起步人员使用的有力工具。</p><h3 id="下载地址-1"><a href="#下载地址-1" class="headerlink" title="下载地址"></a>下载地址</h3><h4 id="官网：https-www-jetbrains-com-pycharm-download-section-windows"><a href="#官网：https-www-jetbrains-com-pycharm-download-section-windows" class="headerlink" title="官网：https://www.jetbrains.com/pycharm/download/#section=windows"></a><a href="https://www.jetbrains.com/pycharm/download/#section=windows" target="_blank" rel="noopener">官网</a>：<a href="https://www.jetbrains.com/pycharm/download/#section=windows" target="_blank" rel="noopener">https://www.jetbrains.com/pycharm/download/#section=windows</a></h4><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20200601183343.png" srcset="/img/loading.gif" alt="1"></p><h3 id="开始安装-1"><a href="#开始安装-1" class="headerlink" title="开始安装"></a>开始安装</h3><ul><li><h4 id="运行可执行文件-1"><a href="#运行可执行文件-1" class="headerlink" title="运行可执行文件"></a>运行可执行文件</h4><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20200601183344.png" srcset="/img/loading.gif" alt="1"></p></li><li><h4 id="选择安装地址"><a href="#选择安装地址" class="headerlink" title="选择安装地址"></a>选择安装地址</h4><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20200601183345.png" srcset="/img/loading.gif" alt="1"></p></li><li><h4 id="选择安装版本及依赖"><a href="#选择安装版本及依赖" class="headerlink" title="选择安装版本及依赖"></a>选择安装版本及依赖</h4><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20200601183346.png" srcset="/img/loading.gif" alt="1"></p></li><li><h4 id="生成快捷方式"><a href="#生成快捷方式" class="headerlink" title="生成快捷方式"></a>生成快捷方式</h4><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20200601183347.png" srcset="/img/loading.gif" alt="1"></p></li><li><h4 id="installing-1"><a href="#installing-1" class="headerlink" title="installing"></a>installing</h4><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20200601183348.png" srcset="/img/loading.gif" alt="1"></p></li><li><h4 id="安装完成-1"><a href="#安装完成-1" class="headerlink" title="安装完成"></a>安装完成</h4><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20200601183349.png" srcset="/img/loading.gif" alt="1"></p></li></ul><h3 id="简单使用-1"><a href="#简单使用-1" class="headerlink" title="简单使用"></a>简单使用</h3><ul><li><h4 id="是否导入设置"><a href="#是否导入设置" class="headerlink" title="是否导入设置"></a>是否导入设置</h4><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20200601183350.png" srcset="/img/loading.gif" alt="1"></p></li><li><h4 id="查看并接受协议"><a href="#查看并接受协议" class="headerlink" title="查看并接受协议"></a>查看并接受协议</h4><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20200601183351.png" srcset="/img/loading.gif" alt="1"></p></li><li><h4 id="选择主题"><a href="#选择主题" class="headerlink" title="选择主题"></a>选择主题</h4><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20200601183352.png" srcset="/img/loading.gif" alt="1"></p></li><li><h4 id="选择是否下载插件"><a href="#选择是否下载插件" class="headerlink" title="选择是否下载插件"></a>选择是否下载插件</h4><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20200601183353.png" srcset="/img/loading.gif" alt="1"></p></li><li><h4 id="开始使用"><a href="#开始使用" class="headerlink" title="开始使用"></a>开始使用</h4><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20200601183354.png" srcset="/img/loading.gif" alt="1"></p></li><li><h4 id="创建项目或打开项目"><a href="#创建项目或打开项目" class="headerlink" title="创建项目或打开项目"></a>创建项目或打开项目</h4><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20200601183355.png" srcset="/img/loading.gif" alt="1"></p></li><li><h4 id="项目保存路径及解释器路径"><a href="#项目保存路径及解释器路径" class="headerlink" title="项目保存路径及解释器路径"></a>项目保存路径及解释器路径</h4><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20200601183356.png" srcset="/img/loading.gif" alt="1"></p><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20200601183357.png" srcset="/img/loading.gif" alt="1"></p><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20200601183358.png" srcset="/img/loading.gif" alt="1"></p><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20200601183359.png" srcset="/img/loading.gif" alt="1"></p></li><li><h4 id="创建代码-右键项目文件"><a href="#创建代码-右键项目文件" class="headerlink" title="创建代码-右键项目文件"></a>创建代码-右键项目文件</h4><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20200601183400.png" srcset="/img/loading.gif" alt="1"></p></li><li><h4 id="编写代码并执行"><a href="#编写代码并执行" class="headerlink" title="编写代码并执行"></a>编写代码并执行</h4><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20200601183401.png" srcset="/img/loading.gif" alt="1"></p></li><li><h4 id="修改字体–依次点击File-settings"><a href="#修改字体–依次点击File-settings" class="headerlink" title="修改字体–依次点击File-settings"></a>修改字体–依次点击File-settings</h4><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20200601183402.png" srcset="/img/loading.gif" alt="1"></p></li><li><h4 id="运行结果"><a href="#运行结果" class="headerlink" title="运行结果"></a>运行结果</h4><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20200601183403.png" srcset="/img/loading.gif" alt="1"></p></li><li><h4 id="切换python解释器"><a href="#切换python解释器" class="headerlink" title="切换python解释器"></a>切换python解释器</h4><ul><li><h5 id="依次点击File-settings"><a href="#依次点击File-settings" class="headerlink" title="依次点击File-settings"></a>依次点击File-settings</h5><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20200601183404.png" srcset="/img/loading.gif" alt="1"></p></li><li><h5 id="增加解释器-add"><a href="#增加解释器-add" class="headerlink" title="增加解释器-add"></a>增加解释器-add</h5><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20200601183405.png" srcset="/img/loading.gif" alt="1"></p></li><li><h5 id="找到我们刚刚用Anaconda-新建的TensorFlow-seven的虚拟环境"><a href="#找到我们刚刚用Anaconda-新建的TensorFlow-seven的虚拟环境" class="headerlink" title="找到我们刚刚用Anaconda 新建的TensorFlow-seven的虚拟环境"></a>找到我们刚刚用Anaconda 新建的TensorFlow-seven的虚拟环境</h5><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20200601183406.png" srcset="/img/loading.gif" alt="1"></p></li><li><h5 id="保存并退出"><a href="#保存并退出" class="headerlink" title="保存并退出"></a>保存并退出</h5><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20200601183407.png" srcset="/img/loading.gif" alt="1"></p></li></ul></li></ul><h3 id="总结："><a href="#总结：" class="headerlink" title="总结："></a>总结：</h3><p>上述方法是采用<strong>Anaconda</strong> 配置 的<strong>python</strong>的开发环境</p><p>如果你不用，自己配置，只需要把<strong>pycharm</strong>的解释器路径设置为你自己的<strong>python路径</strong>。</p>]]></content>
    
    
    <categories>
      
      <category>Anaconda</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Anaconda</tag>
      
      <tag>pycharm</tag>
      
      <tag>Windows</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Docker配置TensorFlow-CPU开发环境</title>
    <link href="/2018/10/docker-TensorFlow-cpu/"/>
    <url>/2018/10/docker-TensorFlow-cpu/</url>
    
    <content type="html"><![CDATA[<h3 id="查找TensorFlow镜像"><a href="#查找TensorFlow镜像" class="headerlink" title="查找TensorFlow镜像"></a>查找TensorFlow镜像</h3><pre><code class="hljs bash">seven@seven:~$ sudo docker search tensorflow  -s 10NAME                                DESCRIPTION                                     STARS               OFFICIAL            AUTOMATEDtensorflow/tensorflow               Official docker images <span class="hljs-keyword">for</span> deep learning fra…   1150                                    jupyter/tensorflow-notebook         Jupyter Notebook Scientific Python Stack w/ …   86                                      xblaster/tensorflow-jupyter         Dockerized Jupyter with tensorflow              50                                      [OK]tensorflow/serving                  Official images <span class="hljs-keyword">for</span> TensorFlow Serving (http…   22                                      floydhub/tensorflow                 tensorflow                                      14                                      [OK]bitnami/tensorflow-serving          Bitnami Docker Image <span class="hljs-keyword">for</span> TensorFlow Serving     13                                      [OK]</code></pre><h3 id="PULL-TensorFlow镜像"><a href="#PULL-TensorFlow镜像" class="headerlink" title="PULL TensorFlow镜像"></a>PULL TensorFlow镜像</h3><pre><code class="hljs bash">seven@seven:~$ sudo docker run -it tensorflow/tensorflowUnable to find image <span class="hljs-string">'tensorflow/tensorflow:latest'</span> locallylatest: Pulling from tensorflow/tensorflow3b37166ec614: Pull complete 504facff238f: Pull complete ebbcacd28e10: Pull complete c7fb3351ecad: Pull complete 2e3debadcbf7: Pull complete 568ddecd541b: Pull complete cb5781b11958: Pull complete c6f383503f95: Pull complete 71dcbb855dc9: Pull complete 5cfd34784f24: Pull complete c26bfefb0572: Pull complete 88302acc21c8: Pull complete 4c2b848f6d49: Pull complete ...Status: Downloaded newer image <span class="hljs-keyword">for</span> tensorflow/tensorflow:latest</code></pre><h3 id="创建TensorFlow容器–jupyter-notebook"><a href="#创建TensorFlow容器–jupyter-notebook" class="headerlink" title="创建TensorFlow容器–jupyter notebook"></a>创建TensorFlow容器–jupyter notebook</h3><pre><code class="hljs bash">seven@seven:~$ docker run --name seven-tensorflow -it -p 8888:8888 -v ~/Docker-tensorflow:/demo tensorflow/tensorflow</code></pre><blockquote><p><strong>–name</strong>：创建的容器名，即seven-tensorflow</p><p><strong>-it</strong>：保留命令行运行</p><p><strong>p 8888:8888</strong>：将本地的8888端口和<code>http://localhost:8888/</code>映射</p><p><strong>-v ~/Docker-tensorflow:/demo</strong>:将本地的~/Docker-tensorflow挂载到容器内的/demo下</p><p><strong>tensorflow/tensorflow</strong> ：默认是tensorflow/tensorflow:latest,指定使用的镜像</p></blockquote><p>输入以上命令后，默认容器就被启动了，命令行显示</p><pre><code class="hljs bash">[I 13:16:07.331 NotebookApp] Writing notebook server cookie secret to /root/.<span class="hljs-built_in">local</span>/share/jupyter/runtime/notebook_cookie_secret[I 13:16:07.347 NotebookApp] Serving notebooks from <span class="hljs-built_in">local</span> directory: /notebooks[I 13:16:07.347 NotebookApp] The Jupyter Notebook is running at:[I 13:16:07.347 NotebookApp] http://(7191961747da or 127.0.0.1):8888/?token=2734a3a619e376f876eb72ba562852fa79efd94a5f3f871a[I 13:16:07.347 NotebookApp] Use Control-C to stop this server and shut down all kernels (twice to skip confirmation).[C 13:16:07.347 NotebookApp]         Copy/paste this URL into your browser when you connect <span class="hljs-keyword">for</span> the first time,    to login with a token:        http://(7191961747da or 127.0.0.1):8888/?token=2734a3a619e376f876eb72ba562852fa79efd94a5f3f871a</code></pre><p>拷贝带token的URL在浏览器打开</p><pre><code class="hljs bash">http://(7191961747da or 127.0.0.1):8888/?token=257ce32bf00cd16dee9019462f8753a3b06154618885d682</code></pre><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20200601183150.png" srcset="/img/loading.gif" alt="2"></p><p>接下来，你就可以在jupyter notebook上运行你的TensorFlow代码啦</p><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20200601183151.png" srcset="/img/loading.gif" alt="2"></p><h3 id="关闭容器"><a href="#关闭容器" class="headerlink" title="关闭容器"></a>关闭容器</h3><pre><code class="hljs bash">seven@seven:~$ sudo docker stop seven-tensorflow  seven-tensorflow</code></pre><h3 id="再次打开容器"><a href="#再次打开容器" class="headerlink" title="再次打开容器"></a>再次打开容器</h3><pre><code class="hljs bash">seven@seven:~$ sudo docker start seven-tensortflow</code></pre><p>如果不喜欢用Jupyter Notebook，我们也可以创建基于命令行的容器</p><h3 id="基于命令行的TensorFlow容器"><a href="#基于命令行的TensorFlow容器" class="headerlink" title="基于命令行的TensorFlow容器"></a>基于命令行的TensorFlow容器</h3><pre><code class="hljs bash">seven@seven:~$ sudo docker run -it --name bash_tensorflow tensorflow/tensorflow /bin/bash root@cb158fab4040:/notebooks<span class="hljs-comment">#</span></code></pre><p>这样我们就创建了名为bash_tensorflow的容器， 并已经进入到容器</p><h3 id="验证TensorFlow环境"><a href="#验证TensorFlow环境" class="headerlink" title="验证TensorFlow环境"></a>验证TensorFlow环境</h3><p>默认的是python2.x安装的TensorFlowCPU版</p><pre><code class="hljs bash">root@cb158fab4040:/notebooks<span class="hljs-comment"># python</span>Python 2.7.12 (default, Dec  4 2017, 14:50:18) [GCC 5.4.0 20160609] on linux2Type <span class="hljs-string">"help"</span>, <span class="hljs-string">"copyright"</span>, <span class="hljs-string">"credits"</span> or <span class="hljs-string">"license"</span> <span class="hljs-keyword">for</span> more information.&gt;&gt;&gt; import tensorflow &gt;&gt;&gt; import tensorflow as tf&gt;&gt;&gt; <span class="hljs-built_in">print</span> tf.__version__1.11.0</code></pre>]]></content>
    
    
    <categories>
      
      <category>Docker</category>
      
    </categories>
    
    
    <tags>
      
      <tag>TensorFlow</tag>
      
      <tag>Docker</tag>
      
      <tag>CPU</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Docker安装-windows</title>
    <link href="/2018/10/docker-install-win10/"/>
    <url>/2018/10/docker-install-win10/</url>
    
    <content type="html"><![CDATA[<h3 id="windows安装Docker"><a href="#windows安装Docker" class="headerlink" title="windows安装Docker"></a>windows安装Docker</h3><p>Docker发布了Docker for Windows的正式版，于是就可以在Windows下运行Docker容器。</p><h3 id="版本要求"><a href="#版本要求" class="headerlink" title="版本要求"></a>版本要求</h3><p>要在Windows下运行Docker，需要满足以下先决条件：</p><ul><li>64位Windows 10 Pro、Enterprise或者Education版本（Build 10586以上版本，需要安装1511 November更新）</li><li>在系统中启用Hyper-V。如果没有启用，Docker for Windows在安装过程中会自动启用Hyper-V（这个过程需要重启系统）<br> 不过，如果不是使用的Windows 10，也没有关系，可以使用<a href="https://link.jianshu.com?t=http%3A%2F%2Fwww.docker.com%2Fproducts%2Fdocker-toolbox" target="_blank" rel="noopener">Docker Toolbox</a>作为替代方案。</li></ul><h3 id="启用系统的Hper-V"><a href="#启用系统的Hper-V" class="headerlink" title="启用系统的Hper-V"></a>启用系统的Hper-V</h3><p>打开<strong>程序</strong>和<strong>功能</strong>，右击<strong>开始</strong>选择<strong>应用和功能</strong>。</p><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20200601183117.png" srcset="/img/loading.gif" alt="1"></p><p>打开右边的<strong>程序</strong>和<strong>功能</strong></p><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20200601183118.png" srcset="/img/loading.gif" alt="1"></p><p>打开左边的<strong>启用或关闭windows功能</strong></p><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20200601183119.png" srcset="/img/loading.gif" alt="1"></p><p>启用<strong>Hper-V</strong>服务 </p><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20200601183120.png" srcset="/img/loading.gif" alt="1"></p><p>然后<strong>重启计算机</strong>，使服务生效</p><h4 id="Docker-for-Windows的安装"><a href="#Docker-for-Windows的安装" class="headerlink" title="Docker for Windows的安装"></a>Docker for Windows的安装</h4><p>在Windows 10中，请<a href="https://link.jianshu.com/?t=https%3A%2F%2Fdownload.docker.com%2Fwin%2Fstable%2FInstallDocker.msi" target="_blank" rel="noopener">点击此处</a>下载Docker for Windows的安装包，下载好之后双击 Docker for Windows Installer.exe 开始安装。安装好后，桌面会出现如下快捷方式，表明docker安装成功。</p><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20200601183121.png" srcset="/img/loading.gif" alt="1"></p><p>Docker CE 启动之后会在 Windows 任务栏出现鲸鱼图标。</p><p>等待片刻，点击 Got it 开始使用 Docker CE。</p><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20200601183122.png" srcset="/img/loading.gif" alt="1"></p>]]></content>
    
    
    <categories>
      
      <category>Docker</category>
      
    </categories>
    
    
    <tags>
      
      <tag>windows</tag>
      
      <tag>Docker</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Docker安装-ubuntu</title>
    <link href="/2018/10/docker-install/"/>
    <url>/2018/10/docker-install/</url>
    
    <content type="html"><![CDATA[<h3 id="ubuntu安装Docker"><a href="#ubuntu安装Docker" class="headerlink" title="ubuntu安装Docker"></a>ubuntu安装Docker</h3><p>Docker CE 支持以下版本的 <a href="https://www.ubuntu.com/server" target="_blank" rel="noopener">Ubuntu</a> 操作系统：</p><ul><li>Bionic 18.04 (LTS)</li><li>Xenial 16.04 (LTS)</li><li>Trusty 14.04 (LTS)</li></ul><h3 id="卸载旧版本Docker"><a href="#卸载旧版本Docker" class="headerlink" title="卸载旧版本Docker"></a>卸载旧版本Docker</h3><p>旧版本的 Docker 称为 <code>docker</code> 或者 <code>docker-engine</code></p><pre><code class="hljs shell"><span class="hljs-meta">$</span><span class="bash"> sudo apt-get remove docker \</span>               docker-engine \               docker.io</code></pre><h3 id="安装可选内核模块–ubuntu14-04"><a href="#安装可选内核模块–ubuntu14-04" class="headerlink" title="安装可选内核模块–ubuntu14.04"></a>安装可选内核模块–ubuntu14.04</h3><p>从 Ubuntu 14.04 开始，一部分内核模块移到了可选内核模块包 (<code>linux-modules-extra-*</code>) ，以减少内核软件包的体积。正常安装的系统应该会包含可选内核模块包，而一些裁剪后的系统可能会将其精简掉。<code>AUFS</code> 内核驱动属于可选内核模块的一部分，作为推荐的 Docker 存储层驱动，一般建议安装可选内核模块包以使用 <code>AUFS</code>。</p><p><code>linux-image-generic</code>应该已经安装了相关的<code>linux-image-extra</code>包，但名称已更改为<code>linux-modules-extra</code>。</p><h4 id="升级到最新的内核："><a href="#升级到最新的内核：" class="headerlink" title="升级到最新的内核："></a>升级到最新的内核：</h4><pre><code class="hljs shell"><span class="hljs-meta">$</span><span class="bash"> sudo apt update</span><span class="hljs-meta">$</span><span class="bash"> sudo apt upgrade</span><span class="hljs-meta">$</span><span class="bash"> sudo apt install --reinstall linux-image-generic</span></code></pre><p>如果系统没有安装可选内核模块的话，可以执行下面的命令来安装可选内核模块包：</p><pre><code class="hljs shell"><span class="hljs-meta">$</span><span class="bash"> sudo apt install linux-modules-extra-$(uname -r) linux-image-extra-virtual</span></code></pre><h3 id="ubuntu16-04"><a href="#ubuntu16-04" class="headerlink" title="ubuntu16.04+"></a>ubuntu16.04+</h3><p>Ubuntu 16.04 + 上的 Docker CE 默认使用 <code>overlay2</code> 存储层驱动,无需手动配置。</p><h3 id="使用APT安装"><a href="#使用APT安装" class="headerlink" title="使用APT安装"></a>使用APT安装</h3><p>由于 <code>apt</code> 源使用 HTTPS 以确保软件下载过程中不被篡改。因此，我们首先需要添加使用 HTTPS 传输的软件包以及 CA 证书。</p><pre><code class="hljs shell"><span class="hljs-meta">$</span><span class="bash"> sudo apt-get update</span><span class="hljs-meta">$</span><span class="bash"> sudo apt-get install \</span>    apt-transport-https \    ca-certificates \    curl \    software-properties-common</code></pre><p>鉴于国内网络问题，强烈建议使用国内源，为了确认所下载软件包的合法性，需要添加软件源的 <code>GPG</code> 密钥。</p><pre><code class="hljs shell"><span class="hljs-meta">$</span><span class="bash"> curl -fsSL https://mirrors.ustc.edu.cn/docker-ce/linux/ubuntu/gpg | sudo apt-key add -</span></code></pre><p>然后，我们需要向 <code>source.list</code> 中添加 Docker 软件源</p><pre><code class="hljs shell"><span class="hljs-meta">$</span><span class="bash"> sudo add-apt-repository \</span>    "deb [arch=amd64] https://mirrors.ustc.edu.cn/docker-ce/linux/ubuntu \    $(lsb_release -cs) \    stable"</code></pre><p>以上命令会添加稳定版本的 Docker CE APT 镜像源，如果需要测试或每日构建版本的 Docker CE 请将 stable 改为 test 或者 nightly。</p><h4 id="安装Docker-CE"><a href="#安装Docker-CE" class="headerlink" title="安装Docker CE"></a>安装Docker CE</h4><p>更新 apt 软件包缓存，并安装 <code>docker-ce</code>：</p><pre><code class="hljs shell"><span class="hljs-meta">$</span><span class="bash"> sudo apt-get update</span><span class="hljs-meta">$</span><span class="bash"> sudo apt-get install docker-ce</span></code></pre><h3 id="使用脚本自动安装"><a href="#使用脚本自动安装" class="headerlink" title="使用脚本自动安装"></a>使用脚本自动安装</h3><p>在测试或开发环境中 Docker 官方为了简化安装流程，提供了一套便捷的安装脚本，Ubuntu 系统上可以使用这套脚本安装：</p><pre><code class="hljs shell"><span class="hljs-meta">$</span><span class="bash"> curl -fsSL get.docker.com -o get-docker.sh</span><span class="hljs-meta">$</span><span class="bash"> sudo sh get-docker.sh --mirror Aliyun</span></code></pre><p>执行这个命令后，脚本就会自动的将一切准备工作做好，并且把 Docker CE 的 Edge 版本安装在系统中。</p><h3 id="启动Docker-CE"><a href="#启动Docker-CE" class="headerlink" title="启动Docker CE"></a>启动Docker CE</h3><pre><code class="hljs SHELL"><span class="hljs-meta">$</span><span class="bash"> sudo systemctl <span class="hljs-built_in">enable</span> docker</span><span class="hljs-meta">$</span><span class="bash"> sudo systemctl start docker</span></code></pre><p>Ubuntu 14.04 请使用以下命令启动：</p><pre><code class="hljs shell"><span class="hljs-meta">$</span><span class="bash"> sudo service docker start</span></code></pre><h3 id="建立-docker-用户组"><a href="#建立-docker-用户组" class="headerlink" title="建立 docker 用户组"></a>建立 docker 用户组</h3><p>默认情况下，<code>docker</code> 命令会使用 <a href="https://en.wikipedia.org/wiki/Unix_domain_socket" target="_blank" rel="noopener">Unix socket</a> 与 Docker 引擎通讯。而只有 <code>root</code> 用户和 <code>docker</code> 组的用户才可以访问 Docker 引擎的 Unix socket。出于安全考虑，一般 Linux 系统上不会直接使用 <code>root</code> 用户。因此，更好地做法是将需要使用 <code>docker</code> 的用户加入 <code>docker</code> 用户组。</p><h4 id="建立-Docker组："><a href="#建立-Docker组：" class="headerlink" title="建立 Docker组："></a>建立 Docker组：</h4><pre><code class="hljs shell"><span class="hljs-meta">$</span><span class="bash"> sudo groupadd docker</span></code></pre><h4 id="将当前用户加入-docker-组："><a href="#将当前用户加入-docker-组：" class="headerlink" title="将当前用户加入 docker 组："></a>将当前用户加入 <code>docker</code> 组：</h4><pre><code class="hljs shell"><span class="hljs-meta">$</span><span class="bash"> sudo usermod -aG docker <span class="hljs-variable">$USER</span></span></code></pre><p>退出当前终端并重新登录，进行如下测试。</p><h3 id="测试Docker是否安装正确"><a href="#测试Docker是否安装正确" class="headerlink" title="测试Docker是否安装正确"></a>测试Docker是否安装正确</h3><pre><code class="hljs shell"><span class="hljs-meta">$</span><span class="bash"> docker run hello-world</span>Unable to find image 'hello-world:latest' locallylatest: Pulling from library/hello-worldd1725b59e92d: Pull completeDigest: sha256:0add3ace90ecb4adbf7777e9aacf18357296e799f81cabc9fde470971e499788Status: Downloaded newer image for hello-world:latestHello from Docker!This message shows that your installation appears to be working correctly.To generate this message, Docker took the following steps: 1. The Docker client contacted the Docker daemon. 2. The Docker daemon pulled the "hello-world" image from the Docker Hub.    (amd64) 3. The Docker daemon created a new container from that image which runs the    executable that produces the output you are currently reading. 4. The Docker daemon streamed that output to the Docker client, which sent it    to your terminal.To try something more ambitious, you can run an Ubuntu container with:<span class="hljs-meta"> $</span><span class="bash"> docker run -it ubuntu bash</span>Share images, automate workflows, and more with a free Docker ID: https://hub.docker.com/For more examples and ideas, visit: https://docs.docker.com/get-started/</code></pre><p>若能正常输出以上信息，则说明安装成功。</p>]]></content>
    
    
    <categories>
      
      <category>Docker</category>
      
    </categories>
    
    
    <tags>
      
      <tag>ubuntu</tag>
      
      <tag>Docker</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Docker镜像管理</title>
    <link href="/2018/10/docker-source/"/>
    <url>/2018/10/docker-source/</url>
    
    <content type="html"><![CDATA[<h3 id="Docker镜像管理"><a href="#Docker镜像管理" class="headerlink" title="Docker镜像管理"></a>Docker镜像管理</h3><p>Docker镜像是一个不包含Linux内核而又精简的Linux的操作系统。</p><h3 id="Dock镜像下载"><a href="#Dock镜像下载" class="headerlink" title="Dock镜像下载"></a>Dock镜像下载</h3><p>Docker Hub 是由Docker公司负责维护的公共注册中心，包含大量的容器镜像，Docker工具默认从这个公共镜像库下载镜像：<a href="https://hub.docker.com/explore" target="_blank" rel="noopener">https://hub.docker.com/explore</a></p><p>默认是国外的源，下载会很慢，可以使用国内的源提供下载速度：参考上一节<a href="">镜像加速器</a></p><h3 id="镜像的工作原理"><a href="#镜像的工作原理" class="headerlink" title="镜像的工作原理"></a>镜像的工作原理</h3><p>当我们启动一个新的容器时，Docker会加载只读镜像，并在其之上添加一个读写层，并将镜像中的目录复制一份到/var/lib/docker/aufs/mnt/容器ID为目录下，我们可以使用chroot进入此目录。</p><p>如果运行中的容器修改一个已经存在的文件，那么会将该文件从下面的只读层复制到读写层，只读层的这个文件就会覆盖，但还存在，这就实现了文件系统隔离，当删除容器后，读写层的数据将会删除，只读镜像不变。</p><h3 id="镜像的文件存储结构"><a href="#镜像的文件存储结构" class="headerlink" title="镜像的文件存储结构"></a>镜像的文件存储结构</h3><p>docker相关文件存放在：/var/lib/docker目录下</p><pre><code class="hljs bash">/var/lib/docker/aufs/diff <span class="hljs-comment"># 每层与其父层之间的文件差异</span>/var/lib/docker/aufs/layers/ <span class="hljs-comment"># 每层一个文件，记录其父层一直到根层之间的ID，大部分文件的最后一行都已，表示继承来自同一层</span>/var/lib/docker/aufs/mnt <span class="hljs-comment"># 联合挂载点，从只读层复制到最上层可读写层的文件系统数据</span></code></pre><p>在建立镜像时，每次写操作，都被视作一种增量操作，即在原有的数据层上添加一个新层；所以一个镜像会有若干个层组成。每次commit提交就会对产生一个ID，就相当于在上一层有加了一层，可以通过这个ID对镜像回滚。</p><h3 id="镜像管理命令"><a href="#镜像管理命令" class="headerlink" title="镜像管理命令"></a>镜像管理命令</h3><p>列举一些常用的Docker镜像管理命令，完整内容请参考<a href="https://docs.docker.com/get-started/#docker-concepts" target="_blank" rel="noopener">官方文档</a></p><h3 id="Docker-search-命令"><a href="#Docker-search-命令" class="headerlink" title="Docker search 命令"></a>Docker search 命令</h3><p><strong>docker search :</strong> 从Docker Hub查找镜像</p><h4 id="语法："><a href="#语法：" class="headerlink" title="语法："></a>语法：</h4><pre><code class="hljs bash">docker search [OPTIONS] TERM</code></pre><blockquote><p>OPTIONS说明：</p><p><strong>–automated :</strong>只列出 automated build类型的镜像；</p><p><strong>–no-trunc :</strong>显示完整的镜像描述；</p><p>*<em>-s : *</em> 列出star数不小于指定值的镜像。</p></blockquote><h4 id="实例"><a href="#实例" class="headerlink" title="实例"></a>实例</h4><p>从Docker Hub 查找所有镜像包含MySQL， 并且star大于100</p><pre><code class="hljs bash">seven@seven:~$ sudo docker search mysql -s 100Flag --stars has been deprecated, use --filter=stars=3 insteadNAME                         DESCRIPTION                                     STARS               OFFICIAL            AUTOMATEDmysql                        MySQL is a widely used, open-source relation…   7213                [OK]                mariadb                      MariaDB is a community-developed fork of MyS…   2311                [OK]                mysql/mysql-server           Optimized MySQL Server Docker images. Create…   531                                     [OK]percona                      Percona Server is a fork of the MySQL relati…   382                 [OK]                zabbix/zabbix-server-mysql   Zabbix Server with MySQL database support       136                                     [OK]</code></pre><h3 id="Docker-pull-命令"><a href="#Docker-pull-命令" class="headerlink" title="Docker pull 命令"></a>Docker pull 命令</h3><p><strong>docker pull :</strong> 从镜像仓库中拉取或者更新指定镜像</p><h4 id="语法：-1"><a href="#语法：-1" class="headerlink" title="语法："></a>语法：</h4><pre><code class="hljs basn">docker pull [OPTIONS] NAME[:TAG|@DIGEST]</code></pre><blockquote><p>OPTIONS说明：</p><p><strong>-a :</strong>拉取所有 tagged 镜像</p><p><strong>–disable-content-trust :</strong>忽略镜像的校验,默认开启</p></blockquote><h4 id="实例："><a href="#实例：" class="headerlink" title="实例："></a>实例：</h4><p>从Docker Hub下载mysql。</p><pre><code class="hljs bash">seven@seven:~$ sudo docker pull mysqlUsing default tag: latestlatest: Pulling from library/mysqlf17d81b4b692: Pull complete c691115e6ae9: Pull complete 41544cb19235: Pull complete 254d04f5f66d: Pull complete 4fe240edfdc9: Pull complete 0cd4fcc94b67: Pull complete 8df36ec4b34a: Pull complete 720bf9851f6a: Pull complete e933e0a4fddf: Pull complete 9ffdbf5f677f: Pull complete a403e1df0389: Pull complete 4669c5f285a6: Pull complete Digest: sha256:811483efcd38de17d93193b4b4bc4ba290a931215c4c8512cbff624e5967a7ddStatus: Downloaded newer image <span class="hljs-keyword">for</span> mysql:latest</code></pre><h3 id="Docker-push-命令"><a href="#Docker-push-命令" class="headerlink" title="Docker push 命令"></a>Docker push 命令</h3><p><strong>docker push :</strong> 将本地的镜像上传到镜像仓库,要先登陆到镜像仓库</p><h4 id="语法：-2"><a href="#语法：-2" class="headerlink" title="语法："></a>语法：</h4><pre><code class="hljs bash">docker push [OPTIONS] NAME[:TAG]</code></pre><blockquote><p>OPTIONS说明：</p><p><strong>–disable-content-trust :</strong>忽略镜像的校验,默认开启</p></blockquote><h3 id="Docker-images-命令"><a href="#Docker-images-命令" class="headerlink" title="Docker images 命令"></a>Docker images 命令</h3><p>Docker images: 查看本机的镜像</p><h4 id="语法：-3"><a href="#语法：-3" class="headerlink" title="语法："></a>语法：</h4><pre><code class="hljs inform7">docker images <span class="hljs-comment">[OPTIONS]</span> <span class="hljs-comment">[REPOSITORY<span class="hljs-comment">[:TAG]</span>]</span></code></pre><blockquote><p>OPTIONS说明：</p><p><strong>-a :</strong>列出本地所有的镜像（含中间映像层，默认情况下，过滤掉中间映像层）；</p><p><strong>–digests :</strong>显示镜像的摘要信息；</p><p><strong>-f :</strong>显示满足条件的镜像；</p><p><strong>–format :</strong>指定返回值的模板文件；</p><p><strong>–no-trunc :</strong>显示完整的镜像信息；</p><p><strong>-q :</strong>只显示镜像ID。</p></blockquote><h3 id="实例：-1"><a href="#实例：-1" class="headerlink" title="实例："></a>实例：</h3><p>查看本地镜像列表</p><pre><code class="hljs bash">seven@seven:~$ sudo docker images REPOSITORY          TAG                 IMAGE ID            CREATED             SIZEmysql               latest              2dd01afbe8df        4 days ago          485MBubuntu              latest              ea4c82dcd15a        10 days ago         85.8MBhello-world         latest              4ab4c602aa5e        7 weeks ago         1.84kB</code></pre><h3 id="Docker-rmi-命令"><a href="#Docker-rmi-命令" class="headerlink" title="Docker rmi 命令"></a>Docker rmi 命令</h3><p>*<em>docker rmi: *</em>删除本地一个或多少镜像。</p><h4 id="语法：-4"><a href="#语法：-4" class="headerlink" title="语法："></a>语法：</h4><pre><code class="hljs bash">docker rmi [OPTIONS] IMAGE [IMAGE...]</code></pre><blockquote><p>OPTIONS说明：</p><p><strong>-f :</strong>强制删除；</p><p><strong>–no-prune :</strong>不移除该镜像的过程镜像，默认移除；</p></blockquote><h3 id="Docker-export-命令"><a href="#Docker-export-命令" class="headerlink" title="Docker export 命令"></a>Docker export 命令</h3><p><strong>docker export :</strong>将文件系统作为一个tar归档文件导出到STDOUT。</p><h3 id="语法"><a href="#语法" class="headerlink" title="语法"></a>语法</h3><pre><code class="hljs bash">docker <span class="hljs-built_in">export</span> [OPTIONS] CONTAINER</code></pre><blockquote><p> OPTIONS说明：</p><p><strong>-o :</strong>将输入内容写到文件。</p></blockquote><h3 id="Docker-import-命令"><a href="#Docker-import-命令" class="headerlink" title="Docker import 命令"></a>Docker import 命令</h3><p><strong>docker import :</strong> 从归档文件中创建镜像。</p><p>语法</p><pre><code class="hljs gradle">docker <span class="hljs-keyword">import</span> [<span class="hljs-keyword">OPTIONS</span>] <span class="hljs-keyword">file</span>|URL|- [REPOSITORY[:TAG]]</code></pre><blockquote><p>OPTIONS说明：</p><p><strong>-c :</strong>应用docker 指令创建镜像；</p><p><strong>-m :</strong>提交时的说明文字；</p></blockquote><h3 id="Docker-save-命令"><a href="#Docker-save-命令" class="headerlink" title="Docker save 命令"></a>Docker save 命令</h3><p><strong>docker save :</strong> 将指定镜像保存成 tar 归档文件。</p><h4 id="语法-1"><a href="#语法-1" class="headerlink" title="语法"></a>语法</h4><pre><code class="hljs css"><span class="hljs-selector-tag">docker</span> <span class="hljs-selector-tag">save</span> <span class="hljs-selector-attr">[OPTIONS]</span> <span class="hljs-selector-tag">IMAGE</span> <span class="hljs-selector-attr">[IMAGE...]</span></code></pre><blockquote><p>OPTIONS说明：</p><p><strong>-o :</strong>输出到的文件。</p></blockquote><h3 id="Docker-load命令"><a href="#Docker-load命令" class="headerlink" title="Docker load命令"></a>Docker load命令</h3><p>docker load:从归档文件中导入镜像。</p><h4 id="语法：-5"><a href="#语法：-5" class="headerlink" title="语法："></a>语法：</h4><pre><code class="hljs bash">docker load [OPTIONS]</code></pre><blockquote><p>Options:<br>  -i: 从tar存档文件读取的输入字符串，而不是STDIN<br>  -q:  Suppress the load output</p></blockquote>]]></content>
    
    
    <categories>
      
      <category>Docker</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Docker</tag>
      
      <tag>镜像</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Docker镜像加速器</title>
    <link href="/2018/10/docker-speed/"/>
    <url>/2018/10/docker-speed/</url>
    
    <content type="html"><![CDATA[<h3 id="镜像加速器"><a href="#镜像加速器" class="headerlink" title="镜像加速器"></a>镜像加速器</h3><p>国内从 Docker Hub 拉取镜像有时会遇到困难，此时可以配置镜像加速器。Docker 官方和国内很多云服务商都提供了国内加速器服务，例如：</p><ul><li><a href="https://docs.docker.com/registry/recipes/mirror/#use-case-the-china-registry-mirror" target="_blank" rel="noopener">Docker 官方提供的中国 registry mirror <code>https://registry.docker-cn.com</code></a></li><li><a href="https://kirk-enterprise.github.io/hub-docs/#/user-guide/mirror" target="_blank" rel="noopener">七牛云加速器 <code>https://reg-mirror.qiniu.com/</code></a></li></ul><blockquote><p>当配置某一个加速器地址之后，若发现拉取不到镜像，请切换到另一个加速器地址。</p><p>国内各大云服务商均提供了 Docker 镜像加速服务，建议根据运行 Docker 的云平台选择对应的镜像加速服务。</p></blockquote><p>我们以 Docker 官方加速器 <code>https://registry.docker-cn.com</code> 为例进行介绍。</p><h3 id="Ubuntu-14-04、Debian-7-Wheezy"><a href="#Ubuntu-14-04、Debian-7-Wheezy" class="headerlink" title="Ubuntu 14.04、Debian 7 Wheezy"></a>Ubuntu 14.04、Debian 7 Wheezy</h3><p>对于使用 <a href="http://upstart.ubuntu.com/" target="_blank" rel="noopener">upstart</a> 的系统而言，编辑 <code>/etc/default/docker</code> 文件，在其中的 <code>DOCKER_OPTS</code> 中配置加速器地址：</p><pre><code class="hljs bash">DOCKER_OPTS=<span class="hljs-string">"--registry-mirror=https://registry.docker-cn.com"</span></code></pre><p>重新启动服务。</p><pre><code class="hljs bash">$ sudo service docker restart</code></pre><h3 id="Ubuntu-16-04-、Debian-8-、CentOS-7"><a href="#Ubuntu-16-04-、Debian-8-、CentOS-7" class="headerlink" title="Ubuntu 16.04+、Debian 8+、CentOS 7"></a>Ubuntu 16.04+、Debian 8+、CentOS 7</h3><p>对于使用 <a href="https://www.freedesktop.org/wiki/Software/systemd/" target="_blank" rel="noopener">systemd</a> 的系统，请在 <code>/etc/docker/daemon.json</code> 中写入如下内容（如果文件不存在请新建该文件）</p><pre><code class="hljs json">&#123;  <span class="hljs-attr">"registry-mirrors"</span>: [    <span class="hljs-string">"https://registry.docker-cn.com"</span>  ]&#125;</code></pre><blockquote><p>注意，一定要保证该文件符合 json 规范，否则 Docker 将不能启动。</p></blockquote><p>之后重新启动服务。</p><pre><code class="hljs bash">$ sudo systemctl daemon-reload$ sudo systemctl restart docker</code></pre><blockquote><p>注意：如果您之前查看旧教程，修改了 <code>docker.service</code> 文件内容，请去掉您添加的内容（<code>--registry-mirror=https://registry.docker-cn.com</code>），这里不再赘述。</p></blockquote><h3 id="Windows-10"><a href="#Windows-10" class="headerlink" title="Windows 10"></a>Windows 10</h3><p>对于使用 Windows 10 的系统，在系统右下角托盘 Docker 图标内右键菜单选择 <code>Settings</code>，打开配置窗口后左侧导航菜单选择 <code>Daemon</code>。在 <code>Registry mirrors</code> 一栏中填写加速器地址 <code>https://registry.docker-cn.com</code>，之后点击 <code>Apply</code> 保存后 Docker 就会重启并应用配置的镜像地址了。</p><h3 id="macOS"><a href="#macOS" class="headerlink" title="macOS"></a>macOS</h3><p>对于使用 macOS 的用户，在任务栏点击 Docker for mac 应用图标 -&gt; Perferences… -&gt; Daemon -&gt; Registry mirrors。在列表中填写加速器地址 <code>https://registry.docker-cn.com</code>。修改完成之后，点击 <code>Apply &amp; Restart</code> 按钮，Docker 就会重启并应用配置的镜像地址了。</p><h3 id="检查加速器是否生效"><a href="#检查加速器是否生效" class="headerlink" title="检查加速器是否生效"></a>检查加速器是否生效</h3><p>配置加速器之后，如果拉取镜像仍然十分缓慢，请手动检查加速器配置是否生效，在命令行执行</p><pre><code class="hljs bash">$  docker info</code></pre><p>如果从结果中看到了如下内容，说明配置成功。</p><pre><code class="hljs bash">Registry Mirrors: https://registry.docker-cn.com/</code></pre>]]></content>
    
    
    <categories>
      
      <category>Docker</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Docker</tag>
      
      <tag>镜像</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Docker容器管理</title>
    <link href="/2018/10/docker-vessel/"/>
    <url>/2018/10/docker-vessel/</url>
    
    <content type="html"><![CDATA[<h3 id="启动容器"><a href="#启动容器" class="headerlink" title="启动容器"></a>启动容器</h3><p>启动容器有两种方式，一种是基于镜像新建一个容器并启动，另外一个是将在终止状态（<code>stopped</code>）的容器重新启动。</p><h3 id="新建并启动"><a href="#新建并启动" class="headerlink" title="新建并启动"></a>新建并启动</h3><p>所需要的命令主要为 <code>docker run</code>。</p><pre><code class="hljs bash">seven@seven:~$ sudo docker run -it ubunturoot@c964215eb3c2:/<span class="hljs-comment">#</span></code></pre><p>其中，<code>-t</code> 选项让Docker分配一个伪终端（pseudo-tty）并绑定到容器的标准输入上， <code>-i</code> 则让容器的标准输入保持打开。</p><p>在交互模式下，用户可以通过所创建的终端来输入命令，例如</p><pre><code class="hljs bash">root@c964215eb3c2:/<span class="hljs-comment"># ls</span>bin  boot  dev  etc  home  lib  lib64  media  mnt  opt  proc  root  run  sbin  srv  sys  tmp  usr  varroot@c964215eb3c2:/<span class="hljs-comment"># dir</span>bin  boot  devetc  home  liblib64  media  mnt  optproc  root  run  sbin  srv  sys  tmp  usr  varroot@c964215eb3c2:/<span class="hljs-comment"># pwd</span>/</code></pre><p>当利用 <code>docker run</code> 来创建容器时，Docker 在后台运行的标准操作包括：</p><ul><li>检查本地是否存在指定的镜像，不存在就从公有仓库下载</li><li>利用镜像创建并启动一个容器</li><li>分配一个文件系统，并在只读的镜像层外面挂载一层可读写层</li><li>从宿主主机配置的网桥接口中桥接一个虚拟接口到容器中去</li><li>从地址池配置一个 ip 地址给容器</li><li>执行用户指定的应用程序</li><li>执行完毕后容器被终止</li></ul><h3 id="启动已终止容器"><a href="#启动已终止容器" class="headerlink" title="启动已终止容器"></a>启动已终止容器</h3><p>所需要的命令主要为：<code>docker container start</code></p><pre><code class="hljs bash">seven@seven:~$ sudo docker container start -i  trusting_boydroot@c964215eb3c2:/<span class="hljs-comment">#</span></code></pre><p>其中： <code>-i</code> 则让容器的标准输入保持打开。</p><h3 id="终止容器"><a href="#终止容器" class="headerlink" title="终止容器"></a>终止容器</h3><p>可以使用 <code>docker container stop</code> 来终止一个运行中的容器。</p><p>此外，当 Docker 容器中指定的应用终结时，容器也自动终止。</p><p>当只启动了一个终端的容器，用户通过 <code>exit</code> 命令或 <code>Ctrl+d</code> 来退出终端时，所创建的容器立刻终止。</p><p>终止状态的容器可以用 <code>docker container ls -a</code> 命令看到。例如</p><pre><code class="hljs bash">seven@seven:~$ sudo docker container ls -aCONTAINER ID        IMAGE               COMMAND             CREATED             STATUS                          PORTS               NAMESc964215eb3c2        ubuntu              <span class="hljs-string">"/bin/bash"</span>         12 minutes ago      Exited (0) About a minute ago                       trusting_boyd</code></pre><p>处于终止状态的容器，可以通过 <code>docker container start</code> 命令来重新启动。</p><p>此外，<code>docker container restart</code> 命令会将一个运行态的容器终止，然后再重新启动它。</p><h3 id="进入容器"><a href="#进入容器" class="headerlink" title="进入容器"></a>进入容器</h3><p>在使用 <code>-d</code> 参数时，容器启动后会进入后台。</p><p>某些时候需要进入容器进行操作，包括使用 <code>docker attach</code> 命令或 <code>docker exec</code> 命令，推荐大家使用 <code>docker exec</code> 命令，原因会在下面说明。</p><h3 id="attach-命令"><a href="#attach-命令" class="headerlink" title="attach 命令"></a><code>attach</code> 命令</h3><p><code>docker attach</code> 是 Docker 自带的命令。下面示例如何使用该命令。</p><pre><code class="hljs bash">seven@seven:~$ sudo docker run -dit ubuntuaaa535c389754179db273c48bafbe8b2c514cb0699f5e77b1249394715965f5fseven@seven:~$ sudo docker container lsCONTAINER ID        IMAGE               COMMAND             CREATED             STATUS              PORTS               NAMESaaa535c38975        ubuntu              <span class="hljs-string">"/bin/bash"</span>         19 seconds ago      Up 17 seconds                           vigilant_coriseven@seven:~$ sudo docker attach vigilant_coriroot@aaa535c38975:/<span class="hljs-comment">#</span></code></pre><p>或者进入我们刚创建那个容器：</p><pre><code class="hljs bash">seven@seven:~$ sudo docker container start trusting_boydtrusting_boydseven@seven:~$ sudo docker attach trusting_boydroot@c964215eb3c2:/<span class="hljs-comment">#</span></code></pre><h3 id="exec-命令"><a href="#exec-命令" class="headerlink" title="exec 命令"></a><code>exec</code> 命令</h3><h4 id="i-t-参数"><a href="#i-t-参数" class="headerlink" title="-i -t 参数"></a>-i -t 参数</h4><p><code>docker exec</code> 后边可以跟多个参数，这里主要说明 <code>-i</code> <code>-t</code> 参数。</p><p>只用 <code>-i</code> 参数时，由于没有分配伪终端，界面没有我们熟悉的 Linux 命令提示符，但命令执行结果仍然可以返回。</p><p>当 <code>-i</code> <code>-t</code> 参数一起使用时，则可以看到我们熟悉的 Linux 命令提示符。</p><pre><code class="hljs bash">seven@seven:~$ sudo docker <span class="hljs-built_in">exec</span> -i trusting_boyd  ls binbootdevetchomeliblib64mediamntoptprocrootrunsbinsrvsystmpusrvar</code></pre><h3 id="导出容器"><a href="#导出容器" class="headerlink" title="导出容器"></a>导出容器</h3><p>如果要导出本地某个容器，可以使用 <code>docker export</code> 命令。</p><pre><code class="hljs bash">seven@seven:~/demo$ sudo docker <span class="hljs-built_in">export</span> trusting_boyd &gt; ubuntu.tarseven@seven:~/demo$ lsubuntu.tar</code></pre><p>这样将导出容器快照到本地文件。</p><h3 id="导入容器"><a href="#导入容器" class="headerlink" title="导入容器"></a>导入容器</h3><p>可以使用 <code>docker import</code> 从容器快照文件中再导入为镜像，例如</p><pre><code class="hljs bash">seven@seven:~/demo$ cat ubuntu.tar | sudo docker import - <span class="hljs-built_in">test</span>/ubuntu:v1.0sha256:2d78a6138d933a64fef619687dfb31f4323f607e35c980d1e70ec5a79a7cb39fseven@seven:~/demo$ sudo docker images REPOSITORY          TAG                 IMAGE ID            CREATED              SIZE<span class="hljs-built_in">test</span>/ubuntu         v1.0                2d78a6138d93        About a minute ago   69.8MBmysql               latest              2dd01afbe8df        4 days ago           485MBubuntu              latest              ea4c82dcd15a        10 days ago          85.8MBhello-world         latest              4ab4c602aa5e        7 weeks ago          1.84kB</code></pre><p>此外，也可以通过指定 URL 或者某个目录来导入，例如</p><pre><code class="hljs bash">$ docker import http://example.com/exampleimage.tgz example/imagerepo</code></pre><p><em>注：用户既可以使用 docker load 来导入镜像存储文件到本地镜像库，也可以使用 docker import 来导入一个容器快照到本地镜像库。这两者的区别在于容器快照文件将丢弃所有的历史记录和元数据信息（即仅保存容器当时的快照状态），而镜像存储文件将保存完整记录，体积也要大。此外，从容器快照文件导入时可以重新指定标签等元数据信息。</em></p><h3 id="删除容器"><a href="#删除容器" class="headerlink" title="删除容器"></a>删除容器</h3><p>可以使用 <code>docker container rm</code> 来删除一个处于终止状态的容器。例如</p><pre><code class="hljs bash">$ docker container rm  trusting_newtontrusting_newton</code></pre><p>如果要删除一个运行中的容器，可以添加 <code>-f</code> 参数。Docker 会发送 <code>SIGKILL</code> 信号给容器。</p><h3 id="清理所有容器"><a href="#清理所有容器" class="headerlink" title="清理所有容器"></a>清理所有容器</h3><pre><code class="hljs bash">seven@seven:~/demo$  sudo docker rm -f $(sudo docker ps -q -a)aaa535c38975c964215eb3c2</code></pre><h3 id="容器安装软件"><a href="#容器安装软件" class="headerlink" title="容器安装软件"></a>容器安装软件</h3><p>就比如安装vim软件， 在使用docker容器时，有时候里边没有安装vim，敲vim命令时提示说：vim: command not found，这个时候就需要安装vim：</p><pre><code class="hljs bash">$ apt-get install vim Reading package lists... Done Building dependency tree        Reading state information... Done E: Unable to locate package vim</code></pre><p>解决这个问题的方法：执行更新命令</p><pre><code class="hljs bash">$ apt-get update</code></pre><p>这个命令的作用是：<strong>同步 /etc/apt/sources.list 和 /etc/apt/sources.list.d 中列出的源的索引</strong>，这样才能获取到最新的软件包。</p><p>等更新完毕以后再敲命令：</p><pre><code class="hljs bash">root@4c7345f38eab:~/.pip<span class="hljs-comment"># apt-get install vim</span>Reading package lists... DoneBuilding dependency tree       Reading state information... DoneThe following additional packages will be installed:  libgpm2 vim-common vim-runtime xxdSuggested packages:  gpm ctags vim-doc vim-scriptsThe following NEW packages will be installed:  libgpm2 vim vim-common vim-runtime xxd0 upgraded, 5 newly installed, 0 to remove and 1 not upgraded.Need to get 6766 kB of archives.After this operation, 31.2 MB of additional disk space will be used.Do you want to <span class="hljs-built_in">continue</span>? [Y/n] y...</code></pre><p>接下来你就可以在docker容器里面，安装你所需要的软件了。</p>]]></content>
    
    
    <categories>
      
      <category>Docker</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Docker</tag>
      
      <tag>容器</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>计算机视觉-openCV：基于Haar+Adaboost人脸识别</title>
    <link href="/2018/10/openCV-face/"/>
    <url>/2018/10/openCV-face/</url>
    
    <content type="html"><![CDATA[<h3 id="openCV基于Haar-Adaboost人脸识别"><a href="#openCV基于Haar-Adaboost人脸识别" class="headerlink" title="openCV基于Haar+Adaboost人脸识别"></a>openCV基于Haar+Adaboost人脸识别</h3><pre><code class="hljs python"><span class="hljs-keyword">import</span> cv2<span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<span class="hljs-comment"># load xml 1 file name</span>face_xml = cv2.CascadeClassifier(<span class="hljs-string">'haarcascade_frontalface_default.xml'</span>)eye_xml = cv2.CascadeClassifier(<span class="hljs-string">'haarcascade_eye.xml'</span>)<span class="hljs-comment"># load jpg</span>img = cv2.imread(<span class="hljs-string">'face.jpg'</span>)cv2.imshow(<span class="hljs-string">'src'</span>,img)<span class="hljs-comment"># haar gray</span>gray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)<span class="hljs-comment"># detect faces 1 data 2 scale 3 5</span>faces = face_xml.detectMultiScale(gray,<span class="hljs-number">1.3</span>,<span class="hljs-number">5</span>)print(<span class="hljs-string">'face='</span>,len(faces))<span class="hljs-comment"># draw</span><span class="hljs-keyword">for</span> (x,y,w,h) <span class="hljs-keyword">in</span> faces:    cv2.rectangle(img,(x,y),(x+w,y+h),(<span class="hljs-number">255</span>,<span class="hljs-number">0</span>,<span class="hljs-number">0</span>),<span class="hljs-number">2</span>)    roi_face = gray[y:y+h,x:x+w]    roi_color = img[y:y+h,x:x+w]    <span class="hljs-comment"># 1 gray</span>    eyes = eye_xml.detectMultiScale(roi_face)    print(<span class="hljs-string">'eye='</span>,len(eyes))    <span class="hljs-keyword">for</span> (e_x,e_y,e_w,e_h) <span class="hljs-keyword">in</span> eyes:        cv2.rectangle(roi_color,(e_x,e_y),(e_x+e_w,e_y+e_h),(<span class="hljs-number">0</span>,<span class="hljs-number">255</span>,<span class="hljs-number">0</span>),<span class="hljs-number">2</span>)cv2.imshow(<span class="hljs-string">'dst'</span>,img)</code></pre><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20200601183011.png" srcset="/img/loading.gif" alt="1"></p><p>ps: 需要xml文件的，可留言邮箱</p>]]></content>
    
    
    <categories>
      
      <category>opencv</category>
      
    </categories>
    
    
    <tags>
      
      <tag>计算机视觉</tag>
      
      <tag>opencv</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>计算机视觉-openCV：基于Hog+SVM小狮子识别</title>
    <link href="/2018/10/openCV-svm-hog/"/>
    <url>/2018/10/openCV-svm-hog/</url>
    
    <content type="html"><![CDATA[<h3 id="openCV：基于Hog-SVM小狮子识别"><a href="#openCV：基于Hog-SVM小狮子识别" class="headerlink" title="openCV：基于Hog+SVM小狮子识别"></a>openCV：基于Hog+SVM小狮子识别</h3><pre><code class="hljs python"><span class="hljs-keyword">import</span> cv2<span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt<span class="hljs-comment"># 1 par</span>PosNum = <span class="hljs-number">820</span>NegNum = <span class="hljs-number">1931</span>winSize = (<span class="hljs-number">64</span>,<span class="hljs-number">128</span>)blockSize = (<span class="hljs-number">16</span>,<span class="hljs-number">16</span>)<span class="hljs-comment"># 105</span>blockStride = (<span class="hljs-number">8</span>,<span class="hljs-number">8</span>)<span class="hljs-comment">#4 cell</span>cellSize = (<span class="hljs-number">8</span>,<span class="hljs-number">8</span>)nBin = <span class="hljs-number">9</span><span class="hljs-comment">#9 bin 3780</span><span class="hljs-comment"># 2 hog create hog 1 win 2 block 3 blockStride 4 cell 5 bin</span>hog = cv2.HOGDescriptor(winSize,blockSize,blockStride,cellSize,nBin)<span class="hljs-comment"># 3 svm</span>svm = cv2.ml.SVM_create()<span class="hljs-comment"># 4 computer hog</span>featureNum = int(((<span class="hljs-number">128</span><span class="hljs-number">-16</span>)/<span class="hljs-number">8</span>+<span class="hljs-number">1</span>)*((<span class="hljs-number">64</span><span class="hljs-number">-16</span>)/<span class="hljs-number">8</span>+<span class="hljs-number">1</span>)*<span class="hljs-number">4</span>*<span class="hljs-number">9</span>) <span class="hljs-comment">#3780</span>featureArray = np.zeros(((PosNum+NegNum),featureNum),np.float32)labelArray = np.zeros(((PosNum+NegNum),<span class="hljs-number">1</span>),np.int32)<span class="hljs-comment"># svm 监督学习 样本 标签 svm -》image hog  </span><span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> range(<span class="hljs-number">0</span>,PosNum):    fileName = <span class="hljs-string">'pos/'</span>+str(i+<span class="hljs-number">1</span>)+<span class="hljs-string">'.jpg'</span>    img = cv2.imread(fileName)    hist = hog.compute(img,(<span class="hljs-number">8</span>,<span class="hljs-number">8</span>))<span class="hljs-comment"># 3780</span>    <span class="hljs-keyword">for</span> j <span class="hljs-keyword">in</span> range(<span class="hljs-number">0</span>,featureNum):        featureArray[i,j] = hist[j]    <span class="hljs-comment"># featureArray hog [1,:] hog1 [2,:]hog2 </span>    labelArray[i,<span class="hljs-number">0</span>] = <span class="hljs-number">1</span>    <span class="hljs-comment"># 正样本 label 1</span>    <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> range(<span class="hljs-number">0</span>,NegNum):    fileName = <span class="hljs-string">'neg/'</span>+str(i+<span class="hljs-number">1</span>)+<span class="hljs-string">'.jpg'</span>    img = cv2.imread(fileName)    hist = hog.compute(img,(<span class="hljs-number">8</span>,<span class="hljs-number">8</span>))<span class="hljs-comment"># 3780</span>    <span class="hljs-keyword">for</span> j <span class="hljs-keyword">in</span> range(<span class="hljs-number">0</span>,featureNum):        featureArray[i+PosNum,j] = hist[j]    labelArray[i+PosNum,<span class="hljs-number">0</span>] = <span class="hljs-number">-1</span><span class="hljs-comment"># 负样本 label -1</span>svm.setType(cv2.ml.SVM_C_SVC)svm.setKernel(cv2.ml.SVM_LINEAR)svm.setC(<span class="hljs-number">0.01</span>)<span class="hljs-comment"># 6 train</span>ret = svm.train(featureArray,cv2.ml.ROW_SAMPLE,labelArray)<span class="hljs-comment"># 7 myHog ：《-myDetect</span><span class="hljs-comment"># myDetect-《resultArray  rho</span><span class="hljs-comment"># myHog-》detectMultiScale</span><span class="hljs-comment"># 7 检测  核心：create Hog -》 myDetect—》array-》</span><span class="hljs-comment"># resultArray-》resultArray = -1*alphaArray*supportVArray</span><span class="hljs-comment"># rho-》svm-〉svm.train</span>alpha = np.zeros((<span class="hljs-number">1</span>),np.float32)rho = svm.getDecisionFunction(<span class="hljs-number">0</span>,alpha)print(rho)print(alpha)alphaArray = np.zeros((<span class="hljs-number">1</span>,<span class="hljs-number">1</span>),np.float32)supportVArray = np.zeros((<span class="hljs-number">1</span>,featureNum),np.float32)resultArray = np.zeros((<span class="hljs-number">1</span>,featureNum),np.float32)alphaArray[<span class="hljs-number">0</span>,<span class="hljs-number">0</span>] = alpharesultArray = <span class="hljs-number">-1</span>*alphaArray*supportVArray<span class="hljs-comment"># detect</span>myDetect = np.zeros((<span class="hljs-number">3781</span>),np.float32)<span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> range(<span class="hljs-number">0</span>,<span class="hljs-number">3780</span>):    myDetect[i] = resultArray[<span class="hljs-number">0</span>,i]myDetect[<span class="hljs-number">3780</span>] = rho[<span class="hljs-number">0</span>]<span class="hljs-comment"># rho svm （判决）</span>myHog = cv2.HOGDescriptor()myHog.setSVMDetector(myDetect)<span class="hljs-comment"># load </span>imageSrc = cv2.imread(<span class="hljs-string">'Test2.jpg'</span>,<span class="hljs-number">1</span>)<span class="hljs-comment"># (8,8) win </span>objs = myHog.detectMultiScale(imageSrc,<span class="hljs-number">0</span>,(<span class="hljs-number">8</span>,<span class="hljs-number">8</span>),(<span class="hljs-number">32</span>,<span class="hljs-number">32</span>),<span class="hljs-number">1.05</span>,<span class="hljs-number">2</span>)<span class="hljs-comment"># xy wh 三维 最后一维</span>x = int(objs[<span class="hljs-number">0</span>][<span class="hljs-number">0</span>][<span class="hljs-number">0</span>])y = int(objs[<span class="hljs-number">0</span>][<span class="hljs-number">0</span>][<span class="hljs-number">1</span>])w = int(objs[<span class="hljs-number">0</span>][<span class="hljs-number">0</span>][<span class="hljs-number">2</span>])h = int(objs[<span class="hljs-number">0</span>][<span class="hljs-number">0</span>][<span class="hljs-number">3</span>])<span class="hljs-comment"># 绘制展示</span>cv2.rectangle(imageSrc,(x,y),(x+w,y+h),(<span class="hljs-number">255</span>,<span class="hljs-number">0</span>,<span class="hljs-number">0</span>),<span class="hljs-number">2</span>)cv2.imshow(<span class="hljs-string">'dst'</span>,imageSrc)</code></pre><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20200601183028.png" srcset="/img/loading.gif" alt="1"></p>]]></content>
    
    
    <categories>
      
      <category>opencv</category>
      
    </categories>
    
    
    <tags>
      
      <tag>计算机视觉</tag>
      
      <tag>opencv</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>计算机视觉-openCV：SVM支持向量机</title>
    <link href="/2018/10/openCV-svm/"/>
    <url>/2018/10/openCV-svm/</url>
    
    <content type="html"><![CDATA[<h3 id="openCV：SVM支持向量机"><a href="#openCV：SVM支持向量机" class="headerlink" title="openCV：SVM支持向量机"></a>openCV：SVM支持向量机</h3><pre><code class="hljs python"><span class="hljs-comment"># 身高体重 训练 预测 </span><span class="hljs-keyword">import</span> cv2<span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt<span class="hljs-comment">#1 准备data</span>rand1 = np.array([[<span class="hljs-number">155</span>,<span class="hljs-number">48</span>],[<span class="hljs-number">159</span>,<span class="hljs-number">50</span>],[<span class="hljs-number">164</span>,<span class="hljs-number">53</span>],[<span class="hljs-number">168</span>,<span class="hljs-number">56</span>],[<span class="hljs-number">172</span>,<span class="hljs-number">60</span>]])rand2 = np.array([[<span class="hljs-number">152</span>,<span class="hljs-number">53</span>],[<span class="hljs-number">156</span>,<span class="hljs-number">55</span>],[<span class="hljs-number">160</span>,<span class="hljs-number">56</span>],[<span class="hljs-number">172</span>,<span class="hljs-number">64</span>],[<span class="hljs-number">176</span>,<span class="hljs-number">65</span>]])<span class="hljs-comment"># 2 label</span>label = np.array([[<span class="hljs-number">0</span>],[<span class="hljs-number">0</span>],[<span class="hljs-number">0</span>],[<span class="hljs-number">0</span>],[<span class="hljs-number">0</span>],[<span class="hljs-number">1</span>],[<span class="hljs-number">1</span>],[<span class="hljs-number">1</span>],[<span class="hljs-number">1</span>],[<span class="hljs-number">1</span>]])<span class="hljs-comment"># 3 data</span>data = np.vstack((rand1,rand2))data = np.array(data,dtype=<span class="hljs-string">'float32'</span>)<span class="hljs-comment"># svm 所有的数据都要有label</span><span class="hljs-comment"># [155,48] -- 0 女生 [152,53] ---1  男生</span><span class="hljs-comment"># 监督学习 0 负样本 1 正样本</span><span class="hljs-comment"># 4 训练</span>svm = cv2.ml.SVM_create() <span class="hljs-comment"># ml  机器学习模块 SVM_create() 创建</span><span class="hljs-comment"># 属性设置</span>svm.setType(cv2.ml.SVM_C_SVC) <span class="hljs-comment"># svm type</span>svm.setKernel(cv2.ml.SVM_LINEAR) <span class="hljs-comment"># line</span>svm.setC(<span class="hljs-number">0.01</span>)<span class="hljs-comment"># 训练</span>result = svm.train(data,cv2.ml.ROW_SAMPLE,label)<span class="hljs-comment"># 预测</span>pt_data = np.vstack([[<span class="hljs-number">167</span>,<span class="hljs-number">55</span>],[<span class="hljs-number">162</span>,<span class="hljs-number">57</span>]]) <span class="hljs-comment">#0 女生 1男生</span>pt_data = np.array(pt_data,dtype=<span class="hljs-string">'float32'</span>)print(pt_data)(par1,par2) = svm.predict(pt_data)print(par2)</code></pre><h3 id="预测结果："><a href="#预测结果：" class="headerlink" title="预测结果："></a>预测结果：</h3><pre><code class="hljs python">[[<span class="hljs-number">167.</span>  <span class="hljs-number">55.</span>] [<span class="hljs-number">162.</span>  <span class="hljs-number">57.</span>]][[<span class="hljs-number">0.</span>] [<span class="hljs-number">1.</span>]]</code></pre>]]></content>
    
    
    <categories>
      
      <category>opencv</category>
      
    </categories>
    
    
    <tags>
      
      <tag>计算机视觉</tag>
      
      <tag>opencv</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>计算机视觉-openCV视频处理</title>
    <link href="/2018/10/openCV-video/"/>
    <url>/2018/10/openCV-video/</url>
    
    <content type="html"><![CDATA[<h3 id="openCV视频分解图片"><a href="#openCV视频分解图片" class="headerlink" title="openCV视频分解图片"></a>openCV视频分解图片</h3><pre><code class="hljs python"><span class="hljs-keyword">import</span> cv2cap = cv2.VideoCapture(<span class="hljs-string">"1.mp4"</span>)<span class="hljs-comment"># 获取一个视频打开cap 1 file name</span>isOpened = cap.isOpened<span class="hljs-comment"># 判断是否打开‘</span>print(isOpened)fps = cap.get(cv2.CAP_PROP_FPS)<span class="hljs-comment">#帧率</span>width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))<span class="hljs-comment">#w h</span>height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))print(fps,width,height)i = <span class="hljs-number">0</span><span class="hljs-keyword">while</span>(isOpened):    <span class="hljs-keyword">if</span> i == <span class="hljs-number">10</span>:        <span class="hljs-keyword">break</span>    <span class="hljs-keyword">else</span>:        i = i+<span class="hljs-number">1</span>    (flag,frame) = cap.read()<span class="hljs-comment"># 读取每一张 flag frame </span>    fileName = <span class="hljs-string">'image'</span>+str(i)+<span class="hljs-string">'.jpg'</span>    print(fileName)    <span class="hljs-keyword">if</span> flag == <span class="hljs-literal">True</span>:        cv2.imwrite(fileName,frame,[cv2.IMWRITE_JPEG_QUALITY,<span class="hljs-number">100</span>])print(<span class="hljs-string">'end!'</span>)</code></pre><h3 id="openCV图片合成视频"><a href="#openCV图片合成视频" class="headerlink" title="openCV图片合成视频"></a>openCV图片合成视频</h3><pre><code class="hljs python"><span class="hljs-keyword">import</span> cv2img = cv2.imread(<span class="hljs-string">'image1.jpg'</span>)imgInfo = img.shapesize = (imgInfo[<span class="hljs-number">1</span>],imgInfo[<span class="hljs-number">0</span>])print(size)videoWrite = cv2.VideoWriter(<span class="hljs-string">'2.mp4'</span>,<span class="hljs-number">-1</span>,<span class="hljs-number">5</span>,size)<span class="hljs-comment"># 写入对象 1 file name</span><span class="hljs-comment"># 2 编码器 3 帧率 4 size</span><span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> range(<span class="hljs-number">1</span>,<span class="hljs-number">11</span>):    fileName = <span class="hljs-string">'image'</span>+str(i)+<span class="hljs-string">'.jpg'</span>    img = cv2.imread(fileName)    videoWrite.write(img)<span class="hljs-comment"># 写入方法 1 jpg data</span>print(<span class="hljs-string">'end!'</span>)</code></pre>]]></content>
    
    
    <categories>
      
      <category>opencv</category>
      
    </categories>
    
    
    <tags>
      
      <tag>计算机视觉</tag>
      
      <tag>opencv</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>计算机视觉-openCV图像美化</title>
    <link href="/2018/10/openCV-whitening/"/>
    <url>/2018/10/openCV-whitening/</url>
    
    <content type="html"><![CDATA[<h3 id="openCV彩色图片直方图"><a href="#openCV彩色图片直方图" class="headerlink" title="openCV彩色图片直方图"></a>openCV彩色图片直方图</h3><pre><code class="hljs python"><span class="hljs-keyword">import</span> cv2<span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">ImageHist</span><span class="hljs-params">(image,type)</span>:</span>    color = (<span class="hljs-number">255</span>,<span class="hljs-number">255</span>,<span class="hljs-number">255</span>)    windowName = <span class="hljs-string">'Gray'</span>    <span class="hljs-keyword">if</span> type == <span class="hljs-number">31</span>:        color = (<span class="hljs-number">255</span>,<span class="hljs-number">0</span>,<span class="hljs-number">0</span>)        windowName = <span class="hljs-string">'B Hist'</span>    <span class="hljs-keyword">elif</span> type == <span class="hljs-number">32</span>:        color = (<span class="hljs-number">0</span>,<span class="hljs-number">255</span>,<span class="hljs-number">0</span>)        windowName = <span class="hljs-string">'G Hist'</span>    <span class="hljs-keyword">elif</span> type == <span class="hljs-number">33</span>:        color = (<span class="hljs-number">0</span>,<span class="hljs-number">0</span>,<span class="hljs-number">255</span>)        windowName = <span class="hljs-string">'R Hist'</span>    <span class="hljs-comment"># 1 image 2 [0] 3 mask None 4 256 5 0-255</span>    hist = cv2.calcHist([image],[<span class="hljs-number">0</span>],<span class="hljs-literal">None</span>,[<span class="hljs-number">256</span>],[<span class="hljs-number">0.0</span>,<span class="hljs-number">255.0</span>])    minV,maxV,minL,maxL = cv2.minMaxLoc(hist)  <span class="hljs-comment"># 获取像素的最大值和最小值以便后面归一化处理</span>    histImg = np.zeros([<span class="hljs-number">256</span>,<span class="hljs-number">256</span>,<span class="hljs-number">3</span>],np.uint8)    <span class="hljs-comment"># 数据归一化处理</span>    <span class="hljs-keyword">for</span> h <span class="hljs-keyword">in</span> range(<span class="hljs-number">256</span>):        intenNormal = int(hist[h]*<span class="hljs-number">256</span>/maxV)        cv2.line(histImg,(h,<span class="hljs-number">256</span>),(h,<span class="hljs-number">256</span>-intenNormal),color)    cv2.imshow(windowName,histImg)    <span class="hljs-keyword">return</span> histImgimg = cv2.imread(<span class="hljs-string">'image0.jpg'</span>,<span class="hljs-number">1</span>)channels = cv2.split(img)<span class="hljs-comment"># RGB - R G B</span><span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> range(<span class="hljs-number">0</span>,<span class="hljs-number">3</span>):    ImageHist(channels[i],<span class="hljs-number">31</span>+i)</code></pre><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20200601183052.png" srcset="/img/loading.gif" alt="1"></p><h3 id="openCV直方图均衡化-灰度"><a href="#openCV直方图均衡化-灰度" class="headerlink" title="openCV直方图均衡化-灰度"></a>openCV直方图均衡化-灰度</h3><pre><code class="hljs python"><span class="hljs-keyword">import</span> cv2<span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> npimg = cv2.imread(<span class="hljs-string">'image0.jpg'</span>,<span class="hljs-number">1</span>)gray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY) <span class="hljs-comment"># 图片灰度化</span>cv2.imshow(<span class="hljs-string">'src'</span>,gray)dst = cv2.equalizeHist(gray) <span class="hljs-comment"># api 完成直方图均衡化</span>cv2.imshow(<span class="hljs-string">'dst'</span>,dst)</code></pre><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20200601183053.png" srcset="/img/loading.gif" alt="1"></p><h3 id="openCV直方图均衡化-彩色"><a href="#openCV直方图均衡化-彩色" class="headerlink" title="openCV直方图均衡化-彩色"></a>openCV直方图均衡化-彩色</h3><pre><code class="hljs python"><span class="hljs-keyword">import</span> cv2<span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> npimg = cv2.imread(<span class="hljs-string">'image0.jpg'</span>,<span class="hljs-number">1</span>)cv2.imshow(<span class="hljs-string">'src'</span>,img)(b,g,r) = cv2.split(img) <span class="hljs-comment"># 通道分解</span><span class="hljs-comment"># 图片单通道处理</span>bH = cv2.equalizeHist(b)gH = cv2.equalizeHist(g)rH = cv2.equalizeHist(r)result = cv2.merge((bH,gH,rH))<span class="hljs-comment"># 通道合成</span>cv2.imshow(<span class="hljs-string">'dst'</span>,result)</code></pre><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20200601183054.png" srcset="/img/loading.gif" alt="1"></p><h3 id="openCV直方图均衡化-YUV"><a href="#openCV直方图均衡化-YUV" class="headerlink" title="openCV直方图均衡化-YUV"></a>openCV直方图均衡化-YUV</h3><pre><code class="hljs python"><span class="hljs-keyword">import</span> cv2<span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> npimg = cv2.imread(<span class="hljs-string">'image0.jpg'</span>,<span class="hljs-number">1</span>)imgYUV = cv2.cvtColor(img,cv2.COLOR_BGR2YCrCb) <span class="hljs-comment">#  </span>cv2.imshow(<span class="hljs-string">'src'</span>,img)channelYUV = cv2.split(imgYUV) <span class="hljs-comment"># 图片分解</span>channelYUV[<span class="hljs-number">0</span>] = cv2.equalizeHist(channelYUV[<span class="hljs-number">0</span>]) <span class="hljs-comment"># 直方图均衡化</span>channels = cv2.merge(channelYUV) <span class="hljs-comment"># 合成</span>result = cv2.cvtColor(channels,cv2.COLOR_YCrCb2BGR)cv2.imshow(<span class="hljs-string">'dst'</span>,result)</code></pre><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20200601183055.png" srcset="/img/loading.gif" alt="1"></p><h3 id="openCV图片修补"><a href="#openCV图片修补" class="headerlink" title="openCV图片修补"></a>openCV图片修补</h3><pre><code class="hljs python"><span class="hljs-keyword">import</span> cv2 <span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> npimg = cv2.imread(<span class="hljs-string">'damaged.jpg'</span>,<span class="hljs-number">1</span>)cv2.imshow(<span class="hljs-string">'src'</span>,img)imgInfo = img.shapeheight = imgInfo[<span class="hljs-number">0</span>]width = imgInfo[<span class="hljs-number">1</span>]paint = np.zeros((height,width,<span class="hljs-number">1</span>),np.uint8)<span class="hljs-comment"># 描绘图片坏的数组</span><span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> range(<span class="hljs-number">200</span>,<span class="hljs-number">300</span>):    paint[i,<span class="hljs-number">200</span>] = <span class="hljs-number">255</span>    paint[i,<span class="hljs-number">200</span>+<span class="hljs-number">1</span>] = <span class="hljs-number">255</span>    paint[i,<span class="hljs-number">200</span><span class="hljs-number">-1</span>] = <span class="hljs-number">255</span><span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> range(<span class="hljs-number">150</span>,<span class="hljs-number">250</span>):    paint[<span class="hljs-number">250</span>,i] = <span class="hljs-number">255</span>    paint[<span class="hljs-number">250</span>+<span class="hljs-number">1</span>,i] = <span class="hljs-number">255</span>    paint[<span class="hljs-number">250</span><span class="hljs-number">-1</span>,i] = <span class="hljs-number">255</span>cv2.imshow(<span class="hljs-string">'paint'</span>,paint)<span class="hljs-comment">#1 src 2 mask</span>imgDst = cv2.inpaint(img,paint,<span class="hljs-number">3</span>,cv2.INPAINT_TELEA)cv2.imshow(<span class="hljs-string">'image'</span>,imgDst)</code></pre><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20200601183056.png" srcset="/img/loading.gif" alt="2"></p><h3 id="openCV灰度直方图"><a href="#openCV灰度直方图" class="headerlink" title="openCV灰度直方图"></a>openCV灰度直方图</h3><pre><code class="hljs python"><span class="hljs-comment"># 本质：统计每个像素灰度 出现的概率 0-255 p</span><span class="hljs-keyword">import</span> cv2<span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> pltimg = cv2.imread(<span class="hljs-string">'image0.jpg'</span>,<span class="hljs-number">1</span>)imgInfo = img.shapeheight = imgInfo[<span class="hljs-number">0</span>]width = imgInfo[<span class="hljs-number">1</span>]gray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY) <span class="hljs-comment"># 图片灰度化</span>count = np.zeros(<span class="hljs-number">256</span>,np.float)<span class="hljs-comment"># 获取灰度像素</span><span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> range(<span class="hljs-number">0</span>,height):    <span class="hljs-keyword">for</span> j <span class="hljs-keyword">in</span> range(<span class="hljs-number">0</span>,width):        pixel = gray[i,j]        index = int(pixel)        count[index] = count[index]+<span class="hljs-number">1</span><span class="hljs-comment"># 灰度等级出现的概率</span><span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> range(<span class="hljs-number">0</span>,<span class="hljs-number">255</span>):    count[i] = count[i]/(height*width)x = np.linspace(<span class="hljs-number">0</span>,<span class="hljs-number">255</span>,<span class="hljs-number">256</span>)y = countplt.bar(x,y,<span class="hljs-number">0.9</span>,alpha=<span class="hljs-number">1</span>,color=<span class="hljs-string">'b'</span>)plt.show()</code></pre><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20200601183057.png" srcset="/img/loading.gif" alt="2"></p><h3 id="openCV彩色直方图"><a href="#openCV彩色直方图" class="headerlink" title="openCV彩色直方图"></a>openCV彩色直方图</h3><pre><code class="hljs python"><span class="hljs-keyword">import</span> cv2<span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> pltimg = cv2.imread(<span class="hljs-string">'image0.jpg'</span>,<span class="hljs-number">1</span>)imgInfo = img.shapeheight = imgInfo[<span class="hljs-number">0</span>]width = imgInfo[<span class="hljs-number">1</span>]count_b = np.zeros(<span class="hljs-number">256</span>,np.float)count_g = np.zeros(<span class="hljs-number">256</span>,np.float)count_r = np.zeros(<span class="hljs-number">256</span>,np.float)<span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> range(<span class="hljs-number">0</span>,height):    <span class="hljs-keyword">for</span> j <span class="hljs-keyword">in</span> range(<span class="hljs-number">0</span>,width):        (b,g,r) = img[i,j]        index_b = int(b)        index_g = int(g)        index_r = int(r)        count_b[index_b] = count_b[index_b]+<span class="hljs-number">1</span>        count_g[index_g] = count_g[index_g]+<span class="hljs-number">1</span>        count_r[index_r] = count_r[index_r]+<span class="hljs-number">1</span><span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> range(<span class="hljs-number">0</span>,<span class="hljs-number">256</span>):    count_b[i] = count_b[i]/(height*width)    count_g[i] = count_g[i]/(height*width)    count_r[i] = count_r[i]/(height*width)x = np.linspace(<span class="hljs-number">0</span>,<span class="hljs-number">255</span>,<span class="hljs-number">256</span>)y1 = count_bplt.figure()plt.bar(x,y1,<span class="hljs-number">0.9</span>,alpha=<span class="hljs-number">1</span>,color=<span class="hljs-string">'b'</span>)y2 = count_gplt.figure()plt.bar(x,y2,<span class="hljs-number">0.9</span>,alpha=<span class="hljs-number">1</span>,color=<span class="hljs-string">'g'</span>)y3 = count_rplt.figure()plt.bar(x,y3,<span class="hljs-number">0.9</span>,alpha=<span class="hljs-number">1</span>,color=<span class="hljs-string">'r'</span>)plt.show()</code></pre><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20200601183058.png" srcset="/img/loading.gif" alt="3"></p><h3 id="openCV灰度直方图均衡化"><a href="#openCV灰度直方图均衡化" class="headerlink" title="openCV灰度直方图均衡化"></a>openCV灰度直方图均衡化</h3><pre><code class="hljs python"><span class="hljs-keyword">import</span> cv2<span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> pltimg = cv2.imread(<span class="hljs-string">'image0.jpg'</span>,<span class="hljs-number">1</span>)imgInfo = img.shapeheight = imgInfo[<span class="hljs-number">0</span>]width = imgInfo[<span class="hljs-number">1</span>]gray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)cv2.imshow(<span class="hljs-string">'src'</span>,gray)count = np.zeros(<span class="hljs-number">256</span>,np.float)<span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> range(<span class="hljs-number">0</span>,height):    <span class="hljs-keyword">for</span> j <span class="hljs-keyword">in</span> range(<span class="hljs-number">0</span>,width):        pixel = gray[i,j]        index = int(pixel)        count[index] = count[index]+<span class="hljs-number">1</span><span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> range(<span class="hljs-number">0</span>,<span class="hljs-number">255</span>):    count[i] = count[i]/(height*width)<span class="hljs-comment">#计算累计概率</span>sum1 = float(<span class="hljs-number">0</span>)<span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> range(<span class="hljs-number">0</span>,<span class="hljs-number">256</span>):    sum1 = sum1+count[i]    count[i] = sum1<span class="hljs-comment">#print(count)</span><span class="hljs-comment"># 计算映射表</span>map1 = np.zeros(<span class="hljs-number">256</span>,np.uint16)<span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> range(<span class="hljs-number">0</span>,<span class="hljs-number">256</span>):    map1[i] = np.uint16(count[i]*<span class="hljs-number">255</span>)<span class="hljs-comment"># 映射</span><span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> range(<span class="hljs-number">0</span>,height):    <span class="hljs-keyword">for</span> j <span class="hljs-keyword">in</span> range(<span class="hljs-number">0</span>,width):        pixel = gray[i,j]        gray[i,j] = map1[pixel]cv2.imshow(<span class="hljs-string">'dst'</span>,gray)</code></pre><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20200601183059.png" srcset="/img/loading.gif" alt="4"></p><h3 id="openCV彩色直方图均衡化"><a href="#openCV彩色直方图均衡化" class="headerlink" title="openCV彩色直方图均衡化"></a>openCV彩色直方图均衡化</h3><pre><code class="hljs python"><span class="hljs-keyword">import</span> cv2<span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> pltimg = cv2.imread(<span class="hljs-string">'image0.jpg'</span>,<span class="hljs-number">1</span>)cv2.imshow(<span class="hljs-string">'src'</span>,img)imgInfo = img.shapeheight = imgInfo[<span class="hljs-number">0</span>]width = imgInfo[<span class="hljs-number">1</span>]count_b = np.zeros(<span class="hljs-number">256</span>,np.float)count_g = np.zeros(<span class="hljs-number">256</span>,np.float)count_r = np.zeros(<span class="hljs-number">256</span>,np.float)<span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> range(<span class="hljs-number">0</span>,height):    <span class="hljs-keyword">for</span> j <span class="hljs-keyword">in</span> range(<span class="hljs-number">0</span>,width):        (b,g,r) = img[i,j]        index_b = int(b)        index_g = int(g)        index_r = int(r)        count_b[index_b] = count_b[index_b]+<span class="hljs-number">1</span>        count_g[index_g] = count_g[index_g]+<span class="hljs-number">1</span>        count_r[index_r] = count_r[index_r]+<span class="hljs-number">1</span><span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> range(<span class="hljs-number">0</span>,<span class="hljs-number">255</span>):    count_b[i] = count_b[i]/(height*width)    count_g[i] = count_g[i]/(height*width)    count_r[i] = count_r[i]/(height*width)<span class="hljs-comment">#计算累计概率</span>sum_b = float(<span class="hljs-number">0</span>)sum_g = float(<span class="hljs-number">0</span>)sum_r = float(<span class="hljs-number">0</span>)<span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> range(<span class="hljs-number">0</span>,<span class="hljs-number">256</span>):    sum_b = sum_b+count_b[i]    sum_g = sum_g+count_g[i]    sum_r = sum_r+count_r[i]    count_b[i] = sum_b    count_g[i] = sum_g    count_r[i] = sum_r<span class="hljs-comment">#print(count)</span><span class="hljs-comment"># 计算映射表</span>map_b = np.zeros(<span class="hljs-number">256</span>,np.uint16)map_g = np.zeros(<span class="hljs-number">256</span>,np.uint16)map_r = np.zeros(<span class="hljs-number">256</span>,np.uint16)<span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> range(<span class="hljs-number">0</span>,<span class="hljs-number">256</span>):    map_b[i] = np.uint16(count_b[i]*<span class="hljs-number">255</span>)    map_g[i] = np.uint16(count_g[i]*<span class="hljs-number">255</span>)    map_r[i] = np.uint16(count_r[i]*<span class="hljs-number">255</span>)<span class="hljs-comment"># 映射</span>dst = np.zeros((height,width,<span class="hljs-number">3</span>),np.uint8)<span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> range(<span class="hljs-number">0</span>,height):    <span class="hljs-keyword">for</span> j <span class="hljs-keyword">in</span> range(<span class="hljs-number">0</span>,width):        (b,g,r) = img[i,j]        b = map_b[b]        g = map_g[g]        r = map_r[r]        dst[i,j] = (b,g,r)cv2.imshow(<span class="hljs-string">'dst'</span>,dst)</code></pre><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20200601183100.png" srcset="/img/loading.gif" alt="4"></p><h3 id="openCV亮度增强"><a href="#openCV亮度增强" class="headerlink" title="openCV亮度增强"></a>openCV亮度增强</h3><pre><code class="hljs python"><span class="hljs-keyword">import</span> cv2<span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> npimg = cv2.imread(<span class="hljs-string">'image0.jpg'</span>,<span class="hljs-number">1</span>)imgInfo = img.shapeheight = imgInfo[<span class="hljs-number">0</span>]width = imgInfo[<span class="hljs-number">1</span>]cv2.imshow(<span class="hljs-string">'src'</span>,img)dst = np.zeros((height,width,<span class="hljs-number">3</span>),np.uint8)<span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> range(<span class="hljs-number">0</span>,height):    <span class="hljs-keyword">for</span> j <span class="hljs-keyword">in</span> range(<span class="hljs-number">0</span>,width):        (b,g,r) = img[i,j]        bb = int(b)+<span class="hljs-number">40</span>        gg = int(g)+<span class="hljs-number">40</span>        rr = int(r)+<span class="hljs-number">40</span>        <span class="hljs-keyword">if</span> bb&gt;<span class="hljs-number">255</span>:            bb = <span class="hljs-number">255</span>        <span class="hljs-keyword">if</span> gg&gt;<span class="hljs-number">255</span>:            gg = <span class="hljs-number">255</span>        <span class="hljs-keyword">if</span> rr&gt;<span class="hljs-number">255</span>:            rr = <span class="hljs-number">255</span>        dst[i,j] = (bb,gg,rr)cv2.imshow(<span class="hljs-string">'dst'</span>,dst)</code></pre><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20200601183101.png" srcset="/img/loading.gif" alt="4"></p><h3 id="openCV磨皮美白-双边滤波"><a href="#openCV磨皮美白-双边滤波" class="headerlink" title="openCV磨皮美白-双边滤波"></a>openCV磨皮美白-双边滤波</h3><pre><code class="hljs python"><span class="hljs-keyword">import</span> cv2img = cv2.imread(<span class="hljs-string">'1.png'</span>,<span class="hljs-number">1</span>)cv2.imshow(<span class="hljs-string">'src'</span>,img)dst = cv2.bilateralFilter(img,<span class="hljs-number">15</span>,<span class="hljs-number">35</span>,<span class="hljs-number">35</span>) <span class="hljs-comment"># 滤波函数</span>cv2.imshow(<span class="hljs-string">'dst'</span>,dst)</code></pre><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20200601183102.png" srcset="/img/loading.gif" alt="4"></p><h3 id="openCV高斯滤波"><a href="#openCV高斯滤波" class="headerlink" title="openCV高斯滤波"></a>openCV高斯滤波</h3><pre><code class="hljs python"><span class="hljs-keyword">import</span> cv2<span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> npimg = cv2.imread(<span class="hljs-string">'image11.jpg'</span>,<span class="hljs-number">1</span>)cv2.imshow(<span class="hljs-string">'src'</span>,img)dst = cv2.GaussianBlur(img,(<span class="hljs-number">5</span>,<span class="hljs-number">5</span>),<span class="hljs-number">1.5</span>)cv2.imshow(<span class="hljs-string">'dst'</span>,dst)</code></pre><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20200601183103.png" srcset="/img/loading.gif" alt="4"></p><h3 id="openCV均值滤波"><a href="#openCV均值滤波" class="headerlink" title="openCV均值滤波"></a>openCV均值滤波</h3><pre><code class="hljs python"><span class="hljs-comment">#均值 6*6 1 。 * 【6*6】/36 = mean -》P</span><span class="hljs-keyword">import</span> cv2<span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> npimg = cv2.imread(<span class="hljs-string">'image11.jpg'</span>,<span class="hljs-number">1</span>)cv2.imshow(<span class="hljs-string">'src'</span>,img)imgInfo = img.shapeheight = imgInfo[<span class="hljs-number">0</span>]width = imgInfo[<span class="hljs-number">1</span>]dst = np.zeros((height,width,<span class="hljs-number">3</span>),np.uint8)<span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> range(<span class="hljs-number">3</span>,height<span class="hljs-number">-3</span>):    <span class="hljs-keyword">for</span> j <span class="hljs-keyword">in</span> range(<span class="hljs-number">3</span>,width<span class="hljs-number">-3</span>):        sum_b = int(<span class="hljs-number">0</span>)        sum_g = int(<span class="hljs-number">0</span>)        sum_r = int(<span class="hljs-number">0</span>)        <span class="hljs-keyword">for</span> m <span class="hljs-keyword">in</span> range(<span class="hljs-number">-3</span>,<span class="hljs-number">3</span>):<span class="hljs-comment">#-3 -2 -1 0 1 2</span>            <span class="hljs-keyword">for</span> n <span class="hljs-keyword">in</span> range(<span class="hljs-number">-3</span>,<span class="hljs-number">3</span>):                (b,g,r) = img[i+m,j+n]                sum_b = sum_b+int(b)                sum_g = sum_g+int(g)                sum_r = sum_r+int(r)                    b = np.uint8(sum_b/<span class="hljs-number">36</span>)        g = np.uint8(sum_g/<span class="hljs-number">36</span>)        r = np.uint8(sum_r/<span class="hljs-number">36</span>)        dst[i,j] = (b,g,r)cv2.imshow(<span class="hljs-string">'dst'</span>,dst)</code></pre><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20200601183104.png" srcset="/img/loading.gif" alt="4"></p><h3 id="openCV中值滤波"><a href="#openCV中值滤波" class="headerlink" title="openCV中值滤波"></a>openCV中值滤波</h3><pre><code class="hljs python"><span class="hljs-keyword">import</span> cv2<span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> npimg = cv2.imread(<span class="hljs-string">'image11.jpg'</span>,<span class="hljs-number">1</span>)imgInfo = img.shapeheight = imgInfo[<span class="hljs-number">0</span>]width = imgInfo[<span class="hljs-number">1</span>]img = cv2.cvtColor(img,cv2.COLOR_RGB2GRAY)cv2.imshow(<span class="hljs-string">'src'</span>,img)dst = np.zeros((height,width,<span class="hljs-number">3</span>),np.uint8)collect = np.zeros(<span class="hljs-number">9</span>,np.uint8)<span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> range(<span class="hljs-number">1</span>,height<span class="hljs-number">-1</span>):    <span class="hljs-keyword">for</span> j <span class="hljs-keyword">in</span> range(<span class="hljs-number">1</span>,width<span class="hljs-number">-1</span>):        k = <span class="hljs-number">0</span>        <span class="hljs-keyword">for</span> m <span class="hljs-keyword">in</span> range(<span class="hljs-number">-1</span>,<span class="hljs-number">2</span>):            <span class="hljs-keyword">for</span> n <span class="hljs-keyword">in</span> range(<span class="hljs-number">-1</span>,<span class="hljs-number">2</span>):                gray = img[i+m,j+n]                collect[k] = gray                k = k+<span class="hljs-number">1</span>        <span class="hljs-comment"># 0 1 2 3 4 5 6 7 8</span>        <span class="hljs-comment">#   1 </span>        <span class="hljs-keyword">for</span> k <span class="hljs-keyword">in</span> range(<span class="hljs-number">0</span>,<span class="hljs-number">9</span>):            p1 = collect[k]            <span class="hljs-keyword">for</span> t <span class="hljs-keyword">in</span> range(k+<span class="hljs-number">1</span>,<span class="hljs-number">9</span>):                <span class="hljs-keyword">if</span> p1&lt;collect[t]:                    mid = collect[t]                    collect[t] = p1                    p1 = mid        dst[i,j] = collect[<span class="hljs-number">4</span>]cv2.imshow(<span class="hljs-string">'dst'</span>,dst)</code></pre><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20200601183105.png" srcset="/img/loading.gif" alt="4"></p>]]></content>
    
    
    <categories>
      
      <category>opencv</category>
      
    </categories>
    
    
    <tags>
      
      <tag>计算机视觉</tag>
      
      <tag>opencv</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>计算机视觉-openCV图片几何变换</title>
    <link href="/2018/10/openCV-geometry/"/>
    <url>/2018/10/openCV-geometry/</url>
    
    <content type="html"><![CDATA[<h3 id="openCV图片缩放一"><a href="#openCV图片缩放一" class="headerlink" title="openCV图片缩放一"></a>openCV图片缩放一</h3><pre><code class="hljs python"><span class="hljs-keyword">import</span> cv2  <span class="hljs-comment"># 导入cv库</span>img = cv2.imread(<span class="hljs-string">'image0.jpg'</span>,<span class="hljs-number">1</span>)  <span class="hljs-comment"># 读取图片文件， 1：彩色， 0：灰色</span>imgInfo = img.shape <span class="hljs-comment"># 获取图片的维度</span>print(imgInfo)height = imgInfo[<span class="hljs-number">0</span>] width = imgInfo[<span class="hljs-number">1</span>]mode = imgInfo[<span class="hljs-number">2</span>]<span class="hljs-comment"># 1 放大 缩小 2 等比例 非 2:3 </span>dstHeight = int(height*<span class="hljs-number">0.5</span>)dstWidth = int(width*<span class="hljs-number">0.5</span>)<span class="hljs-comment">#最近临域插值 双线性插值 像素关系重采样 立方插值</span>dst = cv2.resize(img,(dstWidth,dstHeight))cv2.imshow(<span class="hljs-string">'image'</span>,dst)</code></pre><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20200601182901.png" srcset="/img/loading.gif" alt="1"></p><h3 id="openCV图片缩放二"><a href="#openCV图片缩放二" class="headerlink" title="openCV图片缩放二"></a>openCV图片缩放二</h3><pre><code class="hljs python"><span class="hljs-keyword">import</span> cv2  <span class="hljs-comment"># 导入cv库</span><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np img = cv2.imread(<span class="hljs-string">'image0.jpg'</span>,<span class="hljs-number">1</span>)  <span class="hljs-comment"># 读取图片文件， 1：彩色， 0：灰色</span>imgInfo = img.shape <span class="hljs-comment"># 获取图片的维度</span>height = imgInfo[<span class="hljs-number">0</span>]width = imgInfo[<span class="hljs-number">1</span>]<span class="hljs-comment"># 定义目标图片的高度和宽度</span>dstHeight = int(height/<span class="hljs-number">2</span>)dstWidth = int(width/<span class="hljs-number">2</span>)dstImage = np.zeros((dstHeight,dstWidth,<span class="hljs-number">3</span>),np.uint8) <span class="hljs-comment"># 0-255 准备好缩放后的图片数据维度</span><span class="hljs-comment"># 计算新的目标图片的坐标</span><span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> range(<span class="hljs-number">0</span>,dstHeight):<span class="hljs-comment">#行</span>    <span class="hljs-keyword">for</span> j <span class="hljs-keyword">in</span> range(<span class="hljs-number">0</span>,dstWidth):<span class="hljs-comment">#列 </span>        iNew = int(i*(height*<span class="hljs-number">1.0</span>/dstHeight))         jNew = int(j*(width*<span class="hljs-number">1.0</span>/dstWidth))        dstImage[i,j] = img[iNew,jNew]cv2.imshow(<span class="hljs-string">'dst'</span>,dstImage)</code></pre><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20200601182902.png" srcset="/img/loading.gif" alt="1"></p><h3 id="openCV图片剪切"><a href="#openCV图片剪切" class="headerlink" title="openCV图片剪切"></a>openCV图片剪切</h3><pre><code class="hljs python"><span class="hljs-keyword">import</span> cv2  <span class="hljs-comment"># 导入cv库</span>img = cv2.imread(<span class="hljs-string">'image0.jpg'</span>,<span class="hljs-number">1</span>)  <span class="hljs-comment"># 读取图片文件， 1：彩色， 0：灰色</span>imgInfo = img.shapedst = img[<span class="hljs-number">100</span>:<span class="hljs-number">200</span>,<span class="hljs-number">100</span>:<span class="hljs-number">300</span>] <span class="hljs-comment"># 获取宽度100-200， 高度100-300的图像</span>cv2.imshow(<span class="hljs-string">'image'</span>,dst)</code></pre><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20200601182903.png" srcset="/img/loading.gif" alt="1"></p><h3 id="openCV图片读取与展示"><a href="#openCV图片读取与展示" class="headerlink" title="openCV图片读取与展示"></a>openCV图片读取与展示</h3><pre><code class="hljs python"><span class="hljs-keyword">import</span> cv2  <span class="hljs-comment"># 导入cv库</span>img = cv2.imread(<span class="hljs-string">'image0.jpg'</span>,<span class="hljs-number">1</span>)  <span class="hljs-comment"># 读取图片文件， 1：彩色， 0：灰色</span>cv2.imshow(<span class="hljs-string">'image'</span>,img)  <span class="hljs-comment"># 显示图片</span></code></pre><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20200601182904.png" srcset="/img/loading.gif" alt="1"></p><h3 id="openCV图片移位一"><a href="#openCV图片移位一" class="headerlink" title="openCV图片移位一"></a>openCV图片移位一</h3><pre><code class="hljs python"><span class="hljs-keyword">import</span> cv2 <span class="hljs-comment"># 导入cv库</span><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> npimg = cv2.imread(<span class="hljs-string">'image0.jpg'</span>,<span class="hljs-number">1</span>) <span class="hljs-comment"># 读取图片文件， 1：彩色， 0：灰色</span>cv2.imshow(<span class="hljs-string">'src'</span>,img)imgInfo = img.shape  <span class="hljs-comment"># 获取图片的维度</span>height = imgInfo[<span class="hljs-number">0</span>]width = imgInfo[<span class="hljs-number">1</span>]<span class="hljs-comment">####</span>matShift = np.float32([[<span class="hljs-number">1</span>,<span class="hljs-number">0</span>,<span class="hljs-number">100</span>],[<span class="hljs-number">0</span>,<span class="hljs-number">1</span>,<span class="hljs-number">200</span>]])<span class="hljs-comment"># 2*3  设置平移的矩阵</span>dst = cv2.warpAffine(img,matShift,(height,width))<span class="hljs-comment">#图片数据 ，移位矩阵 图片的维度信息</span><span class="hljs-comment"># 移位 矩阵</span>cv2.imshow(<span class="hljs-string">'dst'</span>,dst)</code></pre><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20200601182905.png" srcset="/img/loading.gif" alt="2"></p><h3 id="openCV图片移位二"><a href="#openCV图片移位二" class="headerlink" title="openCV图片移位二"></a>openCV图片移位二</h3><pre><code class="hljs python"><span class="hljs-keyword">import</span> cv2 <span class="hljs-comment"># 导入cv库</span><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> npimg = cv2.imread(<span class="hljs-string">'image0.jpg'</span>,<span class="hljs-number">1</span>) <span class="hljs-comment"># 读取图片文件， 1：彩色， 0：灰色</span>cv2.imshow(<span class="hljs-string">'src'</span>,img)imgInfo = img.shape  <span class="hljs-comment"># 获取图片的维度</span>dst = np.zeros(img.shape,np.uint8)height = imgInfo[<span class="hljs-number">0</span>]width = imgInfo[<span class="hljs-number">1</span>]<span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> range(<span class="hljs-number">0</span>,height):    <span class="hljs-keyword">for</span> j <span class="hljs-keyword">in</span> range(<span class="hljs-number">0</span>,width<span class="hljs-number">-100</span>):        dst[i,j+<span class="hljs-number">100</span>]=img[i,j]cv2.imshow(<span class="hljs-string">'image'</span>,dst)</code></pre><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20200601182906.png" srcset="/img/loading.gif" alt="2"></p><h3 id="openCV图片镜像"><a href="#openCV图片镜像" class="headerlink" title="openCV图片镜像"></a>openCV图片镜像</h3><pre><code class="hljs python"><span class="hljs-keyword">import</span> cv2 <span class="hljs-comment"># 导入cv库</span><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> npimg = cv2.imread(<span class="hljs-string">'image0.jpg'</span>,<span class="hljs-number">1</span>) <span class="hljs-comment"># 读取图片文件， 1：彩色， 0：灰色</span>cv2.imshow(<span class="hljs-string">'src'</span>,img)imgInfo = img.shape  <span class="hljs-comment"># 获取图片的维度</span>height = imgInfo[<span class="hljs-number">0</span>]width = imgInfo[<span class="hljs-number">1</span>]deep = imgInfo[<span class="hljs-number">2</span>]newImgInfo = (height*<span class="hljs-number">2</span>,width,deep) <span class="hljs-comment"># 新图片的维度</span>dst = np.zeros(newImgInfo,np.uint8)<span class="hljs-comment">#uint8 # 目标图片的数据维度</span><span class="hljs-comment"># 刷新图片的数据</span><span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> range(<span class="hljs-number">0</span>,height):    <span class="hljs-keyword">for</span> j <span class="hljs-keyword">in</span> range(<span class="hljs-number">0</span>,width):        dst[i,j] = img[i,j]        <span class="hljs-comment">#x y = 2*h - y -1</span>        dst[height*<span class="hljs-number">2</span>-i<span class="hljs-number">-1</span>,j] = img[i,j]<span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> range(<span class="hljs-number">0</span>,width): <span class="hljs-comment"># 添加分割线</span>    dst[height,i] = (<span class="hljs-number">0</span>,<span class="hljs-number">0</span>,<span class="hljs-number">255</span>)<span class="hljs-comment">#BGR</span>cv2.imshow(<span class="hljs-string">'dst'</span>,dst)</code></pre><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20200601182907.png" srcset="/img/loading.gif" alt="3"></p><h3 id="openCV图片缩放"><a href="#openCV图片缩放" class="headerlink" title="openCV图片缩放"></a>openCV图片缩放</h3><pre><code class="hljs python"><span class="hljs-keyword">import</span> cv2 <span class="hljs-comment"># 导入cv库</span><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> npimg = cv2.imread(<span class="hljs-string">'image0.jpg'</span>,<span class="hljs-number">1</span>) <span class="hljs-comment"># 读取图片文件， 1：彩色， 0：灰色</span>cv2.imshow(<span class="hljs-string">'src'</span>,img)imgInfo = img.shape  <span class="hljs-comment"># 获取图片的维度</span>height = imgInfo[<span class="hljs-number">0</span>]width = imgInfo[<span class="hljs-number">1</span>]matScale = np.float32([[<span class="hljs-number">0.5</span>,<span class="hljs-number">0</span>,<span class="hljs-number">0</span>],[<span class="hljs-number">0</span>,<span class="hljs-number">0.5</span>,<span class="hljs-number">0</span>]]) <span class="hljs-comment"># 定义缩放矩阵</span>dst = cv2.warpAffine(img,matScale,(int(width/<span class="hljs-number">2</span>),int(height/<span class="hljs-number">2</span>))) <span class="hljs-comment"># 原始数据，缩放矩阵，目标的宽高信息</span>cv2.imshow(<span class="hljs-string">'dst'</span>,dst)</code></pre><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20200601182908.png" srcset="/img/loading.gif" alt="4"></p><h3 id="openCV图片仿射变换"><a href="#openCV图片仿射变换" class="headerlink" title="openCV图片仿射变换"></a>openCV图片仿射变换</h3><pre><code class="hljs python"><span class="hljs-keyword">import</span> cv2 <span class="hljs-comment"># 导入cv库</span><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> npimg = cv2.imread(<span class="hljs-string">'image0.jpg'</span>,<span class="hljs-number">1</span>) <span class="hljs-comment"># 读取图片文件， 1：彩色， 0：灰色</span>cv2.imshow(<span class="hljs-string">'src'</span>,img)imgInfo = img.shape  <span class="hljs-comment"># 获取图片的维度</span>height = imgInfo[<span class="hljs-number">0</span>]width = imgInfo[<span class="hljs-number">1</span>]<span class="hljs-comment">#src 3-&gt;dst 3 (左上角 左下角 右上角)</span>matSrc = np.float32([[<span class="hljs-number">0</span>,<span class="hljs-number">0</span>],[<span class="hljs-number">0</span>,height<span class="hljs-number">-1</span>],[width<span class="hljs-number">-1</span>,<span class="hljs-number">0</span>]]) <span class="hljs-comment"># 获取原图片三个点坐标</span>matDst = np.float32([[<span class="hljs-number">50</span>,<span class="hljs-number">50</span>],[<span class="hljs-number">300</span>,height<span class="hljs-number">-200</span>],[width<span class="hljs-number">-300</span>,<span class="hljs-number">100</span>]]) <span class="hljs-comment"># 三个点的新坐标</span><span class="hljs-comment">#把两个矩阵组合</span>matAffine = cv2.getAffineTransform(matSrc,matDst) <span class="hljs-comment"># 获取矩阵的组合，</span>dst = cv2.warpAffine(img,matAffine,(width,height)) <span class="hljs-comment"># 仿射变换方法</span>cv2.imshow(<span class="hljs-string">'dst'</span>,dst)</code></pre><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20200601182909.png" srcset="/img/loading.gif" alt="4"></p><h3 id="openCV图片旋转"><a href="#openCV图片旋转" class="headerlink" title="openCV图片旋转"></a>openCV图片旋转</h3><pre><code class="hljs python"><span class="hljs-keyword">import</span> cv2 <span class="hljs-comment"># 导入cv库</span><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> npimg = cv2.imread(<span class="hljs-string">'image0.jpg'</span>,<span class="hljs-number">1</span>) <span class="hljs-comment"># 读取图片文件， 1：彩色， 0：灰色</span>cv2.imshow(<span class="hljs-string">'src'</span>,img)imgInfo = img.shape  <span class="hljs-comment"># 获取图片的维度</span>height = imgInfo[<span class="hljs-number">0</span>]width = imgInfo[<span class="hljs-number">1</span>]<span class="hljs-comment"># 2*3 定义旋转矩阵--旋转的中心点，旋转的角度， 缩放系数</span>matRotate = cv2.getRotationMatrix2D((height*<span class="hljs-number">0.5</span>,width*<span class="hljs-number">0.5</span>),<span class="hljs-number">45</span>,<span class="hljs-number">1</span>)<span class="hljs-comment"># mat rotate 1 center 2 angle 3 scale</span><span class="hljs-comment">#100*100 25 </span>dst = cv2.warpAffine(img,matRotate,(height,width)) <span class="hljs-comment"># 仿射方法</span>cv2.imshow(<span class="hljs-string">'dst'</span>,dst)</code></pre><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20200601182910.png" srcset="/img/loading.gif" alt="4"></p>]]></content>
    
    
    <categories>
      
      <category>opencv</category>
      
    </categories>
    
    
    <tags>
      
      <tag>计算机视觉</tag>
      
      <tag>opencv</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>计算机视觉-openCV安装及入门</title>
    <link href="/2018/10/openCV-install/"/>
    <url>/2018/10/openCV-install/</url>
    
    <content type="html"><![CDATA[<h3 id="openCV-python的安装"><a href="#openCV-python的安装" class="headerlink" title="openCV-python的安装"></a>openCV-python的安装</h3><h4 id="windows安装：下载地址：opencv"><a href="#windows安装：下载地址：opencv" class="headerlink" title="windows安装：下载地址：opencv."></a>windows安装：下载地址：<a href="https://www.lfd.uci.edu/~gohlke/pythonlibs/" target="_blank" rel="noopener">opencv</a>.</h4><pre><code class="hljs shell">pip install  opencv_python-3.4.3-cp36-cp36m-win_amd64.whl</code></pre><h4 id="ubuntu安装"><a href="#ubuntu安装" class="headerlink" title="ubuntu安装"></a>ubuntu安装</h4><pre><code class="hljs shell">pip3 install opencv-python</code></pre><h3 id="openCV图片读取与展示"><a href="#openCV图片读取与展示" class="headerlink" title="openCV图片读取与展示"></a>openCV图片读取与展示</h3><pre><code class="hljs python"><span class="hljs-keyword">import</span> cv2  <span class="hljs-comment"># 导入cv库</span>img = cv2.imread(<span class="hljs-string">'image0.jpg'</span>,<span class="hljs-number">1</span>)  <span class="hljs-comment"># 读取图片文件， 1：彩色， 0：灰色</span>cv2.imshow(<span class="hljs-string">'image'</span>,img)  <span class="hljs-comment"># 显示图片</span></code></pre><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20200601182926.png" srcset="/img/loading.gif" alt="1"></p><h3 id="openCV图片写入"><a href="#openCV图片写入" class="headerlink" title="openCV图片写入"></a>openCV图片写入</h3><pre><code class="hljs python"><span class="hljs-keyword">import</span> cv2 <span class="hljs-comment"># 导入cv库</span>img = cv2.imread(<span class="hljs-string">'image0.jpg'</span>,<span class="hljs-number">1</span>) <span class="hljs-comment"># 读取图片文件， 1：彩色， 0：灰色</span>cv2.imwrite(<span class="hljs-string">'image1.jpg'</span>,img) <span class="hljs-comment"># 写入文件名字 ， 图片数据</span></code></pre><h3 id="openCV更改图像质量-有损压缩"><a href="#openCV更改图像质量-有损压缩" class="headerlink" title="openCV更改图像质量-有损压缩"></a>openCV更改图像质量-有损压缩</h3><pre><code class="hljs python"><span class="hljs-keyword">import</span> cv2 <span class="hljs-comment"># 导入cv库</span>img = cv2.imread(<span class="hljs-string">'image0.jpg'</span>,<span class="hljs-number">1</span>) <span class="hljs-comment"># 读取图片文件， 1：彩色， 0：灰色</span>cv2.imwrite(<span class="hljs-string">'imageTest.jpg'</span>,img,[cv2.IMWRITE_JPEG_QUALITY,<span class="hljs-number">50</span>]) <span class="hljs-comment"># 写入文件名字 ， 图片数据 ， 当前jpg图片保存的质量（范围0-100）</span><span class="hljs-comment">#1M 100k 10k 0-100 有损压缩</span></code></pre><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20200601182927.png" srcset="/img/loading.gif" alt="2"></p><h3 id="openCV更改图像质量-无损压缩"><a href="#openCV更改图像质量-无损压缩" class="headerlink" title="openCV更改图像质量-无损压缩"></a>openCV更改图像质量-无损压缩</h3><pre><code class="hljs python"><span class="hljs-comment"># 1 无损 2 透明度属性</span><span class="hljs-keyword">import</span> cv2  <span class="hljs-comment"># 导入cv库</span>img = cv2.imread(<span class="hljs-string">'image0.jpg'</span>,<span class="hljs-number">1</span>)  <span class="hljs-comment"># 读取图片文件， 1：彩色， 0：灰色</span>cv2.imwrite(<span class="hljs-string">'imageTest.png'</span>,img,[cv2.IMWRITE_PNG_COMPRESSION,<span class="hljs-number">0</span>])  <span class="hljs-comment"># 写入文件名字 ， 图片数据 ， 当前jpg图片保存的质量（范围0-100）</span><span class="hljs-comment"># jpg 0 压缩比高0-100 png 0 压缩比低0-9</span></code></pre><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20200601182928.png" srcset="/img/loading.gif" alt="3"></p><h3 id="openCV图片像素操作"><a href="#openCV图片像素操作" class="headerlink" title="openCV图片像素操作"></a>openCV图片像素操作</h3><pre><code class="hljs python"><span class="hljs-keyword">import</span> cv2  <span class="hljs-comment"># 导入cv库 </span>img = cv2.imread(<span class="hljs-string">'image0.jpg'</span>,<span class="hljs-number">1</span>)  <span class="hljs-comment"># 读取图片文件， 1：彩色， 0：灰色</span>(b,g,r) = img[<span class="hljs-number">100</span>,<span class="hljs-number">100</span>] <span class="hljs-comment"># 获取图片的（100,100）坐标的像素值，按照bgr的形式读取</span>print(b,g,r)<span class="hljs-comment"># bgr</span><span class="hljs-comment">#10 100 --- 110 100</span><span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> range(<span class="hljs-number">1</span>,<span class="hljs-number">100</span>):  <span class="hljs-comment"># 总共一百个像素点</span>    img[<span class="hljs-number">10</span>+i,<span class="hljs-number">100</span>] = (<span class="hljs-number">255</span>,<span class="hljs-number">0</span>,<span class="hljs-number">0</span>)  <span class="hljs-comment"># 写入标准的蓝色</span>cv2.imshow(<span class="hljs-string">'image'</span>,img)</code></pre><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20200601182929.png" srcset="/img/loading.gif" alt="4"></p>]]></content>
    
    
    <categories>
      
      <category>opencv</category>
      
    </categories>
    
    
    <tags>
      
      <tag>计算机视觉</tag>
      
      <tag>opencv</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>计算机视觉-openCV图片特效&amp;绘制线段文字</title>
    <link href="/2018/10/openCV-special/"/>
    <url>/2018/10/openCV-special/</url>
    
    <content type="html"><![CDATA[<h3 id="openCV图片灰度处理一"><a href="#openCV图片灰度处理一" class="headerlink" title="openCV图片灰度处理一"></a>openCV图片灰度处理一</h3><pre><code class="hljs python"><span class="hljs-keyword">import</span> cv2  <span class="hljs-comment"># 导入cv库</span>img0 = cv2.imread(<span class="hljs-string">'image0.jpg'</span>,<span class="hljs-number">0</span>)img1 = cv2.imread(<span class="hljs-string">'image0.jpg'</span>,<span class="hljs-number">1</span>)print(img0.shape)print(img1.shape)cv2.imshow(<span class="hljs-string">'src'</span>,img0)</code></pre><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20200601182945.png" srcset="/img/loading.gif" alt="1"></p><h3 id="openCV图片灰度处理二"><a href="#openCV图片灰度处理二" class="headerlink" title="openCV图片灰度处理二"></a>openCV图片灰度处理二</h3><pre><code class="hljs python"><span class="hljs-keyword">import</span> cv2img = cv2.imread(<span class="hljs-string">'image0.jpg'</span>,<span class="hljs-number">1</span>)dst = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)<span class="hljs-comment"># 颜色空间转换 1 data 2 BGR gray</span>cv2.imshow(<span class="hljs-string">'dst'</span>,dst)</code></pre><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20200601182946.png" srcset="/img/loading.gif" alt="1"></p><h3 id="openCV图片灰度处理三"><a href="#openCV图片灰度处理三" class="headerlink" title="openCV图片灰度处理三"></a>openCV图片灰度处理三</h3><pre><code class="hljs python"><span class="hljs-keyword">import</span> cv2<span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> npimg = cv2.imread(<span class="hljs-string">'image0.jpg'</span>,<span class="hljs-number">1</span>)imgInfo = img.shapeheight = imgInfo[<span class="hljs-number">0</span>]width = imgInfo[<span class="hljs-number">1</span>]<span class="hljs-comment"># RGB R=G=B = gray  (R+G+B)/3</span>dst = np.zeros((height,width,<span class="hljs-number">3</span>),np.uint8)<span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> range(<span class="hljs-number">0</span>,height):    <span class="hljs-keyword">for</span> j <span class="hljs-keyword">in</span> range(<span class="hljs-number">0</span>,width):        (b,g,r) = img[i,j]        gray = (int(b)+int(g)+int(r))/<span class="hljs-number">3</span>        dst[i,j] = np.uint8(gray)cv2.imshow(<span class="hljs-string">'dst'</span>,dst)</code></pre><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20200601182947.png" srcset="/img/loading.gif" alt="1"></p><h3 id="openCV图片灰度处理四"><a href="#openCV图片灰度处理四" class="headerlink" title="openCV图片灰度处理四"></a>openCV图片灰度处理四</h3><pre><code class="hljs python"><span class="hljs-comment">#方法4 gray = r*0.299+g*0.587+b*0.114</span><span class="hljs-keyword">import</span> cv2<span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> npimg = cv2.imread(<span class="hljs-string">'image0.jpg'</span>,<span class="hljs-number">1</span>)imgInfo = img.shapeheight = imgInfo[<span class="hljs-number">0</span>]width = imgInfo[<span class="hljs-number">1</span>]dst = np.zeros((height,width,<span class="hljs-number">3</span>),np.uint8)<span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> range(<span class="hljs-number">0</span>,height):    <span class="hljs-keyword">for</span> j <span class="hljs-keyword">in</span> range(<span class="hljs-number">0</span>,width):        (b,g,r) = img[i,j]        b = int(b)        g = int(g)        r = int(r)        gray = r*<span class="hljs-number">0.299</span>+g*<span class="hljs-number">0.587</span>+b*<span class="hljs-number">0.114</span>        dst[i,j] = np.uint8(gray)cv2.imshow(<span class="hljs-string">'dst'</span>,dst)</code></pre><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20200601182948.png" srcset="/img/loading.gif" alt="1"></p><h3 id="openCV图片灰度处理五-算法优化版"><a href="#openCV图片灰度处理五-算法优化版" class="headerlink" title="openCV图片灰度处理五-算法优化版"></a>openCV图片灰度处理五-算法优化版</h3><pre><code class="hljs python"><span class="hljs-comment"># r*0.299+g*0.587+b*0.114</span><span class="hljs-keyword">import</span> cv2<span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> npimg = cv2.imread(<span class="hljs-string">'image0.jpg'</span>,<span class="hljs-number">1</span>)imgInfo = img.shapeheight = imgInfo[<span class="hljs-number">0</span>]width = imgInfo[<span class="hljs-number">1</span>]<span class="hljs-comment"># RGB R=G=B = gray  (R+G+B)/3</span>dst = np.zeros((height,width,<span class="hljs-number">3</span>),np.uint8)<span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> range(<span class="hljs-number">0</span>,height):    <span class="hljs-keyword">for</span> j <span class="hljs-keyword">in</span> range(<span class="hljs-number">0</span>,width):        (b,g,r) = img[i,j]        b = int(b)        g = int(g)        r = int(r)        <span class="hljs-comment">#</span>        <span class="hljs-comment">#gray = (r*1+g*2+b*1)/4</span>        gray = (r+(g&lt;&lt;<span class="hljs-number">1</span>)+b)&gt;&gt;<span class="hljs-number">2</span>        dst[i,j] = np.uint8(gray)cv2.imshow(<span class="hljs-string">'dst'</span>,dst)</code></pre><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20200601182949.png" srcset="/img/loading.gif" alt="2"></p><h3 id="openCV图片颜色反转-灰色"><a href="#openCV图片颜色反转-灰色" class="headerlink" title="openCV图片颜色反转-灰色"></a>openCV图片颜色反转-灰色</h3><pre><code class="hljs python"><span class="hljs-keyword">import</span> cv2<span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> npimg = cv2.imread(<span class="hljs-string">'image0.jpg'</span>,<span class="hljs-number">1</span>)imgInfo = img.shapeheight = imgInfo[<span class="hljs-number">0</span>]width = imgInfo[<span class="hljs-number">1</span>]gray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY) <span class="hljs-comment"># 原始图片，灰度api</span>dst = np.zeros((height,width,<span class="hljs-number">1</span>),np.uint8) <span class="hljs-comment"># 反转矩阵</span><span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> range(<span class="hljs-number">0</span>,height):    <span class="hljs-keyword">for</span> j <span class="hljs-keyword">in</span> range(<span class="hljs-number">0</span>,width):        grayPixel = gray[i,j]          dst[i,j] = <span class="hljs-number">255</span>-grayPixelcv2.imshow(<span class="hljs-string">'dst'</span>,dst)</code></pre><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20200601182950.png" srcset="/img/loading.gif" alt="2"></p><h3 id="openCV图片颜色反转-彩色"><a href="#openCV图片颜色反转-彩色" class="headerlink" title="openCV图片颜色反转-彩色"></a>openCV图片颜色反转-彩色</h3><pre><code class="hljs python"><span class="hljs-keyword">import</span> cv2<span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> npimg = cv2.imread(<span class="hljs-string">'image0.jpg'</span>,<span class="hljs-number">1</span>)imgInfo = img.shapeheight = imgInfo[<span class="hljs-number">0</span>]width = imgInfo[<span class="hljs-number">1</span>]dst = np.zeros((height,width,<span class="hljs-number">3</span>),np.uint8)<span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> range(<span class="hljs-number">0</span>,height):    <span class="hljs-keyword">for</span> j <span class="hljs-keyword">in</span> range(<span class="hljs-number">0</span>,width):        (b,g,r) = img[i,j]        dst[i,j] = (<span class="hljs-number">255</span>-b,<span class="hljs-number">255</span>-g,<span class="hljs-number">255</span>-r)cv2.imshow(<span class="hljs-string">'dst'</span>,dst)</code></pre><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20200601182951.png" srcset="/img/loading.gif" alt="3"></p><h3 id="openCV图片马赛克"><a href="#openCV图片马赛克" class="headerlink" title="openCV图片马赛克"></a>openCV图片马赛克</h3><pre><code class="hljs python"><span class="hljs-keyword">import</span> cv2 <span class="hljs-comment"># 导入cv库</span><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> npimg = cv2.imread(<span class="hljs-string">'image0.jpg'</span>,<span class="hljs-number">1</span>) <span class="hljs-comment"># 读取图片文件， 1：彩色， 0：灰色</span>cv2.imshow(<span class="hljs-string">'src'</span>,img)imgInfo = img.shape  <span class="hljs-comment"># 获取图片的维度</span>height = imgInfo[<span class="hljs-number">0</span>]width = imgInfo[<span class="hljs-number">1</span>]<span class="hljs-keyword">for</span> m <span class="hljs-keyword">in</span> range(<span class="hljs-number">100</span>,<span class="hljs-number">300</span>):    <span class="hljs-keyword">for</span> n <span class="hljs-keyword">in</span> range(<span class="hljs-number">100</span>,<span class="hljs-number">200</span>):        <span class="hljs-comment"># pixel -&gt;10*10</span>        <span class="hljs-keyword">if</span> m%<span class="hljs-number">10</span> == <span class="hljs-number">0</span> <span class="hljs-keyword">and</span> n%<span class="hljs-number">10</span>==<span class="hljs-number">0</span>:            <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> range(<span class="hljs-number">0</span>,<span class="hljs-number">10</span>):                <span class="hljs-keyword">for</span> j <span class="hljs-keyword">in</span> range(<span class="hljs-number">0</span>,<span class="hljs-number">10</span>):                    (b,g,r) = img[m,n]                    img[i+m,j+n] = (b,g,r)cv2.imshow(<span class="hljs-string">'dst'</span>,img)</code></pre><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20200601182952.png" srcset="/img/loading.gif" alt="4"></p><h3 id="openCV图片毛玻璃"><a href="#openCV图片毛玻璃" class="headerlink" title="openCV图片毛玻璃"></a>openCV图片毛玻璃</h3><pre><code class="hljs python"><span class="hljs-keyword">import</span> cv2 <span class="hljs-comment"># 导入cv库</span><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> npimg = cv2.imread(<span class="hljs-string">'image0.jpg'</span>,<span class="hljs-number">1</span>) <span class="hljs-comment"># 读取图片文件， 1：彩色， 0：灰色</span>cv2.imshow(<span class="hljs-string">'src'</span>,img)imgInfo = img.shape  <span class="hljs-comment"># 获取图片的维度</span>height = imgInfo[<span class="hljs-number">0</span>]width = imgInfo[<span class="hljs-number">1</span>]dst = np.zeros((height,width,<span class="hljs-number">3</span>),np.uint8)mm = <span class="hljs-number">8</span><span class="hljs-keyword">for</span> m <span class="hljs-keyword">in</span> range(<span class="hljs-number">0</span>,height-mm):    <span class="hljs-keyword">for</span> n <span class="hljs-keyword">in</span> range(<span class="hljs-number">0</span>,width-mm):        index = int(random.random()*<span class="hljs-number">8</span>)<span class="hljs-comment">#0-8</span>        (b,g,r) = img[m+index,n+index]        dst[m,n] = (b,g,r)cv2.imshow(<span class="hljs-string">'dst'</span>,dst)</code></pre><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20200601182953.png" srcset="/img/loading.gif" alt="4"></p><h3 id="openCV图片融合"><a href="#openCV图片融合" class="headerlink" title="openCV图片融合"></a>openCV图片融合</h3><pre><code class="hljs python"><span class="hljs-comment"># dst  = src1*a+src2*(1-a)</span><span class="hljs-keyword">import</span> cv2<span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> npimg0 = cv2.imread(<span class="hljs-string">'image0.jpg'</span>,<span class="hljs-number">1</span>)img1 = cv2.imread(<span class="hljs-string">'image1.jpg'</span>,<span class="hljs-number">1</span>)imgInfo = img0.shapeheight = imgInfo[<span class="hljs-number">0</span>]width = imgInfo[<span class="hljs-number">1</span>]<span class="hljs-comment"># ROI</span>roiH = int(height/<span class="hljs-number">2</span>)roiW = int(width/<span class="hljs-number">2</span>)img0ROI = img0[<span class="hljs-number">0</span>:roiH,<span class="hljs-number">0</span>:roiW]img1ROI = img1[<span class="hljs-number">0</span>:roiH,<span class="hljs-number">0</span>:roiW]<span class="hljs-comment"># dst</span>dst = np.zeros((roiH,roiW,<span class="hljs-number">3</span>),np.uint8)dst = cv2.addWeighted(img0ROI,<span class="hljs-number">0.5</span>,img1ROI,<span class="hljs-number">0.5</span>,<span class="hljs-number">0</span>)<span class="hljs-comment">#add src1*a+src2*(1-a)</span><span class="hljs-comment"># 1 src1 2 a 3 src2 4 1-a</span>cv2.imshow(<span class="hljs-string">'dst'</span>,dst)</code></pre><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20200601182954.png" srcset="/img/loading.gif" alt="4"></p><h3 id="openCV边缘检测一"><a href="#openCV边缘检测一" class="headerlink" title="openCV边缘检测一"></a>openCV边缘检测一</h3><pre><code class="hljs python"><span class="hljs-keyword">import</span> cv2<span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<span class="hljs-keyword">import</span> randomimg = cv2.imread(<span class="hljs-string">'image0.jpg'</span>,<span class="hljs-number">1</span>)imgInfo = img.shapeheight = imgInfo[<span class="hljs-number">0</span>]width = imgInfo[<span class="hljs-number">1</span>]cv2.imshow(<span class="hljs-string">'src'</span>,img)<span class="hljs-comment">#canny 1 gray 2 高斯 3 canny </span>gray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)imgG = cv2.GaussianBlur(gray,(<span class="hljs-number">3</span>,<span class="hljs-number">3</span>),<span class="hljs-number">0</span>)dst = cv2.Canny(img,<span class="hljs-number">50</span>,<span class="hljs-number">50</span>) <span class="hljs-comment">#图片卷积——》th</span>cv2.imshow(<span class="hljs-string">'dst'</span>,dst)</code></pre><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20200601182955.png" srcset="/img/loading.gif" alt="4"></p><h3 id="openCV边缘检测二"><a href="#openCV边缘检测二" class="headerlink" title="openCV边缘检测二"></a>openCV边缘检测二</h3><pre><code class="hljs python"><span class="hljs-keyword">import</span> cv2<span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<span class="hljs-keyword">import</span> random<span class="hljs-keyword">import</span> mathimg = cv2.imread(<span class="hljs-string">'image0.jpg'</span>,<span class="hljs-number">1</span>)imgInfo = img.shapeheight = imgInfo[<span class="hljs-number">0</span>]width = imgInfo[<span class="hljs-number">1</span>]cv2.imshow(<span class="hljs-string">'src'</span>,img)<span class="hljs-comment"># sobel 1 算子模版 2 图片卷积 3 阈值判决 </span><span class="hljs-comment"># [1 2 1          [ 1 0 -1</span><span class="hljs-comment">#  0 0 0            2 0 -2</span><span class="hljs-comment"># -1 -2 -1 ]       1 0 -1 ]</span>              <span class="hljs-comment"># [1 2 3 4] [a b c d] a*1+b*2+c*3+d*4 = dst</span><span class="hljs-comment"># sqrt(a*a+b*b) = f&gt;th</span>gray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)dst = np.zeros((height,width,<span class="hljs-number">1</span>),np.uint8)<span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> range(<span class="hljs-number">0</span>,height<span class="hljs-number">-2</span>):    <span class="hljs-keyword">for</span> j <span class="hljs-keyword">in</span> range(<span class="hljs-number">0</span>,width<span class="hljs-number">-2</span>):        gy = gray[i,j]*<span class="hljs-number">1</span>+gray[i,j+<span class="hljs-number">1</span>]*<span class="hljs-number">2</span>+gray[i,j+<span class="hljs-number">2</span>]*<span class="hljs-number">1</span>-gray[i+<span class="hljs-number">2</span>,j]*<span class="hljs-number">1</span>-gray[i+<span class="hljs-number">2</span>,j+<span class="hljs-number">1</span>]*<span class="hljs-number">2</span>-gray[i+<span class="hljs-number">2</span>,j+<span class="hljs-number">2</span>]*<span class="hljs-number">1</span>        gx = gray[i,j]+gray[i+<span class="hljs-number">1</span>,j]*<span class="hljs-number">2</span>+gray[i+<span class="hljs-number">2</span>,j]-gray[i,j+<span class="hljs-number">2</span>]-gray[i+<span class="hljs-number">1</span>,j+<span class="hljs-number">2</span>]*<span class="hljs-number">2</span>-gray[i+<span class="hljs-number">2</span>,j+<span class="hljs-number">2</span>]        grad = math.sqrt(gx*gx+gy*gy)        <span class="hljs-keyword">if</span> grad&gt;<span class="hljs-number">50</span>:            dst[i,j] = <span class="hljs-number">255</span>        <span class="hljs-keyword">else</span>:            dst[i,j] = <span class="hljs-number">0</span>cv2.imshow(<span class="hljs-string">'dst'</span>,dst)</code></pre><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20200601182956.png" srcset="/img/loading.gif" alt="4"></p><h3 id="openCV图像浮雕风格"><a href="#openCV图像浮雕风格" class="headerlink" title="openCV图像浮雕风格"></a>openCV图像浮雕风格</h3><pre><code class="hljs python"><span class="hljs-keyword">import</span> cv2<span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> npimg = cv2.imread(<span class="hljs-string">'image0.jpg'</span>,<span class="hljs-number">1</span>)imgInfo = img.shapeheight = imgInfo[<span class="hljs-number">0</span>]width = imgInfo[<span class="hljs-number">1</span>]gray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)<span class="hljs-comment"># newP = gray0-gray1+150</span>dst = np.zeros((height,width,<span class="hljs-number">1</span>),np.uint8)<span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> range(<span class="hljs-number">0</span>,height):    <span class="hljs-keyword">for</span> j <span class="hljs-keyword">in</span> range(<span class="hljs-number">0</span>,width<span class="hljs-number">-1</span>):        grayP0 = int(gray[i,j])        grayP1 = int(gray[i,j+<span class="hljs-number">1</span>])        newP = grayP0-grayP1+<span class="hljs-number">150</span>        <span class="hljs-keyword">if</span> newP &gt; <span class="hljs-number">255</span>:            newP = <span class="hljs-number">255</span>        <span class="hljs-keyword">if</span> newP &lt; <span class="hljs-number">0</span>:            newP = <span class="hljs-number">0</span>        dst[i,j] = newPcv2.imshow(<span class="hljs-string">'dst'</span>,dst)</code></pre><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20200601182957.png" srcset="/img/loading.gif" alt="4"></p><h3 id="openCV图片颜色风格"><a href="#openCV图片颜色风格" class="headerlink" title="openCV图片颜色风格"></a>openCV图片颜色风格</h3><pre><code class="hljs python"><span class="hljs-keyword">import</span> cv2<span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> npimg = cv2.imread(<span class="hljs-string">'image0.jpg'</span>,<span class="hljs-number">1</span>)cv2.imshow(<span class="hljs-string">'src'</span>,img)imgInfo = img.shapeheight = imgInfo[<span class="hljs-number">0</span>]width = imgInfo[<span class="hljs-number">1</span>]<span class="hljs-comment">#rgb -》RGB new “蓝色”</span><span class="hljs-comment"># b=b*1.5</span><span class="hljs-comment"># g = g*1.3</span>dst = np.zeros((height,width,<span class="hljs-number">3</span>),np.uint8)<span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> range(<span class="hljs-number">0</span>,height):    <span class="hljs-keyword">for</span> j <span class="hljs-keyword">in</span> range(<span class="hljs-number">0</span>,width):        (b,g,r) = img[i,j]        b = b*<span class="hljs-number">1.5</span>        g = g*<span class="hljs-number">1.3</span>        <span class="hljs-keyword">if</span> b&gt;<span class="hljs-number">255</span>:            b = <span class="hljs-number">255</span>        <span class="hljs-keyword">if</span> g&gt;<span class="hljs-number">255</span>:            g = <span class="hljs-number">255</span>        dst[i,j]=(b,g,r)cv2.imshow(<span class="hljs-string">'dst'</span>,dst)</code></pre><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20200601182958.png" srcset="/img/loading.gif" alt="4"></p><h3 id="openCV油画特效"><a href="#openCV油画特效" class="headerlink" title="openCV油画特效"></a>openCV油画特效</h3><pre><code class="hljs python"><span class="hljs-keyword">import</span> cv2<span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> npimg = cv2.imread(<span class="hljs-string">'image00.jpg'</span>,<span class="hljs-number">1</span>)cv2.imshow(<span class="hljs-string">'src'</span>,img)imgInfo = img.shapeheight = imgInfo[<span class="hljs-number">0</span>]width = imgInfo[<span class="hljs-number">1</span>]gray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)dst = np.zeros((height,width,<span class="hljs-number">3</span>),np.uint8)<span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> range(<span class="hljs-number">4</span>,height<span class="hljs-number">-4</span>):    <span class="hljs-keyword">for</span> j <span class="hljs-keyword">in</span> range(<span class="hljs-number">4</span>,width<span class="hljs-number">-4</span>):        array1 = np.zeros(<span class="hljs-number">8</span>,np.uint8)        <span class="hljs-keyword">for</span> m <span class="hljs-keyword">in</span> range(<span class="hljs-number">-4</span>,<span class="hljs-number">4</span>):            <span class="hljs-keyword">for</span> n <span class="hljs-keyword">in</span> range(<span class="hljs-number">-4</span>,<span class="hljs-number">4</span>):                p1 = int(gray[i+m,j+n]/<span class="hljs-number">32</span>)                array1[p1] = array1[p1]+<span class="hljs-number">1</span>        currentMax = array1[<span class="hljs-number">0</span>]        l = <span class="hljs-number">0</span>        <span class="hljs-keyword">for</span> k <span class="hljs-keyword">in</span> range(<span class="hljs-number">0</span>,<span class="hljs-number">8</span>):            <span class="hljs-keyword">if</span> currentMax&lt;array1[k]:                currentMax = array1[k]                l = k        <span class="hljs-comment"># 简化 均值</span>        <span class="hljs-keyword">for</span> m <span class="hljs-keyword">in</span> range(<span class="hljs-number">-4</span>,<span class="hljs-number">4</span>):            <span class="hljs-keyword">for</span> n <span class="hljs-keyword">in</span> range(<span class="hljs-number">-4</span>,<span class="hljs-number">4</span>):                <span class="hljs-keyword">if</span> gray[i+m,j+n]&gt;=(l*<span class="hljs-number">32</span>) <span class="hljs-keyword">and</span> gray[i+m,j+n]&lt;=((l+<span class="hljs-number">1</span>)*<span class="hljs-number">32</span>):                    (b,g,r) = img[i+m,j+n]        dst[i,j] = (b,g,r)cv2.imshow(<span class="hljs-string">'dst'</span>,dst)</code></pre><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20200601182959.png" srcset="/img/loading.gif" alt="4"></p><h3 id="openCV线段绘制"><a href="#openCV线段绘制" class="headerlink" title="openCV线段绘制"></a>openCV线段绘制</h3><pre><code class="hljs python"><span class="hljs-keyword">import</span> cv2<span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> npnewImageInfo = (<span class="hljs-number">500</span>,<span class="hljs-number">500</span>,<span class="hljs-number">3</span>)dst = np.zeros(newImageInfo,np.uint8)<span class="hljs-comment"># line</span><span class="hljs-comment"># 绘制线段 1 dst 2 begin 3 end 4 color</span>cv2.line(dst,(<span class="hljs-number">100</span>,<span class="hljs-number">100</span>),(<span class="hljs-number">400</span>,<span class="hljs-number">400</span>),(<span class="hljs-number">0</span>,<span class="hljs-number">0</span>,<span class="hljs-number">255</span>))<span class="hljs-comment"># 5 line w</span>cv2.line(dst,(<span class="hljs-number">100</span>,<span class="hljs-number">200</span>),(<span class="hljs-number">400</span>,<span class="hljs-number">200</span>),(<span class="hljs-number">0</span>,<span class="hljs-number">255</span>,<span class="hljs-number">255</span>),<span class="hljs-number">20</span>)<span class="hljs-comment"># 6 line type</span>cv2.line(dst,(<span class="hljs-number">100</span>,<span class="hljs-number">300</span>),(<span class="hljs-number">400</span>,<span class="hljs-number">300</span>),(<span class="hljs-number">0</span>,<span class="hljs-number">255</span>,<span class="hljs-number">0</span>),<span class="hljs-number">20</span>,cv2.LINE_AA)cv2.line(dst,(<span class="hljs-number">200</span>,<span class="hljs-number">150</span>),(<span class="hljs-number">50</span>,<span class="hljs-number">250</span>),(<span class="hljs-number">25</span>,<span class="hljs-number">100</span>,<span class="hljs-number">255</span>))cv2.line(dst,(<span class="hljs-number">50</span>,<span class="hljs-number">250</span>),(<span class="hljs-number">400</span>,<span class="hljs-number">380</span>),(<span class="hljs-number">25</span>,<span class="hljs-number">100</span>,<span class="hljs-number">255</span>))cv2.line(dst,(<span class="hljs-number">400</span>,<span class="hljs-number">380</span>),(<span class="hljs-number">200</span>,<span class="hljs-number">150</span>),(<span class="hljs-number">25</span>,<span class="hljs-number">100</span>,<span class="hljs-number">255</span>))cv2.imshow(<span class="hljs-string">'dst'</span>,dst)</code></pre><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20200601183000.png" srcset="/img/loading.gif" alt="4"></p><h3 id="openCV矩形圆形绘制"><a href="#openCV矩形圆形绘制" class="headerlink" title="openCV矩形圆形绘制"></a>openCV矩形圆形绘制</h3><pre><code class="hljs python"><span class="hljs-keyword">import</span> cv2<span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> npnewImageInfo = (<span class="hljs-number">500</span>,<span class="hljs-number">500</span>,<span class="hljs-number">3</span>)dst = np.zeros(newImageInfo,np.uint8)<span class="hljs-comment">#  1 2 左上角 3 右下角 4 5 fill -1 &gt;0 line w</span>cv2.rectangle(dst,(<span class="hljs-number">50</span>,<span class="hljs-number">100</span>),(<span class="hljs-number">200</span>,<span class="hljs-number">300</span>),(<span class="hljs-number">255</span>,<span class="hljs-number">0</span>,<span class="hljs-number">0</span>),<span class="hljs-number">5</span>)<span class="hljs-comment"># 2 center 3 r </span>cv2.circle(dst,(<span class="hljs-number">250</span>,<span class="hljs-number">250</span>),(<span class="hljs-number">50</span>),(<span class="hljs-number">0</span>,<span class="hljs-number">255</span>,<span class="hljs-number">0</span>),<span class="hljs-number">2</span>)<span class="hljs-comment"># 2 center 3 轴 4 angle 5 begin 6 end 7 </span>cv2.ellipse(dst,(<span class="hljs-number">256</span>,<span class="hljs-number">256</span>),(<span class="hljs-number">150</span>,<span class="hljs-number">100</span>),<span class="hljs-number">0</span>,<span class="hljs-number">0</span>,<span class="hljs-number">180</span>,(<span class="hljs-number">255</span>,<span class="hljs-number">255</span>,<span class="hljs-number">0</span>),<span class="hljs-number">-1</span>)points = np.array([[<span class="hljs-number">150</span>,<span class="hljs-number">50</span>],[<span class="hljs-number">140</span>,<span class="hljs-number">140</span>],[<span class="hljs-number">200</span>,<span class="hljs-number">170</span>],[<span class="hljs-number">250</span>,<span class="hljs-number">250</span>],[<span class="hljs-number">150</span>,<span class="hljs-number">50</span>]],np.int32)print(points.shape)points = points.reshape((<span class="hljs-number">-1</span>,<span class="hljs-number">1</span>,<span class="hljs-number">2</span>))print(points.shape)cv2.polylines(dst,[points],<span class="hljs-literal">True</span>,(<span class="hljs-number">0</span>,<span class="hljs-number">255</span>,<span class="hljs-number">255</span>))cv2.imshow(<span class="hljs-string">'dst'</span>,dst)</code></pre><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20200601183001.png" srcset="/img/loading.gif" alt="4"></p><h3 id="openCV文字绘制"><a href="#openCV文字绘制" class="headerlink" title="openCV文字绘制"></a>openCV文字绘制</h3><pre><code class="hljs python"><span class="hljs-keyword">import</span> cv2 <span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> npimg = cv2.imread(<span class="hljs-string">'image0.jpg'</span>,<span class="hljs-number">1</span>)font = cv2.FONT_HERSHEY_SIMPLEXcv2.rectangle(img,(<span class="hljs-number">200</span>,<span class="hljs-number">100</span>),(<span class="hljs-number">500</span>,<span class="hljs-number">400</span>),(<span class="hljs-number">0</span>,<span class="hljs-number">255</span>,<span class="hljs-number">0</span>),<span class="hljs-number">3</span>)<span class="hljs-comment"># 1 dst 2 文字内容 3 坐标 4 5 字体大小 6 color 7 粗细 8 line type</span>cv2.putText(img,<span class="hljs-string">'this is flow'</span>,(<span class="hljs-number">100</span>,<span class="hljs-number">300</span>),font,<span class="hljs-number">1</span>,(<span class="hljs-number">200</span>,<span class="hljs-number">100</span>,<span class="hljs-number">255</span>),<span class="hljs-number">2</span>,cv2.LINE_AA)cv2.imshow(<span class="hljs-string">'src'</span>,img)</code></pre><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20200601183002.png" srcset="/img/loading.gif" alt="4"></p><h3 id="openCV图片绘制"><a href="#openCV图片绘制" class="headerlink" title="openCV图片绘制"></a>openCV图片绘制</h3><pre><code class="hljs python"><span class="hljs-keyword">import</span> cv2 img = cv2.imread(<span class="hljs-string">'image0.jpg'</span>,<span class="hljs-number">1</span>)height = int(img.shape[<span class="hljs-number">0</span>]*<span class="hljs-number">0.2</span>)width = int(img.shape[<span class="hljs-number">1</span>]*<span class="hljs-number">0.2</span>)imgResize = cv2.resize(img,(width,height))<span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> range(<span class="hljs-number">0</span>,height):    <span class="hljs-keyword">for</span> j <span class="hljs-keyword">in</span> range(<span class="hljs-number">0</span>,width):        img[i+<span class="hljs-number">200</span>,j+<span class="hljs-number">350</span>] = imgResize[i,j]cv2.imshow(<span class="hljs-string">'src'</span>,img)</code></pre><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20200601183003.png" srcset="/img/loading.gif" alt="4"></p>]]></content>
    
    
    <categories>
      
      <category>opencv</category>
      
    </categories>
    
    
    <tags>
      
      <tag>计算机视觉</tag>
      
      <tag>opencv</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>pip换源</title>
    <link href="/2018/10/pip/"/>
    <url>/2018/10/pip/</url>
    
    <content type="html"><![CDATA[<h3 id="pip在windows和linux上换源的方法："><a href="#pip在windows和linux上换源的方法：" class="headerlink" title="pip在windows和linux上换源的方法："></a>pip在windows和linux上换源的方法：</h3><h4 id="pip国内的一些镜像"><a href="#pip国内的一些镜像" class="headerlink" title="pip国内的一些镜像"></a><strong>pip国内的一些镜像</strong></h4><ul><li>  阿里云 <a href="http://mirrors.aliyun.com/pypi/simple/" target="_blank" rel="noopener">http://mirrors.aliyun.com/pypi/simple/</a> </li><li>  中国科技大学 <a href="https://pypi.mirrors.ustc.edu.cn/simple/" target="_blank" rel="noopener">https://pypi.mirrors.ustc.edu.cn/simple/</a> </li><li>  豆瓣(douban) <a href="http://pypi.douban.com/simple/" target="_blank" rel="noopener">http://pypi.douban.com/simple/</a> </li><li>  清华大学 <a href="https://pypi.tuna.tsinghua.edu.cn/simple/" target="_blank" rel="noopener">https://pypi.tuna.tsinghua.edu.cn/simple/</a> </li><li>  中国科学技术大学 <a href="http://pypi.mirrors.ustc.edu.cn/simple/" target="_blank" rel="noopener">http://pypi.mirrors.ustc.edu.cn/simple/</a></li></ul><h3 id="修改源方法："><a href="#修改源方法：" class="headerlink" title="修改源方法："></a><strong>修改源方法：</strong></h3><h4 id="临时使用："><a href="#临时使用：" class="headerlink" title="临时使用："></a><strong>临时使用：</strong></h4><p> 可以在使用pip的时候在后面加上-i参数，指定pip源 </p><pre><code class="hljs bash">pip install numpy -i &lt;https://pypi.tuna.tsinghua.edu.cn/simple&gt;</code></pre><h4 id="永久修改："><a href="#永久修改：" class="headerlink" title="永久修改："></a><strong>永久修改：</strong></h4><h4 id="linux"><a href="#linux" class="headerlink" title="linux:"></a><strong>linux:</strong></h4><p> 修改或创建 ~/.pip/pip.conf  ：</p><pre><code class="hljs shell">[global]index-url = https://pypi.tuna.tsinghua.edu.cn/simple</code></pre><h4 id="windows"><a href="#windows" class="headerlink" title="windows:"></a><strong>windows:</strong></h4><p> 直接在user目录中创建一个pip目录，如：C:\Users\xx\pip，新建文件pip.ini，内容如下</p><pre><code class="hljs shell">[global]index-url = https://pypi.tuna.tsinghua.edu.cn/simple</code></pre>]]></content>
    
    
    <categories>
      
      <category>linux</category>
      
    </categories>
    
    
    <tags>
      
      <tag>linux</tag>
      
      <tag>pip</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>ubuntu换源</title>
    <link href="/2018/10/source/"/>
    <url>/2018/10/source/</url>
    
    <content type="html"><![CDATA[<h3 id="查找国内的开源镜像提供的源地址"><a href="#查找国内的开源镜像提供的源地址" class="headerlink" title="查找国内的开源镜像提供的源地址"></a><strong>查找国内的开源镜像提供的源地址</strong></h3><blockquote><p>Ubuntu的源的list文件位于 /etc/apt/sources.list</p></blockquote><ul><li><strong>高校镜像源</strong><br><a href="https://link.zhihu.com/?target=https%3A//mirrors.tuna.tsinghua.edu.cn/help/ubuntu/">清华大学开源镜像站</a><br><a href="https://link.zhihu.com/?target=https%3A//mirrors.ustc.edu.cn/repogen/">中科大开源镜像站</a></li><li><strong>企业镜像源</strong><br><a href="https://link.zhihu.com/?target=http%3A//mirrors.aliyun.com/help/ubuntu">阿里云开源镜像站</a><br><a href="https://link.zhihu.com/?target=http%3A//mirrors.163.com/.help/ubuntu.html">网易开源镜像站</a></li></ul><p><strong>这里以清华源为例</strong></p><p>先去查一下清华源的帮助文档，戳 <a href="https://link.zhihu.com/?target=https%3A//mirrors.tuna.tsinghua.edu.cn/help/ubuntu/">清华大学开源镜像站</a>，选择相匹配的Ubuntu的版本，会得到软件源镜像的地址。</p><p>接下来直接替换一下系统的source.list文件</p><pre><code class="hljs shell"><span class="hljs-meta">#</span><span class="bash"> 备份一下</span>sudo cp /etc/apt/sources.list /etc/apt/sources.list.bak<span class="hljs-meta">#</span><span class="bash"> 修改</span>sudo vim /etc/apt/sources.list</code></pre><p>将文档里面所有的内容删除，然后替换为清华镜像提供的软件源镜像的地址。<br>例如我用的 18.04 版， 替换为：</p><pre><code class="hljs shell"><span class="hljs-meta">#</span><span class="bash"> 默认注释了源码镜像以提高 apt update 速度，如有需要可自行取消注释</span>deb https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ bionic main restricted universe multiverse<span class="hljs-meta">#</span><span class="bash"> deb-src https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ bionic main restricted universe multiverse</span>deb https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ bionic-updates main restricted universe multiverse<span class="hljs-meta">#</span><span class="bash"> deb-src https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ bionic-updates main restricted universe multiverse</span>deb https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ bionic-backports main restricted universe multiverse<span class="hljs-meta">#</span><span class="bash"> deb-src https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ bionic-backports main restricted universe multiverse</span>deb https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ bionic-security main restricted universe multiverse<span class="hljs-meta">#</span><span class="bash"> deb-src https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ bionic-security main restricted universe multiverse</span></code></pre><p>然后再update一下，你会发现速度还是有明显的提升的。</p><pre><code class="hljs bash">sudo apt update</code></pre><h3 id="一行命令搞定：阿里源"><a href="#一行命令搞定：阿里源" class="headerlink" title="一行命令搞定：阿里源"></a>一行命令搞定：阿里源</h3><pre><code class="hljs bash">sudo python3 -c <span class="hljs-string">"d='mirrors.aliyun.com';import re;from pathlib import Path;p=Path('/etc/apt/sources.list');s=p.read_text();bak=p.with_name(p.name+'.bak');bak.exists() or bak.write_text(s);p.write_text(re.sub(r'(cn.archive|security|archive)\.ubuntu\.com', d, s))"</span></code></pre>]]></content>
    
    
    <categories>
      
      <category>linux</category>
      
    </categories>
    
    
    <tags>
      
      <tag>linux</tag>
      
      <tag>ubuntu</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>TensorFlow安装问题检测</title>
    <link href="/2018/10/tensorflow-question/"/>
    <url>/2018/10/tensorflow-question/</url>
    
    <content type="html"><![CDATA[<h4 id="当你安装TensorFlow后，import时需要未知的错误，以下代码会为你检测出来并提供正确的解决方式"><a href="#当你安装TensorFlow后，import时需要未知的错误，以下代码会为你检测出来并提供正确的解决方式" class="headerlink" title="当你安装TensorFlow后，import时需要未知的错误，以下代码会为你检测出来并提供正确的解决方式"></a>当你安装TensorFlow后，import时需要未知的错误，以下代码会为你检测出来并提供正确的解决方式</h4><pre><code class="hljs python"><span class="hljs-keyword">import</span> ctypes<span class="hljs-keyword">import</span> imp<span class="hljs-keyword">import</span> sys<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">main</span><span class="hljs-params">()</span>:</span>  <span class="hljs-keyword">try</span>:    <span class="hljs-keyword">import</span> tensorflow <span class="hljs-keyword">as</span> tf    print(<span class="hljs-string">"TensorFlow successfully installed."</span>)    <span class="hljs-keyword">if</span> tf.test.is_built_with_cuda():      print(<span class="hljs-string">"The installed version of TensorFlow includes GPU support."</span>)    <span class="hljs-keyword">else</span>:      print(<span class="hljs-string">"The installed version of TensorFlow does not include GPU support."</span>)    sys.exit(<span class="hljs-number">0</span>)  <span class="hljs-keyword">except</span> ImportError:    print(<span class="hljs-string">"ERROR: Failed to import the TensorFlow module."</span>)  print(<span class="hljs-string">"""</span><span class="hljs-string">WARNING! This script is no longer maintained! </span><span class="hljs-string">=============================================</span><span class="hljs-string">Since TensorFlow 1.4, the self-check has been integrated with TensorFlow itself,</span><span class="hljs-string">and any missing DLLs will be reported when you execute the `import tensorflow`</span><span class="hljs-string">statement. The error messages printed below refer to TensorFlow 1.3 and earlier,</span><span class="hljs-string">and are inaccurate for later versions of TensorFlow."""</span>)      candidate_explanation = <span class="hljs-literal">False</span>  python_version = sys.version_info.major, sys.version_info.minor  print(<span class="hljs-string">"\n- Python version is %d.%d."</span> % python_version)  <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> (python_version == (<span class="hljs-number">3</span>, <span class="hljs-number">5</span>) <span class="hljs-keyword">or</span> python_version == (<span class="hljs-number">3</span>, <span class="hljs-number">6</span>)):    candidate_explanation = <span class="hljs-literal">True</span>    print(<span class="hljs-string">"- The official distribution of TensorFlow for Windows requires "</span>          <span class="hljs-string">"Python version 3.5 or 3.6."</span>)    <span class="hljs-keyword">try</span>:    _, pathname, _ = imp.find_module(<span class="hljs-string">"tensorflow"</span>)    print(<span class="hljs-string">"\n- TensorFlow is installed at: %s"</span> % pathname)  <span class="hljs-keyword">except</span> ImportError:    candidate_explanation = <span class="hljs-literal">False</span>    print(<span class="hljs-string">"""</span><span class="hljs-string">- No module named TensorFlow is installed in this Python environment. You may</span><span class="hljs-string">  install it using the command `pip install tensorflow`."""</span>)  <span class="hljs-keyword">try</span>:    msvcp140 = ctypes.WinDLL(<span class="hljs-string">"msvcp140.dll"</span>)  <span class="hljs-keyword">except</span> OSError:    candidate_explanation = <span class="hljs-literal">True</span>    print(<span class="hljs-string">"""</span><span class="hljs-string">- Could not load 'msvcp140.dll'. TensorFlow requires that this DLL be</span><span class="hljs-string">  installed in a directory that is named in your %PATH% environment</span><span class="hljs-string">  variable. You may install this DLL by downloading Microsoft Visual</span><span class="hljs-string">  C++ 2015 Redistributable Update 3 from this URL:</span><span class="hljs-string">  https://www.microsoft.com/en-us/download/details.aspx?id=53587"""</span>)  <span class="hljs-keyword">try</span>:    cudart64_80 = ctypes.WinDLL(<span class="hljs-string">"cudart64_80.dll"</span>)  <span class="hljs-keyword">except</span> OSError:    candidate_explanation = <span class="hljs-literal">True</span>    print(<span class="hljs-string">"""</span><span class="hljs-string">- Could not load 'cudart64_80.dll'. The GPU version of TensorFlow</span><span class="hljs-string">  requires that this DLL be installed in a directory that is named in</span><span class="hljs-string">  your %PATH% environment variable. Download and install CUDA 8.0 from</span><span class="hljs-string">  this URL: https://developer.nvidia.com/cuda-toolkit"""</span>)  <span class="hljs-keyword">try</span>:    nvcuda = ctypes.WinDLL(<span class="hljs-string">"nvcuda.dll"</span>)  <span class="hljs-keyword">except</span> OSError:    candidate_explanation = <span class="hljs-literal">True</span>    print(<span class="hljs-string">"""</span><span class="hljs-string">- Could not load 'nvcuda.dll'. The GPU version of TensorFlow requires that</span><span class="hljs-string">  this DLL be installed in a directory that is named in your %PATH%</span><span class="hljs-string">  environment variable. Typically it is installed in 'C:\Windows\System32'.</span><span class="hljs-string">  If it is not present, ensure that you have a CUDA-capable GPU with the</span><span class="hljs-string">  correct driver installed."""</span>)  cudnn5_found = <span class="hljs-literal">False</span>  <span class="hljs-keyword">try</span>:    cudnn5 = ctypes.WinDLL(<span class="hljs-string">"cudnn64_5.dll"</span>)    cudnn5_found = <span class="hljs-literal">True</span>  <span class="hljs-keyword">except</span> OSError:    candidate_explanation = <span class="hljs-literal">True</span>    print(<span class="hljs-string">"""</span><span class="hljs-string">- Could not load 'cudnn64_5.dll'. The GPU version of TensorFlow</span><span class="hljs-string">  requires that this DLL be installed in a directory that is named in</span><span class="hljs-string">  your %PATH% environment variable. Note that installing cuDNN is a</span><span class="hljs-string">  separate step from installing CUDA, and it is often found in a</span><span class="hljs-string">  different directory from the CUDA DLLs. You may install the</span><span class="hljs-string">  necessary DLL by downloading cuDNN 5.1 from this URL:</span><span class="hljs-string">  https://developer.nvidia.com/cudnn"""</span>)  cudnn6_found = <span class="hljs-literal">False</span>  <span class="hljs-keyword">try</span>:    cudnn = ctypes.WinDLL(<span class="hljs-string">"cudnn64_6.dll"</span>)    cudnn6_found = <span class="hljs-literal">True</span>  <span class="hljs-keyword">except</span> OSError:    candidate_explanation = <span class="hljs-literal">True</span>  <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> cudnn5_found <span class="hljs-keyword">or</span> <span class="hljs-keyword">not</span> cudnn6_found:    print()    <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> cudnn5_found <span class="hljs-keyword">and</span> <span class="hljs-keyword">not</span> cudnn6_found:      print(<span class="hljs-string">"- Could not find cuDNN."</span>)    <span class="hljs-keyword">elif</span> <span class="hljs-keyword">not</span> cudnn5_found:      print(<span class="hljs-string">"- Could not find cuDNN 5.1."</span>)    <span class="hljs-keyword">else</span>:      print(<span class="hljs-string">"- Could not find cuDNN 6."</span>)      print(<span class="hljs-string">"""</span><span class="hljs-string">  The GPU version of TensorFlow requires that the correct cuDNN DLL be installed</span><span class="hljs-string">  in a directory that is named in your %PATH% environment variable. Note that</span><span class="hljs-string">  installing cuDNN is a separate step from installing CUDA, and it is often</span><span class="hljs-string">  found in a different directory from the CUDA DLLs. The correct version of</span><span class="hljs-string">  cuDNN depends on your version of TensorFlow:</span><span class="hljs-string">  </span><span class="hljs-string">  * TensorFlow 1.2.1 or earlier requires cuDNN 5.1. ('cudnn64_5.dll')</span><span class="hljs-string">  * TensorFlow 1.3 or later requires cuDNN 6. ('cudnn64_6.dll')</span><span class="hljs-string">    </span><span class="hljs-string">  You may install the necessary DLL by downloading cuDNN from this URL:</span><span class="hljs-string">  https://developer.nvidia.com/cudnn"""</span>)      <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> candidate_explanation:    print(<span class="hljs-string">"""</span><span class="hljs-string">- All required DLLs appear to be present. Please open an issue on the</span><span class="hljs-string">  TensorFlow GitHub page: https://github.com/tensorflow/tensorflow/issues"""</span>)  sys.exit(<span class="hljs-number">-1</span>)<span class="hljs-keyword">if</span> __name__ == <span class="hljs-string">"__main__"</span>:  main()</code></pre>]]></content>
    
    
    <categories>
      
      <category>TensorFlow</category>
      
    </categories>
    
    
    <tags>
      
      <tag>linux</tag>
      
      <tag>pip</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Pytorch实现AlexNet</title>
    <link href="/2018/09/Pytorch-AlexNet/"/>
    <url>/2018/09/Pytorch-AlexNet/</url>
    
    <content type="html"><![CDATA[<h3 id="示例代码："><a href="#示例代码：" class="headerlink" title="示例代码："></a>示例代码：</h3><pre><code class="hljs python"><span class="hljs-keyword">import</span> torch.nn <span class="hljs-keyword">as</span> nn<span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">AlexNet</span><span class="hljs-params">(nn.Module)</span>:</span>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span><span class="hljs-params">(self)</span>:</span>        super(AlexNet, self).__init__()        self.conv1 = nn.Sequential(            nn.Conv2d(in_channels=<span class="hljs-number">3</span>,                      out_channels=<span class="hljs-number">96</span>,                      kernel_size=<span class="hljs-number">2</span>,                      stride=<span class="hljs-number">1</span>,                      padding=<span class="hljs-number">1</span>),            nn.ReLU(),            nn.MaxPool2d(kernel_size=<span class="hljs-number">2</span>)        )        self.conv2 = nn.Sequential(            nn.Conv2d(in_channels=<span class="hljs-number">96</span>,                      out_channels=<span class="hljs-number">256</span>,                      kernel_size=<span class="hljs-number">2</span>,                      stride=<span class="hljs-number">1</span>,                      padding=<span class="hljs-number">1</span>),            nn.ReLU(),            nn.MaxPool2d(kernel_size=<span class="hljs-number">2</span>)        )        self.conv3 = nn.Sequential(            nn.Conv2d(in_channels=<span class="hljs-number">256</span>,                      out_channels=<span class="hljs-number">384</span>,                      kernel_size=<span class="hljs-number">3</span>,                      stride=<span class="hljs-number">1</span>,                      padding=<span class="hljs-number">1</span>),            nn.ReLU(),        )        self.conv4 = nn.Sequential(            nn.Conv2d(in_channels=<span class="hljs-number">384</span>,                      out_channels=<span class="hljs-number">384</span>,                      kernel_size=<span class="hljs-number">3</span>,                      stride=<span class="hljs-number">1</span>,                      padding=<span class="hljs-number">1</span>),            nn.ReLU(),        )        self.conv5 = nn.Sequential(            nn.Conv2d(in_channels=<span class="hljs-number">384</span>,                      out_channels=<span class="hljs-number">256</span>,                      kernel_size=<span class="hljs-number">2</span>,                      stride=<span class="hljs-number">1</span>,                      padding=<span class="hljs-number">1</span>),            nn.ReLU(),            nn.MaxPool2d(kernel_size=<span class="hljs-number">2</span>)        )        self.fc1 = nn.Sequential(            nn.Linear(<span class="hljs-number">4</span> * <span class="hljs-number">4</span> * <span class="hljs-number">256</span>, <span class="hljs-number">4096</span>),            nn.ReLU(),            nn.Dropout(p=<span class="hljs-number">0.8</span>)        )        self.fc2 = nn.Sequential(            nn.Linear(<span class="hljs-number">4096</span>, <span class="hljs-number">1024</span>),            nn.ReLU(),            nn.Dropout(p=<span class="hljs-number">0.8</span>)        )        self.out = nn.Linear(<span class="hljs-number">1024</span>, <span class="hljs-number">10</span>)    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">forward</span><span class="hljs-params">(self, inputs)</span>:</span>        network = self.conv1(inputs)        network = self.conv2(network)        network = self.conv3(network)        network = self.conv4(network)        network = self.conv5(network)        network = network.view(network.size(<span class="hljs-number">0</span>), <span class="hljs-number">-1</span>)        network = self.fc1(network)        network = self.fc2(network)        out = self.out(network)        <span class="hljs-keyword">return</span> out, network</code></pre>]]></content>
    
    
    <categories>
      
      <category>Pytorch</category>
      
    </categories>
    
    
    <tags>
      
      <tag>深度学习</tag>
      
      <tag>Pytorch</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Pytorch实现CIFAR10之读取模型训练本地图片</title>
    <link href="/2018/09/Pytorch-Cifar10-test/"/>
    <url>/2018/09/Pytorch-Cifar10-test/</url>
    
    <content type="html"><![CDATA[<h3 id="示例代码："><a href="#示例代码：" class="headerlink" title="示例代码："></a>示例代码：</h3><pre><code class="hljs python"><span class="hljs-comment">#!/usr/bin/env python</span><span class="hljs-comment"># -*- coding: utf-8 -*-</span><span class="hljs-comment"># @Time    : 2018/9/14 12:51</span><span class="hljs-comment"># @Author  : Seven</span><span class="hljs-comment"># @Site    : </span><span class="hljs-comment"># @File    : test.py</span><span class="hljs-comment"># @Software: PyCharm</span><span class="hljs-keyword">import</span> torch<span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<span class="hljs-keyword">from</span> PIL <span class="hljs-keyword">import</span> Image<span class="hljs-comment"># 读取模型</span>model = torch.load(<span class="hljs-string">'LeNet.pkl'</span>)classes = (<span class="hljs-string">'plane'</span>, <span class="hljs-string">'car'</span>, <span class="hljs-string">'bird'</span>, <span class="hljs-string">'cat'</span>, <span class="hljs-string">'deer'</span>, <span class="hljs-string">'dog'</span>, <span class="hljs-string">'frog'</span>, <span class="hljs-string">'horse'</span>, <span class="hljs-string">'ship'</span>, <span class="hljs-string">'truck'</span>)<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">local_photos</span><span class="hljs-params">()</span>:</span>    <span class="hljs-comment"># input</span>    im = Image.open(<span class="hljs-string">'plane.jpg'</span>)    <span class="hljs-comment"># im = im.convert('L')</span>    im = im.resize((<span class="hljs-number">32</span>, <span class="hljs-number">32</span>))    <span class="hljs-comment"># im.show()</span>    im = np.array(im).astype(np.float32)    im = np.reshape(im, [<span class="hljs-number">-1</span>, <span class="hljs-number">32</span>*<span class="hljs-number">32</span>*<span class="hljs-number">3</span>])    im = (im - (<span class="hljs-number">255</span> / <span class="hljs-number">2.0</span>)) / <span class="hljs-number">255</span>    batch_xs = np.reshape(im, [<span class="hljs-number">-1</span>, <span class="hljs-number">3</span>, <span class="hljs-number">32</span>, <span class="hljs-number">32</span>])    batch_xs = torch.FloatTensor(batch_xs)    <span class="hljs-comment"># 预测</span>    pred_y, _ = model(batch_xs)    pred_y = torch.max(pred_y, <span class="hljs-number">1</span>)[<span class="hljs-number">1</span>].data.numpy().squeeze()    print(<span class="hljs-string">"The predict is : "</span>, classes[pred_y])local_photos()</code></pre><h3 id="测试图片："><a href="#测试图片：" class="headerlink" title="测试图片："></a>测试图片：</h3><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20200601182717.jpg" srcset="/img/loading.gif" alt="images"></p><h3 id="测试结果："><a href="#测试结果：" class="headerlink" title="测试结果："></a>测试结果：</h3><pre><code class="hljs ada">The predict <span class="hljs-keyword">is</span> :  <span class="hljs-type">plane</span></code></pre>]]></content>
    
    
    <categories>
      
      <category>Pytorch</category>
      
    </categories>
    
    
    <tags>
      
      <tag>深度学习</tag>
      
      <tag>Pytorch</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Pytorch实现CIFAR10之训练模型</title>
    <link href="/2018/09/Pytorch-Cifar10-train/"/>
    <url>/2018/09/Pytorch-Cifar10-train/</url>
    
    <content type="html"><![CDATA[<h3 id="示例代码："><a href="#示例代码：" class="headerlink" title="示例代码："></a>示例代码：</h3><pre><code class="hljs python">lr = <span class="hljs-number">0.001</span>best_acc = <span class="hljs-number">0</span>  <span class="hljs-comment"># best test accuracy</span>epochs = <span class="hljs-number">20</span>criterion = nn.CrossEntropyLoss()optimizer = optim.Adam(net.parameters(), lr=lr, weight_decay=<span class="hljs-number">5e-4</span>)<span class="hljs-comment"># Training</span><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">train</span><span class="hljs-params">()</span>:</span>    <span class="hljs-keyword">for</span> epoch <span class="hljs-keyword">in</span> range(epochs):        print(<span class="hljs-string">'\nEpoch: &#123;&#125;'</span>.format(epoch + <span class="hljs-number">1</span>))        train_loss = <span class="hljs-number">0</span>        correct = <span class="hljs-number">0</span>        total = <span class="hljs-number">0</span>        <span class="hljs-keyword">for</span> step, (inputs, targets) <span class="hljs-keyword">in</span> enumerate(trainloader):            outputs = net(inputs)[<span class="hljs-number">0</span>]            loss = criterion(outputs, targets)            optimizer.zero_grad()            loss.backward()            optimizer.step()            train_loss += loss.item()            _, predicted = outputs.max(<span class="hljs-number">1</span>)            total += targets.size(<span class="hljs-number">0</span>)            correct += predicted.eq(targets).sum().item()            <span class="hljs-keyword">if</span> step % <span class="hljs-number">50</span> == <span class="hljs-number">0</span>:                print(<span class="hljs-string">"step:&#123;&#125; "</span>.format(step))                print(<span class="hljs-string">"Loss:%.4f "</span> % (train_loss / (step + <span class="hljs-number">1</span>)))                print(<span class="hljs-string">"train Accuracy: %4f"</span> % (<span class="hljs-number">100.</span>*correct/total))    print(<span class="hljs-string">'Finished Training'</span>)<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">test</span><span class="hljs-params">()</span>:</span>    net.eval()    test_loss = <span class="hljs-number">0</span>    correct = <span class="hljs-number">0</span>    total = <span class="hljs-number">0</span>    <span class="hljs-keyword">with</span> torch.no_grad():        <span class="hljs-keyword">for</span> step, (inputs, targets) <span class="hljs-keyword">in</span> enumerate(testloader):            outputs, _ = net(inputs)            loss = criterion(outputs, targets)            test_loss += loss.item()            _, predicted = outputs.max(<span class="hljs-number">1</span>)            total += targets.size(<span class="hljs-number">0</span>)            correct += predicted.eq(targets).sum().item()            <span class="hljs-keyword">if</span> step % <span class="hljs-number">50</span> == <span class="hljs-number">0</span>:                print(<span class="hljs-string">"step:&#123;&#125; "</span>.format(step))                print(<span class="hljs-string">"Loss:%.4f "</span> % (test_loss / (step + <span class="hljs-number">1</span>)))                print(<span class="hljs-string">"Test Accuracy: %4f"</span> % (<span class="hljs-number">100.</span>*correct/total))    print(<span class="hljs-string">"TEST Finished"</span>)</code></pre>]]></content>
    
    
    <categories>
      
      <category>Pytorch</category>
      
    </categories>
    
    
    <tags>
      
      <tag>深度学习</tag>
      
      <tag>Pytorch</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Pytorch实现DenseNet</title>
    <link href="/2018/09/Pytorch-DenseNet/"/>
    <url>/2018/09/Pytorch-DenseNet/</url>
    
    <content type="html"><![CDATA[<h3 id="示例代码："><a href="#示例代码：" class="headerlink" title="示例代码："></a>示例代码：</h3><pre><code class="hljs python"><span class="hljs-comment">#!/usr/bin/env python</span><span class="hljs-comment"># -*- coding: utf-8 -*-</span><span class="hljs-comment"># @Time    : 2018/9/14 16:02</span><span class="hljs-comment"># @Author  : Seven</span><span class="hljs-comment"># @Site    : </span><span class="hljs-comment"># @File    : DenseNet.py</span><span class="hljs-comment"># @Software: PyCharm</span><span class="hljs-keyword">import</span> math<span class="hljs-keyword">import</span> torch<span class="hljs-keyword">import</span> torch.nn <span class="hljs-keyword">as</span> nn<span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">Bn_act_conv_drop</span><span class="hljs-params">(nn.Module)</span>:</span>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span><span class="hljs-params">(self, inputs, outs, kernel_size, padding)</span>:</span>        super(Bn_act_conv_drop, self).__init__()        self.bn = nn.Sequential(            nn.BatchNorm2d(inputs),            nn.ReLU()        )        self.conv = nn.Sequential(            nn.Conv2d(                in_channels=inputs,                out_channels=outs,                kernel_size=kernel_size,                padding=padding,                stride=<span class="hljs-number">1</span>),            nn.ReLU(),            nn.Dropout()        )    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">forward</span><span class="hljs-params">(self, inputs)</span>:</span>        network = self.bn(inputs)        network = self.conv(network)        <span class="hljs-keyword">return</span> network<span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">Transition</span><span class="hljs-params">(nn.Module)</span>:</span>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span><span class="hljs-params">(self, inputs, outs)</span>:</span>        super(Transition, self).__init__()        self.conv = Bn_act_conv_drop(inputs, outs, kernel_size=<span class="hljs-number">1</span>, padding=<span class="hljs-number">0</span>)        self.avgpool = nn.AvgPool2d(kernel_size=<span class="hljs-number">2</span>, stride=<span class="hljs-number">2</span>)    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">forward</span><span class="hljs-params">(self, inputs)</span>:</span>        network = self.conv(inputs)        network = self.avgpool(network)        <span class="hljs-keyword">return</span> network<span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">Block</span><span class="hljs-params">(nn.Module)</span>:</span>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span><span class="hljs-params">(self, inputs, growth)</span>:</span>        super(Block, self).__init__()        self.conv1 = Bn_act_conv_drop(inputs, <span class="hljs-number">4</span>*growth, kernel_size=<span class="hljs-number">1</span>, padding=<span class="hljs-number">0</span>)        self.conv2 = Bn_act_conv_drop(<span class="hljs-number">4</span>*growth, growth, kernel_size=<span class="hljs-number">3</span>, padding=<span class="hljs-number">1</span>)    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">forward</span><span class="hljs-params">(self, inputs)</span>:</span>        network = self.conv1(inputs)        network = self.conv2(network)        out = torch.cat([network, inputs], <span class="hljs-number">1</span>)        <span class="hljs-keyword">return</span> out<span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">DenseNet</span><span class="hljs-params">(nn.Module)</span>:</span>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span><span class="hljs-params">(self, blocks, growth)</span>:</span>        super(DenseNet, self).__init__()        num_planes = <span class="hljs-number">2</span>*growth        inputs = <span class="hljs-number">3</span>        self.conv = nn.Sequential(            nn.Conv2d(                in_channels=inputs,                out_channels=num_planes,                kernel_size=<span class="hljs-number">3</span>,                <span class="hljs-comment"># stride=2,</span>                padding=<span class="hljs-number">1</span>),            nn.ReLU(),            <span class="hljs-comment"># nn.MaxPool2d(kernel_size=2, stride=2)</span>        )        self.block1 = self._block(blocks[<span class="hljs-number">0</span>], num_planes, growth)        num_planes += blocks[<span class="hljs-number">0</span>] * growth        out_planes = int(math.floor(num_planes * <span class="hljs-number">0.5</span>))        self.tran1 = Transition(inputs=num_planes, outs=out_planes)        num_planes = out_planes        self.block2 = self._block(blocks[<span class="hljs-number">1</span>], num_planes, growth)        num_planes += blocks[<span class="hljs-number">1</span>] * growth        out_planes = int(math.floor(num_planes * <span class="hljs-number">0.5</span>))        self.tran2 = Transition(inputs=num_planes, outs=out_planes)        num_planes = out_planes        self.block3 = self._block(blocks[<span class="hljs-number">2</span>], num_planes, growth)        num_planes += blocks[<span class="hljs-number">2</span>] * growth        out_planes = int(math.floor(num_planes * <span class="hljs-number">0.5</span>))        self.tran3 = Transition(inputs=num_planes, outs=out_planes)        num_planes = out_planes        self.block4 = self._block(blocks[<span class="hljs-number">3</span>], num_planes, growth)        num_planes += blocks[<span class="hljs-number">3</span>] * growth        self.bn = nn.Sequential(            nn.BatchNorm2d(num_planes),            nn.ReLU()        )        self.avgpool = nn.AvgPool2d(kernel_size=<span class="hljs-number">4</span>)        self.linear = nn.Linear(num_planes, <span class="hljs-number">10</span>)    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">forward</span><span class="hljs-params">(self, inputs)</span>:</span>        network = self.conv(inputs)        network = self.block1(network)        network = self.tran1(network)        network = self.block2(network)        network = self.tran2(network)        network = self.block3(network)        network = self.tran3(network)        network = self.block4(network)        network = self.bn(network)        network = self.avgpool(network)        network = network.view(network.size(<span class="hljs-number">0</span>), <span class="hljs-number">-1</span>)        out = self.linear(network)        <span class="hljs-keyword">return</span> out, network<span class="hljs-meta">    @staticmethod</span>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">_block</span><span class="hljs-params">(layers, inputs, growth)</span>:</span>        block_layer = []        <span class="hljs-keyword">for</span> layer <span class="hljs-keyword">in</span> range(layers):            network = Block(inputs, growth)            block_layer.append(network)            inputs += growth        block_layer = nn.Sequential(*block_layer)        <span class="hljs-keyword">return</span> block_layer<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">DenseNet121</span><span class="hljs-params">()</span>:</span>    <span class="hljs-keyword">return</span> DenseNet(blocks=[<span class="hljs-number">6</span>, <span class="hljs-number">12</span>, <span class="hljs-number">24</span>, <span class="hljs-number">16</span>], growth=<span class="hljs-number">32</span>)<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">DenseNet169</span><span class="hljs-params">()</span>:</span>    <span class="hljs-keyword">return</span> DenseNet(blocks=[<span class="hljs-number">6</span>, <span class="hljs-number">12</span>, <span class="hljs-number">32</span>, <span class="hljs-number">32</span>], growth=<span class="hljs-number">32</span>)<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">DenseNet201</span><span class="hljs-params">()</span>:</span>    <span class="hljs-keyword">return</span> DenseNet(blocks=[<span class="hljs-number">6</span>, <span class="hljs-number">12</span>, <span class="hljs-number">48</span>, <span class="hljs-number">32</span>], growth=<span class="hljs-number">32</span>)<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">DenseNet161</span><span class="hljs-params">()</span>:</span>    <span class="hljs-keyword">return</span> DenseNet(blocks=[<span class="hljs-number">6</span>, <span class="hljs-number">12</span>, <span class="hljs-number">36</span>, <span class="hljs-number">24</span>], growth=<span class="hljs-number">48</span>)<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">DenseNet_cifar</span><span class="hljs-params">()</span>:</span>    <span class="hljs-keyword">return</span> DenseNet(blocks=[<span class="hljs-number">6</span>, <span class="hljs-number">12</span>, <span class="hljs-number">24</span>, <span class="hljs-number">16</span>], growth=<span class="hljs-number">12</span>)</code></pre>]]></content>
    
    
    <categories>
      
      <category>Pytorch</category>
      
    </categories>
    
    
    <tags>
      
      <tag>深度学习</tag>
      
      <tag>Pytorch</tag>
      
      <tag>DenseNet</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Pytorch实现GoogleNet</title>
    <link href="/2018/09/Pytorch-GoogleNet/"/>
    <url>/2018/09/Pytorch-GoogleNet/</url>
    
    <content type="html"><![CDATA[<h3 id="示例代码："><a href="#示例代码：" class="headerlink" title="示例代码："></a>示例代码：</h3><pre><code class="hljs python"><span class="hljs-comment">#!/usr/bin/env python</span><span class="hljs-comment"># -*- coding: utf-8 -*-</span><span class="hljs-comment"># @Time    : 2018/9/14 15:11</span><span class="hljs-comment"># @Author  : Seven</span><span class="hljs-comment"># @Site    : </span><span class="hljs-comment"># @File    : GoogleNet.py</span><span class="hljs-comment"># @Software: PyCharm</span><span class="hljs-keyword">import</span> torch<span class="hljs-keyword">import</span> torch.nn <span class="hljs-keyword">as</span> nn<span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">Inception</span><span class="hljs-params">(nn.Module)</span>:</span>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span><span class="hljs-params">(self, in_planes, n1x1, n3x3red, n3x3, n5x5red, n5x5, pool_planes)</span>:</span>        super(Inception, self).__init__()        <span class="hljs-comment"># 1x1 conv branch</span>        self.b1 = nn.Sequential(            nn.Conv2d(in_planes, n1x1, kernel_size=<span class="hljs-number">1</span>),            nn.BatchNorm2d(n1x1),            nn.ReLU(inplace=<span class="hljs-literal">True</span>),        )        <span class="hljs-comment"># 1x1 conv -&gt; 3x3 conv branch</span>        self.b2 = nn.Sequential(            nn.Conv2d(in_planes, n3x3red, kernel_size=<span class="hljs-number">1</span>),            nn.BatchNorm2d(n3x3red),            nn.ReLU(inplace=<span class="hljs-literal">True</span>),            nn.Conv2d(n3x3red, n3x3, kernel_size=<span class="hljs-number">3</span>, padding=<span class="hljs-number">1</span>),            nn.BatchNorm2d(n3x3),            nn.ReLU(inplace=<span class="hljs-literal">True</span>),        )        <span class="hljs-comment"># 1x1 conv -&gt; 5x5 conv branch</span>        self.b3 = nn.Sequential(            nn.Conv2d(in_planes, n5x5red, kernel_size=<span class="hljs-number">1</span>),            nn.BatchNorm2d(n5x5red),            nn.ReLU(inplace=<span class="hljs-literal">True</span>),            nn.Conv2d(n5x5red, n5x5, kernel_size=<span class="hljs-number">3</span>, padding=<span class="hljs-number">1</span>),            nn.BatchNorm2d(n5x5),            nn.ReLU(inplace=<span class="hljs-literal">True</span>),            nn.Conv2d(n5x5, n5x5, kernel_size=<span class="hljs-number">3</span>, padding=<span class="hljs-number">1</span>),            nn.BatchNorm2d(n5x5),            nn.ReLU(inplace=<span class="hljs-literal">True</span>),        )        <span class="hljs-comment"># 3x3 pool -&gt; 1x1 conv branch</span>        self.b4 = nn.Sequential(            nn.MaxPool2d(<span class="hljs-number">3</span>, stride=<span class="hljs-number">1</span>, padding=<span class="hljs-number">1</span>),            nn.Conv2d(in_planes, pool_planes, kernel_size=<span class="hljs-number">1</span>),            nn.BatchNorm2d(pool_planes),            nn.ReLU(inplace=<span class="hljs-literal">True</span>),        )    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">forward</span><span class="hljs-params">(self, x)</span>:</span>        y1 = self.b1(x)        y2 = self.b2(x)        y3 = self.b3(x)        y4 = self.b4(x)        <span class="hljs-keyword">return</span> torch.cat([y1, y2, y3, y4], <span class="hljs-number">1</span>)<span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">GoogLeNet</span><span class="hljs-params">(nn.Module)</span>:</span>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span><span class="hljs-params">(self)</span>:</span>        super(GoogLeNet, self).__init__()        self.pre_layers = nn.Sequential(            nn.Conv2d(<span class="hljs-number">3</span>, <span class="hljs-number">192</span>, kernel_size=<span class="hljs-number">3</span>, padding=<span class="hljs-number">1</span>),            nn.BatchNorm2d(<span class="hljs-number">192</span>),            nn.ReLU(inplace=<span class="hljs-literal">True</span>),        )        self.a3 = Inception(<span class="hljs-number">192</span>,  <span class="hljs-number">64</span>,  <span class="hljs-number">96</span>, <span class="hljs-number">128</span>, <span class="hljs-number">16</span>, <span class="hljs-number">32</span>, <span class="hljs-number">32</span>)        self.b3 = Inception(<span class="hljs-number">256</span>, <span class="hljs-number">128</span>, <span class="hljs-number">128</span>, <span class="hljs-number">192</span>, <span class="hljs-number">32</span>, <span class="hljs-number">96</span>, <span class="hljs-number">64</span>)        self.maxpool = nn.MaxPool2d(<span class="hljs-number">3</span>, stride=<span class="hljs-number">2</span>, padding=<span class="hljs-number">1</span>)        self.a4 = Inception(<span class="hljs-number">480</span>, <span class="hljs-number">192</span>,  <span class="hljs-number">96</span>, <span class="hljs-number">208</span>, <span class="hljs-number">16</span>,  <span class="hljs-number">48</span>,  <span class="hljs-number">64</span>)        self.b4 = Inception(<span class="hljs-number">512</span>, <span class="hljs-number">160</span>, <span class="hljs-number">112</span>, <span class="hljs-number">224</span>, <span class="hljs-number">24</span>,  <span class="hljs-number">64</span>,  <span class="hljs-number">64</span>)        self.c4 = Inception(<span class="hljs-number">512</span>, <span class="hljs-number">128</span>, <span class="hljs-number">128</span>, <span class="hljs-number">256</span>, <span class="hljs-number">24</span>,  <span class="hljs-number">64</span>,  <span class="hljs-number">64</span>)        self.d4 = Inception(<span class="hljs-number">512</span>, <span class="hljs-number">112</span>, <span class="hljs-number">144</span>, <span class="hljs-number">288</span>, <span class="hljs-number">32</span>,  <span class="hljs-number">64</span>,  <span class="hljs-number">64</span>)        self.e4 = Inception(<span class="hljs-number">528</span>, <span class="hljs-number">256</span>, <span class="hljs-number">160</span>, <span class="hljs-number">320</span>, <span class="hljs-number">32</span>, <span class="hljs-number">128</span>, <span class="hljs-number">128</span>)        self.a5 = Inception(<span class="hljs-number">832</span>, <span class="hljs-number">256</span>, <span class="hljs-number">160</span>, <span class="hljs-number">320</span>, <span class="hljs-number">32</span>, <span class="hljs-number">128</span>, <span class="hljs-number">128</span>)        self.b5 = Inception(<span class="hljs-number">832</span>, <span class="hljs-number">384</span>, <span class="hljs-number">192</span>, <span class="hljs-number">384</span>, <span class="hljs-number">48</span>, <span class="hljs-number">128</span>, <span class="hljs-number">128</span>)        self.avgpool = nn.AvgPool2d(kernel_size=<span class="hljs-number">8</span>, stride=<span class="hljs-number">1</span>)        self.linear = nn.Linear(<span class="hljs-number">1024</span>, <span class="hljs-number">10</span>)    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">forward</span><span class="hljs-params">(self, inputs)</span>:</span>        network = self.pre_layers(inputs)        network = self.a3(network)        network = self.b3(network)        network = self.maxpool(network)        network = self.a4(network)        network = self.b4(network)        network = self.c4(network)        network = self.d4(network)        network = self.e4(network)        network = self.maxpool(network)        network = self.a5(network)        network = self.b5(network)        network = self.avgpool(network)        network = network.view(network.size(<span class="hljs-number">0</span>), <span class="hljs-number">-1</span>)        out = self.linear(network)        <span class="hljs-keyword">return</span> out, network</code></pre>]]></content>
    
    
    <categories>
      
      <category>Pytorch</category>
      
    </categories>
    
    
    <tags>
      
      <tag>深度学习</tag>
      
      <tag>Pytorch</tag>
      
      <tag>GoogleNet</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Pytorch实现LeNet</title>
    <link href="/2018/09/Pytorch-LeNet/"/>
    <url>/2018/09/Pytorch-LeNet/</url>
    
    <content type="html"><![CDATA[<h3 id="示例代码："><a href="#示例代码：" class="headerlink" title="示例代码："></a>示例代码：</h3><pre><code class="hljs python"><span class="hljs-comment">#!/usr/bin/env python</span><span class="hljs-comment"># -*- coding: utf-8 -*-</span><span class="hljs-comment"># @Time    : 2018/9/13 20:26</span><span class="hljs-comment"># @Author  : Seven</span><span class="hljs-comment"># @Site    : </span><span class="hljs-comment"># @File    : LeNet.py</span><span class="hljs-comment"># @Software: PyCharm</span><span class="hljs-keyword">import</span> torch.nn <span class="hljs-keyword">as</span> nn<span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">LeNet</span><span class="hljs-params">(nn.Module)</span>:</span>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span><span class="hljs-params">(self)</span>:</span>        super(LeNet, self).__init__()        <span class="hljs-comment"># 32*32*3 --28*28*6--&gt; 14*14*6</span>        self.conv1 = nn.Sequential(            nn.Conv2d(in_channels=<span class="hljs-number">3</span>,                      out_channels=<span class="hljs-number">6</span>,                      kernel_size=<span class="hljs-number">5</span>,                      stride=<span class="hljs-number">1</span>,                      padding=<span class="hljs-number">0</span>),            nn.ReLU(),            nn.MaxPool2d(kernel_size=<span class="hljs-number">2</span>),        )        <span class="hljs-comment"># 14*14*6 --10*10*16--&gt; 5*5*16</span>        self.conv2 = nn.Sequential(            nn.Conv2d(in_channels=<span class="hljs-number">6</span>,                      out_channels=<span class="hljs-number">16</span>,                      kernel_size=<span class="hljs-number">5</span>,                      stride=<span class="hljs-number">1</span>,                      padding=<span class="hljs-number">0</span>),            nn.ReLU(),            nn.MaxPool2d(kernel_size=<span class="hljs-number">2</span>),        )        <span class="hljs-comment"># 5*5*16 --&gt; 120</span>        self.fc1 = nn.Sequential(            nn.Linear(<span class="hljs-number">5</span> * <span class="hljs-number">5</span> * <span class="hljs-number">16</span>, <span class="hljs-number">120</span>),            nn.ReLU(),            nn.Dropout(p=<span class="hljs-number">0.8</span>)        )        <span class="hljs-comment"># 120 --&gt; 84</span>        self.fc2 = nn.Sequential(            nn.Linear(<span class="hljs-number">120</span>, <span class="hljs-number">84</span>),            nn.ReLU(),            nn.Dropout(p=<span class="hljs-number">0.8</span>)        )        <span class="hljs-comment"># 84 --&gt; 10</span>        self.out = nn.Linear(<span class="hljs-number">84</span>, <span class="hljs-number">10</span>)    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">forward</span><span class="hljs-params">(self, inputs)</span>:</span>        network = self.conv1(inputs)        network = self.conv2(network)        network = network.view(network.size(<span class="hljs-number">0</span>), <span class="hljs-number">-1</span>)        network = self.fc1(network)        network = self.fc2(network)        out = self.out(network)        <span class="hljs-keyword">return</span> out, network</code></pre>]]></content>
    
    
    <categories>
      
      <category>Pytorch</category>
      
    </categories>
    
    
    <tags>
      
      <tag>深度学习</tag>
      
      <tag>LeNet</tag>
      
      <tag>Pytorch</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Pytorch实现ResNet</title>
    <link href="/2018/09/Pytorch-ResNet/"/>
    <url>/2018/09/Pytorch-ResNet/</url>
    
    <content type="html"><![CDATA[<h3 id="示例代码："><a href="#示例代码：" class="headerlink" title="示例代码："></a>示例代码：</h3><pre><code class="hljs python"><span class="hljs-comment">#!/usr/bin/env python</span><span class="hljs-comment"># -*- coding: utf-8 -*-</span><span class="hljs-comment"># @Time    : 2018/9/14 21:57</span><span class="hljs-comment"># @Author  : Seven</span><span class="hljs-comment"># @Site    : </span><span class="hljs-comment"># @File    : ResNet.py</span><span class="hljs-comment"># @Software: PyCharm</span><span class="hljs-keyword">import</span> torch.nn <span class="hljs-keyword">as</span> nn<span class="hljs-keyword">import</span> torch.nn.functional <span class="hljs-keyword">as</span> F<span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">BasicBlock</span><span class="hljs-params">(nn.Module)</span>:</span>    growth = <span class="hljs-number">1</span>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span><span class="hljs-params">(self, inputs, outs, stride=<span class="hljs-number">1</span>)</span>:</span>        super(BasicBlock, self).__init__()        self.left = nn.Sequential(            nn.Conv2d(inputs, outs, kernel_size=<span class="hljs-number">3</span>, stride=stride, padding=<span class="hljs-number">1</span>, bias=<span class="hljs-literal">False</span>),            nn.BatchNorm2d(outs),            nn.ReLU(),            nn.Conv2d(outs, outs, kernel_size=<span class="hljs-number">3</span>, stride=<span class="hljs-number">1</span>, padding=<span class="hljs-number">1</span>, bias=<span class="hljs-literal">False</span>),            nn.BatchNorm2d(outs)        )        self.shortcut = nn.Sequential()        <span class="hljs-keyword">if</span> stride != <span class="hljs-number">1</span> <span class="hljs-keyword">or</span> inputs != outs:            self.shortcut = nn.Sequential(                nn.Conv2d(inputs, outs, kernel_size=<span class="hljs-number">1</span>, stride=stride, bias=<span class="hljs-literal">False</span>),                nn.BatchNorm2d(outs)            )    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">forward</span><span class="hljs-params">(self, inputs)</span>:</span>        network = self.left(inputs)        network += self.shortcut(inputs)        out = F.relu(network)        <span class="hljs-keyword">return</span> out<span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">UpgradeBlock</span><span class="hljs-params">(nn.Module)</span>:</span>    growth = <span class="hljs-number">4</span>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span><span class="hljs-params">(self, inputs, outs, stride=<span class="hljs-number">1</span>)</span>:</span>        super(UpgradeBlock, self).__init__()        self.left = nn.Sequential(            nn.Conv2d(inputs, outs, kernel_size=<span class="hljs-number">1</span>, bias=<span class="hljs-literal">False</span>),            nn.BatchNorm2d(outs),            nn.ReLU(),            nn.Conv2d(outs, outs, kernel_size=<span class="hljs-number">3</span>, stride=stride, padding=<span class="hljs-number">1</span>, bias=<span class="hljs-literal">False</span>),            nn.BatchNorm2d(outs),            nn.ReLU(),            nn.Conv2d(outs, self.growth*outs, kernel_size=<span class="hljs-number">1</span>, bias=<span class="hljs-literal">False</span>),            nn.BatchNorm2d(self.growth*outs)        )        self.shortcut = nn.Sequential()        <span class="hljs-keyword">if</span> stride != <span class="hljs-number">1</span> <span class="hljs-keyword">or</span> inputs != self.growth * outs:            self.shortcut = nn.Sequential(                nn.Conv2d(inputs, self.growth * outs, kernel_size=<span class="hljs-number">1</span>, stride=stride, bias=<span class="hljs-literal">False</span>),                nn.BatchNorm2d(self.growth * outs)            )    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">forward</span><span class="hljs-params">(self, inputs)</span>:</span>        network = self.left(inputs)        network += self.shortcut(inputs)        out = F.relu(network)        <span class="hljs-keyword">return</span> out<span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">ResNet</span><span class="hljs-params">(nn.Module)</span>:</span>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span><span class="hljs-params">(self, block, layers)</span>:</span>        super(ResNet, self).__init__()        self.inputs = <span class="hljs-number">64</span>        self.conv1 = nn.Sequential(            nn.Conv2d(in_channels=<span class="hljs-number">3</span>, out_channels=<span class="hljs-number">64</span>, kernel_size=<span class="hljs-number">3</span>, padding=<span class="hljs-number">1</span>, stride=<span class="hljs-number">1</span>, bias=<span class="hljs-literal">False</span>),            nn.MaxPool2d(kernel_size=<span class="hljs-number">3</span>, stride=<span class="hljs-number">1</span>, padding=<span class="hljs-number">1</span>),            nn.BatchNorm2d(<span class="hljs-number">64</span>),            nn.ReLU()        )        self.conv2 = self._block(block, layers=layers[<span class="hljs-number">0</span>], channels=<span class="hljs-number">64</span>, stride=<span class="hljs-number">1</span>)        self.conv3 = self._block(block, layers=layers[<span class="hljs-number">1</span>], channels=<span class="hljs-number">128</span>, stride=<span class="hljs-number">2</span>)        self.conv4 = self._block(block, layers=layers[<span class="hljs-number">2</span>], channels=<span class="hljs-number">256</span>, stride=<span class="hljs-number">2</span>)        self.conv5 = self._block(block, layers=layers[<span class="hljs-number">3</span>], channels=<span class="hljs-number">512</span>, stride=<span class="hljs-number">2</span>)        self.linear = nn.Linear(<span class="hljs-number">512</span>*block.growth, <span class="hljs-number">10</span>)    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">forward</span><span class="hljs-params">(self, inputs)</span>:</span>        network = self.conv1(inputs)        network = self.conv2(network)        network = self.conv3(network)        network = self.conv4(network)        network = self.conv5(network)        network = F.avg_pool2d(network, kernel_size=network.shape[<span class="hljs-number">2</span>])        network = network.view(network.size(<span class="hljs-number">0</span>), <span class="hljs-number">-1</span>)        out = self.linear(network)        <span class="hljs-keyword">return</span> out, network    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">_block</span><span class="hljs-params">(self, block, layers, channels, stride)</span>:</span>        strides = [stride] + [<span class="hljs-number">1</span>] * (layers - <span class="hljs-number">1</span>)  <span class="hljs-comment"># strides=[1,1]</span>        layers = []        <span class="hljs-keyword">for</span> stride <span class="hljs-keyword">in</span> strides:            layers.append(block(self.inputs, channels, stride))            self.inputs = channels*block.growth        <span class="hljs-keyword">return</span> nn.Sequential(*layers)<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">ResNet18</span><span class="hljs-params">()</span>:</span>    <span class="hljs-keyword">return</span> ResNet(block=BasicBlock, layers=[<span class="hljs-number">2</span>, <span class="hljs-number">2</span>, <span class="hljs-number">2</span>, <span class="hljs-number">2</span>])<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">ResNet34</span><span class="hljs-params">()</span>:</span>    <span class="hljs-keyword">return</span> ResNet(block=BasicBlock, layers=[<span class="hljs-number">3</span>, <span class="hljs-number">4</span>, <span class="hljs-number">6</span>, <span class="hljs-number">3</span>])<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">ResNet50</span><span class="hljs-params">()</span>:</span>    <span class="hljs-keyword">return</span> ResNet(UpgradeBlock, [<span class="hljs-number">3</span>, <span class="hljs-number">4</span>, <span class="hljs-number">6</span>, <span class="hljs-number">3</span>])<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">ResNet101</span><span class="hljs-params">()</span>:</span>    <span class="hljs-keyword">return</span> ResNet(UpgradeBlock, [<span class="hljs-number">3</span>, <span class="hljs-number">4</span>, <span class="hljs-number">23</span>, <span class="hljs-number">3</span>])<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">ResNet152</span><span class="hljs-params">()</span>:</span>    <span class="hljs-keyword">return</span> ResNet(UpgradeBlock, [<span class="hljs-number">3</span>, <span class="hljs-number">8</span>, <span class="hljs-number">36</span>, <span class="hljs-number">3</span>])</code></pre>]]></content>
    
    
    <categories>
      
      <category>Pytorch</category>
      
    </categories>
    
    
    <tags>
      
      <tag>深度学习</tag>
      
      <tag>Pytorch</tag>
      
      <tag>ResNet</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Pytorch实现ResNextNet</title>
    <link href="/2018/09/Pytorch-ResNextNet/"/>
    <url>/2018/09/Pytorch-ResNextNet/</url>
    
    <content type="html"><![CDATA[<h3 id="示例代码："><a href="#示例代码：" class="headerlink" title="示例代码："></a>示例代码：</h3><pre><code class="hljs python"><span class="hljs-comment">#!/usr/bin/env python</span><span class="hljs-comment"># -*- coding: utf-8 -*-</span><span class="hljs-comment"># @Time    : 2018/9/15 20:48</span><span class="hljs-comment"># @Author  : Seven</span><span class="hljs-comment"># @Site    : </span><span class="hljs-comment"># @File    : ResnextNet.py</span><span class="hljs-comment"># @Software: PyCharm</span><span class="hljs-keyword">import</span> torch<span class="hljs-keyword">import</span> torch.nn <span class="hljs-keyword">as</span> nn<span class="hljs-keyword">import</span> torch.nn.functional <span class="hljs-keyword">as</span> F<span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">Block</span><span class="hljs-params">(nn.Module)</span>:</span>    growth = <span class="hljs-number">2</span>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span><span class="hljs-params">(self, inputs, cardinality=<span class="hljs-number">32</span>, block_width=<span class="hljs-number">4</span>, stride=<span class="hljs-number">1</span>)</span>:</span>        super(Block, self).__init__()        outs = cardinality*block_width        self.left = nn.Sequential(            nn.Conv2d(inputs, outs, kernel_size=<span class="hljs-number">1</span>, bias=<span class="hljs-literal">False</span>),            nn.BatchNorm2d(outs),            nn.ReLU(),            nn.Conv2d(outs, outs, kernel_size=<span class="hljs-number">3</span>, stride=stride, padding=<span class="hljs-number">1</span>, groups=cardinality, bias=<span class="hljs-literal">False</span>),            nn.BatchNorm2d(outs),            nn.ReLU(),            nn.Conv2d(outs, self.growth * outs, kernel_size=<span class="hljs-number">1</span>, bias=<span class="hljs-literal">False</span>),            nn.BatchNorm2d(self.growth * outs)        )        self.shortcut = nn.Sequential()        <span class="hljs-keyword">if</span> stride != <span class="hljs-number">1</span> <span class="hljs-keyword">or</span> inputs != self.growth * outs:            self.shortcut = nn.Sequential(                nn.Conv2d(inputs, self.growth * outs, kernel_size=<span class="hljs-number">1</span>, stride=stride, bias=<span class="hljs-literal">False</span>),                nn.BatchNorm2d(self.growth * outs)            )    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">forward</span><span class="hljs-params">(self, inputs)</span>:</span>        network = self.left(inputs)        network += self.shortcut(inputs)        out = F.relu(network)        <span class="hljs-keyword">return</span> out<span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">ResnextNet</span><span class="hljs-params">(nn.Module)</span>:</span>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span><span class="hljs-params">(self, layers, cardinality, block_width)</span>:</span>        super(ResnextNet, self).__init__()        self.inputs = <span class="hljs-number">64</span>        self.cardinality = cardinality        self.block_width = block_width        self.conv1 = nn.Sequential(            nn.Conv2d(in_channels=<span class="hljs-number">3</span>, out_channels=<span class="hljs-number">64</span>, kernel_size=<span class="hljs-number">3</span>, padding=<span class="hljs-number">1</span>, stride=<span class="hljs-number">1</span>, bias=<span class="hljs-literal">False</span>),            nn.MaxPool2d(kernel_size=<span class="hljs-number">3</span>, stride=<span class="hljs-number">1</span>, padding=<span class="hljs-number">1</span>),            nn.BatchNorm2d(<span class="hljs-number">64</span>),            nn.ReLU()        )        self.conv2 = self._block(layers=layers[<span class="hljs-number">0</span>], stride=<span class="hljs-number">1</span>)        self.conv3 = self._block(layers=layers[<span class="hljs-number">1</span>], stride=<span class="hljs-number">2</span>)        self.conv4 = self._block(layers=layers[<span class="hljs-number">2</span>], stride=<span class="hljs-number">2</span>)        self.conv5 = self._block(layers=layers[<span class="hljs-number">3</span>], stride=<span class="hljs-number">2</span>)        self.linear = nn.Linear(<span class="hljs-number">8</span> * cardinality * block_width, <span class="hljs-number">10</span>)    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">forward</span><span class="hljs-params">(self, inputs)</span>:</span>        network = self.conv1(inputs)        network = self.conv2(network)        network = self.conv3(network)        network = self.conv4(network)        network = self.conv5(network)        print(network.shape)        network = F.avg_pool2d(network, kernel_size=network.shape[<span class="hljs-number">2</span>]//<span class="hljs-number">2</span>)        print(network.shape)        network = network.view(network.size(<span class="hljs-number">0</span>), <span class="hljs-number">-1</span>)        out = self.linear(network)        <span class="hljs-keyword">return</span> out, network    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">_block</span><span class="hljs-params">(self, layers, stride)</span>:</span>        strides = [stride] + [<span class="hljs-number">1</span>] * (layers - <span class="hljs-number">1</span>)  <span class="hljs-comment"># strides=[1,1]</span>        layers = []        <span class="hljs-keyword">for</span> stride <span class="hljs-keyword">in</span> strides:            layers.append(Block(self.inputs, self.cardinality, self.block_width, stride))            self.inputs = self.block_width*self.cardinality*Block.growth        <span class="hljs-keyword">return</span> nn.Sequential(*layers)<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">ResNext50_32x4d</span><span class="hljs-params">()</span>:</span>    <span class="hljs-keyword">return</span> ResnextNet(layers=[<span class="hljs-number">3</span>, <span class="hljs-number">4</span>, <span class="hljs-number">6</span>, <span class="hljs-number">3</span>], cardinality=<span class="hljs-number">32</span>, block_width=<span class="hljs-number">4</span>)<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">ResNext50_4x32d</span><span class="hljs-params">()</span>:</span>    <span class="hljs-keyword">return</span> ResnextNet(layers=[<span class="hljs-number">3</span>, <span class="hljs-number">4</span>, <span class="hljs-number">6</span>, <span class="hljs-number">3</span>], cardinality=<span class="hljs-number">4</span>, block_width=<span class="hljs-number">32</span>)<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">ResNext50_64x4d</span><span class="hljs-params">()</span>:</span>    <span class="hljs-keyword">return</span> ResnextNet(layers=[<span class="hljs-number">3</span>, <span class="hljs-number">4</span>, <span class="hljs-number">6</span>, <span class="hljs-number">3</span>], cardinality=<span class="hljs-number">64</span>, block_width=<span class="hljs-number">4</span>)<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">ResNext101_32x4d</span><span class="hljs-params">()</span>:</span>    <span class="hljs-keyword">return</span> ResnextNet(layers=[<span class="hljs-number">3</span>, <span class="hljs-number">4</span>, <span class="hljs-number">23</span>, <span class="hljs-number">3</span>], cardinality=<span class="hljs-number">32</span>, block_width=<span class="hljs-number">4</span>)<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">ResNext101_64x4d</span><span class="hljs-params">()</span>:</span>    <span class="hljs-keyword">return</span> ResnextNet(layers=[<span class="hljs-number">3</span>, <span class="hljs-number">4</span>, <span class="hljs-number">23</span>, <span class="hljs-number">3</span>], cardinality=<span class="hljs-number">64</span>, block_width=<span class="hljs-number">4</span>)</code></pre>]]></content>
    
    
    <categories>
      
      <category>Pytorch</category>
      
    </categories>
    
    
    <tags>
      
      <tag>深度学习</tag>
      
      <tag>Pytorch</tag>
      
      <tag>ResNextNet</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Pytorch实现CIFAR-10之数据预处理</title>
    <link href="/2018/09/Pytorch-cifar10-data/"/>
    <url>/2018/09/Pytorch-cifar10-data/</url>
    
    <content type="html"><![CDATA[<h3 id="示例代码："><a href="#示例代码：" class="headerlink" title="示例代码："></a>示例代码：</h3><pre><code class="hljs python"><span class="hljs-comment">#!/usr/bin/env python</span><span class="hljs-comment"># -*- coding: utf-8 -*-</span><span class="hljs-comment"># @Time    : 2018/9/13 19:45</span><span class="hljs-comment"># @Author  : Seven</span><span class="hljs-comment"># @Site    : </span><span class="hljs-comment"># @File    : train.py</span><span class="hljs-comment"># @Software: PyCharm</span><span class="hljs-keyword">import</span> torch<span class="hljs-keyword">import</span> os<span class="hljs-keyword">import</span> torchvision<span class="hljs-keyword">import</span> torchvision.transforms <span class="hljs-keyword">as</span> transforms<span class="hljs-keyword">import</span> torch.utils.data <span class="hljs-keyword">as</span> Data<span class="hljs-keyword">import</span> torch.nn <span class="hljs-keyword">as</span> nn<span class="hljs-keyword">import</span> torch.optim <span class="hljs-keyword">as</span> optimprint(<span class="hljs-string">'==&gt; Preparing data..'</span>)transform_train = transforms.Compose([    transforms.RandomCrop(<span class="hljs-number">32</span>, padding=<span class="hljs-number">4</span>),    transforms.RandomHorizontalFlip(),    transforms.ToTensor(),    transforms.Normalize((<span class="hljs-number">0.4914</span>, <span class="hljs-number">0.4822</span>, <span class="hljs-number">0.4465</span>), (<span class="hljs-number">0.2023</span>, <span class="hljs-number">0.1994</span>, <span class="hljs-number">0.2010</span>)),])transform_test = transforms.Compose([    transforms.ToTensor(),    transforms.Normalize((<span class="hljs-number">0.4914</span>, <span class="hljs-number">0.4822</span>, <span class="hljs-number">0.4465</span>), (<span class="hljs-number">0.2023</span>, <span class="hljs-number">0.1994</span>, <span class="hljs-number">0.2010</span>)),])DOWNLOAD_MNIST = <span class="hljs-literal">False</span><span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span>(os.path.exists(<span class="hljs-string">'./data/'</span>)) <span class="hljs-keyword">or</span> <span class="hljs-keyword">not</span> os.listdir(<span class="hljs-string">'./data/'</span>):  <span class="hljs-comment"># 判断数据是否存在</span>    DOWNLOAD_MNIST = <span class="hljs-literal">True</span>trainset = torchvision.datasets.CIFAR10(root=<span class="hljs-string">'./data'</span>,                                        train=<span class="hljs-literal">True</span>,                                        download=DOWNLOAD_MNIST,                                        transform=transform_train)trainloader = torch.utils.data.DataLoader(trainset,                                          batch_size=<span class="hljs-number">128</span>,                                          shuffle=<span class="hljs-literal">True</span>,                                          num_workers=<span class="hljs-number">0</span>)testset = torchvision.datasets.CIFAR10(root=<span class="hljs-string">'./data'</span>,                                       train=<span class="hljs-literal">False</span>,                                       download=DOWNLOAD_MNIST,                                       transform=transform_test)testloader = torch.utils.data.DataLoader(testset,                                         batch_size=<span class="hljs-number">100</span>,                                         shuffle=<span class="hljs-literal">False</span>,                                         num_workers=<span class="hljs-number">0</span>)classes = (<span class="hljs-string">'plane'</span>, <span class="hljs-string">'car'</span>, <span class="hljs-string">'bird'</span>, <span class="hljs-string">'cat'</span>, <span class="hljs-string">'deer'</span>, <span class="hljs-string">'dog'</span>, <span class="hljs-string">'frog'</span>, <span class="hljs-string">'horse'</span>, <span class="hljs-string">'ship'</span>, <span class="hljs-string">'truck'</span>)</code></pre>]]></content>
    
    
    <categories>
      
      <category>Pytorch</category>
      
    </categories>
    
    
    <tags>
      
      <tag>深度学习</tag>
      
      <tag>Pytorch</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Pytorch实现VGGNet</title>
    <link href="/2018/09/Pytorch-VGGNet/"/>
    <url>/2018/09/Pytorch-VGGNet/</url>
    
    <content type="html"><![CDATA[<h3 id="示例代码："><a href="#示例代码：" class="headerlink" title="示例代码："></a>示例代码：</h3><pre><code class="hljs python"><span class="hljs-comment">#!/usr/bin/env python</span><span class="hljs-comment"># -*- coding: utf-8 -*-</span><span class="hljs-comment"># @Time    : 2018/9/14 13:33</span><span class="hljs-comment"># @Author  : Seven</span><span class="hljs-comment"># @Site    : </span><span class="hljs-comment"># @File    : VGG.py</span><span class="hljs-comment"># @Software: PyCharm</span><span class="hljs-keyword">import</span> torch.nn <span class="hljs-keyword">as</span> nnmodel = &#123;    <span class="hljs-string">'VGG11'</span>: [<span class="hljs-number">64</span>, <span class="hljs-string">'M'</span>, <span class="hljs-number">128</span>, <span class="hljs-string">'M'</span>, <span class="hljs-number">256</span>, <span class="hljs-number">256</span>, <span class="hljs-string">'M'</span>, <span class="hljs-number">512</span>, <span class="hljs-number">512</span>, <span class="hljs-string">'M'</span>, <span class="hljs-number">512</span>, <span class="hljs-number">512</span>, <span class="hljs-string">'M'</span>],    <span class="hljs-string">'VGG13'</span>: [<span class="hljs-number">64</span>, <span class="hljs-number">64</span>, <span class="hljs-string">'M'</span>, <span class="hljs-number">128</span>, <span class="hljs-number">128</span>, <span class="hljs-string">'M'</span>, <span class="hljs-number">256</span>, <span class="hljs-number">256</span>, <span class="hljs-string">'M'</span>, <span class="hljs-number">512</span>, <span class="hljs-number">512</span>, <span class="hljs-string">'M'</span>, <span class="hljs-number">512</span>, <span class="hljs-number">512</span>, <span class="hljs-string">'M'</span>],    <span class="hljs-string">'VGG16'</span>: [<span class="hljs-number">64</span>, <span class="hljs-number">64</span>, <span class="hljs-string">'M'</span>, <span class="hljs-number">128</span>, <span class="hljs-number">128</span>, <span class="hljs-string">'M'</span>, <span class="hljs-number">256</span>, <span class="hljs-number">256</span>, <span class="hljs-number">256</span>, <span class="hljs-string">'M'</span>, <span class="hljs-number">512</span>, <span class="hljs-number">512</span>, <span class="hljs-number">512</span>, <span class="hljs-string">'M'</span>, <span class="hljs-number">512</span>, <span class="hljs-number">512</span>, <span class="hljs-number">512</span>, <span class="hljs-string">'M'</span>],    <span class="hljs-string">'VGG19'</span>: [<span class="hljs-number">64</span>, <span class="hljs-number">64</span>, <span class="hljs-string">'M'</span>, <span class="hljs-number">128</span>, <span class="hljs-number">128</span>, <span class="hljs-string">'M'</span>, <span class="hljs-number">256</span>, <span class="hljs-number">256</span>, <span class="hljs-number">256</span>, <span class="hljs-number">256</span>, <span class="hljs-string">'M'</span>, <span class="hljs-number">512</span>, <span class="hljs-number">512</span>, <span class="hljs-number">512</span>, <span class="hljs-number">512</span>, <span class="hljs-string">'M'</span>, <span class="hljs-number">512</span>, <span class="hljs-number">512</span>, <span class="hljs-number">512</span>, <span class="hljs-number">512</span>, <span class="hljs-string">'M'</span>],&#125;<span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">VGG</span><span class="hljs-params">(nn.Module)</span>:</span>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span><span class="hljs-params">(self, vgg_name)</span>:</span>        super(VGG, self).__init__()        self.features = self._make_layers(model[vgg_name])        self.classifier = nn.Linear(<span class="hljs-number">512</span>, <span class="hljs-number">10</span>)    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">forward</span><span class="hljs-params">(self, x)</span>:</span>        network = self.features(x)        network = network.view(network.size(<span class="hljs-number">0</span>), <span class="hljs-number">-1</span>)        out = self.classifier(network)        <span class="hljs-keyword">return</span> out, network<span class="hljs-meta">    @staticmethod</span>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">_make_layers</span><span class="hljs-params">(models)</span>:</span>        layers = []        in_channels = <span class="hljs-number">3</span>        <span class="hljs-keyword">for</span> layer <span class="hljs-keyword">in</span> models:            <span class="hljs-keyword">if</span> layer == <span class="hljs-string">'M'</span>:                layers += [nn.MaxPool2d(kernel_size=<span class="hljs-number">2</span>, stride=<span class="hljs-number">2</span>)]            <span class="hljs-keyword">else</span>:                layers += [nn.Conv2d(in_channels, layer, kernel_size=<span class="hljs-number">3</span>, padding=<span class="hljs-number">1</span>),                           nn.BatchNorm2d(layer),                           nn.ReLU(inplace=<span class="hljs-literal">True</span>)]                in_channels = layer        layers += [nn.AvgPool2d(kernel_size=<span class="hljs-number">1</span>, stride=<span class="hljs-number">1</span>)]        <span class="hljs-keyword">return</span> nn.Sequential(*layers)</code></pre>]]></content>
    
    
    <categories>
      
      <category>Pytorch</category>
      
    </categories>
    
    
    <tags>
      
      <tag>深度学习</tag>
      
      <tag>Pytorch</tag>
      
      <tag>VGGNet</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>TensorFlow-cifar10-图像分类之网络结构</title>
    <link href="/2018/09/TensorFlow-cifar-network/"/>
    <url>/2018/09/TensorFlow-cifar-network/</url>
    
    <content type="html"><![CDATA[<h3 id="LeNet网络："><a href="#LeNet网络：" class="headerlink" title="LeNet网络："></a><strong>LeNet网络</strong>：</h3><pre><code class="hljs python"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">LeNet</span><span class="hljs-params">(inputs)</span>:</span>    mu = <span class="hljs-number">0</span>    sigma = <span class="hljs-number">0.1</span>    print(inputs.shape)    <span class="hljs-comment"># <span class="hljs-doctag">TODO:</span> 第一层卷积：输入=32x32x3, 输出=28x28x6</span>    conv1_w = tf.Variable(tf.truncated_normal(shape=[<span class="hljs-number">5</span>, <span class="hljs-number">5</span>, <span class="hljs-number">3</span>, <span class="hljs-number">6</span>], mean=mu, stddev=sigma))    conv1_b = tf.Variable(tf.zeros(<span class="hljs-number">6</span>))    conv1 = tf.nn.conv2d(inputs, conv1_w, strides=[<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>], padding=<span class="hljs-string">'VALID'</span>) + conv1_b    print(conv1.shape)    <span class="hljs-comment"># 激活函数</span>    conv1_out = tf.nn.relu(conv1)    <span class="hljs-comment"># 池化层， 输入=28x28x6, 输出=14x14x6</span>    pool_1 = tf.nn.max_pool(conv1_out, ksize=[<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">2</span>, <span class="hljs-number">1</span>], strides=[<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">2</span>, <span class="hljs-number">1</span>], padding=<span class="hljs-string">'VALID'</span>)    print(pool_1.shape)    <span class="hljs-comment"># <span class="hljs-doctag">TODO:</span> 第二层卷积： 输入=14x14x6， 输出=10x10x16</span>    conv2_w = tf.Variable(tf.truncated_normal(shape=[<span class="hljs-number">5</span>, <span class="hljs-number">5</span>, <span class="hljs-number">6</span>, <span class="hljs-number">16</span>], mean=mu, stddev=sigma))    conv2_b = tf.Variable(tf.zeros(<span class="hljs-number">16</span>))    conv2 = tf.nn.conv2d(pool_1, conv2_w, strides=[<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>], padding=<span class="hljs-string">'VALID'</span>) + conv2_b    print(conv2.shape)    <span class="hljs-comment"># 激活函数</span>    conv2_out = tf.nn.relu(conv2)    <span class="hljs-comment"># 池化层， 输入=10x10x16, 输出=5x5x16</span>    pool_2 = tf.nn.max_pool(conv2_out, ksize=[<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">2</span>, <span class="hljs-number">1</span>], strides=[<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">2</span>, <span class="hljs-number">1</span>], padding=<span class="hljs-string">'VALID'</span>)    print(pool_2.shape)    <span class="hljs-comment"># Flatten 输入=5x5x16， 输出=400</span>    pool_2_flat = tf.reshape(pool_2, [<span class="hljs-number">-1</span>, <span class="hljs-number">400</span>])    <span class="hljs-comment"># <span class="hljs-doctag">TODO:</span> 第三层全连接层， 输入=400， 输出=120</span>    fc1_w = tf.Variable(tf.truncated_normal(shape=[<span class="hljs-number">400</span>, <span class="hljs-number">120</span>], mean=mu, stddev=sigma))    fc1_b = tf.Variable(tf.zeros(<span class="hljs-number">120</span>))    fc1 = tf.matmul(pool_2_flat, fc1_w) + fc1_b    <span class="hljs-comment"># 激活函数</span>    fc1_out = tf.nn.relu(fc1)    print(fc1_out.shape)    <span class="hljs-comment"># <span class="hljs-doctag">TODO:</span> 第四层全连接层： 输入=120， 输出=84</span>    fc2_w = tf.Variable(tf.truncated_normal(shape=[<span class="hljs-number">120</span>, <span class="hljs-number">84</span>], mean=mu, stddev=sigma))    fc2_b = tf.Variable(tf.zeros(<span class="hljs-number">84</span>))    fc2 = tf.matmul(fc1_out, fc2_w) + fc2_b    <span class="hljs-comment"># 激活函数</span>    fc2_out = tf.nn.relu(fc2)    print(fc2_out.shape)    <span class="hljs-comment"># <span class="hljs-doctag">TODO:</span> 第五层全连接层： 输入=84， 输出=10</span>    fc3_w = tf.Variable(tf.truncated_normal(shape=[<span class="hljs-number">84</span>, <span class="hljs-number">10</span>], mean=mu, stddev=sigma))    fc3_b = tf.Variable(tf.zeros(<span class="hljs-number">10</span>))    fc3_out = tf.matmul(fc2_out, fc3_w) + fc3_b    print(fc3_out.shape)    <span class="hljs-keyword">return</span> fc3_out</code></pre><h3 id="AlexNet网络："><a href="#AlexNet网络：" class="headerlink" title="AlexNet网络："></a><strong>AlexNet网络</strong>：</h3><pre><code class="hljs python"><span class="hljs-comment"># 卷积操作</span><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">conv2d</span><span class="hljs-params">(name, l_input, w, b)</span>:</span>    <span class="hljs-keyword">return</span> tf.nn.relu(tf.nn.bias_add(tf.nn.conv2d(l_input, w, strides=[<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>], padding=<span class="hljs-string">'SAME'</span>), b), name=name)<span class="hljs-comment"># 最大下采样操作</span><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">max_pool</span><span class="hljs-params">(name, l_input, k)</span>:</span>    <span class="hljs-keyword">return</span> tf.nn.max_pool(l_input, ksize=[<span class="hljs-number">1</span>, k, k, <span class="hljs-number">1</span>], strides=[<span class="hljs-number">1</span>, k, k, <span class="hljs-number">1</span>], padding=<span class="hljs-string">'SAME'</span>, name=name)<span class="hljs-comment"># 归一化操作</span><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">norm</span><span class="hljs-params">(name, l_input, lsize=<span class="hljs-number">4</span>)</span>:</span>    <span class="hljs-keyword">return</span> tf.nn.lrn(l_input, lsize, bias=<span class="hljs-number">1.0</span>, alpha=<span class="hljs-number">0.001</span> / <span class="hljs-number">9.0</span>, beta=<span class="hljs-number">0.75</span>, name=name)<span class="hljs-comment"># 定义整个网络</span><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">alex_net</span><span class="hljs-params">(_X, _weights, _biases, _dropout)</span>:</span>    <span class="hljs-comment"># 向量转为矩阵</span>    <span class="hljs-comment"># _X = tf.reshape(_X, shape=[-1, 28, 28, 3])</span>    print(_X.shape)    <span class="hljs-comment"># <span class="hljs-doctag">TODO:</span> 第一层卷积：</span>    conv1 = conv2d(<span class="hljs-string">'conv1'</span>, _X, _weights[<span class="hljs-string">'wc1'</span>], _biases[<span class="hljs-string">'bc1'</span>])    <span class="hljs-comment"># 下采样层</span>    pool1 = max_pool(<span class="hljs-string">'pool1'</span>, conv1, k=<span class="hljs-number">2</span>)    <span class="hljs-comment"># 归一化层</span>    norm1 = norm(<span class="hljs-string">'norm1'</span>, pool1, lsize=<span class="hljs-number">4</span>)    print(norm1.shape)    <span class="hljs-comment"># <span class="hljs-doctag">TODO:</span> 第二层卷积：</span>    conv2 = conv2d(<span class="hljs-string">'conv2'</span>, norm1, _weights[<span class="hljs-string">'wc2'</span>], _biases[<span class="hljs-string">'bc2'</span>])    <span class="hljs-comment"># 下采样</span>    pool2 = max_pool(<span class="hljs-string">'pool2'</span>, conv2, k=<span class="hljs-number">2</span>)    <span class="hljs-comment"># 归一化</span>    norm2 = norm(<span class="hljs-string">'norm2'</span>, pool2, lsize=<span class="hljs-number">4</span>)    print(norm2.shape)    <span class="hljs-comment"># <span class="hljs-doctag">TODO:</span> 第三层卷积：</span>    conv3 = conv2d(<span class="hljs-string">'conv3'</span>, norm2, _weights[<span class="hljs-string">'wc3'</span>], _biases[<span class="hljs-string">'bc3'</span>])    <span class="hljs-comment"># 归一化</span>    norm3 = norm(<span class="hljs-string">'norm3'</span>, conv3, lsize=<span class="hljs-number">4</span>)    print(norm3.shape)    <span class="hljs-comment"># <span class="hljs-doctag">TODO:</span> 第四层卷积</span>    <span class="hljs-comment"># 卷积</span>    conv4 = conv2d(<span class="hljs-string">'conv4'</span>, norm3, _weights[<span class="hljs-string">'wc4'</span>], _biases[<span class="hljs-string">'bc4'</span>])    <span class="hljs-comment"># 归一化</span>    norm4 = norm(<span class="hljs-string">'norm4'</span>, conv4, lsize=<span class="hljs-number">4</span>)    print(norm4.shape)    <span class="hljs-comment"># <span class="hljs-doctag">TODO:</span> 第五层卷积</span>    <span class="hljs-comment"># 卷积</span>    conv5 = conv2d(<span class="hljs-string">'conv5'</span>, norm4, _weights[<span class="hljs-string">'wc5'</span>], _biases[<span class="hljs-string">'bc5'</span>])    <span class="hljs-comment"># 下采样</span>    pool5 = max_pool(<span class="hljs-string">'pool5'</span>, conv5, k=<span class="hljs-number">2</span>)    <span class="hljs-comment"># 归一化</span>    norm5 = norm(<span class="hljs-string">'norm5'</span>, pool5, lsize=<span class="hljs-number">4</span>)    print(norm5.shape)    <span class="hljs-comment"># <span class="hljs-doctag">TODO:</span> 第六层全连接层</span>    <span class="hljs-comment"># 先把特征图转为向量</span>    dense1 = tf.reshape(norm5, [<span class="hljs-number">-1</span>, _weights[<span class="hljs-string">'wd1'</span>].get_shape().as_list()[<span class="hljs-number">0</span>]])    dense1 = tf.nn.relu(tf.matmul(dense1, _weights[<span class="hljs-string">'wd1'</span>]) + _biases[<span class="hljs-string">'bd1'</span>], name=<span class="hljs-string">'fc1'</span>)    dense1 = tf.nn.dropout(dense1, _dropout)    print(dense1.shape)    <span class="hljs-comment"># <span class="hljs-doctag">TODO:</span> 第七层全连接层：</span>    dense2 = tf.nn.relu(tf.matmul(dense1, _weights[<span class="hljs-string">'wd2'</span>]) + _biases[<span class="hljs-string">'bd2'</span>], name=<span class="hljs-string">'fc2'</span>)  <span class="hljs-comment"># Relu activation</span>    dense2 = tf.nn.dropout(dense2, _dropout)    print(dense2.shape)    <span class="hljs-comment"># <span class="hljs-doctag">TODO:</span> 第八层全连接层：</span>    <span class="hljs-comment"># 网络输出层</span>    out = tf.matmul(dense2, _weights[<span class="hljs-string">'out'</span>]) + _biases[<span class="hljs-string">'out'</span>]    print(out.shape)    <span class="hljs-keyword">return</span> out</code></pre><h3 id="VGG16Net网络："><a href="#VGG16Net网络：" class="headerlink" title="VGG16Net网络："></a><strong>VGG16Net网络</strong>：</h3><pre><code class="hljs python"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">VGG16</span><span class="hljs-params">(inputs)</span>:</span>    print(inputs.shape)    <span class="hljs-comment"># (32x32x3) --&gt; (32x32x64)</span>    <span class="hljs-keyword">with</span> tf.name_scope(<span class="hljs-string">'conv_1'</span>):         conv_1_out = tf.layers.conv2d(inputs, <span class="hljs-number">64</span>, [<span class="hljs-number">3</span>, <span class="hljs-number">3</span>],                                       padding=<span class="hljs-string">'same'</span>,                                       activation=tf.nn.relu,                                       kernel_initializer=tf.truncated_normal_initializer(mean=<span class="hljs-number">0.</span>, stddev=<span class="hljs-number">0.1</span>))    print(conv_1_out.shape)    <span class="hljs-comment"># (32x32x64) --&gt; (32x32x64)</span>    <span class="hljs-keyword">with</span> tf.name_scope(<span class="hljs-string">'conv_2'</span>):        conv_2_out = tf.layers.conv2d(conv_1_out, <span class="hljs-number">64</span>, [<span class="hljs-number">3</span>, <span class="hljs-number">3</span>],                                      padding=<span class="hljs-string">'same'</span>,                                      activation=tf.nn.relu,                                      kernel_initializer=tf.truncated_normal_initializer(mean=<span class="hljs-number">0.</span>, stddev=<span class="hljs-number">0.1</span>))    print(conv_2_out.shape)    <span class="hljs-comment"># (32x32x64) --&gt; (16x16x64)</span>    <span class="hljs-keyword">with</span> tf.name_scope(<span class="hljs-string">'pool_1'</span>):        pool_1_out = tf.layers.max_pooling2d(conv_2_out, pool_size=[<span class="hljs-number">2</span>, <span class="hljs-number">2</span>], strides=[<span class="hljs-number">2</span>, <span class="hljs-number">2</span>], padding=<span class="hljs-string">'same'</span>)    print(pool_1_out.shape)    <span class="hljs-comment"># (16x16x64) --&gt; (16x16x128)</span>    <span class="hljs-keyword">with</span> tf.name_scope(<span class="hljs-string">'conv_3'</span>):         conv_3_out = tf.layers.conv2d(pool_1_out, <span class="hljs-number">128</span>, [<span class="hljs-number">3</span>, <span class="hljs-number">3</span>],                                       padding=<span class="hljs-string">'same'</span>,                                       activation=tf.nn.relu,                                       kernel_initializer=tf.truncated_normal_initializer(mean=<span class="hljs-number">0.</span>, stddev=<span class="hljs-number">0.1</span>))    print(conv_3_out.shape)    <span class="hljs-comment"># (16x16x128) --&gt; (16x16x128)</span>    <span class="hljs-keyword">with</span> tf.name_scope(<span class="hljs-string">'conv_4'</span>):         conv_4_out = tf.layers.conv2d(conv_3_out, <span class="hljs-number">128</span>, [<span class="hljs-number">3</span>, <span class="hljs-number">3</span>],                                       padding=<span class="hljs-string">'same'</span>,                                       activation=tf.nn.relu,                                       kernel_initializer=tf.truncated_normal_initializer(mean=<span class="hljs-number">0.</span>, stddev=<span class="hljs-number">0.1</span>))    print(conv_4_out.shape)    <span class="hljs-comment"># (16x16x128) --&gt; (8x8x128)</span>    <span class="hljs-keyword">with</span> tf.name_scope(<span class="hljs-string">'pool_2'</span>):        pool_2_out = tf.layers.max_pooling2d(conv_4_out, pool_size=[<span class="hljs-number">2</span>, <span class="hljs-number">2</span>], strides=[<span class="hljs-number">2</span>, <span class="hljs-number">2</span>], padding=<span class="hljs-string">'same'</span>)    print(pool_2_out.shape)    <span class="hljs-comment"># (8x8x128) --&gt; (8x8x256)</span>    <span class="hljs-keyword">with</span> tf.name_scope(<span class="hljs-string">'conv_5'</span>):         conv_5_out = tf.layers.conv2d(pool_2_out, <span class="hljs-number">256</span>, [<span class="hljs-number">3</span>, <span class="hljs-number">3</span>],                                       padding=<span class="hljs-string">'same'</span>,                                       activation=tf.nn.relu,                                       kernel_initializer=tf.truncated_normal_initializer(mean=<span class="hljs-number">0.</span>, stddev=<span class="hljs-number">0.1</span>))    print(conv_5_out.shape)    <span class="hljs-comment"># (8x8x256) --&gt; (8x8x256)</span>    <span class="hljs-keyword">with</span> tf.name_scope(<span class="hljs-string">'conv_6'</span>):         conv_6_out = tf.layers.conv2d(conv_5_out, <span class="hljs-number">256</span>, [<span class="hljs-number">3</span>, <span class="hljs-number">3</span>],                                       padding=<span class="hljs-string">'same'</span>,                                       activation=tf.nn.relu,                                       kernel_initializer=tf.truncated_normal_initializer(mean=<span class="hljs-number">0.</span>, stddev=<span class="hljs-number">0.1</span>))    print(conv_6_out.shape)    <span class="hljs-comment"># (8x8x256) --&gt; (8x8x256)</span>    <span class="hljs-keyword">with</span> tf.name_scope(<span class="hljs-string">'conv_7'</span>):        conv_7_out = tf.layers.conv2d(conv_6_out, <span class="hljs-number">256</span>, [<span class="hljs-number">3</span>, <span class="hljs-number">3</span>],                                      padding=<span class="hljs-string">'same'</span>,                                      activation=tf.nn.relu,                                      kernel_initializer=tf.truncated_normal_initializer(mean=<span class="hljs-number">0.</span>, stddev=<span class="hljs-number">0.1</span>))    print(conv_7_out.shape)    <span class="hljs-comment"># (8x8x256) --&gt; (4x4x256)</span>    <span class="hljs-keyword">with</span> tf.name_scope(<span class="hljs-string">'pool_3'</span>):        pool_3_out = tf.layers.max_pooling2d(conv_7_out, pool_size=[<span class="hljs-number">2</span>, <span class="hljs-number">2</span>], strides=[<span class="hljs-number">2</span>, <span class="hljs-number">2</span>], padding=<span class="hljs-string">'same'</span>)    print(pool_3_out.shape)    <span class="hljs-comment"># (4x4x256) --&gt; (4x4x512)</span>    <span class="hljs-keyword">with</span> tf.name_scope(<span class="hljs-string">'conv_8'</span>):        conv_8_out = tf.layers.conv2d(pool_3_out, <span class="hljs-number">512</span>, [<span class="hljs-number">3</span>, <span class="hljs-number">3</span>],                                      padding=<span class="hljs-string">'same'</span>,                                      activation=tf.nn.relu,                                      kernel_initializer=tf.truncated_normal_initializer(mean=<span class="hljs-number">0.</span>, stddev=<span class="hljs-number">0.1</span>))    print(conv_8_out.shape)    <span class="hljs-comment"># (4x4x512) --&gt; (4x4x512)</span>    <span class="hljs-keyword">with</span> tf.name_scope(<span class="hljs-string">'conv_9'</span>):        conv_9_out = tf.layers.conv2d(conv_8_out, <span class="hljs-number">512</span>, [<span class="hljs-number">3</span>, <span class="hljs-number">3</span>],                                      padding=<span class="hljs-string">'same'</span>,                                      activation=tf.nn.relu,                                      kernel_initializer=tf.truncated_normal_initializer(mean=<span class="hljs-number">0.</span>, stddev=<span class="hljs-number">0.1</span>))    print(conv_9_out.shape)    <span class="hljs-comment"># (4x4x512) --&gt; (4x4x512)</span>    <span class="hljs-keyword">with</span> tf.name_scope(<span class="hljs-string">'conv_10'</span>):        conv_10_out = tf.layers.conv2d(conv_9_out, <span class="hljs-number">512</span>, [<span class="hljs-number">3</span>, <span class="hljs-number">3</span>],                                       padding=<span class="hljs-string">'same'</span>,                                       activation=tf.nn.relu,                                       kernel_initializer=tf.truncated_normal_initializer(mean=<span class="hljs-number">0.</span>, stddev=<span class="hljs-number">0.1</span>))    print(conv_10_out.shape)    <span class="hljs-comment"># (4x4x512) --&gt; (2x2x512)</span>    <span class="hljs-keyword">with</span> tf.name_scope(<span class="hljs-string">'pool_4'</span>):        pool_4_out = tf.layers.max_pooling2d(conv_10_out, pool_size=[<span class="hljs-number">2</span>, <span class="hljs-number">2</span>], strides=[<span class="hljs-number">2</span>, <span class="hljs-number">2</span>], padding=<span class="hljs-string">'same'</span>)    print(pool_4_out.shape)    <span class="hljs-comment"># (2x2x512) --&gt; (2x2x512)</span>    <span class="hljs-keyword">with</span> tf.name_scope(<span class="hljs-string">'conv_11'</span>):        conv_11_out = tf.layers.conv2d(pool_4_out, <span class="hljs-number">512</span>, [<span class="hljs-number">3</span>, <span class="hljs-number">3</span>],                                       padding=<span class="hljs-string">'same'</span>,                                       activation=tf.nn.relu,                                       kernel_initializer=tf.truncated_normal_initializer(mean=<span class="hljs-number">0.</span>, stddev=<span class="hljs-number">0.1</span>))    print(conv_11_out.shape)    <span class="hljs-comment"># (2x2x512) --&gt; (2x2x512)</span>    <span class="hljs-keyword">with</span> tf.name_scope(<span class="hljs-string">'conv_12'</span>):        conv_12_out = tf.layers.conv2d(conv_11_out, <span class="hljs-number">512</span>, [<span class="hljs-number">3</span>, <span class="hljs-number">3</span>],                                       padding=<span class="hljs-string">'same'</span>,                                       activation=tf.nn.relu,                                       kernel_initializer=tf.truncated_normal_initializer(mean=<span class="hljs-number">0.</span>, stddev=<span class="hljs-number">0.1</span>))    print(conv_12_out.shape)    <span class="hljs-comment"># (2x2x512) --&gt; (2x2x512)</span>    <span class="hljs-keyword">with</span> tf.name_scope(<span class="hljs-string">'conv_13'</span>):        conv_13_out = tf.layers.conv2d(conv_12_out, <span class="hljs-number">512</span>, [<span class="hljs-number">3</span>, <span class="hljs-number">3</span>],                                       padding=<span class="hljs-string">'same'</span>,                                       activation=tf.nn.relu,                                       kernel_initializer=tf.truncated_normal_initializer(mean=<span class="hljs-number">0.</span>, stddev=<span class="hljs-number">0.1</span>))    print(conv_13_out.shape)    <span class="hljs-comment"># (2x2x512) --&gt; (1x1x512)</span>    <span class="hljs-keyword">with</span> tf.name_scope(<span class="hljs-string">'pool_5'</span>):        pool_5_out = tf.layers.max_pooling2d(conv_13_out, pool_size=[<span class="hljs-number">2</span>, <span class="hljs-number">2</span>], strides=[<span class="hljs-number">2</span>, <span class="hljs-number">2</span>], padding=<span class="hljs-string">'same'</span>)    print(pool_5_out.shape)    <span class="hljs-comment"># (1x1x512) --&gt; 512</span>    <span class="hljs-keyword">with</span> tf.name_scope(<span class="hljs-string">'fc_1'</span>):        pool_5_outz_flat = tf.layers.flatten(pool_5_out)        fc_1_out = tf.layers.dense(pool_5_outz_flat, <span class="hljs-number">512</span>, activation=tf.nn.relu)        fc_1_drop = tf.nn.dropout(fc_1_out, keep_prob=config.keep_prob)    print(fc_1_drop.shape)    <span class="hljs-comment"># 512 --&gt; 512</span>    <span class="hljs-keyword">with</span> tf.name_scope(<span class="hljs-string">'fc_2'</span>):        fc_2_out = tf.layers.dense(fc_1_drop, <span class="hljs-number">512</span>, activation=tf.nn.relu)        fc_2_drop = tf.nn.dropout(fc_2_out, keep_prob=config.keep_prob)    print(fc_2_drop.shape)    <span class="hljs-comment"># 512 --&gt; 10</span>    <span class="hljs-keyword">with</span> tf.name_scope(<span class="hljs-string">'fc_3'</span>):        fc_3_out = tf.layers.dense(fc_2_drop, <span class="hljs-number">10</span>, activation=<span class="hljs-literal">None</span>)    print(fc_3_out.shape)    <span class="hljs-keyword">return</span> fc_3_out</code></pre><h3 id="VGG19Net"><a href="#VGG19Net" class="headerlink" title="VGG19Net:"></a><strong>VGG19Net</strong>:</h3><pre><code class="hljs python"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">VGG19</span><span class="hljs-params">(inputs)</span>:</span>    print(inputs.shape)    <span class="hljs-comment"># (32x32x3) --&gt; (32x32x64)</span>    <span class="hljs-keyword">with</span> tf.name_scope(<span class="hljs-string">'conv_1'</span>):         conv_1_out = tf.layers.conv2d(inputs, <span class="hljs-number">64</span>, [<span class="hljs-number">3</span>, <span class="hljs-number">3</span>],                                       padding=<span class="hljs-string">'same'</span>,                                       activation=tf.nn.relu,                                       kernel_initializer=tf.truncated_normal_initializer(mean=<span class="hljs-number">0.</span>, stddev=<span class="hljs-number">0.1</span>))    print(conv_1_out.shape)    <span class="hljs-comment"># (32x32x64) --&gt; (32x32x64)</span>    <span class="hljs-keyword">with</span> tf.name_scope(<span class="hljs-string">'conv_2'</span>):        conv_2_out = tf.layers.conv2d(conv_1_out, <span class="hljs-number">64</span>, [<span class="hljs-number">3</span>, <span class="hljs-number">3</span>],                                      padding=<span class="hljs-string">'same'</span>,                                      activation=tf.nn.relu,                                      kernel_initializer=tf.truncated_normal_initializer(mean=<span class="hljs-number">0.</span>, stddev=<span class="hljs-number">0.1</span>))    print(conv_2_out.shape)    <span class="hljs-comment"># (32x32x64) --&gt; (16x16x64)</span>    <span class="hljs-keyword">with</span> tf.name_scope(<span class="hljs-string">'pool_1'</span>):        pool_1_out = tf.layers.max_pooling2d(conv_2_out, pool_size=[<span class="hljs-number">2</span>, <span class="hljs-number">2</span>], strides=[<span class="hljs-number">2</span>, <span class="hljs-number">2</span>], padding=<span class="hljs-string">'same'</span>)    print(pool_1_out.shape)    <span class="hljs-comment"># (16x16x64) --&gt; (16x16x128)</span>    <span class="hljs-keyword">with</span> tf.name_scope(<span class="hljs-string">'conv_3'</span>):         conv_3_out = tf.layers.conv2d(pool_1_out, <span class="hljs-number">128</span>, [<span class="hljs-number">3</span>, <span class="hljs-number">3</span>],                                       padding=<span class="hljs-string">'same'</span>,                                       activation=tf.nn.relu,                                       kernel_initializer=tf.truncated_normal_initializer(mean=<span class="hljs-number">0.</span>, stddev=<span class="hljs-number">0.1</span>))    print(conv_3_out.shape)    <span class="hljs-comment"># (16x16x128) --&gt; (16x16x128)</span>    <span class="hljs-keyword">with</span> tf.name_scope(<span class="hljs-string">'conv_4'</span>):         conv_4_out = tf.layers.conv2d(conv_3_out, <span class="hljs-number">128</span>, [<span class="hljs-number">3</span>, <span class="hljs-number">3</span>],                                       padding=<span class="hljs-string">'same'</span>,                                       activation=tf.nn.relu,                                       kernel_initializer=tf.truncated_normal_initializer(mean=<span class="hljs-number">0.</span>, stddev=<span class="hljs-number">0.1</span>))    print(conv_4_out.shape)    <span class="hljs-comment"># (16x16x128) --&gt; (8x8x128)</span>    <span class="hljs-keyword">with</span> tf.name_scope(<span class="hljs-string">'pool_2'</span>):        pool_2_out = tf.layers.max_pooling2d(conv_4_out, pool_size=[<span class="hljs-number">2</span>, <span class="hljs-number">2</span>], strides=[<span class="hljs-number">2</span>, <span class="hljs-number">2</span>], padding=<span class="hljs-string">'same'</span>)    print(pool_2_out.shape)    <span class="hljs-comment"># (8x8x128) --&gt; (8x8x256)</span>    <span class="hljs-keyword">with</span> tf.name_scope(<span class="hljs-string">'conv_5'</span>):         conv_5_out = tf.layers.conv2d(pool_2_out, <span class="hljs-number">256</span>, [<span class="hljs-number">3</span>, <span class="hljs-number">3</span>],                                       padding=<span class="hljs-string">'same'</span>,                                       activation=tf.nn.relu,                                       kernel_initializer=tf.truncated_normal_initializer(mean=<span class="hljs-number">0.</span>, stddev=<span class="hljs-number">0.1</span>))    print(conv_5_out.shape)    <span class="hljs-comment"># (8x8x256) --&gt; (8x8x256)</span>    <span class="hljs-keyword">with</span> tf.name_scope(<span class="hljs-string">'conv_6'</span>):         conv_6_out = tf.layers.conv2d(conv_5_out, <span class="hljs-number">256</span>, [<span class="hljs-number">3</span>, <span class="hljs-number">3</span>],                                       padding=<span class="hljs-string">'same'</span>,                                       activation=tf.nn.relu,                                       kernel_initializer=tf.truncated_normal_initializer(mean=<span class="hljs-number">0.</span>, stddev=<span class="hljs-number">0.1</span>))    print(conv_6_out.shape)    <span class="hljs-comment"># (8x8x256) --&gt; (8x8x256)</span>    <span class="hljs-keyword">with</span> tf.name_scope(<span class="hljs-string">'conv_7'</span>):        conv_7_out = tf.layers.conv2d(conv_6_out, <span class="hljs-number">256</span>, [<span class="hljs-number">3</span>, <span class="hljs-number">3</span>],                                      padding=<span class="hljs-string">'same'</span>,                                      activation=tf.nn.relu,                                      kernel_initializer=tf.truncated_normal_initializer(mean=<span class="hljs-number">0.</span>, stddev=<span class="hljs-number">0.1</span>))    print(conv_7_out.shape)    <span class="hljs-comment"># (8x8x256) --&gt; (8x8x256)</span>    <span class="hljs-keyword">with</span> tf.name_scope(<span class="hljs-string">'conv_8'</span>):        conv_8_out = tf.layers.conv2d(conv_7_out, <span class="hljs-number">256</span>, [<span class="hljs-number">3</span>, <span class="hljs-number">3</span>],                                      padding=<span class="hljs-string">'same'</span>,                                      activation=tf.nn.relu,                                      kernel_initializer=tf.truncated_normal_initializer(mean=<span class="hljs-number">0.</span>, stddev=<span class="hljs-number">0.1</span>))    print(conv_8_out.shape)    <span class="hljs-comment"># (8x8x256) --&gt; (4x4x256)</span>    <span class="hljs-keyword">with</span> tf.name_scope(<span class="hljs-string">'pool_3'</span>):        pool_3_out = tf.layers.max_pooling2d(conv_8_out, pool_size=[<span class="hljs-number">2</span>, <span class="hljs-number">2</span>], strides=[<span class="hljs-number">2</span>, <span class="hljs-number">2</span>], padding=<span class="hljs-string">'same'</span>)    print(pool_3_out.shape)    <span class="hljs-comment"># (4x4x256) --&gt; (4x4x512)</span>    <span class="hljs-keyword">with</span> tf.name_scope(<span class="hljs-string">'conv_9'</span>):        conv_9_out = tf.layers.conv2d(pool_3_out, <span class="hljs-number">512</span>, [<span class="hljs-number">3</span>, <span class="hljs-number">3</span>],                                      padding=<span class="hljs-string">'same'</span>,                                      activation=tf.nn.relu,                                      kernel_initializer=tf.truncated_normal_initializer(mean=<span class="hljs-number">0.</span>, stddev=<span class="hljs-number">0.1</span>))    print(conv_9_out.shape)    <span class="hljs-comment"># (4x4x512) --&gt; (4x4x512)</span>    <span class="hljs-keyword">with</span> tf.name_scope(<span class="hljs-string">'conv_10'</span>):        conv_10_out = tf.layers.conv2d(conv_9_out, <span class="hljs-number">512</span>, [<span class="hljs-number">3</span>, <span class="hljs-number">3</span>],                                       padding=<span class="hljs-string">'same'</span>,                                       activation=tf.nn.relu,                                       kernel_initializer=tf.truncated_normal_initializer(mean=<span class="hljs-number">0.</span>, stddev=<span class="hljs-number">0.1</span>))    print(conv_10_out.shape)    <span class="hljs-comment"># (4x4x512) --&gt; (4x4x512)</span>    <span class="hljs-keyword">with</span> tf.name_scope(<span class="hljs-string">'conv_11'</span>):        conv_11_out = tf.layers.conv2d(conv_10_out, <span class="hljs-number">512</span>, [<span class="hljs-number">3</span>, <span class="hljs-number">3</span>],                                       padding=<span class="hljs-string">'same'</span>,                                       activation=tf.nn.relu,                                       kernel_initializer=tf.truncated_normal_initializer(mean=<span class="hljs-number">0.</span>, stddev=<span class="hljs-number">0.1</span>))    print(conv_11_out.shape)    <span class="hljs-comment"># (4x4x512) --&gt; (4x4x512)</span>    <span class="hljs-keyword">with</span> tf.name_scope(<span class="hljs-string">'conv_12'</span>):        conv_12_out = tf.layers.conv2d(conv_11_out, <span class="hljs-number">512</span>, [<span class="hljs-number">3</span>, <span class="hljs-number">3</span>],                                       padding=<span class="hljs-string">'same'</span>,                                       activation=tf.nn.relu,                                       kernel_initializer=tf.truncated_normal_initializer(mean=<span class="hljs-number">0.</span>, stddev=<span class="hljs-number">0.1</span>))    print(conv_12_out.shape)    <span class="hljs-comment"># (4x4x512) --&gt; (2x2x512)</span>    <span class="hljs-keyword">with</span> tf.name_scope(<span class="hljs-string">'pool_4'</span>):        pool_4_out = tf.layers.max_pooling2d(conv_12_out, pool_size=[<span class="hljs-number">2</span>, <span class="hljs-number">2</span>], strides=[<span class="hljs-number">2</span>, <span class="hljs-number">2</span>], padding=<span class="hljs-string">'same'</span>)    print(pool_4_out.shape)    <span class="hljs-comment"># (2x2x512) --&gt; (2x2x512)</span>    <span class="hljs-keyword">with</span> tf.name_scope(<span class="hljs-string">'conv_13'</span>):        conv_13_out = tf.layers.conv2d(pool_4_out, <span class="hljs-number">512</span>, [<span class="hljs-number">3</span>, <span class="hljs-number">3</span>],                                       padding=<span class="hljs-string">'same'</span>,                                       activation=tf.nn.relu,                                       kernel_initializer=tf.truncated_normal_initializer(mean=<span class="hljs-number">0.</span>, stddev=<span class="hljs-number">0.1</span>))    print(conv_13_out.shape)    <span class="hljs-comment"># (2x2x512) --&gt; (2x2x512)</span>    <span class="hljs-keyword">with</span> tf.name_scope(<span class="hljs-string">'conv_14'</span>):        conv_14_out = tf.layers.conv2d(conv_13_out, <span class="hljs-number">512</span>, [<span class="hljs-number">3</span>, <span class="hljs-number">3</span>],                                       padding=<span class="hljs-string">'same'</span>,                                       activation=tf.nn.relu,                                       kernel_initializer=tf.truncated_normal_initializer(mean=<span class="hljs-number">0.</span>, stddev=<span class="hljs-number">0.1</span>))    print(conv_14_out.shape)    <span class="hljs-comment"># (2x2x512) --&gt; (2x2x512)</span>    <span class="hljs-keyword">with</span> tf.name_scope(<span class="hljs-string">'conv_15'</span>):        conv_15_out = tf.layers.conv2d(conv_14_out, <span class="hljs-number">512</span>, [<span class="hljs-number">3</span>, <span class="hljs-number">3</span>],                                       padding=<span class="hljs-string">'same'</span>,                                       activation=tf.nn.relu,                                       kernel_initializer=tf.truncated_normal_initializer(mean=<span class="hljs-number">0.</span>, stddev=<span class="hljs-number">0.1</span>))    print(conv_15_out.shape)    <span class="hljs-comment"># (2x2x512) --&gt; (2x2x512)</span>    <span class="hljs-keyword">with</span> tf.name_scope(<span class="hljs-string">'conv_16'</span>):        conv_16_out = tf.layers.conv2d(conv_15_out, <span class="hljs-number">512</span>, [<span class="hljs-number">3</span>, <span class="hljs-number">3</span>],                                       padding=<span class="hljs-string">'same'</span>,                                       activation=tf.nn.relu,                                       kernel_initializer=tf.truncated_normal_initializer(mean=<span class="hljs-number">0.</span>, stddev=<span class="hljs-number">0.1</span>))    print(conv_16_out.shape)    <span class="hljs-comment"># (2x2x512) --&gt; (1x1x512)</span>    <span class="hljs-keyword">with</span> tf.name_scope(<span class="hljs-string">'pool_5'</span>):        pool_5_out = tf.layers.max_pooling2d(conv_16_out, pool_size=[<span class="hljs-number">2</span>, <span class="hljs-number">2</span>], strides=[<span class="hljs-number">2</span>, <span class="hljs-number">2</span>], padding=<span class="hljs-string">'same'</span>)    print(pool_5_out.shape)    <span class="hljs-comment"># (1x1x512) --&gt; 512</span>    <span class="hljs-keyword">with</span> tf.name_scope(<span class="hljs-string">'fc_1'</span>):        pool_5_outz_flat = tf.layers.flatten(pool_5_out)        fc_1_out = tf.layers.dense(pool_5_outz_flat, <span class="hljs-number">512</span>, activation=tf.nn.relu)        fc_1_drop = tf.nn.dropout(fc_1_out, keep_prob=config.keep_prob)    print(fc_1_drop.shape)    <span class="hljs-comment"># 512 --&gt; 512</span>    <span class="hljs-keyword">with</span> tf.name_scope(<span class="hljs-string">'fc_2'</span>):        fc_2_out = tf.layers.dense(fc_1_drop, <span class="hljs-number">512</span>, activation=tf.nn.relu)        fc_2_drop = tf.nn.dropout(fc_2_out, keep_prob=config.keep_prob)    print(fc_2_drop.shape)    <span class="hljs-comment"># 512 --&gt; 10</span>    <span class="hljs-keyword">with</span> tf.name_scope(<span class="hljs-string">'fc_3'</span>):        fc_3_out = tf.layers.dense(fc_2_drop, <span class="hljs-number">10</span>, activation=<span class="hljs-literal">None</span>)    print(fc_3_out.shape)    <span class="hljs-keyword">return</span> fc_3_out</code></pre>]]></content>
    
    
    <categories>
      
      <category>TensorFlow</category>
      
    </categories>
    
    
    <tags>
      
      <tag>深度学习</tag>
      
      <tag>TensorFlow</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>TensorFlow-cifar10-图像分类之训练模型及可视化数据</title>
    <link href="/2018/09/TensorFlow-cifar-train/"/>
    <url>/2018/09/TensorFlow-cifar-train/</url>
    
    <content type="html"><![CDATA[<h3 id="训练文件"><a href="#训练文件" class="headerlink" title="训练文件:"></a><strong>训练文件</strong>:</h3><pre><code class="hljs python"><span class="hljs-comment">#!/usr/bin/env python</span><span class="hljs-comment"># -*- coding: utf-8 -*-</span><span class="hljs-comment"># @Time    : 2018/9/7 14:07</span><span class="hljs-comment"># @Author  : Seven</span><span class="hljs-comment"># @Site    : </span><span class="hljs-comment"># @File    : TrainModel.py</span><span class="hljs-comment"># @Software: PyCharm</span><span class="hljs-comment"># <span class="hljs-doctag">TODO:</span> 模型训练</span><span class="hljs-comment"># TODO：导入环境</span><span class="hljs-keyword">from</span> keras.preprocessing.image <span class="hljs-keyword">import</span> ImageDataGenerator<span class="hljs-keyword">from</span> keras.models <span class="hljs-keyword">import</span> Model<span class="hljs-keyword">import</span> time<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">run</span><span class="hljs-params">(network, X_train, y_train, X_test, y_test, augmentation=False)</span>:</span>    print(<span class="hljs-string">'X_train shape:'</span>, X_train.shape)    print(<span class="hljs-string">'Y_train shape:'</span>, y_train.shape)    print(X_train.shape[<span class="hljs-number">0</span>], <span class="hljs-string">'x_training samples'</span>)    print(X_test.shape[<span class="hljs-number">0</span>], <span class="hljs-string">'validation samples'</span>)    <span class="hljs-comment"># <span class="hljs-doctag">TODO:</span> 规一化处理</span>    X_train = X_train.astype(<span class="hljs-string">'float32'</span>)    X_test = X_test.astype(<span class="hljs-string">'float32'</span>)    X_train /= <span class="hljs-number">255</span>    X_test /= <span class="hljs-number">255</span>    <span class="hljs-comment"># TODO： 初始化模型</span>    inputs, logits, name = network    model = Model(inputs=inputs, outputs=logits, name=<span class="hljs-string">'model'</span>)    <span class="hljs-comment"># <span class="hljs-doctag">TODO:</span> 计算损失值并初始化optimizer</span>    model.compile(loss=<span class="hljs-string">'categorical_crossentropy'</span>,                  optimizer=<span class="hljs-string">'adadelta'</span>,                  metrics=[<span class="hljs-string">'accuracy'</span>])    model.summary()    print(<span class="hljs-string">'FUNCTION READY!!'</span>)    <span class="hljs-comment"># <span class="hljs-doctag">TODO:</span> 开始训练</span>    print(<span class="hljs-string">'TRAINING....'</span>)    epoch = <span class="hljs-number">100</span>    batch_size = <span class="hljs-number">256</span>    start = time.time()    <span class="hljs-comment"># 数据增强</span>    <span class="hljs-keyword">if</span> augmentation:        aug_gen = ImageDataGenerator(            featurewise_center=<span class="hljs-literal">False</span>,  <span class="hljs-comment"># set input mean to 0 over the dataset</span>            samplewise_center=<span class="hljs-literal">False</span>,  <span class="hljs-comment"># set each sample mean to 0</span>            featurewise_std_normalization=<span class="hljs-literal">False</span>,  <span class="hljs-comment"># divide inputs by std of the dataset</span>            samplewise_std_normalization=<span class="hljs-literal">False</span>,  <span class="hljs-comment"># divide each input by its std</span>            zca_whitening=<span class="hljs-literal">False</span>,  <span class="hljs-comment"># apply ZCA whitening</span>            rotation_range=<span class="hljs-number">0</span>,  <span class="hljs-comment"># randomly rotate images in the range (degrees, 0 to 180)</span>            width_shift_range=<span class="hljs-number">0.1</span>,  <span class="hljs-comment"># randomly shift images horizontally (fraction of total width)</span>            height_shift_range=<span class="hljs-number">0.1</span>,  <span class="hljs-comment"># randomly shift images vertically (fraction of total height)</span>            horizontal_flip=<span class="hljs-literal">True</span>,  <span class="hljs-comment"># randomly flip images</span>            vertical_flip=<span class="hljs-literal">False</span>,  <span class="hljs-comment"># randomly flip images</span>        )        aug_gen.fit(X_train)        generator = aug_gen.flow(X_train, y_train, batch_size=batch_size)        out = model.fit_generator(generator=generator,                                  steps_per_epoch=<span class="hljs-number">50000</span> // batch_size,                                  epochs=epoch,                                  validation_data=(X_test, y_test))        <span class="hljs-comment"># <span class="hljs-doctag">TODO:</span> 保存模型</span>        model.save(<span class="hljs-string">'CIFAR10_model_with_data_augmentation_%s.h5'</span> % name)    <span class="hljs-comment"># 不使用数据增强</span>    <span class="hljs-keyword">else</span>:        out = model.fit(x=X_train, y=y_train,                        batch_size=batch_size,                        epochs=epoch,                        validation_data=(X_test, y_test),                        shuffle=<span class="hljs-literal">True</span>)        <span class="hljs-comment"># <span class="hljs-doctag">TODO:</span> 保存模型</span>        model.save(<span class="hljs-string">'CIFAR10_model_no_data_augmentation_%s.h5'</span> % name)    loss, accuracy = model.evaluate(X_train, y_train, verbose=<span class="hljs-number">0</span>)    print(<span class="hljs-string">"Training Accuracy = %.2f %%     loss = %f"</span> % (accuracy * <span class="hljs-number">100</span>, loss))    loss, accuracy = model.evaluate(X_test, y_test, verbose=<span class="hljs-number">0</span>)    print(<span class="hljs-string">"Testing Accuracy = %.2f %%    loss = %f"</span> % (accuracy * <span class="hljs-number">100</span>, loss))    print(<span class="hljs-string">'@ Total Time Spent: %.2f seconds'</span> % (time.time() - start))    <span class="hljs-keyword">return</span> out, epoch, model</code></pre><h3 id="可视化数据："><a href="#可视化数据：" class="headerlink" title="可视化数据："></a><strong>可视化数据</strong>：</h3><pre><code class="hljs python"><span class="hljs-comment">#!/usr/bin/env python</span><span class="hljs-comment"># -*- coding: utf-8 -*-</span><span class="hljs-comment"># @Time    : 2018/9/7 15:00</span><span class="hljs-comment"># @Author  : Seven</span><span class="hljs-comment"># @Site    : </span><span class="hljs-comment"># @File    : visualization.py</span><span class="hljs-comment"># @Software: PyCharm</span><span class="hljs-comment"># <span class="hljs-doctag">TODO:</span> 可视化数据</span><span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt<span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">plot_acc_loss</span><span class="hljs-params">(data, epoch)</span>:</span>    <span class="hljs-string">"""</span><span class="hljs-string">    可视化数据</span><span class="hljs-string">    :param data: 数据</span><span class="hljs-string">    :param epoch: 迭代次数</span><span class="hljs-string">    :return:</span><span class="hljs-string">    """</span>    acc, loss, val_acc, val_loss = data.history[<span class="hljs-string">'acc'</span>], data.history[<span class="hljs-string">'loss'</span>], \                                   data.history[<span class="hljs-string">'val_acc'</span>], data.history[<span class="hljs-string">'val_loss'</span>]    plt.figure(figsize=(<span class="hljs-number">15</span>, <span class="hljs-number">5</span>))    plt.subplot(<span class="hljs-number">121</span>)    plt.plot(range(epoch), acc, label=<span class="hljs-string">'Train'</span>)    plt.plot(range(epoch), val_acc, label=<span class="hljs-string">'Test'</span>)    plt.title(<span class="hljs-string">'Accuracy over '</span> + str(epoch) + <span class="hljs-string">' Epochs'</span>, size=<span class="hljs-number">15</span>)    plt.legend()    plt.grid(<span class="hljs-literal">True</span>)    plt.subplot(<span class="hljs-number">122</span>)    plt.plot(range(epoch), loss, label=<span class="hljs-string">'Train'</span>)    plt.plot(range(epoch), val_loss, label=<span class="hljs-string">'Test'</span>)    plt.title(<span class="hljs-string">'Loss over '</span> + str(epoch) + <span class="hljs-string">' Epochs'</span>, size=<span class="hljs-number">15</span>)    plt.legend()    plt.grid(<span class="hljs-literal">True</span>)    plt.show()<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">plot_image</span><span class="hljs-params">(x_test, y_test, class_name, model)</span>:</span>    rand_id = np.random.choice(range(<span class="hljs-number">10000</span>), size=<span class="hljs-number">10</span>)    X_pred = np.array([x_test[i] <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> rand_id])    y_true = [y_test[i] <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> rand_id]    y_true = np.argmax(y_true, axis=<span class="hljs-number">1</span>)    y_true = [class_name[name] <span class="hljs-keyword">for</span> name <span class="hljs-keyword">in</span> y_true]    y_pred = model.predict(X_pred)    y_pred = np.argmax(y_pred, axis=<span class="hljs-number">1</span>)    y_pred = [class_name[name] <span class="hljs-keyword">for</span> name <span class="hljs-keyword">in</span> y_pred]    plt.figure(figsize=(<span class="hljs-number">15</span>, <span class="hljs-number">7</span>))    <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> range(<span class="hljs-number">10</span>):        plt.subplot(<span class="hljs-number">2</span>, <span class="hljs-number">5</span>, i + <span class="hljs-number">1</span>)        plt.imshow(X_pred[i].reshape(<span class="hljs-number">32</span>, <span class="hljs-number">32</span>, <span class="hljs-number">3</span>), cmap=<span class="hljs-string">'gray'</span>)        plt.title(<span class="hljs-string">'True: %s \n Pred: %s'</span> % (y_true[i], y_pred[i]), size=<span class="hljs-number">15</span>)    plt.show()</code></pre><h3 id="验证本地图片："><a href="#验证本地图片：" class="headerlink" title="验证本地图片："></a><strong>验证本地图片</strong>：</h3><pre><code class="hljs python"><span class="hljs-comment">#!/usr/bin/env python</span><span class="hljs-comment"># -*- coding: utf-8 -*-</span><span class="hljs-comment"># @Time    : 2018/9/7 19:18</span><span class="hljs-comment"># @Author  : Seven</span><span class="hljs-comment"># @Site    : </span><span class="hljs-comment"># @File    : TestModel.py</span><span class="hljs-comment"># @Software: PyCharm</span><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<span class="hljs-keyword">from</span> PIL <span class="hljs-keyword">import</span> Image<span class="hljs-keyword">from</span> keras.models <span class="hljs-keyword">import</span> load_model<span class="hljs-keyword">import</span> Read_data<span class="hljs-keyword">import</span> config<span class="hljs-keyword">import</span> visualization<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">run</span><span class="hljs-params">()</span>:</span>    <span class="hljs-comment"># 加载模型</span>    model = load_model(<span class="hljs-string">'CIFAR10_model_with_data_augmentation_VGG.h5'</span>)    <span class="hljs-comment"># 加载数据</span>    X_train, y_train, X_test, y_test = Read_data.load_data()    visualization.plot_image(X_test, y_test, config.class_name, model)<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">local_photos</span><span class="hljs-params">()</span>:</span>    <span class="hljs-comment"># input</span>    im = Image.open(<span class="hljs-string">'image/dog-1.jpg'</span>)    <span class="hljs-comment"># im.show()</span>    im = im.resize((<span class="hljs-number">32</span>, <span class="hljs-number">32</span>))    <span class="hljs-comment"># print(im.size, im.mode)</span>    im = np.array(im).astype(np.float32)    im = np.reshape(im, [<span class="hljs-number">-1</span>, <span class="hljs-number">32</span>*<span class="hljs-number">32</span>*<span class="hljs-number">3</span>])    im = (im - (<span class="hljs-number">255</span> / <span class="hljs-number">2.0</span>)) / <span class="hljs-number">255</span>    batch_xs = np.reshape(im, [<span class="hljs-number">-1</span>, <span class="hljs-number">32</span>, <span class="hljs-number">32</span>, <span class="hljs-number">3</span>])    model = load_model(<span class="hljs-string">'CIFAR10_model_with_data_augmentation_VGG.h5'</span>)    output = model.predict(batch_xs)    print(output)    print(<span class="hljs-string">'the out put is :'</span>, config.class_name[np.argmax(output)])<span class="hljs-keyword">if</span> __name__ == <span class="hljs-string">'__main__'</span>:    local_photos()</code></pre><h3 id="输出："><a href="#输出：" class="headerlink" title="输出："></a><strong>输出：</strong></h3><pre><code class="hljs python">[[<span class="hljs-number">0.</span> <span class="hljs-number">0.</span> <span class="hljs-number">0.</span> <span class="hljs-number">0.</span> <span class="hljs-number">0.</span> <span class="hljs-number">1.</span> <span class="hljs-number">0.</span> <span class="hljs-number">0.</span> <span class="hljs-number">0.</span> <span class="hljs-number">0.</span>]]the out put <span class="hljs-keyword">is</span> : dog</code></pre>]]></content>
    
    
    <categories>
      
      <category>TensorFlow</category>
      
    </categories>
    
    
    <tags>
      
      <tag>深度学习</tag>
      
      <tag>TensorFlow</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>TensorFlow-cifar10-图像分类之数据预处理及配置</title>
    <link href="/2018/09/TensorFlow-cifar/"/>
    <url>/2018/09/TensorFlow-cifar/</url>
    
    <content type="html"><![CDATA[<h3 id="预处理数据："><a href="#预处理数据：" class="headerlink" title="预处理数据："></a><strong>预处理数据</strong>：</h3><pre><code class="hljs python"><span class="hljs-comment">#!/usr/bin/env python</span><span class="hljs-comment"># -*- coding: utf-8 -*-</span><span class="hljs-comment"># @Time    : 2018/9/6 15:31</span><span class="hljs-comment"># @Author  : Seven</span><span class="hljs-comment"># @Site    : </span><span class="hljs-comment"># @File    : Read_data.py</span><span class="hljs-comment"># @Software: PyCharm</span><span class="hljs-comment"># <span class="hljs-doctag">TODO:</span> 加载数据</span><span class="hljs-keyword">import</span> pickle<span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<span class="hljs-keyword">from</span> sklearn.preprocessing <span class="hljs-keyword">import</span> MinMaxScaler, LabelBinarizer<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">load_cifar10_batch</span><span class="hljs-params">(path, batch_id)</span>:</span>    <span class="hljs-string">"""</span><span class="hljs-string">    加载batch的数据</span><span class="hljs-string">    :param path: 数据存储的目录</span><span class="hljs-string">    :param batch_id:batch的编号</span><span class="hljs-string">    :return:features and labels</span><span class="hljs-string">    """</span>    <span class="hljs-keyword">with</span> open(path + <span class="hljs-string">'/data_batch_'</span> + str(batch_id), mode=<span class="hljs-string">'rb'</span>) <span class="hljs-keyword">as</span> file:        batch = pickle.load(file, encoding=<span class="hljs-string">'latin1'</span>)    <span class="hljs-comment"># features and labels</span>    features = batch[<span class="hljs-string">'data'</span>].reshape((len(batch[<span class="hljs-string">'data'</span>]), <span class="hljs-number">3</span>, <span class="hljs-number">32</span>, <span class="hljs-number">32</span>)).transpose(<span class="hljs-number">0</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>, <span class="hljs-number">1</span>)    labels = batch[<span class="hljs-string">'labels'</span>]    <span class="hljs-keyword">return</span> features, labels<span class="hljs-comment"># 数据预处理</span><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">pre_processing_data</span><span class="hljs-params">(x_train, y_train, x_test, y_test)</span>:</span>    <span class="hljs-comment"># features</span>    minmax = MinMaxScaler()    <span class="hljs-comment"># 重塑数据</span>    <span class="hljs-comment"># (50000, 32, 32, 3) --&gt; (50000, 32*32*3)</span>    x_train_rows = x_train.reshape(x_train.shape[<span class="hljs-number">0</span>], <span class="hljs-number">32</span>*<span class="hljs-number">32</span>*<span class="hljs-number">3</span>)    <span class="hljs-comment"># (10000, 32, 32, 3) --&gt; (10000, 32*32*3)</span>    x_test_rows = x_test.reshape(x_test.shape[<span class="hljs-number">0</span>], <span class="hljs-number">32</span>*<span class="hljs-number">32</span>*<span class="hljs-number">3</span>)    <span class="hljs-comment"># 归一化</span>    x_train_norm = minmax.fit_transform(x_train_rows)    x_test_norm = minmax.fit_transform(x_test_rows)    <span class="hljs-comment"># 重塑数据</span>    x_train = x_train_norm.reshape(x_train_norm.shape[<span class="hljs-number">0</span>], <span class="hljs-number">32</span>, <span class="hljs-number">32</span>, <span class="hljs-number">3</span>)    x_test = x_test_norm.reshape(x_test_norm.shape[<span class="hljs-number">0</span>], <span class="hljs-number">32</span>, <span class="hljs-number">32</span>, <span class="hljs-number">3</span>)    <span class="hljs-comment"># labels</span>    <span class="hljs-comment"># 对标签进行one-hot</span>    n_class = <span class="hljs-number">10</span>    label_binarizer = LabelBinarizer().fit(np.array(range(n_class)))    y_train = label_binarizer.transform(y_train)    y_test = label_binarizer.transform(y_test)    <span class="hljs-keyword">return</span> x_train, y_train, x_test, y_test<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">cifar10_data</span><span class="hljs-params">()</span>:</span>    <span class="hljs-comment"># 加载训练数据</span>    cifar10_path = <span class="hljs-string">'data'</span>    <span class="hljs-comment"># 一共是有5个batch的训练数据</span>    x_train, y_train = load_cifar10_batch(cifar10_path, <span class="hljs-number">1</span>)    <span class="hljs-keyword">for</span> n <span class="hljs-keyword">in</span> range(<span class="hljs-number">2</span>, <span class="hljs-number">6</span>):        features, labels = load_cifar10_batch(cifar10_path, n)        x_train = np.concatenate([x_train, features])        y_train = np.concatenate([y_train, labels])    <span class="hljs-comment"># 加载测试数据</span>    <span class="hljs-keyword">with</span> open(cifar10_path + <span class="hljs-string">'/test_batch'</span>, mode=<span class="hljs-string">'rb'</span>) <span class="hljs-keyword">as</span> file:        batch = pickle.load(file, encoding=<span class="hljs-string">'latin1'</span>)        x_test = batch[<span class="hljs-string">'data'</span>].reshape((len(batch[<span class="hljs-string">'data'</span>]), <span class="hljs-number">3</span>, <span class="hljs-number">32</span>, <span class="hljs-number">32</span>)).transpose(<span class="hljs-number">0</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>, <span class="hljs-number">1</span>)        y_test = batch[<span class="hljs-string">'labels'</span>]    x_train, y_train, x_test, y_test = pre_processing_data(x_train, y_train, x_test, y_test)    <span class="hljs-keyword">return</span> x_train, y_train, x_test, y_test</code></pre><h3 id="配置文件："><a href="#配置文件：" class="headerlink" title="配置文件："></a><strong>配置文件</strong>：</h3><pre><code class="hljs python"><span class="hljs-comment">#!/usr/bin/env python</span><span class="hljs-comment"># -*- coding: utf-8 -*-</span><span class="hljs-comment"># @Time    : 2018/9/6 16:02</span><span class="hljs-comment"># @Author  : Seven</span><span class="hljs-comment"># @Site    : </span><span class="hljs-comment"># @File    : config.py</span><span class="hljs-comment"># @Software: PyCharm</span><span class="hljs-keyword">import</span> tensorflow <span class="hljs-keyword">as</span> tf<span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt<span class="hljs-comment"># 初始化卷积神经网络参数</span>keep_prob = <span class="hljs-number">0.8</span>epochs = <span class="hljs-number">20</span>batch_size = <span class="hljs-number">128</span>n_classes = <span class="hljs-number">10</span>  <span class="hljs-comment"># 总共10类</span><span class="hljs-comment"># 定义输入和标签的placeholder</span>inputs = tf.placeholder(tf.float32, [<span class="hljs-literal">None</span>, <span class="hljs-number">32</span>, <span class="hljs-number">32</span>, <span class="hljs-number">3</span>], name=<span class="hljs-string">'inputs'</span>)targets = tf.placeholder(tf.float32, [<span class="hljs-literal">None</span>, <span class="hljs-number">10</span>], name=<span class="hljs-string">'logits'</span>)learning_rate = <span class="hljs-number">0.001</span><span class="hljs-comment"># 显示图片</span><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">show_images</span><span class="hljs-params">(images)</span>:</span>    fig, axes = plt.subplots(nrows=<span class="hljs-number">3</span>, ncols=<span class="hljs-number">3</span>, sharex=<span class="hljs-literal">True</span>, sharey=<span class="hljs-literal">True</span>, figsize=(<span class="hljs-number">9</span>, <span class="hljs-number">9</span>))    img = images[: <span class="hljs-number">60</span>]    <span class="hljs-keyword">for</span> image, row <span class="hljs-keyword">in</span> zip([img[: <span class="hljs-number">20</span>], img[<span class="hljs-number">20</span>: <span class="hljs-number">40</span>], img[<span class="hljs-number">40</span>: <span class="hljs-number">60</span>]], axes):        <span class="hljs-keyword">for</span> img, ax <span class="hljs-keyword">in</span> zip(image, row):            ax.imshow(img)            ax.get_xaxis().set_visible(<span class="hljs-literal">False</span>)            ax.get_yaxis().set_visible(<span class="hljs-literal">False</span>)    fig.tight_layout(pad=<span class="hljs-number">0.1</span>)    <span class="hljs-comment"># plt.show()</span><span class="hljs-comment"># 存储alexnet所有的网络参数</span>weights = &#123;    <span class="hljs-string">'wc1'</span>: tf.Variable(tf.random_normal(shape=[<span class="hljs-number">11</span>, <span class="hljs-number">11</span>, <span class="hljs-number">3</span>, <span class="hljs-number">96</span>])),    <span class="hljs-string">'wc2'</span>: tf.Variable(tf.random_normal(shape=[<span class="hljs-number">5</span>, <span class="hljs-number">5</span>, <span class="hljs-number">96</span>, <span class="hljs-number">256</span>])),    <span class="hljs-string">'wc3'</span>: tf.Variable(tf.random_normal(shape=[<span class="hljs-number">3</span>, <span class="hljs-number">3</span>, <span class="hljs-number">256</span>, <span class="hljs-number">384</span>])),    <span class="hljs-string">'wc4'</span>: tf.Variable(tf.random_normal(shape=[<span class="hljs-number">3</span>, <span class="hljs-number">3</span>, <span class="hljs-number">384</span>, <span class="hljs-number">384</span>])),    <span class="hljs-string">'wc5'</span>: tf.Variable(tf.random_normal(shape=[<span class="hljs-number">3</span>, <span class="hljs-number">3</span>, <span class="hljs-number">384</span>, <span class="hljs-number">256</span>])),    <span class="hljs-string">'wd1'</span>: tf.Variable(tf.random_normal(shape=[<span class="hljs-number">4</span>*<span class="hljs-number">4</span>*<span class="hljs-number">256</span>, <span class="hljs-number">4096</span>])),    <span class="hljs-string">'wd2'</span>: tf.Variable(tf.random_normal(shape=[<span class="hljs-number">4096</span>, <span class="hljs-number">1024</span>])),    <span class="hljs-string">'out'</span>: tf.Variable(tf.random_normal(shape=[<span class="hljs-number">1024</span>, n_classes]))&#125;biases = &#123;    <span class="hljs-string">'bc1'</span>: tf.Variable(tf.random_normal([<span class="hljs-number">96</span>])),    <span class="hljs-string">'bc2'</span>: tf.Variable(tf.random_normal([<span class="hljs-number">256</span>])),    <span class="hljs-string">'bc3'</span>: tf.Variable(tf.random_normal([<span class="hljs-number">384</span>])),    <span class="hljs-string">'bc4'</span>: tf.Variable(tf.random_normal([<span class="hljs-number">384</span>])),    <span class="hljs-string">'bc5'</span>: tf.Variable(tf.random_normal([<span class="hljs-number">256</span>])),    <span class="hljs-string">'bd1'</span>: tf.Variable(tf.random_normal([<span class="hljs-number">4096</span>])),    <span class="hljs-string">'bd2'</span>: tf.Variable(tf.random_normal([<span class="hljs-number">1024</span>])),    <span class="hljs-string">'out'</span>: tf.Variable(tf.random_normal([n_classes]))&#125;</code></pre>]]></content>
    
    
    <categories>
      
      <category>TensorFlow</category>
      
    </categories>
    
    
    <tags>
      
      <tag>深度学习</tag>
      
      <tag>TensorFlow</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>TensorFlow数据可视化</title>
    <link href="/2018/09/TensorFlow-visualization/"/>
    <url>/2018/09/TensorFlow-visualization/</url>
    
    <content type="html"><![CDATA[<h4 id="代码："><a href="#代码：" class="headerlink" title="代码："></a><strong>代码</strong>：</h4><pre><code class="hljs python"><span class="hljs-comment">#!/usr/bin/env python</span><span class="hljs-comment"># -*- coding: utf-8 -*-</span><span class="hljs-comment"># @Time    : 2018/9/7 16:59</span><span class="hljs-comment"># @Author  : Seven</span><span class="hljs-comment"># @Site    :</span><span class="hljs-comment"># @File    : demo.py</span><span class="hljs-comment"># @Software: PyCharm</span><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<span class="hljs-keyword">import</span> tensorflow <span class="hljs-keyword">as</span> tfn_observations = <span class="hljs-number">100</span>xs = np.linspace(<span class="hljs-number">-3</span>, <span class="hljs-number">3</span>, n_observations)ys = <span class="hljs-number">0.8</span>*xs + <span class="hljs-number">0.1</span> + np.random.uniform(<span class="hljs-number">-0.5</span>, <span class="hljs-number">0.5</span>, n_observations)X = tf.placeholder(tf.float32, name=<span class="hljs-string">'X'</span>)Y = tf.placeholder(tf.float32, name=<span class="hljs-string">'Y'</span>)W = tf.Variable(tf.random_normal([<span class="hljs-number">1</span>]), name=<span class="hljs-string">'weight'</span>)tf.summary.histogram(<span class="hljs-string">'weight'</span>, W)b = tf.Variable(tf.random_normal([<span class="hljs-number">1</span>]), name=<span class="hljs-string">'bias'</span>)tf.summary.histogram(<span class="hljs-string">'bias'</span>, b)Y_pred = tf.add(tf.multiply(X, W), b)loss = tf.square(Y - Y_pred, name=<span class="hljs-string">'loss'</span>)tf.summary.scalar(<span class="hljs-string">'loss'</span>, tf.reshape(loss, []))learning_rate = <span class="hljs-number">0.01</span>optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss)n_samples = xs.shape[<span class="hljs-number">0</span>]init = tf.global_variables_initializer()<span class="hljs-keyword">with</span> tf.Session() <span class="hljs-keyword">as</span> sess:    <span class="hljs-comment"># 记得初始化所有变量</span>    sess.run(init)    merged = tf.summary.merge_all()    log_writer = tf.summary.FileWriter(<span class="hljs-string">"./logs/linear_regression"</span>, sess.graph)    <span class="hljs-comment"># 训练模型</span>    <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> range(<span class="hljs-number">50</span>):        total_loss = <span class="hljs-number">0</span>        <span class="hljs-keyword">for</span> x, y <span class="hljs-keyword">in</span> zip(xs, ys):            <span class="hljs-comment"># 通过feed_dic把数据灌进去</span>            _, loss_value, merged_summary = sess.run([optimizer, loss, merged], feed_dict=&#123;X: x, Y: y&#125;)            total_loss += loss_value        <span class="hljs-keyword">if</span> i % <span class="hljs-number">5</span> == <span class="hljs-number">0</span>:            print(<span class="hljs-string">'Epoch &#123;0&#125;: &#123;1&#125;'</span>.format(i, total_loss / n_samples))            log_writer.add_summary(merged_summary, i)    <span class="hljs-comment"># 关闭writer</span>    log_writer.close()    <span class="hljs-comment"># 取出w和b的值</span>    W, b = sess.run([W, b])print(W, b)print(<span class="hljs-string">"W:"</span>+str(W[<span class="hljs-number">0</span>]))print(<span class="hljs-string">"b:"</span>+str(b[<span class="hljs-number">0</span>]))</code></pre><h4 id="执行结果："><a href="#执行结果：" class="headerlink" title="执行结果："></a><strong>执行结果</strong>：</h4><pre><code class="hljs bash">Epoch 0: [0.5815637]Epoch 5: [0.08926834]Epoch 10: [0.08926827]Epoch 15: [0.08926827]Epoch 20: [0.08926827]Epoch 25: [0.08926827]Epoch 30: [0.08926827]Epoch 35: [0.08926827]Epoch 40: [0.08926827]Epoch 45: [0.08926827][0.7907032] [0.10920969]W:0.7907032b:0.10920969</code></pre><h4 id="Tensoboard："><a href="#Tensoboard：" class="headerlink" title="Tensoboard："></a><strong>Tensoboard</strong>：</h4><h4 id="在终端执行代码："><a href="#在终端执行代码：" class="headerlink" title="在终端执行代码："></a>在终端执行代码：</h4><pre><code class="hljs bash">tensorboard --logdir <span class="hljs-built_in">log</span> (你保存文件所在位置)</code></pre><h4 id="输出："><a href="#输出：" class="headerlink" title="输出："></a>输出：</h4><pre><code class="hljs bash">TensorBoard 0.4.0 at http://seven:6006 (Press CTRL+C to quit)</code></pre><p>然后打开网页：<code>http://seven:6006</code>。</p><h4 id="显示结果："><a href="#显示结果：" class="headerlink" title="显示结果："></a>显示结果：</h4><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20200601182650.png" srcset="/img/loading.gif" alt="image"></p><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20200601182651.png" srcset="/img/loading.gif" alt="image"></p><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20200601182652.png" srcset="/img/loading.gif" alt="image"></p><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20200601182653.png" srcset="/img/loading.gif" alt="image"></p>]]></content>
    
    
    <categories>
      
      <category>TensorFlow</category>
      
    </categories>
    
    
    <tags>
      
      <tag>深度学习</tag>
      
      <tag>TensorFlow</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Keras-cifar10-图像分类</title>
    <link href="/2018/09/keras-cifar/"/>
    <url>/2018/09/keras-cifar/</url>
    
    <content type="html"><![CDATA[<h3 id="预处理数据："><a href="#预处理数据：" class="headerlink" title="预处理数据："></a><strong>预处理数据</strong>：</h3><pre><code class="hljs python"><span class="hljs-comment">#!/usr/bin/env python</span><span class="hljs-comment"># -*- coding: utf-8 -*-</span><span class="hljs-comment"># @Time    : 2018/9/6 15:31</span><span class="hljs-comment"># @Author  : Seven</span><span class="hljs-comment"># @Site    : </span><span class="hljs-comment"># @File    : Read_data.py</span><span class="hljs-comment"># @Software: PyCharm</span><span class="hljs-comment"># <span class="hljs-doctag">TODO:</span> 加载数据</span><span class="hljs-keyword">import</span> pickle<span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<span class="hljs-keyword">from</span> sklearn.preprocessing <span class="hljs-keyword">import</span> MinMaxScaler, LabelBinarizer<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">load_cifar10_batch</span><span class="hljs-params">(path, batch_id)</span>:</span>    <span class="hljs-string">"""</span><span class="hljs-string">    加载batch的数据</span><span class="hljs-string">    :param path: 数据存储的目录</span><span class="hljs-string">    :param batch_id:batch的编号</span><span class="hljs-string">    :return:features and labels</span><span class="hljs-string">    """</span>    <span class="hljs-keyword">with</span> open(path + <span class="hljs-string">'/data_batch_'</span> + str(batch_id), mode=<span class="hljs-string">'rb'</span>) <span class="hljs-keyword">as</span> file:        batch = pickle.load(file, encoding=<span class="hljs-string">'latin1'</span>)    <span class="hljs-comment"># features and labels</span>    features = batch[<span class="hljs-string">'data'</span>].reshape((len(batch[<span class="hljs-string">'data'</span>]), <span class="hljs-number">3</span>, <span class="hljs-number">32</span>, <span class="hljs-number">32</span>)).transpose(<span class="hljs-number">0</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>, <span class="hljs-number">1</span>)    labels = batch[<span class="hljs-string">'labels'</span>]    <span class="hljs-keyword">return</span> features, labels<span class="hljs-comment"># 数据预处理</span><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">pre_processing_data</span><span class="hljs-params">(x_train, y_train, x_test, y_test)</span>:</span>    <span class="hljs-comment"># features</span>    minmax = MinMaxScaler()    <span class="hljs-comment"># 重塑数据</span>    <span class="hljs-comment"># (50000, 32, 32, 3) --&gt; (50000, 32*32*3)</span>    x_train_rows = x_train.reshape(x_train.shape[<span class="hljs-number">0</span>], <span class="hljs-number">32</span>*<span class="hljs-number">32</span>*<span class="hljs-number">3</span>)    <span class="hljs-comment"># (10000, 32, 32, 3) --&gt; (10000, 32*32*3)</span>    x_test_rows = x_test.reshape(x_test.shape[<span class="hljs-number">0</span>], <span class="hljs-number">32</span>*<span class="hljs-number">32</span>*<span class="hljs-number">3</span>)    <span class="hljs-comment"># 归一化</span>    x_train_norm = minmax.fit_transform(x_train_rows)    x_test_norm = minmax.fit_transform(x_test_rows)    <span class="hljs-comment"># 重塑数据</span>    x_train = x_train_norm.reshape(x_train_norm.shape[<span class="hljs-number">0</span>], <span class="hljs-number">32</span>, <span class="hljs-number">32</span>, <span class="hljs-number">3</span>)    x_test = x_test_norm.reshape(x_test_norm.shape[<span class="hljs-number">0</span>], <span class="hljs-number">32</span>, <span class="hljs-number">32</span>, <span class="hljs-number">3</span>)    <span class="hljs-comment"># labels</span>    <span class="hljs-comment"># 对标签进行one-hot</span>    n_class = <span class="hljs-number">10</span>    label_binarizer = LabelBinarizer().fit(np.array(range(n_class)))    y_train = label_binarizer.transform(y_train)    y_test = label_binarizer.transform(y_test)    <span class="hljs-keyword">return</span> x_train, y_train, x_test, y_test<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">cifar10_data</span><span class="hljs-params">()</span>:</span>    <span class="hljs-comment"># 加载训练数据</span>    cifar10_path = <span class="hljs-string">'data'</span>    <span class="hljs-comment"># 一共是有5个batch的训练数据</span>    x_train, y_train = load_cifar10_batch(cifar10_path, <span class="hljs-number">1</span>)    <span class="hljs-keyword">for</span> n <span class="hljs-keyword">in</span> range(<span class="hljs-number">2</span>, <span class="hljs-number">6</span>):        features, labels = load_cifar10_batch(cifar10_path, n)        x_train = np.concatenate([x_train, features])        y_train = np.concatenate([y_train, labels])    <span class="hljs-comment"># 加载测试数据</span>    <span class="hljs-keyword">with</span> open(cifar10_path + <span class="hljs-string">'/test_batch'</span>, mode=<span class="hljs-string">'rb'</span>) <span class="hljs-keyword">as</span> file:        batch = pickle.load(file, encoding=<span class="hljs-string">'latin1'</span>)        x_test = batch[<span class="hljs-string">'data'</span>].reshape((len(batch[<span class="hljs-string">'data'</span>]), <span class="hljs-number">3</span>, <span class="hljs-number">32</span>, <span class="hljs-number">32</span>)).transpose(<span class="hljs-number">0</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>, <span class="hljs-number">1</span>)        y_test = batch[<span class="hljs-string">'labels'</span>]    x_train, y_train, x_test, y_test = pre_processing_data(x_train, y_train, x_test, y_test)    <span class="hljs-keyword">return</span> x_train, y_train, x_test, y_test</code></pre><h3 id="配置文件："><a href="#配置文件：" class="headerlink" title="配置文件："></a><strong>配置文件</strong>：</h3><pre><code class="hljs python"><span class="hljs-comment">#!/usr/bin/env python</span><span class="hljs-comment"># -*- coding: utf-8 -*-</span><span class="hljs-comment"># @Time    : 2018/9/7 14:10</span><span class="hljs-comment"># @Author  : Seven</span><span class="hljs-comment"># @Site    : </span><span class="hljs-comment"># @File    : config.py</span><span class="hljs-comment"># @Software: PyCharm</span><span class="hljs-comment"># <span class="hljs-doctag">TODO:</span> 基本配置</span>nb_classes = <span class="hljs-number">10</span>class_name = &#123;    <span class="hljs-number">0</span>: <span class="hljs-string">'airplane'</span>,    <span class="hljs-number">1</span>: <span class="hljs-string">'automobile'</span>,    <span class="hljs-number">2</span>: <span class="hljs-string">'bird'</span>,    <span class="hljs-number">3</span>: <span class="hljs-string">'cat'</span>,    <span class="hljs-number">4</span>: <span class="hljs-string">'deer'</span>,    <span class="hljs-number">5</span>: <span class="hljs-string">'dog'</span>,    <span class="hljs-number">6</span>: <span class="hljs-string">'frog'</span>,    <span class="hljs-number">7</span>: <span class="hljs-string">'horse'</span>,    <span class="hljs-number">8</span>: <span class="hljs-string">'ship'</span>,    <span class="hljs-number">9</span>: <span class="hljs-string">'truck'</span>,&#125;</code></pre><h3 id="类VGG16Net网络："><a href="#类VGG16Net网络：" class="headerlink" title="类VGG16Net网络："></a><strong>类VGG16Net网络</strong>：</h3><pre><code class="hljs python"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">KerasVGG</span><span class="hljs-params">()</span>:</span>    <span class="hljs-string">"""</span><span class="hljs-string">    模型采用类似于 VGG16 的结构：</span><span class="hljs-string">        使用固定尺寸的小卷积核 (3x3)</span><span class="hljs-string">        以2的幂次递增的卷积核数量 (64, 128, 256)</span><span class="hljs-string">        两层卷积搭配一层池化</span><span class="hljs-string">        全连接层没有采用 VGG16 庞大的三层结构，避免运算量过大，仅使用 128 个节点的单个FC</span><span class="hljs-string">        权重初始化采用He Normal</span><span class="hljs-string">    :return:</span><span class="hljs-string">    """</span>    name = <span class="hljs-string">'VGG'</span>    inputs = Input(shape=(<span class="hljs-number">32</span>, <span class="hljs-number">32</span>, <span class="hljs-number">3</span>))    net = inputs    <span class="hljs-comment"># (32, 32, 3)--&gt;(32, 32, 64)</span>    net = Convolution2D(filters=<span class="hljs-number">64</span>, kernel_size=<span class="hljs-number">3</span>, strides=<span class="hljs-number">1</span>,                        padding=<span class="hljs-string">'same'</span>, activation=<span class="hljs-string">'relu'</span>,                        kernel_initializer=<span class="hljs-string">'he_normal'</span>)(net)    <span class="hljs-comment"># (32, 32, 64)--&gt;(32, 32, 64)</span>    net = Convolution2D(filters=<span class="hljs-number">64</span>, kernel_size=<span class="hljs-number">3</span>, strides=<span class="hljs-number">1</span>,                        padding=<span class="hljs-string">'same'</span>, activation=<span class="hljs-string">'relu'</span>,                        kernel_initializer=<span class="hljs-string">'he_normal'</span>)(net)    <span class="hljs-comment"># (32, 32, 64)--&gt;(16, 16, 64)</span>    net = MaxPooling2D(pool_size=<span class="hljs-number">2</span>, strides=<span class="hljs-number">2</span>, padding=<span class="hljs-string">'valid'</span>)(net)    <span class="hljs-comment"># (16, 16, 64)--&gt;(16, 16, 128)</span>    net = Convolution2D(filters=<span class="hljs-number">128</span>, kernel_size=<span class="hljs-number">3</span>, strides=<span class="hljs-number">1</span>,                        padding=<span class="hljs-string">'same'</span>, activation=<span class="hljs-string">'relu'</span>,                        kernel_initializer=<span class="hljs-string">'he_normal'</span>)(net)    <span class="hljs-comment"># (16, 16, 64)--&gt;(16, 16, 128)</span>    net = Convolution2D(filters=<span class="hljs-number">128</span>, kernel_size=<span class="hljs-number">3</span>, strides=<span class="hljs-number">1</span>,                        padding=<span class="hljs-string">'same'</span>, activation=<span class="hljs-string">'relu'</span>,                        kernel_initializer=<span class="hljs-string">'he_normal'</span>)(net)    <span class="hljs-comment"># (16, 16, 128)--&gt;(8, 8, 128)</span>    net = MaxPooling2D(pool_size=<span class="hljs-number">2</span>, strides=<span class="hljs-number">2</span>, padding=<span class="hljs-string">'valid'</span>)(net)    <span class="hljs-comment"># (8, 8, 128)--&gt;(8, 8, 256)</span>    net = Convolution2D(filters=<span class="hljs-number">256</span>, kernel_size=<span class="hljs-number">3</span>, strides=<span class="hljs-number">1</span>,                        padding=<span class="hljs-string">'same'</span>, activation=<span class="hljs-string">'relu'</span>,                        kernel_initializer=<span class="hljs-string">'he_normal'</span>)(net)    <span class="hljs-comment"># (8, 8, 256)--&gt;(8, 8, 256)</span>    net = Convolution2D(filters=<span class="hljs-number">256</span>, kernel_size=<span class="hljs-number">3</span>, strides=<span class="hljs-number">1</span>,                        padding=<span class="hljs-string">'same'</span>, activation=<span class="hljs-string">'relu'</span>,                        kernel_initializer=<span class="hljs-string">'he_normal'</span>)(net)    <span class="hljs-comment"># (8, 8, 256)--&gt;(4, 4, 256)</span>    net = MaxPooling2D(pool_size=<span class="hljs-number">2</span>, strides=<span class="hljs-number">2</span>, padding=<span class="hljs-string">'valid'</span>)(net)    <span class="hljs-comment"># (4, 4, 256) --&gt; 4*4*256=4096</span>    net = Flatten()(net)    <span class="hljs-comment"># 4096 --&gt; 128</span>    net = Dense(units=<span class="hljs-number">128</span>, activation=<span class="hljs-string">'relu'</span>,                kernel_initializer=<span class="hljs-string">'he_normal'</span>)(net)    <span class="hljs-comment"># Dropout</span>    net = Dropout(<span class="hljs-number">0.5</span>)(net)    <span class="hljs-comment"># 128 --&gt; 10</span>    net = Dense(units=config.nb_classes, activation=<span class="hljs-string">'softmax'</span>,                kernel_initializer=<span class="hljs-string">'he_normal'</span>)(net)    <span class="hljs-keyword">return</span> inputs, net, name</code></pre><h3 id="添加BN层"><a href="#添加BN层" class="headerlink" title="添加BN层:"></a><strong>添加BN层</strong>:</h3><pre><code class="hljs python"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">KerasBN</span><span class="hljs-params">()</span>:</span>    <span class="hljs-string">"""</span><span class="hljs-string">    添加batch norm 层</span><span class="hljs-string">    :return:</span><span class="hljs-string">    """</span>    name = <span class="hljs-string">'BN'</span>    inputs = Input(shape=(<span class="hljs-number">32</span>, <span class="hljs-number">32</span>, <span class="hljs-number">3</span>))    net = inputs    <span class="hljs-comment"># (32, 32, 3)--&gt;(32, 32, 64)</span>    net = Convolution2D(filters=<span class="hljs-number">64</span>, kernel_size=<span class="hljs-number">3</span>, strides=<span class="hljs-number">1</span>,                        padding=<span class="hljs-string">'same'</span>, activation=<span class="hljs-string">'relu'</span>,                        kernel_initializer=<span class="hljs-string">'he_normal'</span>)(net)    net = BatchNormalization()(net)    net = Activation(<span class="hljs-string">'relu'</span>)(net)    <span class="hljs-comment"># (32, 32, 64)--&gt;(32, 32, 64)</span>    net = Convolution2D(filters=<span class="hljs-number">64</span>, kernel_size=<span class="hljs-number">3</span>, strides=<span class="hljs-number">1</span>,                        padding=<span class="hljs-string">'same'</span>, activation=<span class="hljs-string">'relu'</span>,                        kernel_initializer=<span class="hljs-string">'he_normal'</span>)(net)    net = BatchNormalization()(net)    net = Activation(<span class="hljs-string">'relu'</span>)(net)    <span class="hljs-comment"># (32, 32, 64)--&gt;(16, 16, 64)</span>    net = MaxPooling2D(pool_size=<span class="hljs-number">2</span>, strides=<span class="hljs-number">2</span>, padding=<span class="hljs-string">'valid'</span>)(net)    <span class="hljs-comment"># (16, 16, 64)--&gt;(16, 16, 128)</span>    net = Convolution2D(filters=<span class="hljs-number">128</span>, kernel_size=<span class="hljs-number">3</span>, strides=<span class="hljs-number">1</span>,                        padding=<span class="hljs-string">'same'</span>, activation=<span class="hljs-string">'relu'</span>,                        kernel_initializer=<span class="hljs-string">'he_normal'</span>)(net)    net = BatchNormalization()(net)    net = Activation(<span class="hljs-string">'relu'</span>)(net)    <span class="hljs-comment"># (16, 16, 64)--&gt;(16, 16, 128)</span>    net = Convolution2D(filters=<span class="hljs-number">128</span>, kernel_size=<span class="hljs-number">3</span>, strides=<span class="hljs-number">1</span>,                        padding=<span class="hljs-string">'same'</span>, activation=<span class="hljs-string">'relu'</span>,                        kernel_initializer=<span class="hljs-string">'he_normal'</span>)(net)    net = BatchNormalization()(net)    net = Activation(<span class="hljs-string">'relu'</span>)(net)    <span class="hljs-comment"># (16, 16, 128)--&gt;(8, 8, 128)</span>    net = MaxPooling2D(pool_size=<span class="hljs-number">2</span>, strides=<span class="hljs-number">2</span>, padding=<span class="hljs-string">'valid'</span>)(net)    <span class="hljs-comment"># (8, 8, 128)--&gt;(8, 8, 256)</span>    net = Convolution2D(filters=<span class="hljs-number">256</span>, kernel_size=<span class="hljs-number">3</span>, strides=<span class="hljs-number">1</span>,                        padding=<span class="hljs-string">'same'</span>, activation=<span class="hljs-string">'relu'</span>,                        kernel_initializer=<span class="hljs-string">'he_normal'</span>)(net)    net = BatchNormalization()(net)    net = Activation(<span class="hljs-string">'relu'</span>)(net)    <span class="hljs-comment"># (8, 8, 128)--&gt;(8, 8, 256)</span>    net = Convolution2D(filters=<span class="hljs-number">256</span>, kernel_size=<span class="hljs-number">3</span>, strides=<span class="hljs-number">1</span>,                        padding=<span class="hljs-string">'same'</span>, activation=<span class="hljs-string">'relu'</span>,                        kernel_initializer=<span class="hljs-string">'he_normal'</span>)(net)    net = BatchNormalization()(net)    net = Activation(<span class="hljs-string">'relu'</span>)(net)    <span class="hljs-comment"># (8, 8, 256)--&gt;(4, 4, 256)</span>    net = MaxPooling2D(pool_size=<span class="hljs-number">2</span>, strides=<span class="hljs-number">2</span>, padding=<span class="hljs-string">'valid'</span>)(net)    <span class="hljs-comment"># (4, 4, 256) --&gt; 4*4*256=4096</span>    net = Flatten()(net)    <span class="hljs-comment"># 4096 --&gt; 128</span>    net = Dense(units=<span class="hljs-number">128</span>, activation=<span class="hljs-string">'relu'</span>,                kernel_initializer=<span class="hljs-string">'he_normal'</span>)(net)    <span class="hljs-comment"># Dropout</span>    net = Dropout(<span class="hljs-number">0.5</span>)(net)    <span class="hljs-comment"># 128 --&gt; 10</span>    net = Dense(units=config.nb_classes, activation=<span class="hljs-string">'softmax'</span>,                kernel_initializer=<span class="hljs-string">'he_normal'</span>)(net)    <span class="hljs-keyword">return</span> inputs, net, name</code></pre><h3 id="训练文件"><a href="#训练文件" class="headerlink" title="训练文件:"></a><strong>训练文件</strong>:</h3><pre><code class="hljs python"><span class="hljs-comment">#!/usr/bin/env python</span><span class="hljs-comment"># -*- coding: utf-8 -*-</span><span class="hljs-comment"># @Time    : 2018/9/7 14:07</span><span class="hljs-comment"># @Author  : Seven</span><span class="hljs-comment"># @Site    : </span><span class="hljs-comment"># @File    : TrainModel.py</span><span class="hljs-comment"># @Software: PyCharm</span><span class="hljs-comment"># <span class="hljs-doctag">TODO:</span> 模型训练</span><span class="hljs-comment"># TODO：导入环境</span><span class="hljs-keyword">from</span> keras.preprocessing.image <span class="hljs-keyword">import</span> ImageDataGenerator<span class="hljs-keyword">from</span> keras.models <span class="hljs-keyword">import</span> Model<span class="hljs-keyword">import</span> time<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">run</span><span class="hljs-params">(network, X_train, y_train, X_test, y_test, augmentation=False)</span>:</span>    print(<span class="hljs-string">'X_train shape:'</span>, X_train.shape)    print(<span class="hljs-string">'Y_train shape:'</span>, y_train.shape)    print(X_train.shape[<span class="hljs-number">0</span>], <span class="hljs-string">'x_training samples'</span>)    print(X_test.shape[<span class="hljs-number">0</span>], <span class="hljs-string">'validation samples'</span>)    <span class="hljs-comment"># <span class="hljs-doctag">TODO:</span> 规一化处理</span>    X_train = X_train.astype(<span class="hljs-string">'float32'</span>)    X_test = X_test.astype(<span class="hljs-string">'float32'</span>)    X_train /= <span class="hljs-number">255</span>    X_test /= <span class="hljs-number">255</span>    <span class="hljs-comment"># TODO： 初始化模型</span>    inputs, logits, name = network    model = Model(inputs=inputs, outputs=logits, name=<span class="hljs-string">'model'</span>)    <span class="hljs-comment"># <span class="hljs-doctag">TODO:</span> 计算损失值并初始化optimizer</span>    model.compile(loss=<span class="hljs-string">'categorical_crossentropy'</span>,                  optimizer=<span class="hljs-string">'adadelta'</span>,                  metrics=[<span class="hljs-string">'accuracy'</span>])    model.summary()    print(<span class="hljs-string">'FUNCTION READY!!'</span>)    <span class="hljs-comment"># <span class="hljs-doctag">TODO:</span> 开始训练</span>    print(<span class="hljs-string">'TRAINING....'</span>)    epoch = <span class="hljs-number">100</span>    batch_size = <span class="hljs-number">256</span>    start = time.time()    <span class="hljs-comment"># 数据增强</span>    <span class="hljs-keyword">if</span> augmentation:        aug_gen = ImageDataGenerator(            featurewise_center=<span class="hljs-literal">False</span>,  <span class="hljs-comment"># set input mean to 0 over the dataset</span>            samplewise_center=<span class="hljs-literal">False</span>,  <span class="hljs-comment"># set each sample mean to 0</span>            featurewise_std_normalization=<span class="hljs-literal">False</span>,  <span class="hljs-comment"># divide inputs by std of the dataset</span>            samplewise_std_normalization=<span class="hljs-literal">False</span>,  <span class="hljs-comment"># divide each input by its std</span>            zca_whitening=<span class="hljs-literal">False</span>,  <span class="hljs-comment"># apply ZCA whitening</span>            rotation_range=<span class="hljs-number">0</span>,  <span class="hljs-comment"># randomly rotate images in the range (degrees, 0 to 180)</span>            width_shift_range=<span class="hljs-number">0.1</span>,  <span class="hljs-comment"># randomly shift images horizontally (fraction of total width)</span>            height_shift_range=<span class="hljs-number">0.1</span>,  <span class="hljs-comment"># randomly shift images vertically (fraction of total height)</span>            horizontal_flip=<span class="hljs-literal">True</span>,  <span class="hljs-comment"># randomly flip images</span>            vertical_flip=<span class="hljs-literal">False</span>,  <span class="hljs-comment"># randomly flip images</span>        )        aug_gen.fit(X_train)        generator = aug_gen.flow(X_train, y_train, batch_size=batch_size)        out = model.fit_generator(generator=generator,                                  steps_per_epoch=<span class="hljs-number">50000</span> // batch_size,                                  epochs=epoch,                                  validation_data=(X_test, y_test))        <span class="hljs-comment"># <span class="hljs-doctag">TODO:</span> 保存模型</span>        model.save(<span class="hljs-string">'CIFAR10_model_with_data_augmentation_%s.h5'</span> % name)    <span class="hljs-comment"># 不使用数据增强</span>    <span class="hljs-keyword">else</span>:        out = model.fit(x=X_train, y=y_train,                        batch_size=batch_size,                        epochs=epoch,                        validation_data=(X_test, y_test),                        shuffle=<span class="hljs-literal">True</span>)        <span class="hljs-comment"># <span class="hljs-doctag">TODO:</span> 保存模型</span>        model.save(<span class="hljs-string">'CIFAR10_model_no_data_augmentation_%s.h5'</span> % name)    loss, accuracy = model.evaluate(X_train, y_train, verbose=<span class="hljs-number">0</span>)    print(<span class="hljs-string">"Training Accuracy = %.2f %%     loss = %f"</span> % (accuracy * <span class="hljs-number">100</span>, loss))    loss, accuracy = model.evaluate(X_test, y_test, verbose=<span class="hljs-number">0</span>)    print(<span class="hljs-string">"Testing Accuracy = %.2f %%    loss = %f"</span> % (accuracy * <span class="hljs-number">100</span>, loss))    print(<span class="hljs-string">'@ Total Time Spent: %.2f seconds'</span> % (time.time() - start))    <span class="hljs-keyword">return</span> out, epoch, model</code></pre><h3 id="可视化数据："><a href="#可视化数据：" class="headerlink" title="可视化数据："></a><strong>可视化数据</strong>：</h3><pre><code class="hljs python"><span class="hljs-comment">#!/usr/bin/env python</span><span class="hljs-comment"># -*- coding: utf-8 -*-</span><span class="hljs-comment"># @Time    : 2018/9/7 15:00</span><span class="hljs-comment"># @Author  : Seven</span><span class="hljs-comment"># @Site    : </span><span class="hljs-comment"># @File    : visualization.py</span><span class="hljs-comment"># @Software: PyCharm</span><span class="hljs-comment"># <span class="hljs-doctag">TODO:</span> 可视化数据</span><span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt<span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">plot_acc_loss</span><span class="hljs-params">(data, epoch)</span>:</span>    <span class="hljs-string">"""</span><span class="hljs-string">    可视化数据</span><span class="hljs-string">    :param data: 数据</span><span class="hljs-string">    :param epoch: 迭代次数</span><span class="hljs-string">    :return:</span><span class="hljs-string">    """</span>    acc, loss, val_acc, val_loss = data.history[<span class="hljs-string">'acc'</span>], data.history[<span class="hljs-string">'loss'</span>], \                                   data.history[<span class="hljs-string">'val_acc'</span>], data.history[<span class="hljs-string">'val_loss'</span>]    plt.figure(figsize=(<span class="hljs-number">15</span>, <span class="hljs-number">5</span>))    plt.subplot(<span class="hljs-number">121</span>)    plt.plot(range(epoch), acc, label=<span class="hljs-string">'Train'</span>)    plt.plot(range(epoch), val_acc, label=<span class="hljs-string">'Test'</span>)    plt.title(<span class="hljs-string">'Accuracy over '</span> + str(epoch) + <span class="hljs-string">' Epochs'</span>, size=<span class="hljs-number">15</span>)    plt.legend()    plt.grid(<span class="hljs-literal">True</span>)    plt.subplot(<span class="hljs-number">122</span>)    plt.plot(range(epoch), loss, label=<span class="hljs-string">'Train'</span>)    plt.plot(range(epoch), val_loss, label=<span class="hljs-string">'Test'</span>)    plt.title(<span class="hljs-string">'Loss over '</span> + str(epoch) + <span class="hljs-string">' Epochs'</span>, size=<span class="hljs-number">15</span>)    plt.legend()    plt.grid(<span class="hljs-literal">True</span>)    plt.show()<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">plot_image</span><span class="hljs-params">(x_test, y_test, class_name, model)</span>:</span>    rand_id = np.random.choice(range(<span class="hljs-number">10000</span>), size=<span class="hljs-number">10</span>)    X_pred = np.array([x_test[i] <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> rand_id])    y_true = [y_test[i] <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> rand_id]    y_true = np.argmax(y_true, axis=<span class="hljs-number">1</span>)    y_true = [class_name[name] <span class="hljs-keyword">for</span> name <span class="hljs-keyword">in</span> y_true]    y_pred = model.predict(X_pred)    y_pred = np.argmax(y_pred, axis=<span class="hljs-number">1</span>)    y_pred = [class_name[name] <span class="hljs-keyword">for</span> name <span class="hljs-keyword">in</span> y_pred]    plt.figure(figsize=(<span class="hljs-number">15</span>, <span class="hljs-number">7</span>))    <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> range(<span class="hljs-number">10</span>):        plt.subplot(<span class="hljs-number">2</span>, <span class="hljs-number">5</span>, i + <span class="hljs-number">1</span>)        plt.imshow(X_pred[i].reshape(<span class="hljs-number">32</span>, <span class="hljs-number">32</span>, <span class="hljs-number">3</span>), cmap=<span class="hljs-string">'gray'</span>)        plt.title(<span class="hljs-string">'True: %s \n Pred: %s'</span> % (y_true[i], y_pred[i]), size=<span class="hljs-number">15</span>)    plt.show()</code></pre><h3 id="验证本地图片："><a href="#验证本地图片：" class="headerlink" title="验证本地图片："></a><strong>验证本地图片</strong>：</h3><pre><code class="hljs python"><span class="hljs-comment">#!/usr/bin/env python</span><span class="hljs-comment"># -*- coding: utf-8 -*-</span><span class="hljs-comment"># @Time    : 2018/9/7 19:18</span><span class="hljs-comment"># @Author  : Seven</span><span class="hljs-comment"># @Site    : </span><span class="hljs-comment"># @File    : TestModel.py</span><span class="hljs-comment"># @Software: PyCharm</span><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<span class="hljs-keyword">from</span> PIL <span class="hljs-keyword">import</span> Image<span class="hljs-keyword">from</span> keras.models <span class="hljs-keyword">import</span> load_model<span class="hljs-keyword">import</span> Read_data<span class="hljs-keyword">import</span> config<span class="hljs-keyword">import</span> visualization<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">run</span><span class="hljs-params">()</span>:</span>    <span class="hljs-comment"># 加载模型</span>    model = load_model(<span class="hljs-string">'CIFAR10_model_with_data_augmentation_VGG.h5'</span>)    <span class="hljs-comment"># 加载数据</span>    X_train, y_train, X_test, y_test = Read_data.load_data()    visualization.plot_image(X_test, y_test, config.class_name, model)<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">local_photos</span><span class="hljs-params">()</span>:</span>    <span class="hljs-comment"># input</span>    im = Image.open(<span class="hljs-string">'image/dog-1.jpg'</span>)    <span class="hljs-comment"># im.show()</span>    im = im.resize((<span class="hljs-number">32</span>, <span class="hljs-number">32</span>))    <span class="hljs-comment"># print(im.size, im.mode)</span>    im = np.array(im).astype(np.float32)    im = np.reshape(im, [<span class="hljs-number">-1</span>, <span class="hljs-number">32</span>*<span class="hljs-number">32</span>*<span class="hljs-number">3</span>])    im = (im - (<span class="hljs-number">255</span> / <span class="hljs-number">2.0</span>)) / <span class="hljs-number">255</span>    batch_xs = np.reshape(im, [<span class="hljs-number">-1</span>, <span class="hljs-number">32</span>, <span class="hljs-number">32</span>, <span class="hljs-number">3</span>])    model = load_model(<span class="hljs-string">'CIFAR10_model_with_data_augmentation_VGG.h5'</span>)    output = model.predict(batch_xs)    print(output)    print(<span class="hljs-string">'the out put is :'</span>, config.class_name[np.argmax(output)])<span class="hljs-keyword">if</span> __name__ == <span class="hljs-string">'__main__'</span>:    local_photos()</code></pre><h3 id="输出："><a href="#输出：" class="headerlink" title="输出："></a><strong>输出：</strong></h3><pre><code class="hljs python">[[<span class="hljs-number">0.</span> <span class="hljs-number">0.</span> <span class="hljs-number">0.</span> <span class="hljs-number">0.</span> <span class="hljs-number">0.</span> <span class="hljs-number">1.</span> <span class="hljs-number">0.</span> <span class="hljs-number">0.</span> <span class="hljs-number">0.</span> <span class="hljs-number">0.</span>]]the out put <span class="hljs-keyword">is</span> : dog</code></pre>]]></content>
    
    
    <categories>
      
      <category>Keras</category>
      
    </categories>
    
    
    <tags>
      
      <tag>深度学习</tag>
      
      <tag>Keras</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>神经网络之深度卷积生成对抗网络</title>
    <link href="/2018/09/DL-DCGAN/"/>
    <url>/2018/09/DL-DCGAN/</url>
    
    <content type="html"><![CDATA[<h3 id="深度卷积生成对抗网络"><a href="#深度卷积生成对抗网络" class="headerlink" title="深度卷积生成对抗网络"></a>深度卷积生成对抗网络</h3><p>深度卷积生成对抗网络（Deep Convolutional Generative Adversarial Networks, DCGANs）由Alec Radford等于2015年提出。在了解<a href="https://sevenold.github.io/2018/09/DL-GAN/" target="_blank" rel="noopener">GAN</a>的架构后，我们可以较为容易地了解DCGAN的原理。</p><h3 id="稳定的DCGAN架构指南"><a href="#稳定的DCGAN架构指南" class="headerlink" title="稳定的DCGAN架构指南"></a>稳定的DCGAN架构指南</h3><ol><li>将<strong>pooling layer</strong>替换成<strong>convolutions layer</strong><ol><li>对于<strong>生成模型</strong>：允许网络学习自己的<strong>空间下采样</strong>。<br> . 对于<strong>判别模型</strong>：允许网络学习自己的<strong>空间上采样</strong>。    </li></ol></li><li>除了<strong>生成器的输出层</strong>和<strong>判别器的输入层</strong>之外，使用<strong>批量标准化–batchnorm</strong>。<ol><li>解决初始化差的问题。</li><li>帮助梯度传播到每一层。</li><li>防止<strong>生成器</strong>把所有的样本都收敛到同一个点。</li></ol></li><li>在CNN中移除<strong>全连接层</strong>。</li><li>在<strong>生成器</strong>的除了<strong>输出层</strong>外的所有层使用<strong>ReLU</strong>，<strong>输出层</strong>采用<strong>tanh</strong>。</li><li>在<strong>判别器</strong>的所有层中使用<strong>LeakyReLU</strong>。</li></ol><h3 id="DCGAN生成器"><a href="#DCGAN生成器" class="headerlink" title="DCGAN生成器"></a>DCGAN生成器</h3><p>在DCGAN中，生成式模型$G(z)$使用一个比较特殊的深度卷积网络来实现，如下图所示：</p><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20200601182547.png" srcset="/img/loading.gif" alt="images"></p><h3 id="DCGAN判别器"><a href="#DCGAN判别器" class="headerlink" title="DCGAN判别器"></a>DCGAN判别器</h3><p>而判别式模型$D(x)$则仍是一个传统的深度卷积网络，如下图所示:</p><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20200601182548.gif" srcset="/img/loading.gif" alt="images"></p><h3 id="反卷积"><a href="#反卷积" class="headerlink" title="反卷积"></a>反卷积</h3><p>从前面两幅图中可以看出，DCGAN的生成式模型$G(z)$中出现了上采样（upsampling）。</p><p>卷积神经网络的下采样很好理解，加入polling层即可，然而这里的上采样要如何实现呢？</p><p>这里，DCGAN通过“微步幅卷积”（fractionally-strided convolution）进行上采样。</p><p>假设有一个3×3的输入，希望输出的尺寸比这要大，那么可以把这个3×3的输入通过在像素之间插入0的方式来进行扩展，如下图所示。当扩展到7×7的尺寸后，再进行卷积，就可以得到尺寸比原来大的输出。</p><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20200601182549.png" srcset="/img/loading.gif" alt="images"></p><h3 id="DCGAN的特点："><a href="#DCGAN的特点：" class="headerlink" title="DCGAN的特点："></a>DCGAN的特点：</h3><ul><li>判别模型：使用带步长的卷积（strided convolutions）取代了的空间池化（spatial pooling），容许网络学习自己的空间下采样（spatial downsampling）。</li><li>生成模型：使用微步幅卷积（fractional strided），容许它学习自己的空间上采样（spatial upsampling）。</li><li>激活函数： LeakyReLU</li><li>Batch Normalization 批标准化：解决因糟糕的初始化引起的训练问题，使得梯度能传播更深层次。 Batch Normalization证明了生成模型初始化的重要性，避免生成模型崩溃：生成的所有样本都在一个点上（样本相同），这是训练GANs经常遇到的失败现象。</li></ul><h3 id="DCGAN调优技巧"><a href="#DCGAN调优技巧" class="headerlink" title="DCGAN调优技巧"></a>DCGAN调优技巧</h3><ul><li><p>所有模型均采用小批量随机梯度下降（SGD）进行训练，最小批量为128。</p></li><li><p>所有权重均从零中心正态分布初始化，标准偏差为0.02。</p></li><li><p>在LeakyReLU中，所有model的leak斜率均设为0.2。</p></li><li><p>虽然之前的GAN工作已经使用了<strong>momentum</strong>来加速训练，但我们使用了<strong>Adam</strong>优化器和调整的超参数。</p></li><li><p>建议的学习率为0.001，过高，使用0.0002代替。</p></li><li><p>建议将<strong>momentum</strong>项$β_1$保持在0.9，会导致训练振荡和不稳定，所以将其降低到0.5有助于稳定训练。</p></li></ul>]]></content>
    
    
    <categories>
      
      <category>深度学习</category>
      
    </categories>
    
    
    <tags>
      
      <tag>深度学习</tag>
      
      <tag>神经网络</tag>
      
      <tag>深度卷积生成对抗网络</tag>
      
      <tag>DCGAN</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>TensorFlow实现深度卷积生成对抗网络-DCGAN</title>
    <link href="/2018/09/TensorFlow-DCGAN/"/>
    <url>/2018/09/TensorFlow-DCGAN/</url>
    
    <content type="html"><![CDATA[<h4 id="代码："><a href="#代码：" class="headerlink" title="代码："></a><strong>代码</strong>：</h4><pre><code class="hljs python"><span class="hljs-comment">#!/usr/bin/env python</span><span class="hljs-comment"># -*- coding: utf-8 -*-</span><span class="hljs-comment"># @Time    : 2018/9/4 16:29</span><span class="hljs-comment"># @Author  : Seven</span><span class="hljs-comment"># @Site    : </span><span class="hljs-comment"># @File    : DCGAN.py</span><span class="hljs-comment"># @Software: PyCharm</span><span class="hljs-comment"># <span class="hljs-doctag">TODO:</span> 导入环境</span><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<span class="hljs-keyword">import</span> tensorflow <span class="hljs-keyword">as</span> tf<span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt<span class="hljs-keyword">from</span> tensorflow.examples.tutorials.mnist <span class="hljs-keyword">import</span> input_data<span class="hljs-comment"># TODO：数据准备</span>mnist = input_data.read_data_sets(<span class="hljs-string">'data'</span>)<span class="hljs-comment"># <span class="hljs-doctag">TODO:</span>获得输入数据</span><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">get_inputs</span><span class="hljs-params">(noise_dim, image_height, image_width, image_depth)</span>:</span>    <span class="hljs-comment"># 真实数据</span>    inputs_real = tf.placeholder(tf.float32, [<span class="hljs-literal">None</span>, image_height, image_width, image_depth], name=<span class="hljs-string">'inputs_real'</span>)    <span class="hljs-comment"># 噪声数据</span>    inputs_noise = tf.placeholder(tf.float32, [<span class="hljs-literal">None</span>, noise_dim], name=<span class="hljs-string">'inputs_noise'</span>)    <span class="hljs-keyword">return</span> inputs_real, inputs_noise<span class="hljs-comment"># <span class="hljs-doctag">TODO:</span>生成器</span><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">get_generator</span><span class="hljs-params">(noise_img, output_dim, is_train=True, alpha=<span class="hljs-number">0.01</span>)</span>:</span>    <span class="hljs-keyword">with</span> tf.variable_scope(<span class="hljs-string">"generator"</span>, reuse=(<span class="hljs-keyword">not</span> is_train)):        <span class="hljs-comment"># 100 x 1 to 4 x 4 x 512</span>        <span class="hljs-comment"># 全连接层</span>        layer1 = tf.layers.dense(noise_img, <span class="hljs-number">4</span> * <span class="hljs-number">4</span> * <span class="hljs-number">512</span>)        layer1 = tf.reshape(layer1, [<span class="hljs-number">-1</span>, <span class="hljs-number">4</span>, <span class="hljs-number">4</span>, <span class="hljs-number">512</span>])        <span class="hljs-comment"># batch normalization</span>        layer1 = tf.layers.batch_normalization(layer1, training=is_train)        <span class="hljs-comment"># Leaky ReLU</span>        layer1 = tf.maximum(alpha * layer1, layer1)        <span class="hljs-comment"># dropout</span>        layer1 = tf.nn.dropout(layer1, keep_prob=<span class="hljs-number">0.8</span>)        <span class="hljs-comment"># 4 x 4 x 512 to 7 x 7 x 256</span>        layer2 = tf.layers.conv2d_transpose(layer1, <span class="hljs-number">256</span>, <span class="hljs-number">4</span>, strides=<span class="hljs-number">1</span>, padding=<span class="hljs-string">'valid'</span>)        layer2 = tf.layers.batch_normalization(layer2, training=is_train)        layer2 = tf.maximum(alpha * layer2, layer2)        layer2 = tf.nn.dropout(layer2, keep_prob=<span class="hljs-number">0.8</span>)        <span class="hljs-comment"># 7 x 7 256 to 14 x 14 x 128</span>        layer3 = tf.layers.conv2d_transpose(layer2, <span class="hljs-number">128</span>, <span class="hljs-number">3</span>, strides=<span class="hljs-number">2</span>, padding=<span class="hljs-string">'same'</span>)        layer3 = tf.layers.batch_normalization(layer3, training=is_train)        layer3 = tf.maximum(alpha * layer3, layer3)        layer3 = tf.nn.dropout(layer3, keep_prob=<span class="hljs-number">0.8</span>)        <span class="hljs-comment"># 14 x 14 x 128 to 28 x 28 x 1</span>        logits = tf.layers.conv2d_transpose(layer3, output_dim, <span class="hljs-number">3</span>, strides=<span class="hljs-number">2</span>, padding=<span class="hljs-string">'same'</span>)        <span class="hljs-comment"># MNIST原始数据集的像素范围在0-1，这里的生成图片范围为(-1,1)</span>        <span class="hljs-comment"># 因此在训练时，记住要把MNIST像素范围进行resize</span>        outputs = tf.tanh(logits)        <span class="hljs-keyword">return</span> outputs<span class="hljs-comment"># <span class="hljs-doctag">TODO:</span>判别器</span><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">get_discriminator</span><span class="hljs-params">(inputs_img, reuse=False, alpha=<span class="hljs-number">0.01</span>)</span>:</span>    <span class="hljs-keyword">with</span> tf.variable_scope(<span class="hljs-string">"discriminator"</span>, reuse=reuse):        <span class="hljs-comment"># 28 x 28 x 1 to 14 x 14 x 128</span>        <span class="hljs-comment"># 第一层不加入BN</span>        layer1 = tf.layers.conv2d(inputs_img, <span class="hljs-number">128</span>, <span class="hljs-number">3</span>, strides=<span class="hljs-number">2</span>, padding=<span class="hljs-string">'same'</span>)        layer1 = tf.maximum(alpha * layer1, layer1)        layer1 = tf.nn.dropout(layer1, keep_prob=<span class="hljs-number">0.8</span>)        <span class="hljs-comment"># 14 x 14 x 128 to 7 x 7 x 256</span>        layer2 = tf.layers.conv2d(layer1, <span class="hljs-number">256</span>, <span class="hljs-number">3</span>, strides=<span class="hljs-number">2</span>, padding=<span class="hljs-string">'same'</span>)        layer2 = tf.layers.batch_normalization(layer2, training=<span class="hljs-literal">True</span>)        layer2 = tf.maximum(alpha * layer2, layer2)        layer2 = tf.nn.dropout(layer2, keep_prob=<span class="hljs-number">0.8</span>)        <span class="hljs-comment"># 7 x 7 x 256 to 4 x 4 x 512</span>        layer3 = tf.layers.conv2d(layer2, <span class="hljs-number">512</span>, <span class="hljs-number">3</span>, strides=<span class="hljs-number">2</span>, padding=<span class="hljs-string">'same'</span>)        layer3 = tf.layers.batch_normalization(layer3, training=<span class="hljs-literal">True</span>)        layer3 = tf.maximum(alpha * layer3, layer3)        layer3 = tf.nn.dropout(layer3, keep_prob=<span class="hljs-number">0.8</span>)        <span class="hljs-comment"># 4 x 4 x 512 to 4*4*512 x 1</span>        flatten = tf.reshape(layer3, (<span class="hljs-number">-1</span>, <span class="hljs-number">4</span> * <span class="hljs-number">4</span> * <span class="hljs-number">512</span>))        logits = tf.layers.dense(flatten, <span class="hljs-number">1</span>)        outputs = tf.sigmoid(logits)        <span class="hljs-keyword">return</span> logits, outputs<span class="hljs-comment"># <span class="hljs-doctag">TODO:</span> 目标函数</span><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">get_loss</span><span class="hljs-params">(inputs_real, inputs_noise, image_depth, smooth=<span class="hljs-number">0.1</span>)</span>:</span>    g_outputs = get_generator(inputs_noise, image_depth, is_train=<span class="hljs-literal">True</span>)    d_logits_real, d_outputs_real = get_discriminator(inputs_real)    d_logits_fake, d_outputs_fake = get_discriminator(g_outputs, reuse=<span class="hljs-literal">True</span>)    <span class="hljs-comment"># 计算Loss</span>    g_loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=d_logits_fake,                                                                    labels=tf.ones_like(d_outputs_fake) * (<span class="hljs-number">1</span> - smooth)))    d_loss_real = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=d_logits_real,                                                                         labels=tf.ones_like(d_outputs_real) * (                                                                                     <span class="hljs-number">1</span> - smooth)))    d_loss_fake = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=d_logits_fake,                                                                         labels=tf.zeros_like(d_outputs_fake)))    d_loss = tf.add(d_loss_real, d_loss_fake)    <span class="hljs-keyword">return</span> g_loss, d_loss<span class="hljs-comment"># <span class="hljs-doctag">TODO:</span>优化器</span><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">get_optimizer</span><span class="hljs-params">(g_loss, d_loss, learning_rate=<span class="hljs-number">0.001</span>)</span>:</span>    train_vars = tf.trainable_variables()    g_vars = [var <span class="hljs-keyword">for</span> var <span class="hljs-keyword">in</span> train_vars <span class="hljs-keyword">if</span> var.name.startswith(<span class="hljs-string">"generator"</span>)]    d_vars = [var <span class="hljs-keyword">for</span> var <span class="hljs-keyword">in</span> train_vars <span class="hljs-keyword">if</span> var.name.startswith(<span class="hljs-string">"discriminator"</span>)]    <span class="hljs-comment"># Optimizer</span>    <span class="hljs-keyword">with</span> tf.control_dependencies(tf.get_collection(tf.GraphKeys.UPDATE_OPS)):        g_opt = tf.train.AdamOptimizer(learning_rate).minimize(g_loss, var_list=g_vars)        d_opt = tf.train.AdamOptimizer(learning_rate).minimize(d_loss, var_list=d_vars)    <span class="hljs-keyword">return</span> g_opt, d_opt<span class="hljs-comment"># 显示图片</span><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">plot_images</span><span class="hljs-params">(samples)</span>:</span>    fig, axes = plt.subplots(nrows=<span class="hljs-number">5</span>, ncols=<span class="hljs-number">5</span>, sharex=<span class="hljs-literal">True</span>, sharey=<span class="hljs-literal">True</span>, figsize=(<span class="hljs-number">7</span>, <span class="hljs-number">7</span>))    <span class="hljs-keyword">for</span> img, ax <span class="hljs-keyword">in</span> zip(samples, axes.flatten()):        ax.imshow(img.reshape((<span class="hljs-number">28</span>, <span class="hljs-number">28</span>)), cmap=<span class="hljs-string">'Greys_r'</span>)        ax.get_xaxis().set_visible(<span class="hljs-literal">False</span>)        ax.get_yaxis().set_visible(<span class="hljs-literal">False</span>)    fig.tight_layout(pad=<span class="hljs-number">0</span>)    plt.show()<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">show_generator_output</span><span class="hljs-params">(sess, n_images, inputs_noise, output_dim)</span>:</span>    noise_shape = inputs_noise.get_shape().as_list()[<span class="hljs-number">-1</span>]    <span class="hljs-comment"># 生成噪声图片</span>    examples_noise = np.random.uniform(<span class="hljs-number">-1</span>, <span class="hljs-number">1</span>, size=[n_images, noise_shape])    samples = sess.run(get_generator(inputs_noise, output_dim, <span class="hljs-literal">False</span>),                       feed_dict=&#123;inputs_noise: examples_noise&#125;)    result = np.squeeze(samples, <span class="hljs-number">-1</span>)    <span class="hljs-keyword">return</span> result<span class="hljs-comment"># <span class="hljs-doctag">TODO:</span>开始训练</span><span class="hljs-comment"># 定义参数</span>batch_size = <span class="hljs-number">64</span>noise_size = <span class="hljs-number">100</span>epochs = <span class="hljs-number">5</span>n_samples = <span class="hljs-number">25</span>learning_rate = <span class="hljs-number">0.001</span><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">train</span><span class="hljs-params">(noise_size, data_shape, batch_size, n_samples)</span>:</span>    <span class="hljs-comment"># 存储loss</span>    losses = []    steps = <span class="hljs-number">0</span>    inputs_real, inputs_noise = get_inputs(noise_size, data_shape[<span class="hljs-number">1</span>], data_shape[<span class="hljs-number">2</span>], data_shape[<span class="hljs-number">3</span>])    g_loss, d_loss = get_loss(inputs_real, inputs_noise, data_shape[<span class="hljs-number">-1</span>])    print(<span class="hljs-string">"FUNCTION READY!!"</span>)    g_train_opt, d_train_opt = get_optimizer(g_loss, d_loss, learning_rate)    print(<span class="hljs-string">"TRAINING...."</span>)    <span class="hljs-keyword">with</span> tf.Session() <span class="hljs-keyword">as</span> sess:        sess.run(tf.global_variables_initializer())        <span class="hljs-comment"># 迭代epoch</span>        <span class="hljs-keyword">for</span> e <span class="hljs-keyword">in</span> range(epochs):            <span class="hljs-keyword">for</span> batch_i <span class="hljs-keyword">in</span> range(mnist.train.num_examples // batch_size):                steps += <span class="hljs-number">1</span>                batch = mnist.train.next_batch(batch_size)                batch_images = batch[<span class="hljs-number">0</span>].reshape((batch_size, data_shape[<span class="hljs-number">1</span>], data_shape[<span class="hljs-number">2</span>], data_shape[<span class="hljs-number">3</span>]))                <span class="hljs-comment"># scale to -1, 1</span>                batch_images = batch_images * <span class="hljs-number">2</span> - <span class="hljs-number">1</span>                <span class="hljs-comment"># noise</span>                batch_noise = np.random.uniform(<span class="hljs-number">-1</span>, <span class="hljs-number">1</span>, size=(batch_size, noise_size))                <span class="hljs-comment"># run optimizer</span>                sess.run(g_train_opt, feed_dict=&#123;inputs_real: batch_images,                                                 inputs_noise: batch_noise&#125;)                sess.run(d_train_opt, feed_dict=&#123;inputs_real: batch_images,                                                 inputs_noise: batch_noise&#125;)                <span class="hljs-keyword">if</span> steps % <span class="hljs-number">101</span> == <span class="hljs-number">0</span>:                    train_loss_d = d_loss.eval(&#123;inputs_real: batch_images,                                                inputs_noise: batch_noise&#125;)                    train_loss_g = g_loss.eval(&#123;inputs_real: batch_images,                                                inputs_noise: batch_noise&#125;)                    losses.append((train_loss_d, train_loss_g))                    print(<span class="hljs-string">"Epoch &#123;&#125;/&#123;&#125;...."</span>.format(e + <span class="hljs-number">1</span>, epochs),                          <span class="hljs-string">"Discriminator Loss: &#123;:.4f&#125;...."</span>.format(train_loss_d),                          <span class="hljs-string">"Generator Loss: &#123;:.4f&#125;...."</span>.format(train_loss_g))            <span class="hljs-keyword">if</span> e % <span class="hljs-number">1</span> == <span class="hljs-number">0</span>:                <span class="hljs-comment"># 显示图片</span>                samples = show_generator_output(sess, n_samples, inputs_noise, data_shape[<span class="hljs-number">-1</span>])                plot_images(samples)<span class="hljs-keyword">with</span> tf.Graph().as_default():    train(noise_size, [<span class="hljs-number">-1</span>, <span class="hljs-number">28</span>, <span class="hljs-number">28</span>, <span class="hljs-number">1</span>], batch_size, n_samples)    print(<span class="hljs-string">"OPTIMIZER END!!"</span>)</code></pre><h4 id="执行结果："><a href="#执行结果：" class="headerlink" title="执行结果："></a><strong>执行结果</strong>：</h4><pre><code class="hljs bash">Epoch 4/5.... Discriminator Loss: 0.4584.... Generator Loss: 4.8776....Epoch 4/5.... Discriminator Loss: 0.4315.... Generator Loss: 2.9278....Epoch 4/5.... Discriminator Loss: 0.5317.... Generator Loss: 3.4315....Epoch 4/5.... Discriminator Loss: 0.6342.... Generator Loss: 2.7376....Epoch 4/5.... Discriminator Loss: 0.4312.... Generator Loss: 3.9016....Epoch 4/5.... Discriminator Loss: 0.5498.... Generator Loss: 2.3418....Epoch 4/5.... Discriminator Loss: 0.4807.... Generator Loss: 3.4065....Epoch 4/5.... Discriminator Loss: 0.5360.... Generator Loss: 2.2658....Epoch 4/5.... Discriminator Loss: 0.4612.... Generator Loss: 2.8166....Epoch 5/5.... Discriminator Loss: 0.4784.... Generator Loss: 2.9896....Epoch 5/5.... Discriminator Loss: 0.7368.... Generator Loss: 2.1130....Epoch 5/5.... Discriminator Loss: 0.4192.... Generator Loss: 3.9733....Epoch 5/5.... Discriminator Loss: 0.4998.... Generator Loss: 2.1577....Epoch 5/5.... Discriminator Loss: 0.4693.... Generator Loss: 3.1395....Epoch 5/5.... Discriminator Loss: 0.3946.... Generator Loss: 4.0385....Epoch 5/5.... Discriminator Loss: 0.4807.... Generator Loss: 3.8975....Epoch 5/5.... Discriminator Loss: 0.4703.... Generator Loss: 3.6105....OPTIMIZER END!!</code></pre><h4 id="生成的图像："><a href="#生成的图像：" class="headerlink" title="生成的图像："></a><strong>生成的图像</strong>：</h4><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20200601182604.png" srcset="/img/loading.gif" alt="images"></p>]]></content>
    
    
    <categories>
      
      <category>TensorFlow</category>
      
    </categories>
    
    
    <tags>
      
      <tag>深度学习</tag>
      
      <tag>神经网络</tag>
      
      <tag>深度卷积生成对抗网络</tag>
      
      <tag>DCGAN</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>TensorFlow--保存神经网络参数和加载神经网络参数</title>
    <link href="/2018/09/TensorFlow-save-restore/"/>
    <url>/2018/09/TensorFlow-save-restore/</url>
    
    <content type="html"><![CDATA[<h3 id="保存神经网络参数"><a href="#保存神经网络参数" class="headerlink" title="保存神经网络参数"></a>保存神经网络参数</h3><h4 id="代码："><a href="#代码：" class="headerlink" title="代码："></a><strong>代码</strong>：</h4><pre><code class="hljs python"><span class="hljs-comment">#!/usr/bin/env python</span><span class="hljs-comment"># -*- coding: utf-8 -*-</span><span class="hljs-comment"># @Time    : 2018/9/4 16:11</span><span class="hljs-comment"># @Author  : Seven</span><span class="hljs-comment"># @Site    : </span><span class="hljs-comment"># @File    : save.py</span><span class="hljs-comment"># @Software: PyCharm</span><span class="hljs-keyword">import</span> tensorflow <span class="hljs-keyword">as</span> tf<span class="hljs-comment"># 保存神经网络参数</span><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">save_para</span><span class="hljs-params">()</span>:</span>    <span class="hljs-comment"># 定义权重参数</span>    W = tf.Variable([[<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>], [<span class="hljs-number">4</span>, <span class="hljs-number">5</span>, <span class="hljs-number">6</span>]], dtype = tf.float32, name = <span class="hljs-string">'weights'</span>)    <span class="hljs-comment"># 定义偏置参数</span>    b = tf.Variable([[<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>]], dtype = tf.float32, name = <span class="hljs-string">'biases'</span>)    <span class="hljs-comment"># 参数初始化</span>    init = tf.global_variables_initializer()    <span class="hljs-comment"># 定义保存参数的saver</span>    saver = tf.train.Saver()    <span class="hljs-keyword">with</span> tf.Session() <span class="hljs-keyword">as</span> sess:        sess.run(init)        <span class="hljs-comment"># 保存session中的数据</span>        save_path = saver.save(sess, <span class="hljs-string">'./save_net.ckpt'</span>)        <span class="hljs-comment"># 输出保存路径</span>        print(<span class="hljs-string">'Save to path: '</span>, save_path)save_para()</code></pre><h4 id="执行结果："><a href="#执行结果：" class="headerlink" title="执行结果："></a><strong>执行结果</strong>：</h4><pre><code class="hljs bash">Save to path:  ./save_net.ckpt</code></pre><h3 id="加载神经网络参数"><a href="#加载神经网络参数" class="headerlink" title="加载神经网络参数"></a>加载神经网络参数</h3><h4 id="代码：-1"><a href="#代码：-1" class="headerlink" title="代码："></a><strong>代码</strong>：</h4><pre><code class="hljs python"><span class="hljs-comment">#!/usr/bin/env python</span><span class="hljs-comment"># -*- coding: utf-8 -*-</span><span class="hljs-comment"># @Time    : 2018/9/4 16:14</span><span class="hljs-comment"># @Author  : Seven</span><span class="hljs-comment"># @Site    : </span><span class="hljs-comment"># @File    : restore.py</span><span class="hljs-comment"># @Software: PyCharm</span><span class="hljs-keyword">import</span> tensorflow <span class="hljs-keyword">as</span> tf<span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<span class="hljs-comment"># 恢复神经网络参数</span><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">restore_para</span><span class="hljs-params">()</span>:</span>    <span class="hljs-comment"># 定义权重参数</span>    W = tf.Variable(np.arange(<span class="hljs-number">6</span>).reshape((<span class="hljs-number">2</span>, <span class="hljs-number">3</span>)), dtype = tf.float32, name = <span class="hljs-string">'weights'</span>)    <span class="hljs-comment"># 定义偏置参数</span>    b = tf.Variable(np.arange(<span class="hljs-number">3</span>).reshape((<span class="hljs-number">1</span>, <span class="hljs-number">3</span>)), dtype = tf.float32, name = <span class="hljs-string">'biases'</span>)    <span class="hljs-comment"># 定义提取参数的saver</span>    saver = tf.train.Saver()    <span class="hljs-keyword">with</span> tf.Session() <span class="hljs-keyword">as</span> sess:        <span class="hljs-comment"># 加载文件中的参数数据，会根据name加载数据并保存到变量W和b中</span>        save_path = saver.restore(sess, <span class="hljs-string">'save_net.ckpt'</span>)        <span class="hljs-comment"># 输出保存路径</span>        print(<span class="hljs-string">'Weights: '</span>, sess.run(W))        print(<span class="hljs-string">'biases:  '</span>, sess.run(b))restore_para()</code></pre><h4 id="执行结果：-1"><a href="#执行结果：-1" class="headerlink" title="执行结果："></a><strong>执行结果</strong>：</h4><pre><code class="hljs bash">Weights:  [[1. 2. 3.] [4. 5. 6.]]biases:   [[1. 2. 3.]]</code></pre>]]></content>
    
    
    <categories>
      
      <category>TensorFlow</category>
      
    </categories>
    
    
    <tags>
      
      <tag>深度学习</tag>
      
      <tag>TensorFlow</tag>
      
      <tag>模型</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>TensorFlow实现简单的生成对抗网络-GAN</title>
    <link href="/2018/09/TensorFlow-GAN/"/>
    <url>/2018/09/TensorFlow-GAN/</url>
    
    <content type="html"><![CDATA[<h4 id="示例代码："><a href="#示例代码：" class="headerlink" title="示例代码："></a><strong>示例代码</strong>：</h4><pre><code class="hljs python"><span class="hljs-comment">#!/usr/bin/env python</span><span class="hljs-comment"># -*- coding: utf-8 -*-</span><span class="hljs-comment"># @Time    : 2018/9/1 16:38</span><span class="hljs-comment"># @Author  : Seven</span><span class="hljs-comment"># @Site    : </span><span class="hljs-comment"># @File    : GAN.py</span><span class="hljs-comment"># @Software: PyCharm</span><span class="hljs-comment"># <span class="hljs-doctag">TODO:</span> 0.导入环境</span><span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt<span class="hljs-keyword">import</span> tensorflow <span class="hljs-keyword">as</span> tf<span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<span class="hljs-keyword">from</span> tensorflow.examples.tutorials.mnist <span class="hljs-keyword">import</span> input_data<span class="hljs-comment"># <span class="hljs-doctag">TODO:</span> 1：读取数据</span>mnist = input_data.read_data_sets(<span class="hljs-string">'data'</span>)<span class="hljs-comment"># <span class="hljs-doctag">TODO:</span> 2：初始化参数</span>img_size = mnist.train.images[<span class="hljs-number">0</span>].shape[<span class="hljs-number">0</span>]noise_size = <span class="hljs-number">100</span>g_units = <span class="hljs-number">128</span>d_units = <span class="hljs-number">128</span>learning_rate = <span class="hljs-number">0.001</span>alpha = <span class="hljs-number">0.01</span><span class="hljs-comment"># 真实数据和噪音数据的placeholder</span>real_img = tf.placeholder(tf.float32, [<span class="hljs-literal">None</span>, img_size])noise_img = tf.placeholder(tf.float32, [<span class="hljs-literal">None</span>, noise_size])<span class="hljs-comment"># 显示生成的图像</span><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">view_samples</span><span class="hljs-params">(epoch, samples)</span>:</span>    fig, axes = plt.subplots(figsize=(<span class="hljs-number">7</span>, <span class="hljs-number">7</span>), nrows=<span class="hljs-number">5</span>, ncols=<span class="hljs-number">5</span>, sharey=<span class="hljs-literal">True</span>, sharex=<span class="hljs-literal">True</span>)    <span class="hljs-keyword">for</span> ax, img <span class="hljs-keyword">in</span> zip(axes.flatten(), samples[epoch][<span class="hljs-number">1</span>]):  <span class="hljs-comment"># 这里samples[epoch][1]代表生成的图像结果，而[0]代表对应的logits</span>        ax.xaxis.set_visible(<span class="hljs-literal">False</span>)        ax.yaxis.set_visible(<span class="hljs-literal">False</span>)        ax.imshow(img.reshape((<span class="hljs-number">28</span>, <span class="hljs-number">28</span>)), cmap=<span class="hljs-string">'Greys_r'</span>)    plt.show()    <span class="hljs-keyword">return</span> fig, axes<span class="hljs-comment"># <span class="hljs-doctag">TODO:</span> 4.生成器</span><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">get_generator</span><span class="hljs-params">(noise_img, n_units, out_dim, reuse=False, alpha=<span class="hljs-number">0.01</span>)</span>:</span>    <span class="hljs-keyword">with</span> tf.variable_scope(<span class="hljs-string">"generator"</span>, reuse=reuse):        <span class="hljs-comment"># hidden layer</span>        hidden1 = tf.layers.dense(noise_img, n_units)        <span class="hljs-comment"># leaky ReLU</span>        hidden1 = tf.maximum(alpha * hidden1, hidden1)        <span class="hljs-comment"># dropout</span>        hidden1 = tf.layers.dropout(hidden1, rate=<span class="hljs-number">0.2</span>)        <span class="hljs-comment"># logits &amp; outputs</span>        logits = tf.layers.dense(hidden1, out_dim)        outputs = tf.tanh(logits)        <span class="hljs-keyword">return</span> logits, outputs<span class="hljs-comment"># 生成器生成数据</span>g_logits, g_outputs = get_generator(noise_img, g_units, img_size)<span class="hljs-comment"># <span class="hljs-doctag">TODO:</span> 5.判别器</span><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">get_discriminator</span><span class="hljs-params">(img, n_units, reuse=False, alpha=<span class="hljs-number">0.01</span>)</span>:</span>    <span class="hljs-keyword">with</span> tf.variable_scope(<span class="hljs-string">"discriminator"</span>, reuse=reuse):        <span class="hljs-comment"># hidden layer</span>        hidden1 = tf.layers.dense(img, n_units)        hidden1 = tf.maximum(alpha * hidden1, hidden1)        <span class="hljs-comment"># logits &amp; outputs</span>        logits = tf.layers.dense(hidden1, <span class="hljs-number">1</span>)        outputs = tf.sigmoid(logits)        <span class="hljs-keyword">return</span> logits, outputs<span class="hljs-comment"># 判别真实的数据</span>d_logits_real, d_outputs_real = get_discriminator(real_img, d_units)<span class="hljs-comment"># 判别生成器生成的数据</span>d_logits_fake, d_outputs_fake = get_discriminator(g_outputs, d_units, reuse=<span class="hljs-literal">True</span>)<span class="hljs-comment"># <span class="hljs-doctag">TODO:</span> 6.损失值的计算</span><span class="hljs-comment"># 判别器的损失值</span><span class="hljs-comment"># 识别真实图片的损失值</span>d_loss_real = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=d_logits_real,                                                                     labels=tf.ones_like(d_logits_real)))<span class="hljs-comment"># 识别生成的图片的损失值</span>d_loss_fake = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=d_logits_fake,                                                                     labels=tf.zeros_like(d_logits_fake)))<span class="hljs-comment"># 总体loss</span>d_loss = tf.add(d_loss_real, d_loss_fake)<span class="hljs-comment"># generator的loss</span>g_loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=d_logits_fake,                                                                labels=tf.ones_like(d_logits_fake)))<span class="hljs-comment"># <span class="hljs-doctag">TODO:</span>7.始化optimizer</span>train_vars = tf.trainable_variables()<span class="hljs-comment"># generator</span>g_vars = [var <span class="hljs-keyword">for</span> var <span class="hljs-keyword">in</span> train_vars <span class="hljs-keyword">if</span> var.name.startswith(<span class="hljs-string">"generator"</span>)]<span class="hljs-comment"># discriminator</span>d_vars = [var <span class="hljs-keyword">for</span> var <span class="hljs-keyword">in</span> train_vars <span class="hljs-keyword">if</span> var.name.startswith(<span class="hljs-string">"discriminator"</span>)]<span class="hljs-comment"># optimizer</span>d_train_opt = tf.train.AdamOptimizer(learning_rate).minimize(d_loss, var_list=d_vars)g_train_opt = tf.train.AdamOptimizer(learning_rate).minimize(g_loss, var_list=g_vars)print(<span class="hljs-string">"FUNCTION READY!!!"</span>)print(<span class="hljs-string">"TRAINING....."</span>)<span class="hljs-comment"># <span class="hljs-doctag">TODO:</span>8.开始训练</span>batch_size = <span class="hljs-number">64</span><span class="hljs-comment"># 训练迭代轮数</span>epochs = <span class="hljs-number">300</span><span class="hljs-comment"># 抽取样本数</span>n_sample = <span class="hljs-number">25</span><span class="hljs-comment"># 存储测试样例</span>samples = []<span class="hljs-comment"># 存储loss</span>losses = []<span class="hljs-comment"># 初始化所有变量</span>init = tf.global_variables_initializer()show_imgs = []<span class="hljs-keyword">with</span> tf.Session() <span class="hljs-keyword">as</span> sess:    sess.run(init)    <span class="hljs-keyword">for</span> epoch <span class="hljs-keyword">in</span> range(epochs):        <span class="hljs-keyword">for</span> batch_i <span class="hljs-keyword">in</span> range(mnist.train.num_examples // batch_size):            batch = mnist.train.next_batch(batch_size)            batch_images = batch[<span class="hljs-number">0</span>].reshape((batch_size, <span class="hljs-number">784</span>))            <span class="hljs-comment"># 对图像像素进行缩放，这是因为tanh输出的结果介于(-1,1),real和fake图片共享discriminator的参数</span>            batch_images = batch_images * <span class="hljs-number">2</span> - <span class="hljs-number">1</span>            <span class="hljs-comment"># generator的输入噪声</span>            batch_noise = np.random.uniform(<span class="hljs-number">-1</span>, <span class="hljs-number">1</span>, size=(batch_size, noise_size))            <span class="hljs-comment"># Run optimizers</span>            sess.run(d_train_opt, feed_dict=&#123;real_img: batch_images, noise_img: batch_noise&#125;)            sess.run(g_train_opt, feed_dict=&#123;noise_img: batch_noise&#125;)        <span class="hljs-keyword">if</span> (epoch+<span class="hljs-number">1</span>) % <span class="hljs-number">30</span> == <span class="hljs-number">0</span>:            <span class="hljs-comment"># 每一轮结束计算loss</span>            train_loss_d = sess.run(d_loss,                                    feed_dict=&#123;real_img: batch_images,                                               noise_img: batch_noise&#125;)            <span class="hljs-comment"># real img loss</span>            train_loss_d_real = sess.run(d_loss_real,                                         feed_dict=&#123;real_img: batch_images,                                                    noise_img: batch_noise&#125;)            <span class="hljs-comment"># fake img loss</span>            train_loss_d_fake = sess.run(d_loss_fake,                                         feed_dict=&#123;real_img: batch_images,                                                    noise_img: batch_noise&#125;)            <span class="hljs-comment"># generator loss</span>            train_loss_g = sess.run(g_loss,                                    feed_dict=&#123;noise_img: batch_noise&#125;)            print(<span class="hljs-string">"Epoch &#123;&#125;/&#123;&#125;...\n"</span>.format(epoch + <span class="hljs-number">1</span>, epochs),                  <span class="hljs-string">"判别器损失: &#123;:.4f&#125;--&gt;(判别真实的: &#123;:.4f&#125; + 判别生成的: &#123;:.4f&#125;)...\n"</span>.format(train_loss_d, train_loss_d_real,                                                                                train_loss_d_fake),                  <span class="hljs-string">"生成器损失: &#123;:.4f&#125;"</span>.format(train_loss_g))            losses.append((train_loss_d, train_loss_d_real, train_loss_d_fake, train_loss_g))            <span class="hljs-comment"># 抽取样本后期进行观察</span>            sample_noise = np.random.uniform(<span class="hljs-number">-1</span>, <span class="hljs-number">1</span>, size=(n_sample, noise_size))            gen_samples = sess.run(get_generator(noise_img, g_units, img_size, reuse=<span class="hljs-literal">True</span>),                                   feed_dict=&#123;noise_img: sample_noise&#125;)            samples.append(gen_samples)    <span class="hljs-comment"># 显示生成的图像</span>    view_samples(<span class="hljs-number">-1</span>, samples)print(<span class="hljs-string">"OPTIMIZER END"</span>)</code></pre><h4 id="执行结果："><a href="#执行结果：" class="headerlink" title="执行结果："></a><strong>执行结果</strong>：</h4><pre><code class="hljs bash"> 判别器损失: 0.8938--&gt;(判别真实的: 0.4657 + 判别生成的: 0.4281)... 生成器损失: 1.9035Epoch 60/300... 判别器损失: 0.9623--&gt;(判别真实的: 0.5577 + 判别生成的: 0.4046)... 生成器损失: 1.7722Epoch 90/300... 判别器损失: 0.9523--&gt;(判别真实的: 0.3698 + 判别生成的: 0.5825)... 生成器损失: 1.3028Epoch 120/300... 判别器损失: 0.8671--&gt;(判别真实的: 0.3948 + 判别生成的: 0.4723)... 生成器损失: 1.5518Epoch 150/300... 判别器损失: 1.0439--&gt;(判别真实的: 0.3626 + 判别生成的: 0.6813)... 生成器损失: 1.1374Epoch 180/300... 判别器损失: 1.3034--&gt;(判别真实的: 0.6210 + 判别生成的: 0.6824)... 生成器损失: 1.3377Epoch 210/300... 判别器损失: 0.8368--&gt;(判别真实的: 0.4397 + 判别生成的: 0.3971)... 生成器损失: 1.7115Epoch 240/300... 判别器损失: 1.0776--&gt;(判别真实的: 0.5503 + 判别生成的: 0.5273)... 生成器损失: 1.4761Epoch 270/300... 判别器损失: 0.9964--&gt;(判别真实的: 0.5351 + 判别生成的: 0.4612)... 生成器损失: 1.8451Epoch 300/300... 判别器损失: 0.9810--&gt;(判别真实的: 0.5085 + 判别生成的: 0.4725)... 生成器损失: 1.5440OPTIMIZER END</code></pre><h4 id="生成的图像："><a href="#生成的图像：" class="headerlink" title="生成的图像："></a>生成的图像：</h4><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20200601182536.png" srcset="/img/loading.gif" alt="images"></p>]]></content>
    
    
    <categories>
      
      <category>TensorFlow</category>
      
    </categories>
    
    
    <tags>
      
      <tag>深度学习</tag>
      
      <tag>神经网络</tag>
      
      <tag>对抗生成网络</tag>
      
      <tag>GAN</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>神经网络之生成对抗网络</title>
    <link href="/2018/09/DL-GAN/"/>
    <url>/2018/09/DL-GAN/</url>
    
    <content type="html"><![CDATA[<h3 id="生成对抗网络"><a href="#生成对抗网络" class="headerlink" title="生成对抗网络"></a>生成对抗网络</h3><p>生成式对抗网络（GAN, Generative Adversarial Networks ）是一种<strong>深度学习</strong>模型，是近年来复杂分布上无监督学习最具前景的方法之一。</p><p>模型通过框架中（至少）两个模块：<strong>生成模型（Generative Model）和判别模型（Discriminative Model）</strong>的互相<strong>博弈</strong>学习产生相当好的输出。</p><p>原始 GAN 理论中，并不要求 G 和 D 都是神经网络，只需要是能拟合相应生成和判别的函数即可。</p><p>但实用中一般均使用深度神经网络作为 G 和 D 。一个优秀的GAN应用需要有良好的训练方法，否则可能由于神经网络模型的自由性而导致输出不理想。</p><h3 id="GAN的基本框架"><a href="#GAN的基本框架" class="headerlink" title="GAN的基本框架"></a>GAN的基本框架</h3><p>GAN所建立的一个学习框架，实际上我们可以看成<strong>生成模型和判别模型</strong>之间的一个模拟对抗游戏。我们可以把<strong>生成模型</strong>看作一个<strong>伪装者</strong>，而把<strong>判别模型</strong>看成一个<strong>警察</strong>。<strong>生成模型</strong>通过不断地学习来提高自己的<strong>伪装能力</strong>，从而使得生成出来的数据能够更好地“欺骗”<strong>判别模型</strong>。而<strong>判别模型</strong>则通过不断的训练来提高自己的判别能力，能够更准确地判断出数据的来源。GAN就是这样一个不断对抗的网络。GAN的架构如下图所示：</p><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20200601182514.png" srcset="/img/loading.gif" alt="images"></p><p><strong>生成模型</strong>以<strong>随机变量</strong>作为输入，其输出是对真实数据分布的一个估计。</p><p>生成数据和真实数据的采样都由<strong>判别模型</strong>进行判别，并给出真假性的判断和当前的损失。</p><p>利用<strong>反向传播</strong>，GAN对生成模型和判别模型进行交替优化。</p><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20200601182515.png" srcset="/img/loading.gif" alt="images"></p><h3 id="GAN的优化目标"><a href="#GAN的优化目标" class="headerlink" title="GAN的优化目标"></a>GAN的优化目标</h3><p>在对抗生成网络中，有两个博弈的角色分别为<strong>生成式模型(generative model)和判别式模型(discriminative model)</strong>。具体方式为：</p><ul><li>生成模型G捕捉样本数据的分布，判别模型D时一个二分类器，估计一个样本来自于训练数据（而非生成数据）的概率。</li></ul><p>在博弈的过程中我们需要提高两个模型的能力，所以通过不断调整<strong>生成模型G</strong>和<strong>判别模型D</strong>，直到<strong>判别模型D</strong>不能把数据的真假判别出来为止。在调整优化的过程中，我们需要：</p><ul><li>优化<strong>生成模型G</strong>，使得<strong>判别模型D</strong>无法判别出来事件的真假。</li><li>优化<strong>判别模型D</strong>，使得它尽可能的判别出事件的真假。</li></ul><h3 id="举个栗子："><a href="#举个栗子：" class="headerlink" title="举个栗子："></a>举个栗子：</h3><p>假设数据的概率分布为M，但是我们不知道具体的分布和构造是什么样的，就好像是一个黑盒子。为了了解这个黑盒子，我们就可以构建一个对抗生成网络：</p><ul><li><strong>生成模型G</strong>：使用一种我们完全知道的概率分布N来不断学习成为我们不知道的概率分布M.</li><li><strong>判别模型D</strong>：用来判别这个不断学习的概率是我们知道的概率分布N还是我们不知道的概率分布M。</li></ul><p>我们用图像来体现：</p><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20200601182516.png" srcset="/img/loading.gif" alt="images"></p><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20200601182517.png" srcset="/img/loading.gif" alt="images"></p><p>由上图所示：</p><ul><li>黑点所组成的数据分布是我们所不知道的概率分布M所形成的</li><li>绿色的线表示<strong>生成模型G</strong>使用已知的数据和判别模型不断对抗生成的数据分布。</li><li>蓝色的线表示<strong>判断模型D</strong></li><li>a图：初始状态</li><li>b图：生成模型不变，优化判别模型，直到判别的准确率最高</li><li>c图：判别模型不变。优化生成模型。直到生成的数据的真实性越高</li><li>d图：多次迭代后，生成模型产生的数据和概率部分M的数据基本一致，从而判别模型认为生成模型生成的数据就是概率分布M的数据分布。</li></ul><h3 id="GAN的数学推导"><a href="#GAN的数学推导" class="headerlink" title="GAN的数学推导"></a>GAN的数学推导</h3><p>符号定义:</p><ul><li><h4 id="P-data-x-：真实数据的分布"><a href="#P-data-x-：真实数据的分布" class="headerlink" title="$P_{data}(x)$：真实数据的分布"></a>$P_{data}(x)$：真实数据的分布</h4></li><li><h4 id="P-z-Z-：噪声数据"><a href="#P-z-Z-：噪声数据" class="headerlink" title="$P_z(Z)$：噪声数据"></a>$P_z(Z)$：噪声数据</h4></li><li><h4 id="P-g-x-：生成模型生成的数据分布"><a href="#P-g-x-：生成模型生成的数据分布" class="headerlink" title="$P_g(x)$：生成模型生成的数据分布"></a>$P_g(x)$：生成模型生成的数据分布</h4></li><li><h4 id="D-X-：判别器"><a href="#D-X-：判别器" class="headerlink" title="$D(X)$：判别器"></a>$D(X)$：判别器</h4></li><li><h4 id="G-x-：生成器"><a href="#G-x-：生成器" class="headerlink" title="$G(x)$：生成器"></a>$G(x)$：生成器</h4></li></ul><h3 id="定义判别模型和生成模型："><a href="#定义判别模型和生成模型：" class="headerlink" title="定义判别模型和生成模型："></a><strong>定义判别模型和生成模型</strong>：</h3><h4 id="E-x-sim-P-data-x-cdot-logD-x"><a href="#E-x-sim-P-data-x-cdot-logD-x" class="headerlink" title="$E_{x \sim P_{data}}(x) \cdot logD(x)$"></a>$E_{x \sim P_{data}}(x) \cdot logD(x)$</h4><p>由上式可知：当$x \sim P_{data}(x) , D(x)=1 $的时，$E_{x \sim P_{data}}(x)$取得最大值。</p><h4 id="E-x-sim-P-z-z-cdot-log-1-D-G-z"><a href="#E-x-sim-P-z-z-cdot-log-1-D-G-z" class="headerlink" title="$E_{x \sim P_{z}}(z) \cdot log(1-D(G(z)))$"></a>$E_{x \sim P_{z}}(z) \cdot log(1-D(G(z)))$</h4><p>由上式可知：当$x \sim P_{z}(z) , D(G(z))=0 $的时，$E_{x \sim P_{z}}(z)$取得最大值。</p><p>所以为了我们的<strong>判别模型</strong>越来越好，能力越来越强大，定义目标函数为：</p><h4 id="V-G-D-logD-x-log-1-D-G-z"><a href="#V-G-D-logD-x-log-1-D-G-z" class="headerlink" title="$V(G,D)=  logD(x) +  log(1-D(G(z)))$"></a>$V(G,D)=  logD(x) +  log(1-D(G(z)))$</h4><p>要使<strong>判别模型</strong>取得最好，所以需要使$V(G,D)$取得最大，即：</p><h4 id="D-agrmax-DV-G-D"><a href="#D-agrmax-DV-G-D" class="headerlink" title="$D = agrmax_DV(G,D)$"></a>$D = agrmax_DV(G,D)$</h4><p>当<strong>判别模型</strong>最好的时候，最好的<strong>生成模型</strong>就是<strong>目标函数取得最小</strong>的时候：</p><h4 id="G-argmin-G-aggmax-D-V-G-D"><a href="#G-argmin-G-aggmax-D-V-G-D" class="headerlink" title="$G=argmin_G(aggmax_D(V(G, D)))$"></a>$G=argmin_G(aggmax_D(V(G, D)))$</h4><p>所以经过这一系列的讨论，这个问题就变成了最大最小的问题，即：</p><h4 id="min-Gmax-DV-G-D-E-x-sim-P-data-x-cdot-logD-x-E-x-sim-P-z-z-cdot-log-1-D-G-z"><a href="#min-Gmax-DV-G-D-E-x-sim-P-data-x-cdot-logD-x-E-x-sim-P-z-z-cdot-log-1-D-G-z" class="headerlink" title="$min_Gmax_DV(G, D)=E_{x \sim P_{data}}(x) \cdot logD(x)+ E_{x \sim P_{z}}(z) \cdot log(1-D(G(z)))$"></a>$min_Gmax_DV(G, D)=E_{x \sim P_{data}}(x) \cdot logD(x)+ E_{x \sim P_{z}}(z) \cdot log(1-D(G(z)))$</h4><h3 id="最优判别模型："><a href="#最优判别模型：" class="headerlink" title="最优判别模型："></a><strong>最优判别模型</strong>：</h3><p>最终的目标函数：</p><p>$V(G,D)=  \int_x P_{data}(x) \cdot logD(x) +  P_g(x)log(1-D(G(z))) d(x)$</p><p>令：$V(G,D)=f(y), P_{data}(x)=a, P_g(x)=b$</p><p>所以：$f(y)=alogy+blog(1-y)$</p><p>因为:$a+b \ne 0$</p><p>所以最大值：$\frac{a}{a+b}$</p><p>最后，我们得到的<strong>最优判别模型</strong>就是：</p><h4 id="D-x-frac-P-data-X-P-data-X-P-g-x"><a href="#D-x-frac-P-data-X-P-data-X-P-g-x" class="headerlink" title="$D(x)=\frac{P_{data}(X)}{P_{data}(X)+P_g(x)}$"></a>$D(x)=\frac{P_{data}(X)}{P_{data}(X)+P_g(x)}$</h4><p>由于<strong>生成对抗网络</strong>的目的是：得到<strong>生成模型</strong>可以生成非常逼真的数据，也就是说是和真实数据的分布是一样的。因此最优的判别模型的输出为：</p><h4 id="D-x-frac-P-data-P-data-P-g-frac12"><a href="#D-x-frac-P-data-P-data-P-g-frac12" class="headerlink" title="$D(x)=\frac{P_{data}}{P_{data}+P_g}=\frac12$"></a>$D(x)=\frac{P_{data}}{P_{data}+P_g}=\frac12$</h4><p>其中：$P_g和P_{data}$的数据分布是一样的。</p><p>也就是说当D输出为0.5时，说明鉴别模型已经完全分不清真实数据和GAN生成的数据了，此时就是得到了最优生成模型了。</p><h3 id="证明生成模型："><a href="#证明生成模型：" class="headerlink" title="证明生成模型："></a><strong>证明生成模型</strong>：</h3><h4 id="充分性："><a href="#充分性：" class="headerlink" title="充分性："></a>充分性：</h4><p>前面我们已经得到了<strong>最优的判别模型</strong>，我们直接把数据带进目标函数：</p><h4 id="V-G-int-x-P-data-x-cdot-log-frac-12-P-g-x-log-frac-12-d-x-log4"><a href="#V-G-int-x-P-data-x-cdot-log-frac-12-P-g-x-log-frac-12-d-x-log4" class="headerlink" title="$V(G)=\int_x [P_{data}(x) \cdot log(\frac 12) +  P_g(x)log(\frac 12) ]d(x) =-log4$"></a>$V(G)=\int_x [P_{data}(x) \cdot log(\frac 12) +  P_g(x)log(\frac 12) ]d(x) =-log4$</h4><h4 id="必要性："><a href="#必要性：" class="headerlink" title="必要性："></a>必要性：</h4><h4 id="V-G-int-x-P-data-x-cdot-log-frac-P-data-X-P-data-X-P-g-x-P-g-x-log-1-frac-P-data-X-P-data-X-P-g-x-d-x"><a href="#V-G-int-x-P-data-x-cdot-log-frac-P-data-X-P-data-X-P-g-x-P-g-x-log-1-frac-P-data-X-P-data-X-P-g-x-d-x" class="headerlink" title="$V(G)=\int_x [P_{data}(x) \cdot log(\frac{P_{data}(X)}{P_{data}(X)+P_g(x)}) +  P_g(x)log(1-\frac{P_{data}(X)}{P_{data}(X)+P_g(x)}) ] d(x) $"></a>$V(G)=\int_x [P_{data}(x) \cdot log(\frac{P_{data}(X)}{P_{data}(X)+P_g(x)}) +  P_g(x)log(1-\frac{P_{data}(X)}{P_{data}(X)+P_g(x)}) ] d(x) $</h4><h4 id="V-G-int-x-log2-log2-cdot-P-data-x-P-data-x-cdot-log-frac-P-data-X-P-data-X-P-g-x-log2-log2-cdot-P-g-x-P-g-x-log-1-frac-P-data-X-P-data-X-P-g-x-d-x"><a href="#V-G-int-x-log2-log2-cdot-P-data-x-P-data-x-cdot-log-frac-P-data-X-P-data-X-P-g-x-log2-log2-cdot-P-g-x-P-g-x-log-1-frac-P-data-X-P-data-X-P-g-x-d-x" class="headerlink" title="$$V(G)=\int_x [(log2-log2)\cdot P_{data}(x)+P_{data}(x) \cdot log(\frac{P_{data}(X)}{P_{data}(X)+P_g(x)}) + (log2-log2)\cdot P_g(x) + P_g(x)log(1-\frac{P_{data}(X)}{P_{data}(X)+P_g(x)}) ]d(x) $$"></a>$$V(G)=\int_x [(log2-log2)\cdot P_{data}(x)+P_{data}(x) \cdot log(\frac{P_{data}(X)}{P_{data}(X)+P_g(x)}) + (log2-log2)\cdot P_g(x) + P_g(x)log(1-\frac{P_{data}(X)}{P_{data}(X)+P_g(x)}) ]d(x) $$</h4><h4 id="V-G-log2-int-x-P-g-x-P-data-x-d-x-int-x-P-data-x-cdot-log2-log-frac-P-data-X-P-data-X-P-g-x-P-g-x-cdot-log2-log-1-frac-P-data-X-P-data-X-P-g-x-d-x"><a href="#V-G-log2-int-x-P-g-x-P-data-x-d-x-int-x-P-data-x-cdot-log2-log-frac-P-data-X-P-data-X-P-g-x-P-g-x-cdot-log2-log-1-frac-P-data-X-P-data-X-P-g-x-d-x" class="headerlink" title="$V(G)=-log2 \int_x [P_g(x) +P_{data}(x)]d(x)+\int_x [P_{data}(x) \cdot(log2+ log(\frac{P_{data}(X)}{P_{data}(X)+P_g(x)}))+  P_g(x) \cdot (log2+log(1-\frac{P_{data}(X)}{P_{data}(X)+P_g(x)}) )] d(x)$"></a>$V(G)=-log2 \int_x [P_g(x) +P_{data}(x)]d(x)+\int_x [P_{data}(x) \cdot(log2+ log(\frac{P_{data}(X)}{P_{data}(X)+P_g(x)}))+  P_g(x) \cdot (log2+log(1-\frac{P_{data}(X)}{P_{data}(X)+P_g(x)}) )] d(x)$</h4><h4 id="V-G-log4-int-x-P-data-x-cdot-log-frac-P-data-X-P-data-X-frac-P-g-x-2-P-g-x-log-1-frac-P-data-X-P-data-X-frac-P-g-x-2-d-x"><a href="#V-G-log4-int-x-P-data-x-cdot-log-frac-P-data-X-P-data-X-frac-P-g-x-2-P-g-x-log-1-frac-P-data-X-P-data-X-frac-P-g-x-2-d-x" class="headerlink" title="$V(G)=-log4+\int_x [P_{data}(x) \cdot log(\frac{P_{data}(X)}{P_{data}(X)+\frac{P_g(x)}{2}}) +  P_g(x)log(1-\frac{P_{data}(X)}{P_{data}(X)+\frac{P_g(x)}{2}}) ] d(x) $"></a>$V(G)=-log4+\int_x [P_{data}(x) \cdot log(\frac{P_{data}(X)}{P_{data}(X)+\frac{P_g(x)}{2}}) +  P_g(x)log(1-\frac{P_{data}(X)}{P_{data}(X)+\frac{P_g(x)}{2}}) ] d(x) $</h4><h4 id="我们把最终结果转换为KL散度："><a href="#我们把最终结果转换为KL散度：" class="headerlink" title="我们把最终结果转换为KL散度："></a>我们把最终结果转换为<strong>KL散度</strong>：</h4><h4 id="V-G-log4-KL-P-data-mid-frac-P-data-P-g-2-KL-P-g-mid-frac-P-data-P-g-2"><a href="#V-G-log4-KL-P-data-mid-frac-P-data-P-g-2-KL-P-g-mid-frac-P-data-P-g-2" class="headerlink" title="$V(G)=-log4+KL(P_{data} \mid \frac{P_{data}+P_g}{2})+KL(P_g \mid  \frac{P_{data}+P_g}{2} )$"></a>$V(G)=-log4+KL(P_{data} \mid \frac{P_{data}+P_g}{2})+KL(P_g \mid  \frac{P_{data}+P_g}{2} )$</h4><p>因为：KL散度永远大于等于0，所以可以知道目标函数最终最优值为-log4。</p><h3 id="GAN的特性"><a href="#GAN的特性" class="headerlink" title="GAN的特性"></a>GAN的特性</h3><p>优点：</p><ul><li>模型优化只用到了反向传播，而不需要马尔科夫链。</li><li>训练时不需要对隐变量做推断。</li><li>理论上，只要是可微分函数都能用于构建生成模型G和判别模型D，因而能够与深度神经网络结合–&gt;深度产生式模型。</li><li>生成模型G的参数更新不是直接来自于数据样本，而是使用来自判别模型D的反向传播梯度。</li></ul><p>缺点：</p><ul><li>可解释性差，生成模型的分布没有显示的表达。它只是一个黑盒子一样的映射函数：输入是一个随机变量，输出是我们想要的一个数据分布。</li><li>比较难训练，生成模型D和判别模型G之间需要很好的同步。例如，在实际中我们常常需要 D 更新 K次， G 才能更新 1 次，如果没有很好地平衡这两个部件的优化，那么G最后就极大可能会坍缩到一个鞍点。</li></ul>]]></content>
    
    
    <categories>
      
      <category>深度学习</category>
      
    </categories>
    
    
    <tags>
      
      <tag>深度学习</tag>
      
      <tag>神经网络</tag>
      
      <tag>对抗生成网络</tag>
      
      <tag>GAN</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>TensorFlow实现简单的递归神经网络-RNN</title>
    <link href="/2018/08/TensorFlow-RNN/"/>
    <url>/2018/08/TensorFlow-RNN/</url>
    
    <content type="html"><![CDATA[<h3 id="递归神经网络实现MNIST手写数字识别"><a href="#递归神经网络实现MNIST手写数字识别" class="headerlink" title="递归神经网络实现MNIST手写数字识别"></a>递归神经网络实现MNIST手写数字识别</h3><h4 id="代码"><a href="#代码" class="headerlink" title="代码"></a><code>代码</code></h4><pre><code class="hljs python"><span class="hljs-comment">#!/usr/bin/env python</span><span class="hljs-comment"># -*- coding: utf-8 -*-</span><span class="hljs-comment"># @Time    : 2018/8/31 13:00</span><span class="hljs-comment"># @Author  : Seven</span><span class="hljs-comment"># @Site    : </span><span class="hljs-comment"># @File    : RNN.py</span><span class="hljs-comment"># @Software: PyCharm</span><span class="hljs-comment"># <span class="hljs-doctag">TODO:</span> 0.导入环境</span><span class="hljs-keyword">import</span> tensorflow <span class="hljs-keyword">as</span> tf<span class="hljs-keyword">from</span> tensorflow.examples.tutorials.mnist <span class="hljs-keyword">import</span> input_data<span class="hljs-keyword">import</span> osos.environ[<span class="hljs-string">'TF_CPP_MIN_LOG_LEVEL'</span>] = <span class="hljs-string">'2'</span>print(<span class="hljs-string">"Packages imported"</span>)<span class="hljs-comment"># <span class="hljs-doctag">TODO:</span> 1.数据准备</span>mnist = input_data.read_data_sets(<span class="hljs-string">"data/"</span>, one_hot=<span class="hljs-literal">True</span>)<span class="hljs-comment"># <span class="hljs-doctag">TODO:</span> 2.数据处理</span>x_train, y_train = mnist.train.images, mnist.train.labelsx_test, y_test = mnist.test.images, mnist.test.labelstrain_number = x_train.shape[<span class="hljs-number">0</span>]test_number = x_test.shape[<span class="hljs-number">0</span>]dim = y_train.shape[<span class="hljs-number">1</span>]classes_number = y_test.shape[<span class="hljs-number">1</span>]print(<span class="hljs-string">"MNIST LOADED"</span>)<span class="hljs-comment"># <span class="hljs-doctag">TODO:</span> 初始化权重参数</span>input_dim = <span class="hljs-number">28</span>hidden_dim = <span class="hljs-number">128</span>output_dim = classes_numbersteps = <span class="hljs-number">28</span>weights = &#123;    <span class="hljs-string">'hidden'</span>: tf.Variable(tf.random_normal([input_dim, hidden_dim])),    <span class="hljs-string">'out'</span>: tf.Variable(tf.random_normal([hidden_dim, output_dim]))&#125;biases = &#123;    <span class="hljs-string">'hidden'</span>: tf.Variable(tf.random_normal([hidden_dim])),    <span class="hljs-string">'out'</span>: tf.Variable(tf.random_normal([output_dim]))&#125;<span class="hljs-comment"># <span class="hljs-doctag">TODO:</span> 构建网络计算图</span><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">_RNN</span><span class="hljs-params">(_x, _w, _b, _step, _name)</span>:</span>    <span class="hljs-comment"># 1.把输入进行转换</span>    <span class="hljs-comment"># [batchsize, steps, input_dim]==&gt;[steps, batchsize, input_dim]</span>    _x = tf.transpose(_x, [<span class="hljs-number">1</span>, <span class="hljs-number">0</span>, <span class="hljs-number">2</span>])    <span class="hljs-comment"># [steps, batchsize, input_dim]==&gt;[steps*batchsize, input_dim]</span>    _x = tf.reshape(_x, [<span class="hljs-number">-1</span>, input_dim])    <span class="hljs-comment"># 2. input layer ==&gt; hidden layer</span>    hidden_layer = tf.add(tf.matmul(_x, _w[<span class="hljs-string">'hidden'</span>]), _b[<span class="hljs-string">'hidden'</span>])    <span class="hljs-comment"># 把数据进行分割</span>    hidden_layer_data = tf.split(hidden_layer, _step, <span class="hljs-number">0</span>)    <span class="hljs-comment"># LSTM</span>    lstm_cell = tf.nn.rnn_cell.BasicLSTMCell(hidden_dim, forget_bias=<span class="hljs-number">0.8</span>)    lstm_out, lstm_s = tf.nn.static_rnn(lstm_cell, hidden_layer_data, dtype=tf.float32)    <span class="hljs-comment"># 3. output</span>    output = tf.add(tf.matmul(lstm_out[<span class="hljs-number">-1</span>], _w[<span class="hljs-string">'out'</span>]), _b[<span class="hljs-string">'out'</span>])    return_data = &#123;        <span class="hljs-string">'x'</span>: _x, <span class="hljs-string">'hidden'</span>: hidden_layer, <span class="hljs-string">'hidden_data'</span>: hidden_layer_data,        <span class="hljs-string">'lstm_out'</span>: lstm_out, <span class="hljs-string">'lstm_s'</span>: lstm_s, <span class="hljs-string">'output'</span>: output    &#125;    <span class="hljs-keyword">return</span> return_data<span class="hljs-comment"># <span class="hljs-doctag">TODO:</span> 5.计算损失值并初始化optimizer</span>learn_rate = <span class="hljs-number">0.01</span>x = tf.placeholder(tf.float32, [<span class="hljs-literal">None</span>, steps, input_dim])y = tf.placeholder(tf.float32, [<span class="hljs-literal">None</span>, output_dim])MyRNN = _RNN(x, weights, biases, steps, <span class="hljs-string">'basic'</span>)<span class="hljs-comment"># 预测值</span>prediction = MyRNN[<span class="hljs-string">'output'</span>]<span class="hljs-comment"># 计算损失值</span>cross = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=prediction, labels=y))<span class="hljs-comment"># 初始化优化器</span>optimizer = tf.train.AdamOptimizer(learning_rate=learn_rate).minimize(cross)<span class="hljs-comment"># 模型评估</span>accuracy = tf.reduce_mean(tf.cast(tf.equal(tf.argmax(prediction, <span class="hljs-number">1</span>), tf.argmax(y, <span class="hljs-number">1</span>)), tf.float32))print(<span class="hljs-string">"NETWORK READY!!"</span>)<span class="hljs-comment"># <span class="hljs-doctag">TODO:</span> 6.初始化变量</span>init = tf.global_variables_initializer()<span class="hljs-comment"># <span class="hljs-doctag">TODO:</span> 7.模型训练</span>print(<span class="hljs-string">"Start optimization"</span>)<span class="hljs-keyword">with</span> tf.Session() <span class="hljs-keyword">as</span> sess:    sess.run(init)    training_epochs = <span class="hljs-number">50</span>    batch_size = <span class="hljs-number">128</span>    display_step = <span class="hljs-number">10</span>    <span class="hljs-keyword">for</span> epoch <span class="hljs-keyword">in</span> range(training_epochs):        avg_cost = <span class="hljs-number">0.</span>        total_batch = int(mnist.train.num_examples/batch_size)        <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> range(total_batch):            batch_xs, batch_ys = mnist.train.next_batch(batch_size)            batch_xs = batch_xs.reshape((batch_size, steps, input_dim))            feeds = &#123;x: batch_xs, y: batch_ys&#125;            sess.run(optimizer, feed_dict=feeds)            avg_cost += sess.run(cross, feed_dict=feeds) / total_batch        <span class="hljs-keyword">if</span> epoch % display_step == <span class="hljs-number">0</span>:            print(<span class="hljs-string">"Epoch: %03d/%03d cost: %.9f"</span> % (epoch+<span class="hljs-number">1</span>, training_epochs, avg_cost))            feeds = &#123;x: batch_xs, y: batch_ys&#125;            train_acc = sess.run(accuracy, feed_dict=feeds)            print(<span class="hljs-string">" Training accuracy: %.3f"</span> % train_acc)            batch_x_test = mnist.test.images            batch_y_test = mnist.test.labels            batch_x_test = batch_x_test.reshape([<span class="hljs-number">-1</span>, steps, input_dim])            test_acc = sess.run(accuracy, feed_dict=&#123;x: batch_x_test, y: batch_y_test&#125;)            print(<span class="hljs-string">" Test accuracy: %.3f"</span> % test_acc)print(<span class="hljs-string">"Optimization Finished."</span>)</code></pre><h4 id="执行结果"><a href="#执行结果" class="headerlink" title="执行结果"></a><code>执行结果</code></h4><pre><code class="hljs bash">NETWORK READY!!Start optimizationEpoch: 001/050 cost: 2.033586812 Training accuracy: 0.422 Test accuracy: 0.511Epoch: 011/050 cost: 0.101458139 Training accuracy: 0.961 Test accuracy: 0.961Epoch: 021/050 cost: 0.096797398 Training accuracy: 0.977 Test accuracy: 0.956Epoch: 031/050 cost: 0.105833270 Training accuracy: 0.992 Test accuracy: 0.958Epoch: 041/050 cost: 0.117593214 Training accuracy: 0.977 Test accuracy: 0.959Optimization Finished.</code></pre>]]></content>
    
    
    <categories>
      
      <category>TensorFlow</category>
      
    </categories>
    
    
    <tags>
      
      <tag>深度学习</tag>
      
      <tag>神经网络</tag>
      
      <tag>递归神经网络</tag>
      
      <tag>长短时记忆</tag>
      
      <tag>LSTM</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>递归神经网络之LSTM</title>
    <link href="/2018/08/DL-RNN-LSTM/"/>
    <url>/2018/08/DL-RNN-LSTM/</url>
    
    <content type="html"><![CDATA[<h3 id="LSTM模型"><a href="#LSTM模型" class="headerlink" title="LSTM模型"></a>LSTM模型</h3><p>由于RNN也有梯度消失的问题，因此很难处理长序列的数据，大牛们对RNN做了改进，得到了RNN的特例LSTM（Long Short-Term Memory），它可以避免常规RNN的梯度消失，因此在工业界得到了广泛的应用。</p><h3 id="从RNN到LSTM"><a href="#从RNN到LSTM" class="headerlink" title="从RNN到LSTM"></a>从RNN到LSTM</h3><p>在RNN模型中，我们总结了RNN具有如下结构：</p><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20200601182445.png" srcset="/img/loading.gif" alt="images"></p><p>RNN的模型可以简化成如下图的形式，所有 RNN 都具有一种重复神经网络模块的链式的形式。在标准的 RNN 中，这个重复的模块只有一个非常简单的结构，例如一个 tanh 层：</p><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20200601182446.png" srcset="/img/loading.gif" alt="images"></p><p>由于RNN梯度消失的问题，大牛们对于序列索引位置t的隐藏结构做了改进，可以说通过一些技巧让隐藏结构复杂了起来，来避免梯度消失的问题，这样的特殊RNN就是我们的LSTM。由于LSTM有很多的变种，这里我们以最常见的LSTM为例讲述。LSTM的结构如下图：</p><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20200601182447.png" srcset="/img/loading.gif" alt="images"></p><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20200601182448.png" srcset="/img/loading.gif" alt="images"></p><h3 id="LSTM模型结构剖析"><a href="#LSTM模型结构剖析" class="headerlink" title="LSTM模型结构剖析"></a>LSTM模型结构剖析</h3><p>上面我们给出了LSTM的模型结构，下面我们就一点点的剖析LSTM模型在每个序列索引位置t时刻的内部结构。</p><h3 id="LSTM关键–：“细胞状态”"><a href="#LSTM关键–：“细胞状态”" class="headerlink" title="LSTM关键–：“细胞状态”"></a>LSTM关键–：“细胞状态”</h3><p>细胞状态类似于传送带。直接在整个链上运行，只有一些少量的线性交互。信息在上面流传会很容易保持不变。</p><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20200601182449.png" srcset="/img/loading.gif" alt="images"></p><h4 id="LSTM控制“细胞状态”的方式："><a href="#LSTM控制“细胞状态”的方式：" class="headerlink" title="LSTM控制“细胞状态”的方式："></a><strong>LSTM</strong>控制“细胞状态”的方式：</h4><ul><li>通过“门”让信息选择性通过，来去除或者增加信息到细胞状态。</li><li>包含一个<code>SIGMOD</code>神经元层和一个<code>pointwise</code>乘法操作。</li><li><code>SIGMOD</code>层输出0到1之间的概率值，描述每个部分有多少量可以通过。0代表“不许任何量通过”，1就表示“允许任意量通过”。</li></ul><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20200601182450.png" srcset="/img/loading.gif" alt="images"></p><p>除了细胞状态，LSTM图中还有了很多奇怪的结构，这些结构一般称之为门控结构(Gate)。</p><p>LSTM在在每个序列索引位置t的门一般包括<strong>遗忘门</strong>，<strong>输入门</strong>和<strong>输出门</strong>三种。</p><h3 id="LSTM之遗忘门"><a href="#LSTM之遗忘门" class="headerlink" title="LSTM之遗忘门"></a>LSTM之遗忘门</h3><p>遗忘门（forget gate）顾名思义，是控制是否遗忘的，在LSTM中即以一定的概率控制是否遗忘上一层的隐藏细胞状态。遗忘门子结构如下图所示：</p><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20200601182451.png" srcset="/img/loading.gif" alt="images"></p><p>图中输入的有上一序列的隐藏状态$h(t-1)$和本序列数据$x_{(t)}$，通过一个激活函数，一般情况下是<code>SIGMOD</code>，得到遗忘门的输出$f(t)$。由于SIGMOD的输出$f(t)$在[0,1]之间，因此这里的输出$f(t)$代表了遗忘上一层隐藏细胞的概率。</p><p><strong>数学表达式</strong>：</p><h4 id="f-t-sigma-W-fh-t-1-U-fx-t-b-f"><a href="#f-t-sigma-W-fh-t-1-U-fx-t-b-f" class="headerlink" title="$f(t)=\sigma(W_fh(t-1)+U_fx(t)+b_f)$"></a>$f(t)=\sigma(W_fh(t-1)+U_fx(t)+b_f)$</h4><p>其中：$W_f、U_f、b_f$为线性关系的权重项和偏置项，$\sigma$为SIGMOD激活函数。</p><h3 id="LSTM之输入门"><a href="#LSTM之输入门" class="headerlink" title="LSTM之输入门"></a>LSTM之输入门</h3><p>输入门（input gate）负责处理当前序列位置的输入，它的子结构如下图：</p><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20200601182452.png" srcset="/img/loading.gif" alt="images"></p><p>从图中可以看到输入门由两部分组成，第一部分使用了sigmoid激活函数，输出为$i_{(t)}$,第二部分使用了tanh激活函数，输出为$\tilde c_{(t)}$, 两者的结果后面会相乘再去更新细胞状态。</p><ul><li>SIGMOD层决定什么值需要更新。</li><li>Tanh层创建一个新的候选值向量$\tilde c_{(t)}$</li><li>第二步还是为状态更新做准备。</li></ul><p><strong>数学表达式</strong>：</p><h4 id="i-t-sigma-W-ih-t-1-U-ix-t-b-i"><a href="#i-t-sigma-W-ih-t-1-U-ix-t-b-i" class="headerlink" title="$i{(t)} = \sigma(W_ih{(t-1)} + U_ix^{(t)} + b_i)$"></a>$i{(t)} = \sigma(W_ih{(t-1)} + U_ix^{(t)} + b_i)$</h4><h4 id="tilde-c-t-tanh-W-ah-t-1-U-ax-t-b-a"><a href="#tilde-c-t-tanh-W-ah-t-1-U-ax-t-b-a" class="headerlink" title="$\tilde c{(t)} =tanh(W_ah{(t-1)} + U_ax^{(t)} + b_a)$"></a>$\tilde c{(t)} =tanh(W_ah{(t-1)} + U_ax^{(t)} + b_a)$</h4><p>其中$W_i, U_i, b_i, W_a, U_a, b_a$，为线性关系的权重项和偏置项，$\sigma$为SIGMOD激活函数。</p><h3 id="LSTM之更新“细胞状态”"><a href="#LSTM之更新“细胞状态”" class="headerlink" title="LSTM之更新“细胞状态”"></a>LSTM之更新“细胞状态”</h3><p>在研究LSTM输出门之前，我们要先看看LSTM之细胞状态。前面的遗忘门和输入门的结果都会作用于细胞状态 $C_{(t)}$，我们来看看细胞如何从$C_{(t-1)}$到$C_{(t)}$:</p><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20200601182453.png" srcset="/img/loading.gif" alt="images"></p><p>由图可知：细胞状态$C_{(t)}$由两部分组成；第一部分是$C_{(t-1)}$和遗忘门输出$f(t)$的乘积，第二部分是输入门的$i_{(t)}$和$\tilde c_{(t)}$的乘积，总结为如下三点：</p><ul><li>更新$C_{(t-1)}$为$C_{(t)}$。</li><li>把就状态和$f(t)$相乘，丢弃掉我们确定需要丢弃的信息。</li><li>加上$i(t) * \tilde c_{(t)}$。最后得到新的候选值，根据我们决定更新每个状态的程度进行变化。</li></ul><p><strong>数学表达式</strong>：</p><h4 id="C-t-C-t-1-odot-f-t-i-t-odot-tilde-c-t"><a href="#C-t-C-t-1-odot-f-t-i-t-odot-tilde-c-t" class="headerlink" title="$C_{(t)} = C_{(t-1)} \odot f{(t)} + i_{(t)} \odot \tilde c_{(t)}$"></a>$C_{(t)} = C_{(t-1)} \odot f{(t)} + i_{(t)} \odot \tilde c_{(t)}$</h4><p>其中，$\bigodot$为Hadamard积.</p><h3 id="LSTM之输出门"><a href="#LSTM之输出门" class="headerlink" title="LSTM之输出门"></a>LSTM之输出门</h3><p>有了新的隐藏细胞状态$C_{(t)}$，我们就可以来看输出门了，子结构如下：</p><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20200601182454.png" srcset="/img/loading.gif" alt="images"></p><p>从图中可以看出：隐藏状态$h(t)$的更新由两个部分组成：第一部分是$o_{(t)}$，它是由上一序列的隐藏状态$h_{(t-1)}$和本序列的$x_{(t)}$，以及激活函数SIGMOD得到的，第二部分是由隐藏状态$C_{(t)}$和$Tanh$激活函数组成，即：</p><ul><li>最开始先运行一个SIGMOD层来确定细胞状态的那个部分将输出。</li><li>接着用tanh处理细胞状态（得到一个-1到1之间的值），再讲它和SIGMOD门的输出相乘。输出我们确定输出的那部分值。</li></ul><p><strong>数学表达式</strong>：</p><h4 id="o-t-sigma-W-o-h-t-1-x-t-b-o"><a href="#o-t-sigma-W-o-h-t-1-x-t-b-o" class="headerlink" title="$o_t=\sigma(W_o[h_{t-1},x_t]+b_o)$"></a>$o_t=\sigma(W_o[h_{t-1},x_t]+b_o)$</h4><h4 id="h-t-o-t-tanh-C-t"><a href="#h-t-o-t-tanh-C-t" class="headerlink" title="$h_t=o_t*tanh(C_t)$"></a>$h_t=o_t*tanh(C_t)$</h4><h3 id="LSTM的变体"><a href="#LSTM的变体" class="headerlink" title="LSTM的变体"></a>LSTM的变体</h3><ul><li>增加<code>peephole connection</code></li><li>让门层也会接受细胞状态的输入。</li></ul><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20200601182455.png" srcset="/img/loading.gif" alt="images"></p><p><strong>数学表达式</strong>：</p><h4 id="f-t-sigma-W-f-cdot-C-t-1-h-t-1-x-t-b-f"><a href="#f-t-sigma-W-f-cdot-C-t-1-h-t-1-x-t-b-f" class="headerlink" title="$f_t=\sigma(W_f \cdot[C_{t-1}, h_{t-1},x_t]+b_f)$"></a>$f_t=\sigma(W_f \cdot[C_{t-1}, h_{t-1},x_t]+b_f)$</h4><h4 id="i-t-sigma-W-i-cdot-C-t-1-h-t-1-x-t-b-i"><a href="#i-t-sigma-W-i-cdot-C-t-1-h-t-1-x-t-b-i" class="headerlink" title="$i_t=\sigma(W_i \cdot[C_{t-1}, h_{t-1},x_t]+b_i)$"></a>$i_t=\sigma(W_i \cdot[C_{t-1}, h_{t-1},x_t]+b_i)$</h4><h4 id="o-t-sigma-W-o-cdot-C-t-1-h-t-1-x-t-b-o"><a href="#o-t-sigma-W-o-cdot-C-t-1-h-t-1-x-t-b-o" class="headerlink" title="$o_t=\sigma(W_o \cdot[C_{t-1}, h_{t-1},x_t]+b_o)$"></a>$o_t=\sigma(W_o \cdot[C_{t-1}, h_{t-1},x_t]+b_o)$</h4><ul><li>通过使用<code>coupled</code>忘记和输入门</li><li>之前是分开确定需要忘记和添加的信息，然后一同做出决定。</li></ul><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20200601182456.png" srcset="/img/loading.gif" alt="images"></p><p><strong>数学表达式</strong>：</p><h4 id="C-t-f-t-C-t-1-1-f-t-tilde-C-t"><a href="#C-t-f-t-C-t-1-1-f-t-tilde-C-t" class="headerlink" title="$C_t=f_t * C_{t-1}+(1-f_t)  *  \tilde C_t$"></a>$C_t=f_t * C_{t-1}+(1-f_t)  *  \tilde C_t$</h4><h3 id="GRU"><a href="#GRU" class="headerlink" title="GRU"></a>GRU</h3><p><strong>Gatad Reacurrent Unit (GRU)</strong>，2014年提出。</p><ul><li>将忘记门和输入门合成了一个单一的<strong>更新门</strong></li><li>混合了细胞状态和隐藏状态</li><li>比标准的LSTM简单</li></ul><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20200601182457.png" srcset="/img/loading.gif" alt="images"></p><p><strong>数学表达式</strong>：</p><h4 id="z-t-sigma-W-z-cdot-h-t-1-x-t"><a href="#z-t-sigma-W-z-cdot-h-t-1-x-t" class="headerlink" title="$z_t=\sigma(W_z \cdot [h_{t-1},x_t])$"></a>$z_t=\sigma(W_z \cdot [h_{t-1},x_t])$</h4><h4 id="r-t-sigma-W-r-cdot-h-t-1-x-t"><a href="#r-t-sigma-W-r-cdot-h-t-1-x-t" class="headerlink" title="$r_t=\sigma(W_r \cdot [h_{t-1},x_t])$"></a>$r_t=\sigma(W_r \cdot [h_{t-1},x_t])$</h4><h4 id="tilde-h-t-tanh-W-cdot-r-t-h-t-1-x-t"><a href="#tilde-h-t-tanh-W-cdot-r-t-h-t-1-x-t" class="headerlink" title="$\tilde h_t= tanh(W \cdot [r_t*h_{t-1},x_t])$"></a>$\tilde h_t= tanh(W \cdot [r_t*h_{t-1},x_t])$</h4><h4 id="h-t-1-z-t-h-t-1-z-t-tilde-h-t"><a href="#h-t-1-z-t-h-t-1-z-t-tilde-h-t" class="headerlink" title="$h_t=(1-z_t)  * h_{t-1} + z_t  *  \tilde h_t$"></a>$h_t=(1-z_t)  * h_{t-1} + z_t  *  \tilde h_t$</h4>]]></content>
    
    
    <categories>
      
      <category>深度学习</category>
      
    </categories>
    
    
    <tags>
      
      <tag>深度学习</tag>
      
      <tag>神经网络</tag>
      
      <tag>递归神经网络</tag>
      
      <tag>长短时记忆</tag>
      
      <tag>LSTM</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>递归神经网络之BackPropagation Through Time</title>
    <link href="/2018/08/DL-RNN-BPTT/"/>
    <url>/2018/08/DL-RNN-BPTT/</url>
    
    <content type="html"><![CDATA[<h3 id="循环神经网络的训练算法：BPTT"><a href="#循环神经网络的训练算法：BPTT" class="headerlink" title="循环神经网络的训练算法：BPTT"></a>循环神经网络的训练算法：BPTT</h3><p><strong>BackPropagation Through Time (BPTT)</strong>经常出现用于学习递归神经网络（RNN）。</p><p>与前馈神经网络相比，RNN的特点是可以处理过去很长的信息。因此特别适合顺序模型。</p><p>BPTT扩展了普通的BP算法来适应递归神经网络。</p><p>BPTT算法是针对<strong>循环层</strong>的训练算法，它的基本原理和BP算法是一样的，也包含同样的三个步骤：</p><ol><li>前向计算每个神经元的输出值；</li><li>反向计算每个神经元的<strong>误差项</strong>$E$值，它是误差函数<code>E</code>对神经元的<strong>加权输入</strong>的偏导数；</li><li>计算每个权重的梯度。</li></ol><p>最后再用<strong>随机梯度下降</strong>算法更新权重。</p><h3 id="前向计算"><a href="#前向计算" class="headerlink" title="前向计算"></a>前向计算</h3><p>网络结构：</p><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20200601182417.png" srcset="/img/loading.gif" alt="images"></p><p>由图可知：</p><ul><li><h4 id="INPUT-LAYER-gt-HIDDEN-LAYER：-S-t-tanh-U-cdot-X-t-W-cdot-S-t-1"><a href="#INPUT-LAYER-gt-HIDDEN-LAYER：-S-t-tanh-U-cdot-X-t-W-cdot-S-t-1" class="headerlink" title="INPUT LAYER --&gt;HIDDEN LAYER：$S_t=tanh(U \cdot X_t+W \cdot S_{t-1}) $"></a><code>INPUT LAYER --&gt;HIDDEN LAYER</code>：$S_t=tanh(U \cdot X_t+W \cdot S_{t-1}) $</h4></li><li><h4 id="HIDDEN-LAYER-gt-OUTPUT-LAYER：-y-t-softmax-V-cdot-S-t"><a href="#HIDDEN-LAYER-gt-OUTPUT-LAYER：-y-t-softmax-V-cdot-S-t" class="headerlink" title="HIDDEN LAYER --&gt; OUTPUT LAYER：$y_t = softmax(V \cdot S_t)$"></a><code>HIDDEN LAYER --&gt; OUTPUT LAYER</code>：$y_t = softmax(V \cdot S_t)$</h4></li></ul><h3 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h3><p>我们将损失或者误差定义为<strong>交叉熵损失</strong>：</p><p>$E_t(y_t,\widehat y_t) = -y_t \cdot log\widehat y_t$</p><h4 id="E-t-y-t-widehat-y-t-y-t-cdot-log-widehat-y-t"><a href="#E-t-y-t-widehat-y-t-y-t-cdot-log-widehat-y-t" class="headerlink" title="$E_t(y_t,\widehat y_t) = -y_t \cdot log\widehat y_t$"></a>$E_t(y_t,\widehat y_t) = -y_t \cdot log\widehat y_t$</h4><h4 id="E-y-t-widehat-y-t-sum-t-y-t-cdot-log-widehat-y-t"><a href="#E-y-t-widehat-y-t-sum-t-y-t-cdot-log-widehat-y-t" class="headerlink" title="$E(y_t,\widehat y_t) = -\sum_t y_t \cdot log\widehat y_t$"></a>$E(y_t,\widehat y_t) = -\sum_t y_t \cdot log\widehat y_t$</h4><p><code>注意</code>:</p><ul><li>$y_t$：是<code>t</code>时刻的正确值</li><li>$\widehat y_t$：是我们的预测值</li><li>总误差是每个时刻的误差之和</li></ul><h3 id="反向计算"><a href="#反向计算" class="headerlink" title="反向计算"></a>反向计算</h3><p>循环层如下图所示：</p><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20200601182418.png" srcset="/img/loading.gif" alt="images"></p><p>接下来，我们就需要使用链式法则进行反向梯度的参数更新：</p><ul><li><h4 id="bigtriangleup-U-frac-partial-E-partial-U-sum-t-frac-partial-E-t-partial-U"><a href="#bigtriangleup-U-frac-partial-E-partial-U-sum-t-frac-partial-E-t-partial-U" class="headerlink" title="$\bigtriangleup U=\frac{ \partial E}{\partial U} = \sum_t \frac{\partial E_t}{\partial U}$"></a>$\bigtriangleup U=\frac{ \partial E}{\partial U} = \sum_t \frac{\partial E_t}{\partial U}$</h4></li><li><h4 id="bigtriangleup-W-frac-partial-E-partial-W-sum-t-frac-partial-E-t-partial-W"><a href="#bigtriangleup-W-frac-partial-E-partial-W-sum-t-frac-partial-E-t-partial-W" class="headerlink" title="$\bigtriangleup W=\frac{ \partial E}{\partial W} = \sum_t \frac{\partial E_t}{\partial W}$"></a>$\bigtriangleup W=\frac{ \partial E}{\partial W} = \sum_t \frac{\partial E_t}{\partial W}$</h4></li><li><h4 id="bigtriangleup-V-frac-partial-E-partial-V-sum-t-frac-partial-E-t-partial-V"><a href="#bigtriangleup-V-frac-partial-E-partial-V-sum-t-frac-partial-E-t-partial-V" class="headerlink" title="$\bigtriangleup V=\frac{ \partial E}{\partial V} = \sum_t \frac{\partial E_t}{\partial V}$"></a>$\bigtriangleup V=\frac{ \partial E}{\partial V} = \sum_t \frac{\partial E_t}{\partial V}$</h4></li></ul><p>我们令$t=3$为栗子：</p><h4 id="bigtriangleup-V-frac-partial-E-3-partial-V-frac-partial-E-3-partial-widehat-y-3-cdot-frac-partial-widehat-y-3-partial-net-3-cdot-frac-partial-net-3-partial-V-widehat-y-3-y-3-otimes-S-3"><a href="#bigtriangleup-V-frac-partial-E-3-partial-V-frac-partial-E-3-partial-widehat-y-3-cdot-frac-partial-widehat-y-3-partial-net-3-cdot-frac-partial-net-3-partial-V-widehat-y-3-y-3-otimes-S-3" class="headerlink" title="$\bigtriangleup V=\frac{ \partial E_3}{\partial V} = \frac{\partial E_3}{\partial \widehat y_3} \cdot \frac{\partial \widehat y_3}{\partial net_3} \cdot \frac{\partial net_3}{\partial V} = (\widehat y_3 - y_3) \otimes S_3$"></a>$\bigtriangleup V=\frac{ \partial E_3}{\partial V} = \frac{\partial E_3}{\partial \widehat y_3} \cdot \frac{\partial \widehat y_3}{\partial net_3} \cdot \frac{\partial net_3}{\partial V} = (\widehat y_3 - y_3) \otimes S_3$</h4><p><code>注意</code>：$net_3=V \cdot S_3$，$\otimes$是外积，V只和当前的时间有关，所以计算非常简单。</p><h4 id="bigtriangleup-W-frac-partial-E-3-partial-W-frac-partial-E-3-partial-widehat-y-3-cdot-frac-partial-widehat-y-3-partial-S-3-cdot-frac-partial-S-3-partial-W"><a href="#bigtriangleup-W-frac-partial-E-3-partial-W-frac-partial-E-3-partial-widehat-y-3-cdot-frac-partial-widehat-y-3-partial-S-3-cdot-frac-partial-S-3-partial-W" class="headerlink" title="$\bigtriangleup W=\frac{ \partial E_3}{\partial W} = \frac{\partial E_3}{\partial \widehat y_3} \cdot \frac{\partial \widehat y_3}{\partial S_3} \cdot \frac{\partial S_3}{\partial W} $"></a>$\bigtriangleup W=\frac{ \partial E_3}{\partial W} = \frac{\partial E_3}{\partial \widehat y_3} \cdot \frac{\partial \widehat y_3}{\partial S_3} \cdot \frac{\partial S_3}{\partial W} $</h4><p><code>因为</code>：</p><h4 id="S-3-tanh-U-cdot-X-t-W-cdot-S-2-，取决于-S-2-，而-S-2-又依赖于-W、S-1-。"><a href="#S-3-tanh-U-cdot-X-t-W-cdot-S-2-，取决于-S-2-，而-S-2-又依赖于-W、S-1-。" class="headerlink" title="$S_3 = tanh(U \cdot X_t+W \cdot S_2)$，取决于$S_2$，而$S_2$又依赖于$W、S_1$。"></a>$S_3 = tanh(U \cdot X_t+W \cdot S_2)$，取决于$S_2$，而$S_2$又依赖于$W、S_1$。</h4><p><code>所以</code>:</p><h4 id="bigtriangleup-W-frac-partial-E-3-partial-W-sum-k-0-3-frac-partial-E-3-partial-widehat-y-3-cdot-frac-partial-widehat-y-3-partial-S-3-cdot-frac-partial-S-3-partial-S-k-cdot-frac-partial-S-k-partial-net-k-cdot-frac-partial-net-k-partial-W"><a href="#bigtriangleup-W-frac-partial-E-3-partial-W-sum-k-0-3-frac-partial-E-3-partial-widehat-y-3-cdot-frac-partial-widehat-y-3-partial-S-3-cdot-frac-partial-S-3-partial-S-k-cdot-frac-partial-S-k-partial-net-k-cdot-frac-partial-net-k-partial-W" class="headerlink" title="$\bigtriangleup W=\frac{ \partial E_3}{\partial W} =\sum_{k=0}^3 \frac{\partial E_3}{\partial \widehat y_3} \cdot \frac{\partial \widehat y_3}{\partial S_3} \cdot \frac{\partial S_3}{\partial S_k} \cdot \frac{\partial S_k}{\partial net_k} \cdot \frac{\partial net_k}{\partial W} $"></a>$\bigtriangleup W=\frac{ \partial E_3}{\partial W} =\sum_{k=0}^3 \frac{\partial E_3}{\partial \widehat y_3} \cdot \frac{\partial \widehat y_3}{\partial S_3} \cdot \frac{\partial S_3}{\partial S_k} \cdot \frac{\partial S_k}{\partial net_k} \cdot \frac{\partial net_k}{\partial W} $</h4><p><code>总结</code>：W 在每一步中都有使用，所以我们需要$t=3$通过网络反向传播到$t=0$：</p><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20200601182419.png" srcset="/img/loading.gif" alt="images"></p><p><code>注意</code>：这种方式是和我们前面所推导的<a href="https://sevenold.github.io/2018/08/DL-back-propagation/" target="_blank" rel="noopener">深度神经网络的反向传播算法</a>和<a href="https://sevenold.github.io/2018/08/CNN-back-propagation/" target="_blank" rel="noopener">卷积神经网络的反向传播算法</a>是完全相同的。关键的区别就是我们总结了W的每个时刻的渐变，在传统的神经网络中，我们不跨层共享参数，因此我们不需要总结任何东西。</p><h4 id="bigtriangleup-U-frac-partial-E-3-partial-U-sum-k-0-3-frac-partial-E-3-partial-widehat-y-3-cdot-frac-partial-widehat-y-3-partial-S-3-cdot-frac-partial-S-3-partial-S-k-cdot-frac-partial-S-k-partial-net-k-cdot-frac-partial-net-k-partial-U"><a href="#bigtriangleup-U-frac-partial-E-3-partial-U-sum-k-0-3-frac-partial-E-3-partial-widehat-y-3-cdot-frac-partial-widehat-y-3-partial-S-3-cdot-frac-partial-S-3-partial-S-k-cdot-frac-partial-S-k-partial-net-k-cdot-frac-partial-net-k-partial-U" class="headerlink" title="$\bigtriangleup U=\frac{ \partial E_3}{\partial U} =\sum_{k=0}^3 \frac{\partial E_3}{\partial \widehat y_3} \cdot \frac{\partial \widehat y_3}{\partial S_3} \cdot \frac{\partial S_3}{\partial S_k} \cdot \frac{\partial S_k}{\partial net_k} \cdot \frac{\partial net_k}{\partial U} $"></a>$\bigtriangleup U=\frac{ \partial E_3}{\partial U} =\sum_{k=0}^3 \frac{\partial E_3}{\partial \widehat y_3} \cdot \frac{\partial \widehat y_3}{\partial S_3} \cdot \frac{\partial S_3}{\partial S_k} \cdot \frac{\partial S_k}{\partial net_k} \cdot \frac{\partial net_k}{\partial U} $</h4><p><code>总结</code>：U参数W参数的传递过程基本一致。</p><h3 id="参数更新"><a href="#参数更新" class="headerlink" title="参数更新"></a>参数更新</h3><ul><li><h4 id="U-new-U-old-eta-bigtriangleup-U"><a href="#U-new-U-old-eta-bigtriangleup-U" class="headerlink" title="$U_{new}=U_{old}-\eta \bigtriangleup U$"></a>$U_{new}=U_{old}-\eta \bigtriangleup U$</h4></li><li><h4 id="V-new-V-old-eta-bigtriangleup-V"><a href="#V-new-V-old-eta-bigtriangleup-V" class="headerlink" title="$V_{new}=V_{old}-\eta \bigtriangleup V$"></a>$V_{new}=V_{old}-\eta \bigtriangleup V$</h4></li><li><h4 id="W-new-W-old-eta-bigtriangleup-W"><a href="#W-new-W-old-eta-bigtriangleup-W" class="headerlink" title="$W_{new}=W_{old}-\eta \bigtriangleup W$"></a>$W_{new}=W_{old}-\eta \bigtriangleup W$</h4></li></ul><p>$\eta$：学习率</p><h3 id="梯度消失和梯度爆炸"><a href="#梯度消失和梯度爆炸" class="headerlink" title="梯度消失和梯度爆炸"></a>梯度消失和梯度爆炸</h3><p>RNN在训练中很容易发生<strong>梯度爆炸</strong>和<strong>梯度消失</strong>，这导致训练时梯度不能在较长序列中一直传递下去，从而使RNN无法捕捉到长距离的影响。</p><p>通常来说，<strong>梯度爆炸</strong>更容易处理一些。因为梯度爆炸的时候，我们的程序会收到NaN错误。我们也可以设置一个梯度阈值，当梯度超过这个阈值的时候可以直接截取。</p><p><strong>梯度消失</strong>更难检测，而且也更难处理一些。总的来说，我们有三种方法应对梯度消失问题：</p><ol><li>合理的初始化权重值。初始化权重，使每个神经元尽可能不要取极大或极小值，以躲开梯度消失的区域。</li><li>使用relu代替sigmoid和tanh作为激活函数。</li><li>使用其他结构的RNNs，比如长短时记忆网络（LTSM）和Gated Recurrent Unit（GRU），这是最流行的做法。</li></ol>]]></content>
    
    
    <categories>
      
      <category>深度学习</category>
      
    </categories>
    
    
    <tags>
      
      <tag>深度学习</tag>
      
      <tag>神经网络</tag>
      
      <tag>反向传播</tag>
      
      <tag>递归神经网络</tag>
      
      <tag>BPTT</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>递归神经网络之python实现RNN算法</title>
    <link href="/2018/08/DL-RNN-hand/"/>
    <url>/2018/08/DL-RNN-hand/</url>
    
    <content type="html"><![CDATA[<h3 id="PYTHON实现RNN算法"><a href="#PYTHON实现RNN算法" class="headerlink" title="PYTHON实现RNN算法"></a>PYTHON实现RNN算法</h3><h4 id="实现学习二进制的加法。"><a href="#实现学习二进制的加法。" class="headerlink" title="实现学习二进制的加法。"></a>实现学习二进制的加法。</h4><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20200603140953.png" srcset="/img/loading.gif" alt="images"></p><p>如上图所示，我们来手写RNN算法：</p><h4 id="代码："><a href="#代码：" class="headerlink" title="代码："></a>代码：</h4><pre><code class="hljs python"><span class="hljs-comment">#!/usr/bin/env python</span><span class="hljs-comment"># -*- coding: utf-8 -*-</span><span class="hljs-comment"># @Time    : 2018/8/29 13:51</span><span class="hljs-comment"># @Author  : Seven</span><span class="hljs-comment"># @Site    :</span><span class="hljs-comment"># @File    : demo.py</span><span class="hljs-comment"># @Software: PyCharm</span><span class="hljs-comment"># <span class="hljs-doctag">TODO:</span>导入环境</span><span class="hljs-keyword">import</span> copy<span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> npnp.random.seed(<span class="hljs-number">0</span>)<span class="hljs-comment"># 使用SIGMOD函数转换成非线性</span><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">sigmoid</span><span class="hljs-params">(inputs)</span>:</span>    output = <span class="hljs-number">1</span> / (<span class="hljs-number">1</span> + np.exp(-inputs))    <span class="hljs-keyword">return</span> output<span class="hljs-comment"># 将SIGMOD函数的输出转换成其导数</span><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">sigmoid_output_to_derivative</span><span class="hljs-params">(output)</span>:</span>    <span class="hljs-keyword">return</span> output * (<span class="hljs-number">1</span> - output)<span class="hljs-comment"># <span class="hljs-doctag">TODO:</span> 生成训练数据集--&gt;把十进制转换为8位二进制</span>int2binary = &#123;&#125;binary_dim = <span class="hljs-number">8</span>largest_number = pow(<span class="hljs-number">2</span>, binary_dim)  <span class="hljs-comment"># 2**8=256</span><span class="hljs-comment"># 把0-256转换成对应的二进制编码并储存起来</span>binary = np.unpackbits(    np.array([range(largest_number)], dtype=np.uint8).T, axis=<span class="hljs-number">1</span>)<span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> range(largest_number):    int2binary[i] = binary[i]<span class="hljs-comment"># 参数定义</span>alpha = <span class="hljs-number">0.1</span>input_dim = <span class="hljs-number">2</span>hidden_dim = <span class="hljs-number">16</span>output_dim = <span class="hljs-number">1</span><span class="hljs-comment"># 初始化权重参数--在-1到1之间</span>U = <span class="hljs-number">2</span> * np.random.random((input_dim, hidden_dim)) - <span class="hljs-number">1</span>V = <span class="hljs-number">2</span> * np.random.random((hidden_dim, output_dim)) - <span class="hljs-number">1</span>W = <span class="hljs-number">2</span> * np.random.random((hidden_dim, hidden_dim)) - <span class="hljs-number">1</span><span class="hljs-comment"># 更新权重参数</span>U_update = np.zeros_like(U)V_update = np.zeros_like(V)W_update = np.zeros_like(W)<span class="hljs-comment"># <span class="hljs-doctag">TODO:</span>训练</span><span class="hljs-keyword">for</span> step <span class="hljs-keyword">in</span> range(<span class="hljs-number">10000</span>):    <span class="hljs-comment"># 随机生成一个加法a+b=c（因为最大是8位，所以a最大是largest_number的1/2）</span>    a_int = np.random.randint(largest_number / <span class="hljs-number">2</span>)  <span class="hljs-comment"># int version</span>    a = int2binary[a_int]  <span class="hljs-comment"># binary encoding</span>    b_int = np.random.randint(largest_number / <span class="hljs-number">2</span>)  <span class="hljs-comment"># int version</span>    b = int2binary[b_int]  <span class="hljs-comment"># binary encoding</span>    <span class="hljs-comment"># true answer</span>    c_int = a_int + b_int    c = int2binary[c_int]    <span class="hljs-comment"># 存储概率最大的二进制--预测值</span>    d = np.zeros_like(c)    overallError = <span class="hljs-number">0</span>    out_deltas = list()    layer_1_values = list()  <span class="hljs-comment"># 保存’记忆‘</span>    layer_1_values.append(np.zeros(hidden_dim))    <span class="hljs-comment"># 沿着二进制的位置移动--前向传播</span>    <span class="hljs-keyword">for</span> position <span class="hljs-keyword">in</span> range(binary_dim):        <span class="hljs-comment"># 生成输入和输出的值</span>        X = np.array([[a[binary_dim - position - <span class="hljs-number">1</span>], b[binary_dim - position - <span class="hljs-number">1</span>]]])        y = np.array([c[binary_dim - position - <span class="hljs-number">1</span>]]).T        <span class="hljs-comment"># hidden layer --&gt; UX+layer1·W</span>        layer_1 = sigmoid(np.dot(X, U) + np.dot(layer_1_values[<span class="hljs-number">-1</span>], W))        <span class="hljs-comment"># output layer</span>        layer_2 = sigmoid(np.dot(layer_1, V))        <span class="hljs-comment"># 计算损失值</span>        layer_2_error = y - layer_2        out_deltas.append(layer_2_error * sigmoid_output_to_derivative(layer_2))        overallError += np.abs(layer_2_error[<span class="hljs-number">0</span>])        <span class="hljs-comment"># 解码成十进制</span>        d[binary_dim - position - <span class="hljs-number">1</span>] = np.round(layer_2[<span class="hljs-number">0</span>][<span class="hljs-number">0</span>])        <span class="hljs-comment"># 保存记忆，以便下一时刻使用</span>        layer_1_values.append(copy.deepcopy(layer_1))    future_layer_1_delta = np.zeros(hidden_dim)    <span class="hljs-comment"># 反向传播</span>    <span class="hljs-keyword">for</span> position <span class="hljs-keyword">in</span> range(binary_dim):        X = np.array([[a[position], b[position]]])        layer_1 = layer_1_values[-position - <span class="hljs-number">1</span>]        prev_layer_1 = layer_1_values[-position - <span class="hljs-number">2</span>]        <span class="hljs-comment"># error at output layer</span>        layer_2_delta = out_deltas[-position - <span class="hljs-number">1</span>]        <span class="hljs-comment"># error at hidden layer</span>        layer_1_delta = (future_layer_1_delta.dot(W.T) + layer_2_delta.dot(            V.T)) * sigmoid_output_to_derivative(layer_1)        <span class="hljs-comment"># 更新权重参数</span>        V_update += np.atleast_2d(layer_1).T.dot(layer_2_delta)        W_update += np.atleast_2d(prev_layer_1).T.dot(layer_1_delta)        U_update += X.T.dot(layer_1_delta)        future_layer_1_delta = layer_1_delta    U += U_update * alpha    V += V_update * alpha    W += W_update * alpha    U_update *= <span class="hljs-number">0</span>    V_update *= <span class="hljs-number">0</span>    W_update *= <span class="hljs-number">0</span>    <span class="hljs-comment"># print out progress</span>    <span class="hljs-keyword">if</span> step % <span class="hljs-number">1000</span> == <span class="hljs-number">0</span>:        print(<span class="hljs-string">"错误值:"</span> + str(overallError))        print(<span class="hljs-string">"预测值:"</span> + str(d))        print(<span class="hljs-string">"正确值:"</span> + str(c))        out = <span class="hljs-number">0</span>        <span class="hljs-keyword">for</span> index, x <span class="hljs-keyword">in</span> enumerate(reversed(d)):            out += x * pow(<span class="hljs-number">2</span>, index)        print(str(a_int) + <span class="hljs-string">" + "</span> + str(b_int) + <span class="hljs-string">" = "</span> + str(out))        print(<span class="hljs-string">"------------"</span>)</code></pre><h4 id="结果："><a href="#结果：" class="headerlink" title="结果："></a>结果：</h4><pre><code class="hljs bash">损失值:[3.45638663]预测值:[0 0 0 0 0 0 0 1]正确值:[0 1 0 0 0 1 0 1]9 + 60 = 1------------损失值:[3.63389116]预测值:[1 1 1 1 1 1 1 1]正确值:[0 0 1 1 1 1 1 1]28 + 35 = 255------------损失值:[3.91366595]预测值:[0 1 0 0 1 0 0 0]正确值:[1 0 1 0 0 0 0 0]116 + 44 = 72------------损失值:[3.72191702]预测值:[1 1 0 1 1 1 1 1]正确值:[0 1 0 0 1 1 0 1]4 + 73 = 223------------损失值:[3.5852713]预测值:[0 0 0 0 1 0 0 0]正确值:[0 1 0 1 0 0 1 0]71 + 11 = 8------------损失值:[2.53352328]预测值:[1 0 1 0 0 0 1 0]正确值:[1 1 0 0 0 0 1 0]81 + 113 = 162------------损失值:[0.57691441]预测值:[0 1 0 1 0 0 0 1]正确值:[0 1 0 1 0 0 0 1]81 + 0 = 81------------损失值:[1.42589952]预测值:[1 0 0 0 0 0 0 1]正确值:[1 0 0 0 0 0 0 1]4 + 125 = 129------------损失值:[0.47477457]预测值:[0 0 1 1 1 0 0 0]正确值:[0 0 1 1 1 0 0 0]39 + 17 = 56------------损失值:[0.21595037]预测值:[0 0 0 0 1 1 1 0]正确值:[0 0 0 0 1 1 1 0]11 + 3 = 14------------</code></pre>]]></content>
    
    
    <categories>
      
      <category>深度学习</category>
      
    </categories>
    
    
    <tags>
      
      <tag>深度学习</tag>
      
      <tag>神经网络</tag>
      
      <tag>反向传播</tag>
      
      <tag>递归神经网络</tag>
      
      <tag>BPTT</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>神经网络之递归神经网络</title>
    <link href="/2018/08/DL-RNN/"/>
    <url>/2018/08/DL-RNN/</url>
    
    <content type="html"><![CDATA[<h3 id="递归神经网络"><a href="#递归神经网络" class="headerlink" title="递归神经网络"></a>递归神经网络</h3><p><strong>递归神经网络（Recurrent Neural Networks，RNN）</strong>是两种人工神经网络的总称：</p><ul><li><p><strong>时间递归神经网络（recurrent neural network）</strong>：时间递归神经网络的神经元间连接构成有向图</p></li><li><p><strong>结构递归神经网络（recursive neural network)</strong>：结构递归神经网络利用相似的神经网络结构递归构造更为复杂的深度网络。</p></li></ul><p><strong>递归神经网络（RNN）</strong>(也叫循环神经网络）是一类神经网络，包括一层内的加权连接，与传统前馈神经网络相比，加权连接仅反馈到后续层。因为<strong>RNN</strong>包含循环，所以<strong>RNN</strong>就可以在<strong>处理输入信息的时候同时储存信息</strong>。这种记忆使得<strong>RNN</strong>非常适合处理必须考虑事先输入的任务（比如时序数据）。所以循环神经网络在自然语言处理领域非常适合。</p><p><strong>RNN</strong>一般指代<strong>时间递归神经网络</strong>。单纯递归神经网络因为无法处理随着递归，<strong>权重指数级爆炸或消失的问题（Vanishing gradient problem）</strong>，难以捕捉长期时间关联；而结合不同的LSTM可以很好解决这个问题。时间递归神经网络可以描述动态时间行为，因为和前馈神经网络（feedforward neural network）接受较特定结构的输入不同，RNN将状态在自身网络中循环传递，因此可以接受更广泛的时间序列结构输入。</p><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20200601182341.png" srcset="/img/loading.gif" alt="images"></p><h3 id="语言模型"><a href="#语言模型" class="headerlink" title="语言模型"></a>语言模型</h3><p>RNN是在<strong>自然语言处理</strong>领域中最先被用起来的，比如，RNN可以为<strong>语言模型</strong>来建模。</p><p>我们小时候都做过语文的填空题吧</p><pre><code class="hljs maxima">饿死了，我要吃<span class="hljs-symbol">__</span>。</code></pre><p>如果是我，肯定是填<code>肉</code>。哈哈</p><p>但是我不是今天的主角，如果这个问题我们让计算机来回答呢？</p><p>我们给计算机展示这段话，让计算机给出答案。而这个答案最有可能的就是<code>饭</code>，而太可能是<code>睡觉</code>或者<code>做作业</code>.</p><p><strong>语言模型</strong>就是这样的东西：给定一个一句话前面的部分，预测接下来最有可能的一个词是什么。</p><p><strong>语言模型</strong>是对一种语言的特征进行建模，它有很多很多用处。比如在语音转文本(STT)的应用中，声学模型输出的结果，往往是若干个可能的候选词，这时候就需要<strong>语言模型</strong>来从这些候选词中选择一个最可能的。当然，它同样也可以用在图像到文本的识别中(OCR)。</p><p>使用RNN之前，语言模型主要是采用N-Gram。N可以是一个自然数，比如2或者3。它的含义是，假设一个词出现的概率只与前面N个词相关。我们以2-Gram为例。首先，对前面的一句话进行切词：</p><pre><code class="hljs sqf">饿死了，我要<span class="hljs-variable">____</span>。</code></pre><p>如果用2-Gram进行建模，那么电脑在预测的时候，只会看到前面的『要』，然后，电脑会在语料库中，搜索『要』后面最可能的一个词。不管最后电脑选的是不是『饭』，我们都知道这个模型是不靠谱的，因为『要』前面说了那么一大堆实际上是没有用到的。如果是3-Gram模型呢，会搜索『我要吃』后面最可能的词，感觉上比2-Gram靠谱了不少，但还是远远不够的。因为这句话最关键的信息『饿』，远在5个词之前！</p><p>现在读者可能会想，可以提升继续提升N的值呀，比如4-Gram、5-Gram…….。实际上，这个想法是没有实用性的。因为我们想处理任意长度的句子，N设为多少都不合适；另外，模型的大小和N的关系是指数级的，4-Gram模型就会占用海量的存储空间。</p><p>所以，我们的RNN就出场了，RNN理论上<strong>可以往前看(往后看)任意多个词</strong>。</p><h3 id="基本递归神经网络"><a href="#基本递归神经网络" class="headerlink" title="基本递归神经网络"></a>基本递归神经网络</h3><p>下图是一个简单的递归神经网络，它是由输入层、一个隐层和一个输出层组成的。</p><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20200601182342.png" srcset="/img/loading.gif" alt="images"></p><p>由图可知：</p><ul><li><code>x</code>：输入层的值</li><li><code>U</code>：输入层到隐层的权重参数</li><li><code>s</code>：隐层的值</li><li><code>v</code>：隐层到输出层的权重参数</li><li><code>o</code>：输出层的值</li><li><code>W</code>：<strong>递归神经网络</strong>的<strong>隐藏层</strong>的值<code>s</code>不仅仅取决于当前这次的输入<code>x</code>，还取决于上一次<strong>隐藏层</strong>的值<code>s</code>。<strong>权重参数</strong><code>W</code>就是<strong>隐藏层</strong>上一次的值作为这一次的输入的权重。</li></ul><p>现在我们把上面的图展开，<strong>递归神经网络</strong>也可以画出这个结构：</p><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20200601182343.png" srcset="/img/loading.gif" alt="images"></p><p>从上图中，我们可以很直观的看出，这个网络在<code>t</code>时刻接收到$X_t$后，隐层的值是$S_t$，输出值是$O_t$。</p><p><code>关键点</code>：$S_t$的值不仅仅取决于$X_t$，还取决于$S_{t-1}$(就是上一状态的隐层的值)。</p><p><code>公式</code>：循环神经网络的计算公式：</p><ul><li><h4 id="O-t-f-V-cdot-S-t-quad-1"><a href="#O-t-f-V-cdot-S-t-quad-1" class="headerlink" title="$O_t=f(V \cdot S_t) \quad (1)$"></a><strong>$O_t=f(V \cdot S_t) \quad (1)$</strong></h4><ul><li><code>输出层</code>的计算公式，由于输出层是一个<strong>全连接层</strong>，所以说它每个节点都和隐层的节点相连。<code>V</code>是输出层的权重参数，<code>f</code>是激活函数。</li></ul></li><li><h4 id="S-t-f-U-cdot-X-t-W-cdot-S-t-1-quad-2"><a href="#S-t-f-U-cdot-X-t-W-cdot-S-t-1-quad-2" class="headerlink" title="$S_t=f(U \cdot X_t+W \cdot S_{t-1}) \quad (2)$"></a><strong>$S_t=f(U \cdot X_t+W \cdot S_{t-1}) \quad (2)$</strong></h4><ul><li><code>隐层</code>的计算公式，它是一个<code>循环层</code>，<code>U</code>是输入<code>x</code>的权重参数，<code>W</code>是上一次的值<strong>$S_{t-1}$</strong>作为这一次输入的权重参数，<code>f</code>是激活函数。</li></ul></li></ul><p><code>总结</code>：从上面的公式中，我们可以看出，<strong>循环层</strong>和<strong>全连接层</strong>的区别就是<strong>循环层</strong>多了一个<strong>权重参数</strong><code>w</code>。</p><p><code>扩展</code>：如果反复的把（1）式带入 （2）式：</p><h4 id="o-t-f-V-cdot-S-t"><a href="#o-t-f-V-cdot-S-t" class="headerlink" title="${o}_t=f(V\cdot{S}_t) $"></a>${o}_t=f(V\cdot{S}_t) $</h4><p>$ = V \cdot f(U \cdot X_t + W \cdot S_{t-1}) $</p><p>$= V \cdot f(U \cdot X_t+W \cdot f(U \cdot X_{t-1}+W \cdot S_{t-2}))$</p><p>$= V \cdot f(U \cdot X_t+W \cdot f(U \cdot X_{t-1}+W \cdot f(U \cdot X_{t-2}+W \cdot S_{t-3}))) $</p><p>$= V \cdot f(U \cdot X_t+W \cdot f(U \cdot X_{t-1}+W \cdot f(U \cdot X_{t-2}+W \cdot f(U \cdot X_{t-3}+…))))$</p><p><code>总结</code>：从上面可以看出，<strong>递归神经网络</strong>的输出值$o_t$，是受前面几次输入值$X_t、X_{t-1}、X_{t-2}、X_{t-3}$…影响的，这也就是为什么<strong>递归神经网络</strong>可以往前看任意多个<strong>输入值</strong>的原因。</p><h3 id="双向递归神经网络"><a href="#双向递归神经网络" class="headerlink" title="双向递归神经网络"></a>双向递归神经网络</h3><p><code>介绍</code>：它们只是两个堆叠在一起的RNN。然后基于两个RNN的隐藏状态计算输出。</p><p>对于<strong>语言模型</strong>来说，很多时候光看前面的词是不够的，比如下面这句话：</p><pre><code class="hljs sqf">肚子好饿啊。我打算<span class="hljs-variable">___</span>一份外卖。</code></pre><p>可以想象，如果我们只是看横线前面的词，肚子好饿啊，我打算做饭，去吃饭还是什么什么的。这些都是无法确定的。但是如果我也先看横线后面的词【一份外卖】，那么横线上填【点】的概率就大多了。</p><p>但是我们前面所讲的<strong>基本递归神经网络</strong>是无法解决这个问题的，这时我们就需要<strong>双向递归神经网络</strong>，如下图所示：</p><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20200601182344.png" srcset="/img/loading.gif" alt="images"></p><p>呃~~，有点复杂，我们先尝试分析一个特殊的场景，然后再总结规律。我们先看看上图的$y_2$的计算：</p><p>从上图可以看出，<strong>双向递归神经网络</strong>的隐层是需要保持两个值：</p><ul><li>$A$：参与正向计算</li><li>$A’$：参与反向计算</li></ul><p>所以$y_2$的值就取决于$A_2$和$A_2’$。计算方法：</p><ul><li><h4 id="y-2-f-V-cdot-A-2-V’-cdot-A-2’"><a href="#y-2-f-V-cdot-A-2-V’-cdot-A-2’" class="headerlink" title="$y_2=f(V \cdot A_2+V’ \cdot A_2’)$"></a>$y_2=f(V \cdot A_2+V’ \cdot A_2’)$</h4></li></ul><p>$A_2$和$A_2’$则分别计算：</p><ul><li><h4 id="A-2-f-W-cdot-A-1-U-cdot-X-2"><a href="#A-2-f-W-cdot-A-1-U-cdot-X-2" class="headerlink" title="$A_2 = f(W \cdot A_1+U \cdot X_2)$"></a>$A_2 = f(W \cdot A_1+U \cdot X_2)$</h4></li><li><h4 id="A-2’-f-W’-cdot-A-3’-U’-cdot-X-2"><a href="#A-2’-f-W’-cdot-A-3’-U’-cdot-X-2" class="headerlink" title="$A_2’=f(W’ \cdot A_3’+U’ \cdot X_2)$"></a>$A_2’=f(W’ \cdot A_3’+U’ \cdot X_2)$</h4></li></ul><p><code>总结</code>：</p><ul><li>正向计算时：隐层的值$S_t$和$S_{t-1}$有关。</li><li>反向计算时：隐层的值$S_t’$和$S_{t-1}’$有关。</li><li>最终的输出取决于正向和反向计算的<strong>加和</strong>。</li></ul><p><code>扩展</code>：我们仿照（1）和（2）那种方式：</p><h4 id="o-t-f-V-cdot-S-t-V’-cdot-S-t’"><a href="#o-t-f-V-cdot-S-t-V’-cdot-S-t’" class="headerlink" title="$o_t =f(V \cdot S_t+V’ \cdot S_t’) $"></a>$o_t =f(V \cdot S_t+V’ \cdot S_t’) $</h4><h4 id="S-t-f-U-cdot-X-t-W-cdot-S-t-1"><a href="#S-t-f-U-cdot-X-t-W-cdot-S-t-1" class="headerlink" title="$S_t =f(U \cdot X_t+W \cdot S_{t-1}) $"></a>$S_t =f(U \cdot X_t+W \cdot S_{t-1}) $</h4><h4 id="S-t’-f-U’-cdot-X-t-W’-cdot-S-t-1-’"><a href="#S-t’-f-U’-cdot-X-t-W’-cdot-S-t-1-’" class="headerlink" title="$ S_t’=f(U’ \cdot X_t+W’ \cdot S_{t+1}’)$"></a>$ S_t’=f(U’ \cdot X_t+W’ \cdot S_{t+1}’)$</h4><p><code>注意</code>：从上面三个公式我们可以看到，正向计算和反向计算<strong>不共享权重</strong>，也就是说U和U’、W和W’、V和V’都是不同的<strong>权重矩阵</strong>。</p><h3 id="深度递归神经网络"><a href="#深度递归神经网络" class="headerlink" title="深度递归神经网络"></a>深度递归神经网络</h3><p>前面我们介绍的<strong>递归神经网络</strong>只有一个隐藏层，我们当然也可以堆叠两个以上的隐藏层，这样就得到了<strong>深度递归神经网络</strong>。如下图所示：</p><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20200601182345.png" srcset="/img/loading.gif" alt="images"></p><p>我们把第$i$个隐层的值表示为$S_t^{(i)}、S_t’^{(i)}$  ,则<strong>深度递归神经网络</strong>的计算方式就可以表示为：</p><ul><li><h4 id="o-t-f-cdot-V-i-cdot-S-t-i-V’-i-cdot-S-t’-i"><a href="#o-t-f-cdot-V-i-cdot-S-t-i-V’-i-cdot-S-t’-i" class="headerlink" title="${o}_t=f \cdot (V^{(i)} \cdot S_t^{(i)}+V’^{(i)} \cdot S_t’^{(i)})$"></a>${o}_t=f \cdot (V^{(i)} \cdot S_t^{(i)}+V’^{(i)} \cdot S_t’^{(i)})$</h4></li><li><h4 id="S-t-i-f-U-i-cdot-S-t-i-1-W-i-cdot-S-t-1"><a href="#S-t-i-f-U-i-cdot-S-t-i-1-W-i-cdot-S-t-1" class="headerlink" title="$S_t^{(i)}=f(U^{(i)}\cdot S_t^{(i-1)}+W^{(i)}\cdot S_{t-1})$"></a>$S_t^{(i)}=f(U^{(i)}\cdot S_t^{(i-1)}+W^{(i)}\cdot S_{t-1})$</h4></li><li><h4 id="S-t’-i-f-U’-i-cdot-S-t’-i-1-W’-i-cdot-S-t-1-’"><a href="#S-t’-i-f-U’-i-cdot-S-t’-i-1-W’-i-cdot-S-t-1-’" class="headerlink" title="$S_t’^{(i)}=f(U’^{(i)}\cdot S_t’^{(i-1)}+W’^{(i)}\cdot S_{t+1}’)$"></a>$S_t’^{(i)}=f(U’^{(i)}\cdot S_t’^{(i-1)}+W’^{(i)}\cdot S_{t+1}’)$</h4></li><li><p>$…$</p></li><li><h4 id="S-t-1-f-U-1-cdot-X-t-W-1-cdot-S-t-1"><a href="#S-t-1-f-U-1-cdot-X-t-W-1-cdot-S-t-1" class="headerlink" title="$S_t^{(1)}=f(U^{(1)} \cdot X_t+W^{(1)}\cdot S_{t-1})$"></a>$S_t^{(1)}=f(U^{(1)} \cdot X_t+W^{(1)}\cdot S_{t-1})$</h4></li><li><h4 id="S-t’-1-f-U’-1-cdot-X-t-W’-1-cdot-S-t-1-’"><a href="#S-t’-1-f-U’-1-cdot-X-t-W’-1-cdot-S-t-1-’" class="headerlink" title="$S_t’^{(1)}=f(U’^{(1)}\cdot X_t+W’^{(1)}\cdot S_{t+1}’)$"></a>$S_t’^{(1)}=f(U’^{(1)}\cdot X_t+W’^{(1)}\cdot S_{t+1}’)$</h4></li></ul><h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>递归神经网络的形式很自由：不同的递归神经网络可以实现不同的功能</p><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20200601182346.png" srcset="/img/loading.gif" alt="images"></p><p>从上图我们可以总结出：</p><ul><li><code>one to one</code>：一个输入（单一标签）对应一个输出（单一标签）</li><li><code>one to many</code>：一个输入对应多个输出，即这个架构多用于图片的对象识别，即输入一个图片，输出一个文本序列。</li><li><code>many to one</code>：  多个输入对应一个输出，多用于文本分类或视频分类，即输入一段文本或视频片段，输出类别。</li><li><code>many to many</code>：这种结构广泛的用于机器翻译，输入一个文本，输出另一种语言的文本。</li><li><code>many to many</code>：这种广泛的用于序列标注。</li></ul><p>在众多的深度学习网络中，RNN由于能够接收序列输入，也能得到序列输出，在自然语言处理中取得了巨大的成功，并得到广泛的应用。</p>]]></content>
    
    
    <categories>
      
      <category>深度学习</category>
      
    </categories>
    
    
    <tags>
      
      <tag>深度学习</tag>
      
      <tag>神经网络</tag>
      
      <tag>LSTM</tag>
      
      <tag>循环神经网络</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>TensorFlow-keras之CNN实现手写数字识别</title>
    <link href="/2018/08/TensorFlow-keras/"/>
    <url>/2018/08/TensorFlow-keras/</url>
    
    <content type="html"><![CDATA[<h3 id="TensorFlow四种写法之四：keras"><a href="#TensorFlow四种写法之四：keras" class="headerlink" title="TensorFlow四种写法之四：keras"></a>TensorFlow四种写法之四：keras</h3><h4 id="代码："><a href="#代码：" class="headerlink" title="代码："></a>代码：</h4><pre><code class="hljs python"><span class="hljs-comment">#!/usr/bin/env python</span><span class="hljs-comment"># -*- coding: utf-8 -*-</span><span class="hljs-comment"># @Time    : 2018/8/23 19:14</span><span class="hljs-comment"># @Author  : Seven</span><span class="hljs-comment"># @Site    : </span><span class="hljs-comment"># @File    : CNN-keras.py</span><span class="hljs-comment"># @Software: PyCharm</span><span class="hljs-comment"># 0.导入环境</span><span class="hljs-keyword">import</span> os<span class="hljs-keyword">from</span> tensorflow.examples.tutorials.mnist <span class="hljs-keyword">import</span> input_data<span class="hljs-keyword">import</span> tensorflow <span class="hljs-keyword">as</span> tf<span class="hljs-keyword">from</span> keras.layers.core <span class="hljs-keyword">import</span> Dense, Flatten<span class="hljs-keyword">from</span> keras.layers.convolutional <span class="hljs-keyword">import</span> Conv2D<span class="hljs-keyword">from</span> keras.layers.pooling <span class="hljs-keyword">import</span> MaxPooling2D<span class="hljs-keyword">from</span> keras.objectives <span class="hljs-keyword">import</span> categorical_crossentropy<span class="hljs-keyword">from</span> keras <span class="hljs-keyword">import</span> backend <span class="hljs-keyword">as</span> KK.image_data_format()os.environ[<span class="hljs-string">'TF_CPP_MIN_LOG_LEVEL'</span>] = <span class="hljs-string">'2'</span><span class="hljs-comment"># 1.数据准备</span><span class="hljs-comment"># 使用tensorflow自带的工具加载MNIST手写数字集合</span>mnist = input_data.read_data_sets(<span class="hljs-string">'data'</span>, one_hot=<span class="hljs-literal">True</span>)<span class="hljs-comment"># 查看数据的维度和target的维度</span>print(mnist.train.images.shape)print(mnist.train.labels.shape)<span class="hljs-comment"># 2.准备好palceholder</span>x = tf.placeholder(tf.float32, [<span class="hljs-literal">None</span>, <span class="hljs-number">784</span>])y = tf.placeholder(tf.float32, [<span class="hljs-literal">None</span>, <span class="hljs-number">10</span>])learnRate = tf.placeholder(tf.float32)<span class="hljs-comment"># 3.构建网络计算图结构</span><span class="hljs-comment"># 把输入数据reshape--28x28=784, 单通道， -1表示None</span><span class="hljs-keyword">with</span> tf.name_scope(<span class="hljs-string">'reshape'</span>):    x_image = tf.reshape(x, [<span class="hljs-number">-1</span>, <span class="hljs-number">28</span>, <span class="hljs-number">28</span>, <span class="hljs-number">1</span>])<span class="hljs-comment"># 构建第一层卷积计算层--将一个灰度图像映射到32个feature maps, 卷积核为5x5</span>net = Conv2D(<span class="hljs-number">32</span>, kernel_size=[<span class="hljs-number">5</span>, <span class="hljs-number">5</span>], strides=[<span class="hljs-number">1</span>, <span class="hljs-number">1</span>],             activation=<span class="hljs-string">'relu'</span>, padding=<span class="hljs-string">'same'</span>,             input_shape=[<span class="hljs-number">28</span>, <span class="hljs-number">28</span>, <span class="hljs-number">1</span>])(x_image)<span class="hljs-comment"># 构建池化层--采用最大池化</span>net = MaxPooling2D(pool_size=[<span class="hljs-number">2</span>, <span class="hljs-number">2</span>])(net)<span class="hljs-comment"># 构建第二层卷积计算层--maps 32 feature maps to 64.</span>net = Conv2D(<span class="hljs-number">64</span>, kernel_size=[<span class="hljs-number">5</span>, <span class="hljs-number">5</span>], strides=[<span class="hljs-number">1</span>, <span class="hljs-number">1</span>],             activation=<span class="hljs-string">'relu'</span>, padding=<span class="hljs-string">'same'</span>)(net)<span class="hljs-comment"># 构建第二层池化层--采用最大池化</span>net = MaxPooling2D(pool_size=[<span class="hljs-number">2</span>, <span class="hljs-number">2</span>])(net)<span class="hljs-comment"># 构建全连接层--经过的两层的下采样（池化），28x28x1的图像--&gt;7x7x64，然后映射到1024个特征</span>net = Flatten()(net)net = Dense(<span class="hljs-number">1024</span>, activation=<span class="hljs-string">'relu'</span>)(net)<span class="hljs-comment"># 构建第二层全连接层--将1024个特性映射到10个类，每个类对应一个数字</span>net = Dense(<span class="hljs-number">10</span>, activation=<span class="hljs-string">'softmax'</span>)(net)<span class="hljs-comment"># 4.计算损失值并初始化optimizer</span>cross_entropy = tf.reduce_mean(categorical_crossentropy(y, net))l2_loss = tf.add_n([tf.nn.l2_loss(w) <span class="hljs-keyword">for</span> w <span class="hljs-keyword">in</span> tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES)])total_loss = cross_entropy + <span class="hljs-number">7e-5</span>*l2_losstrain_step = tf.train.AdamOptimizer(learnRate).minimize(total_loss)<span class="hljs-comment"># 5.初始化变量</span>init = tf.global_variables_initializer()print(<span class="hljs-string">"FUNCTION READY!!"</span>)<span class="hljs-comment"># 6.在会话中执行网络定义的运算</span><span class="hljs-keyword">with</span> tf.Session() <span class="hljs-keyword">as</span> sess:    sess.run(init)    <span class="hljs-keyword">for</span> step <span class="hljs-keyword">in</span> range(<span class="hljs-number">3000</span>):        batch_xs, batch_ys = mnist.train.next_batch(<span class="hljs-number">100</span>)        lr = <span class="hljs-number">0.01</span>        _, loss, l2_loss_value, total_loss_value = sess.run(            [train_step, cross_entropy, l2_loss, total_loss],            feed_dict=&#123;x: batch_xs, y: batch_ys, learnRate: lr&#125;)        <span class="hljs-keyword">if</span> (step + <span class="hljs-number">1</span>) % <span class="hljs-number">100</span> == <span class="hljs-number">0</span>:            print(<span class="hljs-string">"step %d, entropy loss: %f, l2_loss: %f, total loss: %f"</span> %                  (step + <span class="hljs-number">1</span>, loss, l2_loss_value, total_loss_value))            <span class="hljs-comment"># 验证训练的模型</span>            correct_prediction = tf.equal(tf.argmax(net, <span class="hljs-number">1</span>), tf.argmax(y, <span class="hljs-number">1</span>))            accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))            print(<span class="hljs-string">"Train accuracy:"</span>, sess.run(accuracy, feed_dict=&#123;x: batch_xs, y: batch_ys&#125;))        <span class="hljs-keyword">if</span> (step + <span class="hljs-number">1</span>) % <span class="hljs-number">1000</span> == <span class="hljs-number">0</span>:            print(<span class="hljs-string">"Text accuracy:"</span>, sess.run(accuracy, feed_dict=&#123;x: batch_xs, y: batch_ys&#125;))</code></pre>]]></content>
    
    
    <categories>
      
      <category>TensorFlow</category>
      
    </categories>
    
    
    <tags>
      
      <tag>深度学习</tag>
      
      <tag>卷积神经网络</tag>
      
      <tag>TensorFlow</tag>
      
      <tag>keras</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>TensorFlow-Layer之CNN实现手写数字识别</title>
    <link href="/2018/08/TensorFlow-layer/"/>
    <url>/2018/08/TensorFlow-layer/</url>
    
    <content type="html"><![CDATA[<h3 id="TensorFlow四种写法之二：layer"><a href="#TensorFlow四种写法之二：layer" class="headerlink" title="TensorFlow四种写法之二：layer"></a>TensorFlow四种写法之二：layer</h3><h4 id="代码："><a href="#代码：" class="headerlink" title="代码："></a>代码：</h4><pre><code class="hljs python"><span class="hljs-comment">#!/usr/bin/env python</span><span class="hljs-comment"># -*- coding: utf-8 -*-</span><span class="hljs-comment"># @Time    : 2018/8/22 21:01</span><span class="hljs-comment"># @Author  : Seven</span><span class="hljs-comment"># @Site    : </span><span class="hljs-comment"># @File    : CNN-layers.py</span><span class="hljs-comment"># @Software: PyCharm</span><span class="hljs-comment"># 0.导入环境</span><span class="hljs-keyword">from</span> tensorflow.examples.tutorials.mnist <span class="hljs-keyword">import</span> input_data<span class="hljs-keyword">import</span> tensorflow <span class="hljs-keyword">as</span> tf<span class="hljs-keyword">import</span> osos.environ[<span class="hljs-string">'TF_CPP_MIN_LOG_LEVEL'</span>] = <span class="hljs-string">'2'</span><span class="hljs-comment"># 1.数据准备</span><span class="hljs-comment"># 使用tensorflow自带的工具加载MNIST手写数字集合</span>mnist = input_data.read_data_sets(<span class="hljs-string">'data'</span>, one_hot=<span class="hljs-literal">True</span>)<span class="hljs-comment"># 查看数据的维度和target的维度</span>print(mnist.train.images.shape)print(mnist.train.labels.shape)<span class="hljs-comment"># 2.准备好palceholder</span>x = tf.placeholder(tf.float32, [<span class="hljs-literal">None</span>, <span class="hljs-number">784</span>])y = tf.placeholder(tf.float32, [<span class="hljs-literal">None</span>, <span class="hljs-number">10</span>])learnRate = tf.placeholder(tf.float32)<span class="hljs-comment"># 3.构建网络计算图结构</span><span class="hljs-comment"># 把输入数据reshape--28x28=784, 单通道， -1表示None</span><span class="hljs-keyword">with</span> tf.name_scope(<span class="hljs-string">'reshape'</span>):    x_image = tf.reshape(x, [<span class="hljs-number">-1</span>, <span class="hljs-number">28</span>, <span class="hljs-number">28</span>, <span class="hljs-number">1</span>])<span class="hljs-comment"># 构建第一层卷积计算层--将一个灰度图像映射到32个feature maps, 卷积核为5x5</span><span class="hljs-keyword">with</span> tf.name_scope(<span class="hljs-string">'conv1'</span>):    h_conv1 = tf.layers.conv2d(x_image, <span class="hljs-number">32</span>, [<span class="hljs-number">5</span>, <span class="hljs-number">5</span>], padding=<span class="hljs-string">'SAME'</span>, activation=tf.nn.relu)<span class="hljs-comment"># 构建池化层--采用最大池化</span><span class="hljs-keyword">with</span> tf.name_scope(<span class="hljs-string">'pool1'</span>):    h_pool1 = tf.layers.max_pooling2d(h_conv1, pool_size=[<span class="hljs-number">2</span>, <span class="hljs-number">2</span>], strides=[<span class="hljs-number">2</span>, <span class="hljs-number">2</span>], padding=<span class="hljs-string">'VALID'</span>)<span class="hljs-comment"># 构建第二层卷积计算层--maps 32 feature maps to 64.</span><span class="hljs-keyword">with</span> tf.name_scope(<span class="hljs-string">'conv2'</span>):    h_conv2 = tf.layers.conv2d(h_pool1, <span class="hljs-number">64</span>, [<span class="hljs-number">5</span>, <span class="hljs-number">5</span>], padding=<span class="hljs-string">'SAME'</span>, activation=tf.nn.relu)<span class="hljs-comment"># 构建第二个池化层</span><span class="hljs-keyword">with</span> tf.name_scope(<span class="hljs-string">'pool2'</span>):    h_pool2 = tf.layers.max_pooling2d(h_conv2, pool_size=[<span class="hljs-number">2</span>, <span class="hljs-number">2</span>], strides=[<span class="hljs-number">2</span>, <span class="hljs-number">2</span>], padding=<span class="hljs-string">'VALID'</span>)<span class="hljs-comment"># 构建全连接层--经过的两层的下采样（池化），28x28x1的图像--&gt;7x7x64，然后映射到1024个特征</span><span class="hljs-keyword">with</span> tf.name_scope(<span class="hljs-string">'fc1'</span>):    h_pool2_flat = tf.layers.flatten(h_pool2)    h_fc1 = tf.layers.dense(h_pool2_flat, <span class="hljs-number">1024</span>, activation=tf.nn.relu)<span class="hljs-comment"># Dropout--防止过拟合</span><span class="hljs-keyword">with</span> tf.name_scope(<span class="hljs-string">'dropout'</span>):    keep_prob = tf.placeholder(tf.float32)    h_fc_drop = tf.nn.dropout(h_fc1, keep_prob=keep_prob)<span class="hljs-comment"># 构建全连接层--将1024个特性映射到10个类，每个类对应一个数字</span><span class="hljs-keyword">with</span> tf.name_scope(<span class="hljs-string">'fc2'</span>):    out = tf.layers.dense(h_fc_drop, <span class="hljs-number">10</span>, activation=<span class="hljs-literal">None</span>)<span class="hljs-comment"># 4.计算损失值并初始化optimizer</span>print(y.shape, out.shape)cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y, logits=out))l2_loss = tf.add_n([tf.nn.l2_loss(w) <span class="hljs-keyword">for</span> w <span class="hljs-keyword">in</span> tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES)])total_loss = cross_entropy + <span class="hljs-number">7e-5</span>*l2_losstrain_step = tf.train.AdamOptimizer(learnRate).minimize(total_loss)<span class="hljs-comment"># 5.初始化变量</span>init = tf.global_variables_initializer()<span class="hljs-comment"># 6.在会话中执行网络定义的运算</span><span class="hljs-keyword">with</span> tf.Session() <span class="hljs-keyword">as</span> sess:    sess.run(init)    <span class="hljs-keyword">for</span> step <span class="hljs-keyword">in</span> range(<span class="hljs-number">3000</span>):        batch_xs, batch_ys = mnist.train.next_batch(<span class="hljs-number">100</span>)        lr = <span class="hljs-number">0.01</span>        _, loss, l2_loss_value, total_loss_value = sess.run(            [train_step, cross_entropy, l2_loss, total_loss],            feed_dict=&#123;x: batch_xs, y: batch_ys, learnRate: lr, keep_prob: <span class="hljs-number">0.5</span>&#125;)        <span class="hljs-keyword">if</span> (step+<span class="hljs-number">1</span>) % <span class="hljs-number">100</span> == <span class="hljs-number">0</span>:            print(<span class="hljs-string">"step %d, entropy loss: %f, l2_loss: %f, total loss: %f"</span> %                  (step+<span class="hljs-number">1</span>, loss, l2_loss_value, total_loss_value))            <span class="hljs-comment"># 验证训练的模型</span>            correct_prediction = tf.equal(tf.argmax(out, <span class="hljs-number">1</span>), tf.argmax(y, <span class="hljs-number">1</span>))            accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))            print(<span class="hljs-string">"Train accuracy:"</span>, sess.run(accuracy, feed_dict=&#123;x: batch_xs, y: batch_ys, keep_prob:<span class="hljs-number">0.5</span>&#125;))        <span class="hljs-keyword">if</span> (step + <span class="hljs-number">1</span>) % <span class="hljs-number">1000</span> == <span class="hljs-number">0</span>:            print(<span class="hljs-string">"Text accuracy:"</span>, sess.run(accuracy, feed_dict=&#123;x: batch_xs, y: batch_ys, keep_prob: <span class="hljs-number">0.5</span>&#125;))</code></pre>]]></content>
    
    
    <categories>
      
      <category>TensorFlow</category>
      
    </categories>
    
    
    <tags>
      
      <tag>深度学习</tag>
      
      <tag>卷积神经网络</tag>
      
      <tag>TensorFlow</tag>
      
      <tag>Layer</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>TensorFlow-原生写法之CNN实现手写数字识别</title>
    <link href="/2018/08/TensorFlow-native/"/>
    <url>/2018/08/TensorFlow-native/</url>
    
    <content type="html"><![CDATA[<h3 id="TensorFlow四种写法之一：原生写法"><a href="#TensorFlow四种写法之一：原生写法" class="headerlink" title="TensorFlow四种写法之一：原生写法"></a>TensorFlow四种写法之一：原生写法</h3><h4 id="代码："><a href="#代码：" class="headerlink" title="代码："></a>代码：</h4><pre><code class="hljs python"><span class="hljs-comment">#!/usr/bin/env python</span><span class="hljs-comment"># -*- coding: utf-8 -*-</span><span class="hljs-comment"># @Time    : 2018/8/23 16:49</span><span class="hljs-comment"># @Author  : Seven</span><span class="hljs-comment"># @Site    : </span><span class="hljs-comment"># @File    : CNN-native.py</span><span class="hljs-comment"># @Software: PyCharm</span><span class="hljs-comment"># 0.导入环境</span><span class="hljs-keyword">from</span> tensorflow.examples.tutorials.mnist <span class="hljs-keyword">import</span> input_data<span class="hljs-keyword">import</span> tensorflow <span class="hljs-keyword">as</span> tf<span class="hljs-keyword">import</span> osos.environ[<span class="hljs-string">'TF_CPP_MIN_LOG_LEVEL'</span>] = <span class="hljs-string">'2'</span><span class="hljs-comment"># 1.数据准备</span><span class="hljs-comment"># 使用tensorflow自带的工具加载MNIST手写数字集合</span>mnist = input_data.read_data_sets(<span class="hljs-string">'data'</span>, one_hot=<span class="hljs-literal">True</span>)<span class="hljs-comment"># 查看数据的维度和target的维度</span>print(mnist.train.images.shape)print(mnist.train.labels.shape)<span class="hljs-comment"># 2.准备好palceholder</span>x = tf.placeholder(tf.float32, [<span class="hljs-literal">None</span>, <span class="hljs-number">784</span>])y = tf.placeholder(tf.float32, [<span class="hljs-literal">None</span>, <span class="hljs-number">10</span>])learnRate = tf.placeholder(tf.float32)<span class="hljs-comment"># 3.构建网络计算图结构</span><span class="hljs-comment"># 把输入数据reshape--28x28=784, 单通道， -1表示None</span><span class="hljs-keyword">with</span> tf.name_scope(<span class="hljs-string">'reshape'</span>):    x_image = tf.reshape(x, [<span class="hljs-number">-1</span>, <span class="hljs-number">28</span>, <span class="hljs-number">28</span>, <span class="hljs-number">1</span>])<span class="hljs-comment"># 构建第一层卷积计算层--将一个灰度图像映射到32个feature maps, 卷积核为5x5</span><span class="hljs-keyword">with</span> tf.name_scope(<span class="hljs-string">'conv1'</span>):    shape = [<span class="hljs-number">5</span>, <span class="hljs-number">5</span>, <span class="hljs-number">1</span>, <span class="hljs-number">32</span>]    W_conv1 = tf.Variable(tf.truncated_normal(shape=shape, stddev=<span class="hljs-number">0.1</span>),                          collections=[tf.GraphKeys.GLOBAL_VARIABLES, <span class="hljs-string">'WEIGHTS'</span>])    shape = [<span class="hljs-number">32</span>]    b_conv1 = tf.Variable(tf.constant(<span class="hljs-number">0.1</span>, shape=shape))    net_conv1 = tf.nn.conv2d(x_image, W_conv1, strides=[<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>],                             padding=<span class="hljs-string">'SAME'</span>) + b_conv1    out_conv1 = tf.nn.relu(net_conv1)<span class="hljs-comment"># 构建池化层--采用最大池化</span><span class="hljs-keyword">with</span> tf.name_scope(<span class="hljs-string">'pool1'</span>):    h_pool1 = tf.nn.max_pool(out_conv1, ksize=[<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">2</span>, <span class="hljs-number">1</span>], strides=[<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">2</span>, <span class="hljs-number">1</span>],                             padding=<span class="hljs-string">'VALID'</span>)<span class="hljs-comment"># 构建第二层卷积计算层--maps 32 feature maps to 64.</span><span class="hljs-keyword">with</span> tf.name_scope(<span class="hljs-string">'conv2'</span>):    shape = [<span class="hljs-number">5</span>, <span class="hljs-number">5</span>, <span class="hljs-number">32</span>, <span class="hljs-number">64</span>]    W_conv2 = tf.Variable(tf.truncated_normal(shape=shape, stddev=<span class="hljs-number">0.1</span>),                          collections=[tf.GraphKeys.GLOBAL_VARIABLES, <span class="hljs-string">'WEIGHTS'</span>])    shape = [<span class="hljs-number">64</span>]    b_conv2 = tf.Variable(tf.constant(<span class="hljs-number">0.1</span>, shape=shape))    net_conv2 = tf.nn.conv2d(h_pool1, W_conv2, strides=[<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>],                             padding=<span class="hljs-string">'SAME'</span>) + b_conv2    out_conv2 = tf.nn.relu(net_conv2)<span class="hljs-comment"># 构建第二层池化层--采用最大池化</span><span class="hljs-keyword">with</span> tf.name_scope(<span class="hljs-string">'pool2'</span>):    h_pool2 = tf.nn.max_pool(out_conv2, ksize=[<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">2</span>, <span class="hljs-number">1</span>], strides=[<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">2</span>, <span class="hljs-number">1</span>],                             padding=<span class="hljs-string">'VALID'</span>)<span class="hljs-comment"># 构建全连接层--经过的两层的下采样（池化），28x28x1的图像--&gt;7x7x64，然后映射到1024个特征</span><span class="hljs-keyword">with</span> tf.name_scope(<span class="hljs-string">'fc1'</span>):    shape = [<span class="hljs-number">7</span>*<span class="hljs-number">7</span>*<span class="hljs-number">64</span>, <span class="hljs-number">1024</span>]    W_fc1 = tf.Variable(tf.truncated_normal(shape=shape, stddev=<span class="hljs-number">0.1</span>),                        collections=[tf.GraphKeys.GLOBAL_VARIABLES, <span class="hljs-string">'WEIGHTS'</span>])    shape = [<span class="hljs-number">1024</span>]    b_fc1 = tf.Variable(tf.constant(<span class="hljs-number">0.1</span>, shape=shape))    shape = [<span class="hljs-number">-1</span>, <span class="hljs-number">7</span>*<span class="hljs-number">7</span>*<span class="hljs-number">64</span>]    h_pool2_flat = tf.reshape(h_pool2, shape)    out_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, W_fc1) + b_fc1)<span class="hljs-comment"># Dropout--防止过拟合</span><span class="hljs-keyword">with</span> tf.name_scope(<span class="hljs-string">'dropout'</span>):    keep_prob = tf.placeholder(tf.float32)    out_fc1_drop = tf.nn.dropout(out_fc1, keep_prob=keep_prob)<span class="hljs-comment"># 构建第二层全连接层--将1024个特性映射到10个类，每个类对应一个数字</span><span class="hljs-keyword">with</span> tf.name_scope(<span class="hljs-string">'fc2'</span>):    shape = [<span class="hljs-number">1024</span>, <span class="hljs-number">10</span>]    W_fc2 = tf.Variable(tf.truncated_normal(shape=shape, stddev=<span class="hljs-number">0.1</span>),                        collections=[tf.GraphKeys.GLOBAL_VARIABLES, <span class="hljs-string">'WEIGHTS'</span>])    shape = [<span class="hljs-number">10</span>]    b_fc2 = tf.Variable(tf.constant(<span class="hljs-number">0.1</span>, shape=shape))    out = tf.matmul(out_fc1_drop, W_fc2) + b_fc2<span class="hljs-comment"># 4.计算损失值并初始化optimizer</span>print(y.shape, out.shape)cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y, logits=out))l2_loss = tf.add_n([tf.nn.l2_loss(w) <span class="hljs-keyword">for</span> w <span class="hljs-keyword">in</span> tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES)])total_loss = cross_entropy + <span class="hljs-number">7e-5</span>*l2_losstrain_step = tf.train.AdamOptimizer(learnRate).minimize(total_loss)<span class="hljs-comment"># 5.初始化变量</span>init = tf.global_variables_initializer()print(<span class="hljs-string">"FUNCTION READY!!"</span>)<span class="hljs-comment"># 6.在会话中执行网络定义的运算</span><span class="hljs-keyword">with</span> tf.Session() <span class="hljs-keyword">as</span> sess:    sess.run(init)    <span class="hljs-keyword">for</span> step <span class="hljs-keyword">in</span> range(<span class="hljs-number">3000</span>):        batch_xs, batch_ys = mnist.train.next_batch(<span class="hljs-number">100</span>)        lr = <span class="hljs-number">0.01</span>        _, loss, l2_loss_value, total_loss_value = sess.run(            [train_step, cross_entropy, l2_loss, total_loss],            feed_dict=&#123;x: batch_xs, y: batch_ys, learnRate: lr, keep_prob: <span class="hljs-number">0.5</span>&#125;)        <span class="hljs-keyword">if</span> (step + <span class="hljs-number">1</span>) % <span class="hljs-number">100</span> == <span class="hljs-number">0</span>:            print(<span class="hljs-string">"step %d, entropy loss: %f, l2_loss: %f, total loss: %f"</span> %                  (step + <span class="hljs-number">1</span>, loss, l2_loss_value, total_loss_value))            <span class="hljs-comment"># 验证训练的模型</span>            correct_prediction = tf.equal(tf.argmax(out, <span class="hljs-number">1</span>), tf.argmax(y, <span class="hljs-number">1</span>))            accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))            print(<span class="hljs-string">"Train accuracy:"</span>, sess.run(accuracy, feed_dict=&#123;x: batch_xs, y: batch_ys, keep_prob: <span class="hljs-number">0.5</span>&#125;))        <span class="hljs-keyword">if</span> (step + <span class="hljs-number">1</span>) % <span class="hljs-number">1000</span> == <span class="hljs-number">0</span>:            print(<span class="hljs-string">"Text accuracy:"</span>, sess.run(accuracy, feed_dict=&#123;x: batch_xs, y: batch_ys, keep_prob: <span class="hljs-number">0.5</span>&#125;))</code></pre>]]></content>
    
    
    <categories>
      
      <category>TensorFlow</category>
      
    </categories>
    
    
    <tags>
      
      <tag>深度学习</tag>
      
      <tag>卷积神经网络</tag>
      
      <tag>TensorFlow</tag>
      
      <tag>原生写法</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>TensorFlow-slim之CNN实现手写数字识别</title>
    <link href="/2018/08/TensorFlow-slim/"/>
    <url>/2018/08/TensorFlow-slim/</url>
    
    <content type="html"><![CDATA[<h3 id="TensorFlow四种写法之三：Slim"><a href="#TensorFlow四种写法之三：Slim" class="headerlink" title="TensorFlow四种写法之三：Slim"></a>TensorFlow四种写法之三：Slim</h3><h4 id="代码："><a href="#代码：" class="headerlink" title="代码："></a>代码：</h4><pre><code class="hljs python"><span class="hljs-comment">#!/usr/bin/env python</span><span class="hljs-comment"># -*- coding: utf-8 -*-</span><span class="hljs-comment"># @Time    : 2018/8/23 19:40</span><span class="hljs-comment"># @Author  : Seven</span><span class="hljs-comment"># @Site    : </span><span class="hljs-comment"># @File    : CNN-slim.py</span><span class="hljs-comment"># @Software: PyCharm</span><span class="hljs-comment"># 0.导入环境</span><span class="hljs-keyword">from</span> tensorflow.examples.tutorials.mnist <span class="hljs-keyword">import</span> input_data<span class="hljs-keyword">import</span> tensorflow <span class="hljs-keyword">as</span> tf<span class="hljs-keyword">import</span> osos.environ[<span class="hljs-string">'TF_CPP_MIN_LOG_LEVEL'</span>] = <span class="hljs-string">'2'</span><span class="hljs-comment"># 1.数据准备</span><span class="hljs-comment"># 使用tensorflow自带的工具加载MNIST手写数字集合</span>mnist = input_data.read_data_sets(<span class="hljs-string">'data'</span>, one_hot=<span class="hljs-literal">True</span>)<span class="hljs-comment"># 查看数据的维度和target的维度</span>print(mnist.train.images.shape)print(mnist.train.labels.shape)<span class="hljs-comment"># 2.准备好palceholder</span>x = tf.placeholder(tf.float32, [<span class="hljs-literal">None</span>, <span class="hljs-number">784</span>])y = tf.placeholder(tf.float32, [<span class="hljs-literal">None</span>, <span class="hljs-number">10</span>])learnRate = tf.placeholder(tf.float32)<span class="hljs-comment"># 3.构建网络计算图结构</span><span class="hljs-comment"># 把输入数据reshape--28x28=784, 单通道， -1表示None</span><span class="hljs-keyword">with</span> tf.name_scope(<span class="hljs-string">'reshape'</span>):    x_image = tf.reshape(x, [<span class="hljs-number">-1</span>, <span class="hljs-number">28</span>, <span class="hljs-number">28</span>, <span class="hljs-number">1</span>])    print(x_image.shape)<span class="hljs-comment"># 构建第一层卷积计算层--将一个灰度图像映射到32个feature maps, 卷积核为5x5</span><span class="hljs-keyword">with</span> tf.name_scope(<span class="hljs-string">'conv1'</span>):    h_conv1 = tf.contrib.slim.conv2d(x_image, <span class="hljs-number">32</span>, [<span class="hljs-number">5</span>, <span class="hljs-number">5</span>],                                     padding=<span class="hljs-string">'SAME'</span>,                                     activation_fn=tf.nn.relu)    print(h_conv1.shape)<span class="hljs-comment"># 构建池化层--采用最大池化</span><span class="hljs-keyword">with</span> tf.name_scope(<span class="hljs-string">'pool1'</span>):    h_pool1 = tf.contrib.slim.max_pool2d(h_conv1, [<span class="hljs-number">2</span>, <span class="hljs-number">2</span>], stride=<span class="hljs-number">2</span>, padding=<span class="hljs-string">'VALID'</span>)    print(h_pool1.shape)<span class="hljs-comment"># 构建第二层卷积计算层--maps 32 feature maps to 64.</span><span class="hljs-keyword">with</span> tf.name_scope(<span class="hljs-string">'conv2'</span>):    h_conv2 = tf.contrib.slim.conv2d(h_pool1, <span class="hljs-number">64</span>, [<span class="hljs-number">5</span>, <span class="hljs-number">5</span>],                                     padding=<span class="hljs-string">'SAME'</span>,                                     activation_fn=tf.nn.relu)    print(h_conv2.shape)<span class="hljs-comment"># 构建第二个池化层</span><span class="hljs-keyword">with</span> tf.name_scope(<span class="hljs-string">'pool2'</span>):    h_pool2 = tf.contrib.slim.max_pool2d(h_conv2, [<span class="hljs-number">2</span>, <span class="hljs-number">2</span>], stride=[<span class="hljs-number">2</span>, <span class="hljs-number">2</span>], padding=<span class="hljs-string">'VALID'</span>)    print(h_pool2.shape)<span class="hljs-comment"># 构建全连接层--经过的两层的下采样（池化），28x28x1的图像--&gt;7x7x64，然后映射到1024个特征</span><span class="hljs-keyword">with</span> tf.name_scope(<span class="hljs-string">'fc1'</span>):    h_pool2_flat = tf.contrib.slim.avg_pool2d(h_pool2, h_pool2.shape[<span class="hljs-number">1</span>:<span class="hljs-number">3</span>],                                              stride=[<span class="hljs-number">1</span>, <span class="hljs-number">1</span>], padding=<span class="hljs-string">'VALID'</span>)    h_fc1 = tf.contrib.slim.conv2d(h_pool2_flat, <span class="hljs-number">1024</span>, [<span class="hljs-number">1</span>, <span class="hljs-number">1</span>],                                   activation_fn=tf.nn.relu)    print(h_fc1.shape)<span class="hljs-comment"># Dropout--防止过拟合</span><span class="hljs-keyword">with</span> tf.name_scope(<span class="hljs-string">'dropout'</span>):    keep_prob = tf.placeholder(tf.float32)    h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob=keep_prob)<span class="hljs-comment"># 构建全连接层--将1024个特性映射到10个类，每个类对应一个数字</span><span class="hljs-keyword">with</span> tf.name_scope(<span class="hljs-string">'fc2'</span>):    out = tf.squeeze(tf.contrib.slim.conv2d(h_fc1_drop, <span class="hljs-number">10</span>, [<span class="hljs-number">1</span>, <span class="hljs-number">1</span>], activation_fn=<span class="hljs-literal">None</span>))    print(out.shape)<span class="hljs-comment"># 4.计算损失值并初始化optimizer</span>cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y, logits=out))l2_loss = tf.add_n([tf.nn.l2_loss(w) <span class="hljs-keyword">for</span> w <span class="hljs-keyword">in</span> tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES)])total_loss = cross_entropy + <span class="hljs-number">7e-5</span>*l2_losstrain_step = tf.train.AdamOptimizer(learnRate).minimize(total_loss)<span class="hljs-comment"># 5.初始化变量</span>init = tf.global_variables_initializer()<span class="hljs-comment"># 6.在会话中执行网络定义的运算</span><span class="hljs-keyword">with</span> tf.Session() <span class="hljs-keyword">as</span> sess:    sess.run(init)    <span class="hljs-keyword">for</span> step <span class="hljs-keyword">in</span> range(<span class="hljs-number">3000</span>):        batch_xs, batch_ys = mnist.train.next_batch(<span class="hljs-number">100</span>)        lr = <span class="hljs-number">0.01</span>        _, loss, l2_loss_value, total_loss_value = sess.run(            [train_step, cross_entropy, l2_loss, total_loss],            feed_dict=&#123;x: batch_xs, y: batch_ys, learnRate: lr, keep_prob: <span class="hljs-number">0.5</span>&#125;)        <span class="hljs-keyword">if</span> (step+<span class="hljs-number">1</span>) % <span class="hljs-number">100</span> == <span class="hljs-number">0</span>:            print(<span class="hljs-string">"step %d, entropy loss: %f, l2_loss: %f, total loss: %f"</span> %                  (step+<span class="hljs-number">1</span>, loss, l2_loss_value, total_loss_value))            <span class="hljs-comment"># 验证训练的模型</span>            correct_prediction = tf.equal(tf.argmax(out, <span class="hljs-number">1</span>), tf.argmax(y, <span class="hljs-number">1</span>))            accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))            print(<span class="hljs-string">"Train accuracy:"</span>, sess.run(accuracy, feed_dict=&#123;x: batch_xs, y: batch_ys, keep_prob:<span class="hljs-number">0.5</span>&#125;))        <span class="hljs-keyword">if</span> (step + <span class="hljs-number">1</span>) % <span class="hljs-number">1000</span> == <span class="hljs-number">0</span>:            print(<span class="hljs-string">"Text accuracy:"</span>, sess.run(accuracy, feed_dict=&#123;x: batch_xs, y: batch_ys, keep_prob: <span class="hljs-number">0.5</span>&#125;))</code></pre>]]></content>
    
    
    <categories>
      
      <category>TensorFlow</category>
      
    </categories>
    
    
    <tags>
      
      <tag>深度学习</tag>
      
      <tag>卷积神经网络</tag>
      
      <tag>TensorFlow</tag>
      
      <tag>slim</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>TensorFlow实现卷积神经网络-AlexNet</title>
    <link href="/2018/08/DL-CNN-AlexNet/"/>
    <url>/2018/08/DL-CNN-AlexNet/</url>
    
    <content type="html"><![CDATA[<h3 id="AlexNet背景"><a href="#AlexNet背景" class="headerlink" title="AlexNet背景"></a>AlexNet背景</h3><p>AlexNet是在2012年被发表的一个经典之作，并在当年取得了ImageNet最好成绩，也是在那年之后，更多的更深的神经网路被提出，比如优秀的vgg,GoogleLeNet.</p><p>其官方提供的数据模型，准确率达到57.1%,top 1-5 达到80.2%. 这项对于传统的机器学习分类算法而言，已经相当的出色。</p><h3 id="框架介绍"><a href="#框架介绍" class="headerlink" title="框架介绍:"></a>框架介绍:</h3><p>AlexNet的结构模型如下：</p><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20200601182327.png" srcset="/img/loading.gif" alt="images"></p><h3 id="示例代码："><a href="#示例代码：" class="headerlink" title="示例代码："></a>示例代码：</h3><h4 id="AlexNet实现Minist手写数字识别"><a href="#AlexNet实现Minist手写数字识别" class="headerlink" title="AlexNet实现Minist手写数字识别"></a><code>AlexNet实现Minist手写数字识别</code></h4><pre><code class="hljs python"><span class="hljs-comment">#!/usr/bin/env python</span><span class="hljs-comment"># -*- coding: utf-8 -*-</span><span class="hljs-comment"># @Time    : 2018/8/24 22:11</span><span class="hljs-comment"># @Author  : Seven</span><span class="hljs-comment"># @Site    : </span><span class="hljs-comment"># @File    : AlexNet.py</span><span class="hljs-comment"># @Software: PyCharm</span><span class="hljs-comment"># <span class="hljs-doctag">TODO:</span> 0.导入环境</span><span class="hljs-keyword">from</span> tensorflow.examples.tutorials.mnist <span class="hljs-keyword">import</span> input_data<span class="hljs-keyword">import</span> tensorflow <span class="hljs-keyword">as</span> tf<span class="hljs-keyword">import</span> osos.environ[<span class="hljs-string">'TF_CPP_MIN_LOG_LEVEL'</span>] = <span class="hljs-string">'2'</span><span class="hljs-comment"># <span class="hljs-doctag">TODO:</span> 1.数据准备</span><span class="hljs-comment"># 使用tensorflow自带的工具加载MNIST手写数字集合</span>mnist = input_data.read_data_sets(<span class="hljs-string">'data'</span>, one_hot=<span class="hljs-literal">True</span>)<span class="hljs-comment"># 查看数据的维度和target的维度</span>print(mnist.train.images.shape)print(mnist.train.labels.shape)<span class="hljs-comment"># <span class="hljs-doctag">TODO:</span> 2.初始化变量</span><span class="hljs-comment"># 定义网络超参数</span>learning_rate = <span class="hljs-number">0.001</span>epochs = <span class="hljs-number">200000</span>batch_size = <span class="hljs-number">128</span>display_step = <span class="hljs-number">5</span><span class="hljs-comment"># 定义网络参数</span>n_input = <span class="hljs-number">784</span>  <span class="hljs-comment"># 输入的维度</span>n_classes = <span class="hljs-number">10</span>  <span class="hljs-comment"># 标签的维度</span>dropout = <span class="hljs-number">0.8</span>  <span class="hljs-comment"># Dropout 的概率</span><span class="hljs-comment"># <span class="hljs-doctag">TODO:</span> 3.准备好placeholder</span><span class="hljs-comment"># 占位符输入</span>x = tf.placeholder(tf.float32, [<span class="hljs-literal">None</span>, n_input])y = tf.placeholder(tf.float32, [<span class="hljs-literal">None</span>, n_classes])keep_prob = tf.placeholder(tf.float32)<span class="hljs-comment"># <span class="hljs-doctag">TODO:</span> 4.构建网络计算图结构</span><span class="hljs-comment"># 卷积操作</span><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">conv2d</span><span class="hljs-params">(name, l_input, w, b)</span>:</span>    <span class="hljs-keyword">return</span> tf.nn.relu(tf.nn.bias_add(tf.nn.conv2d(l_input, w, strides=[<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>], padding=<span class="hljs-string">'SAME'</span>), b), name=name)<span class="hljs-comment"># 最大下采样操作</span><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">max_pool</span><span class="hljs-params">(name, l_input, k)</span>:</span>    <span class="hljs-keyword">return</span> tf.nn.max_pool(l_input, ksize=[<span class="hljs-number">1</span>, k, k, <span class="hljs-number">1</span>], strides=[<span class="hljs-number">1</span>, k, k, <span class="hljs-number">1</span>], padding=<span class="hljs-string">'SAME'</span>, name=name)<span class="hljs-comment"># 归一化操作</span><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">norm</span><span class="hljs-params">(name, l_input, lsize=<span class="hljs-number">4</span>)</span>:</span>    <span class="hljs-keyword">return</span> tf.nn.lrn(l_input, lsize, bias=<span class="hljs-number">1.0</span>, alpha=<span class="hljs-number">0.001</span> / <span class="hljs-number">9.0</span>, beta=<span class="hljs-number">0.75</span>, name=name)<span class="hljs-comment"># 存储所有的网络参数</span>weights = &#123;    <span class="hljs-string">'wc1'</span>: tf.Variable(tf.random_normal(shape=[<span class="hljs-number">11</span>, <span class="hljs-number">11</span>, <span class="hljs-number">1</span>, <span class="hljs-number">96</span>])),    <span class="hljs-string">'wc2'</span>: tf.Variable(tf.random_normal(shape=[<span class="hljs-number">5</span>, <span class="hljs-number">5</span>, <span class="hljs-number">96</span>, <span class="hljs-number">256</span>])),    <span class="hljs-string">'wc3'</span>: tf.Variable(tf.random_normal(shape=[<span class="hljs-number">3</span>, <span class="hljs-number">3</span>, <span class="hljs-number">256</span>, <span class="hljs-number">384</span>])),    <span class="hljs-string">'wc4'</span>: tf.Variable(tf.random_normal(shape=[<span class="hljs-number">3</span>, <span class="hljs-number">3</span>, <span class="hljs-number">384</span>, <span class="hljs-number">384</span>])),    <span class="hljs-string">'wc5'</span>: tf.Variable(tf.random_normal(shape=[<span class="hljs-number">3</span>, <span class="hljs-number">3</span>, <span class="hljs-number">384</span>, <span class="hljs-number">256</span>])),    <span class="hljs-string">'wd1'</span>: tf.Variable(tf.random_normal(shape=[<span class="hljs-number">4</span>*<span class="hljs-number">4</span>*<span class="hljs-number">256</span>, <span class="hljs-number">4096</span>])),    <span class="hljs-string">'wd2'</span>: tf.Variable(tf.random_normal(shape=[<span class="hljs-number">4096</span>, <span class="hljs-number">1024</span>])),    <span class="hljs-string">'out'</span>: tf.Variable(tf.random_normal(shape=[<span class="hljs-number">1024</span>, n_classes]))&#125;biases = &#123;    <span class="hljs-string">'bc1'</span>: tf.Variable(tf.random_normal([<span class="hljs-number">96</span>])),    <span class="hljs-string">'bc2'</span>: tf.Variable(tf.random_normal([<span class="hljs-number">256</span>])),    <span class="hljs-string">'bc3'</span>: tf.Variable(tf.random_normal([<span class="hljs-number">384</span>])),    <span class="hljs-string">'bc4'</span>: tf.Variable(tf.random_normal([<span class="hljs-number">384</span>])),    <span class="hljs-string">'bc5'</span>: tf.Variable(tf.random_normal([<span class="hljs-number">256</span>])),    <span class="hljs-string">'bd1'</span>: tf.Variable(tf.random_normal([<span class="hljs-number">4096</span>])),    <span class="hljs-string">'bd2'</span>: tf.Variable(tf.random_normal([<span class="hljs-number">1024</span>])),    <span class="hljs-string">'out'</span>: tf.Variable(tf.random_normal([n_classes]))&#125;<span class="hljs-comment"># 定义整个网络</span><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">alex_net</span><span class="hljs-params">(_X, _weights, _biases, _dropout)</span>:</span>    <span class="hljs-comment"># 向量转为矩阵</span>    _X = tf.reshape(_X, shape=[<span class="hljs-number">-1</span>, <span class="hljs-number">28</span>, <span class="hljs-number">28</span>, <span class="hljs-number">1</span>])    print(_X.shape)    <span class="hljs-comment"># <span class="hljs-doctag">TODO:</span> 第一层卷积：</span>    conv1 = conv2d(<span class="hljs-string">'conv1'</span>, _X, _weights[<span class="hljs-string">'wc1'</span>], _biases[<span class="hljs-string">'bc1'</span>])    <span class="hljs-comment"># 下采样层</span>    pool1 = max_pool(<span class="hljs-string">'pool1'</span>, conv1, k=<span class="hljs-number">2</span>)    <span class="hljs-comment"># 归一化层</span>    norm1 = norm(<span class="hljs-string">'norm1'</span>, pool1, lsize=<span class="hljs-number">4</span>)    print(norm1.shape)    <span class="hljs-comment"># <span class="hljs-doctag">TODO:</span> 第二层卷积：</span>    conv2 = conv2d(<span class="hljs-string">'conv2'</span>, norm1, _weights[<span class="hljs-string">'wc2'</span>], _biases[<span class="hljs-string">'bc2'</span>])    <span class="hljs-comment"># 下采样</span>    pool2 = max_pool(<span class="hljs-string">'pool2'</span>, conv2, k=<span class="hljs-number">2</span>)    <span class="hljs-comment"># 归一化</span>    norm2 = norm(<span class="hljs-string">'norm2'</span>, pool2, lsize=<span class="hljs-number">4</span>)    print(norm2.shape)    <span class="hljs-comment"># <span class="hljs-doctag">TODO:</span> 第三层卷积：</span>    conv3 = conv2d(<span class="hljs-string">'conv3'</span>, norm2, _weights[<span class="hljs-string">'wc3'</span>], _biases[<span class="hljs-string">'bc3'</span>])    <span class="hljs-comment"># 归一化</span>    norm3 = norm(<span class="hljs-string">'norm3'</span>, conv3, lsize=<span class="hljs-number">4</span>)    print(norm3.shape)    <span class="hljs-comment"># <span class="hljs-doctag">TODO:</span> 第四层卷积</span>    <span class="hljs-comment"># 卷积</span>    conv4 = conv2d(<span class="hljs-string">'conv4'</span>, norm3, _weights[<span class="hljs-string">'wc4'</span>], _biases[<span class="hljs-string">'bc4'</span>])    <span class="hljs-comment"># 归一化</span>    norm4 = norm(<span class="hljs-string">'norm4'</span>, conv4, lsize=<span class="hljs-number">4</span>)    print(norm4.shape)    <span class="hljs-comment"># <span class="hljs-doctag">TODO:</span> 第五层卷积</span>    <span class="hljs-comment"># 卷积</span>    conv5 = conv2d(<span class="hljs-string">'conv5'</span>, norm4, _weights[<span class="hljs-string">'wc5'</span>], _biases[<span class="hljs-string">'bc5'</span>])    <span class="hljs-comment"># 下采样</span>    pool5 = max_pool(<span class="hljs-string">'pool5'</span>, conv5, k=<span class="hljs-number">2</span>)    <span class="hljs-comment"># 归一化</span>    norm5 = norm(<span class="hljs-string">'norm5'</span>, pool5, lsize=<span class="hljs-number">4</span>)    print(norm5.shape)    <span class="hljs-comment"># <span class="hljs-doctag">TODO:</span> 第六层全连接层</span>    <span class="hljs-comment"># 先把特征图转为向量</span>    dense1 = tf.reshape(norm5, [<span class="hljs-number">-1</span>, _weights[<span class="hljs-string">'wd1'</span>].get_shape().as_list()[<span class="hljs-number">0</span>]])    dense1 = tf.nn.relu(tf.matmul(dense1, _weights[<span class="hljs-string">'wd1'</span>]) + _biases[<span class="hljs-string">'bd1'</span>], name=<span class="hljs-string">'fc1'</span>)    dense1 = tf.nn.dropout(dense1, _dropout)    print(dense1.shape)    <span class="hljs-comment"># <span class="hljs-doctag">TODO:</span> 第七层全连接层：</span>    dense2 = tf.nn.relu(tf.matmul(dense1, _weights[<span class="hljs-string">'wd2'</span>]) + _biases[<span class="hljs-string">'bd2'</span>], name=<span class="hljs-string">'fc2'</span>)  <span class="hljs-comment"># Relu activation</span>    dense2 = tf.nn.dropout(dense2, _dropout)    print(dense2.shape)    <span class="hljs-comment"># <span class="hljs-doctag">TODO:</span> 第八层全连接层：</span>    <span class="hljs-comment"># 网络输出层</span>    out = tf.matmul(dense2, _weights[<span class="hljs-string">'out'</span>]) + _biases[<span class="hljs-string">'out'</span>]    print(out.shape)    <span class="hljs-keyword">return</span> out<span class="hljs-comment"># <span class="hljs-doctag">TODO:</span> 5.计算损失值并初始化optimizer</span><span class="hljs-comment"># 构建模型</span>logits = alex_net(x, weights, biases, keep_prob)<span class="hljs-comment"># 定义损失函数和学习步骤</span>cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=y))training_operation = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)print(<span class="hljs-string">"FUNCTION READY!!"</span>)<span class="hljs-comment"># <span class="hljs-doctag">TODO:</span> 6.模型评估</span><span class="hljs-comment"># 测试网络</span>correct_prediction = tf.equal(tf.argmax(logits, <span class="hljs-number">1</span>), tf.argmax(y, <span class="hljs-number">1</span>))accuracy_operation = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))<span class="hljs-comment"># <span class="hljs-doctag">TODO:</span> 7.初始化变量</span><span class="hljs-comment"># 初始化所有的共享变量</span>init = tf.global_variables_initializer()<span class="hljs-comment"># <span class="hljs-doctag">TODO:</span> 8.训练模型</span><span class="hljs-comment"># 开启一个训练</span>print(<span class="hljs-string">"Training...."</span>)<span class="hljs-keyword">with</span> tf.Session() <span class="hljs-keyword">as</span> sess:    sess.run(init)    <span class="hljs-comment"># Keep training until reach max iterations</span>    <span class="hljs-keyword">for</span> n <span class="hljs-keyword">in</span> range(epochs):        total_epochs = int(mnist.train.num_examples/batch_size)        <span class="hljs-comment"># 遍历所有epoch</span>        <span class="hljs-keyword">for</span> epoch <span class="hljs-keyword">in</span> range(total_epochs):            batch_xs, batch_ys = mnist.train.next_batch(batch_size)            <span class="hljs-comment"># 获取批数据</span>            sess.run(training_operation, feed_dict=&#123;x: batch_xs, y: batch_ys, keep_prob: dropout&#125;)        <span class="hljs-keyword">if</span> n % display_step == <span class="hljs-number">0</span>:            <span class="hljs-comment"># 计算精度和损失值</span>            loss, acc = sess.run([cost, accuracy_operation], feed_dict=&#123;x: batch_xs, y: batch_ys, keep_prob: <span class="hljs-number">1.</span>&#125;)            print(<span class="hljs-string">"epoch :"</span>, (n+<span class="hljs-number">1</span>))            print(<span class="hljs-string">"Minibatch Loss= &#123;:.6f&#125;"</span>.format(loss))            print(<span class="hljs-string">"Training Accuracy= &#123;:.5f&#125;"</span>.format(acc))    print(<span class="hljs-string">"Optimization Finished!"</span>)    <span class="hljs-comment"># 计算测试精度</span>    print(<span class="hljs-string">"Testing Accuracy:"</span>,          sess.run(accuracy_operation, feed_dict=&#123;x: mnist.test.images[:<span class="hljs-number">256</span>],                                                  y: mnist.test.labels[:<span class="hljs-number">256</span>],                                                  keep_prob: <span class="hljs-number">1.</span>&#125;))</code></pre>]]></content>
    
    
    <categories>
      
      <category>TensorFlow</category>
      
    </categories>
    
    
    <tags>
      
      <tag>深度学习</tag>
      
      <tag>神经网络</tag>
      
      <tag>卷积神经网络</tag>
      
      <tag>TensorFlow</tag>
      
      <tag>AlexNet</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>TensorFlow实现卷积神经网络-LeNet</title>
    <link href="/2018/08/DL-CNN-LeNet/"/>
    <url>/2018/08/DL-CNN-LeNet/</url>
    
    <content type="html"><![CDATA[<h3 id="LeNet5"><a href="#LeNet5" class="headerlink" title="LeNet5"></a><strong>LeNet5</strong></h3><p>LeNet5 诞生于 1994 年，是最早的卷积神经网络之一，并且推动了深度学习领域的发展。自从 1988 年开始，在许多次成功的迭代后，这项由 Yann LeCun 完成的开拓性成果被命名为 LeNet5（参见：Gradient-Based Learning Applied to Document Recognition）。 </p><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20200601182316.png" srcset="/img/loading.gif" alt="images"></p><p>LeNet5 的架构基于这样的观点：图像的特征分布在整张图像上，以及带有可学习参数的卷积是一种用少量参数在多个位置上提取相似特征的有效方式。</p><p>在那时候，没有 GPU 帮助训练，甚至 CPU 的速度也很慢。因此，能够保存参数以及计算过程是一个关键进展。这和将每个像素用作一个大型多层神经网络的单独输入相反。</p><p>LeNet5 阐述了那些像素不应该被使用在第一层，因为图像具有很强的空间相关性，而使用图像中独立的像素作为不同的输入特征则利用不到这些相关性。</p><h3 id="LeNet5特点"><a href="#LeNet5特点" class="headerlink" title="LeNet5特点"></a><strong>LeNet5特点</strong></h3><p>LeNet5特征能够总结为如下几点： </p><p> 1）卷积神经网络使用三个层作为一个系列： 卷积，池化，非线性 </p><p> 2） 使用卷积提取空间特征</p><p>  3）使用映射到空间均值下采样（subsample）  </p><p>4）双曲线（tanh）或S型（sigmoid）形式的非线性  </p><p>5）多层神经网络（MLP）作为最后的分类器  </p><p>6）层与层之间的稀疏连接矩阵避免大的计算成本</p><h3 id="LeNet5实现"><a href="#LeNet5实现" class="headerlink" title="LeNet5实现"></a><strong>LeNet5实现</strong></h3><pre><code class="hljs python"><span class="hljs-comment">#!/usr/bin/env python</span><span class="hljs-comment"># -*- coding: utf-8 -*-</span><span class="hljs-comment"># @Time    : 2018/8/24 16:35</span><span class="hljs-comment"># @Author  : Seven</span><span class="hljs-comment"># @Site    :</span><span class="hljs-comment"># @File    : LeNet.py</span><span class="hljs-comment"># @Software: PyCharm</span><span class="hljs-comment"># <span class="hljs-doctag">TODO:</span> 0.导入环境</span><span class="hljs-keyword">from</span> tensorflow.examples.tutorials.mnist <span class="hljs-keyword">import</span> input_data<span class="hljs-keyword">import</span> tensorflow <span class="hljs-keyword">as</span> tf<span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<span class="hljs-keyword">import</span> random<span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt<span class="hljs-keyword">from</span> sklearn.utils <span class="hljs-keyword">import</span> shuffle<span class="hljs-keyword">import</span> osos.environ[<span class="hljs-string">'TF_CPP_MIN_LOG_LEVEL'</span>] = <span class="hljs-string">'2'</span><span class="hljs-comment"># <span class="hljs-doctag">TODO:</span> 1.数据准备</span><span class="hljs-comment"># 使用tensorflow自带的工具加载MNIST手写数字集合</span>mnist = input_data.read_data_sets(<span class="hljs-string">'data'</span>, reshape=<span class="hljs-literal">False</span>)<span class="hljs-comment"># <span class="hljs-doctag">TODO:</span> 2.数据处理</span>x_train, y_train = mnist.train.images, mnist.train.labelsx_validation, y_validation = mnist.validation.images, mnist.validation.labelsx_test, y_test = mnist.test.images, mnist.test.labels<span class="hljs-comment"># 预先判断下维度是否一致</span><span class="hljs-keyword">assert</span>(len(x_train) == len(y_train))<span class="hljs-keyword">assert</span>(len(x_validation) == len(y_validation))<span class="hljs-keyword">assert</span>(len(x_test) == len(y_test))print(<span class="hljs-string">"Image Shape: &#123;&#125;"</span>.format(x_train[<span class="hljs-number">0</span>].shape))print(<span class="hljs-string">"Training Set:   &#123;&#125; samples"</span>.format(len(x_train)))print(<span class="hljs-string">"Validation Set: &#123;&#125; samples"</span>.format(len(x_validation)))print(<span class="hljs-string">"Test Set:       &#123;&#125; samples"</span>.format(len(x_test)))<span class="hljs-comment"># TensorFlow预加载的MNIST数据为28x28x1图像。</span><span class="hljs-comment"># 但是，LeNet架构只接受32x32xC图像，其中C是颜色通道的数量。</span><span class="hljs-comment"># 为了将MNIST数据重新格式化为LeNet将接受的形状，我们在顶部和底部填充两行零，并在左侧和右侧填充两列零（28 + 2 + 2 = 32）</span>x_train = np.pad(x_train, ((<span class="hljs-number">0</span>, <span class="hljs-number">0</span>), (<span class="hljs-number">2</span>, <span class="hljs-number">2</span>), (<span class="hljs-number">2</span>, <span class="hljs-number">2</span>), (<span class="hljs-number">0</span>, <span class="hljs-number">0</span>)), <span class="hljs-string">'constant'</span>)x_validation = np.pad(x_validation, ((<span class="hljs-number">0</span>, <span class="hljs-number">0</span>), (<span class="hljs-number">2</span>, <span class="hljs-number">2</span>), (<span class="hljs-number">2</span>, <span class="hljs-number">2</span>), (<span class="hljs-number">0</span>, <span class="hljs-number">0</span>)), <span class="hljs-string">'constant'</span>)x_test = np.pad(x_test, ((<span class="hljs-number">0</span>, <span class="hljs-number">0</span>), (<span class="hljs-number">2</span>, <span class="hljs-number">2</span>), (<span class="hljs-number">2</span>, <span class="hljs-number">2</span>), (<span class="hljs-number">0</span>, <span class="hljs-number">0</span>)), <span class="hljs-string">'constant'</span>)print(<span class="hljs-string">"Updated Image Shape: &#123;&#125;"</span>.format(x_train[<span class="hljs-number">0</span>].shape))<span class="hljs-comment">#  查看下灰度图</span>index = random.randint(<span class="hljs-number">0</span>, len(x_train))image = x_train[index].squeeze()plt.figure(figsize=(<span class="hljs-number">1</span>, <span class="hljs-number">1</span>))plt.imshow(image, cmap=<span class="hljs-string">"gray"</span>)<span class="hljs-comment"># plt.show()</span>print(y_train[index])<span class="hljs-comment"># Shuffle the training data.</span>x_train, y_train = shuffle(x_train, y_train)<span class="hljs-comment"># <span class="hljs-doctag">TODO:</span> 3.准备好placeholder</span>x = tf.placeholder(tf.float32, (<span class="hljs-literal">None</span>, <span class="hljs-number">32</span>, <span class="hljs-number">32</span>, <span class="hljs-number">1</span>))y = tf.placeholder(tf.int32, <span class="hljs-literal">None</span>)one_hot_y = tf.one_hot(y, <span class="hljs-number">10</span>)<span class="hljs-comment"># <span class="hljs-doctag">TODO:</span> 4.构建网络计算图结构</span>epochs = <span class="hljs-number">10</span>batch_size = <span class="hljs-number">128</span><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">LeNet</span><span class="hljs-params">(input)</span>:</span>    mu = <span class="hljs-number">0</span>    sigma = <span class="hljs-number">0.1</span>    <span class="hljs-comment"># 层深度定义</span>    layer_depth = &#123;        <span class="hljs-string">'layer1'</span>: <span class="hljs-number">6</span>,        <span class="hljs-string">'layer2'</span>: <span class="hljs-number">16</span>,        <span class="hljs-string">'layer3'</span>: <span class="hljs-number">120</span>,        <span class="hljs-string">'layer_fc1'</span>: <span class="hljs-number">84</span>    &#125;    <span class="hljs-comment"># <span class="hljs-doctag">TODO:</span> 第一层卷积：输入=32x32x1, 输出=28x28x6</span>    conv1_w = tf.Variable(tf.truncated_normal(shape=[<span class="hljs-number">5</span>, <span class="hljs-number">5</span>, <span class="hljs-number">1</span>, <span class="hljs-number">6</span>], mean=mu, stddev=sigma))    conv1_b = tf.Variable(tf.zeros(<span class="hljs-number">6</span>))    conv1 = tf.nn.conv2d(input, conv1_w, strides=[<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>], padding=<span class="hljs-string">'VALID'</span>) + conv1_b    <span class="hljs-comment"># 激活函数</span>    conv1_out = tf.nn.relu(conv1)    <span class="hljs-comment"># 池化层， 输入=28x28x6, 输出=14x14x6</span>    pool_1 = tf.nn.max_pool(conv1_out, ksize=[<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">2</span>, <span class="hljs-number">1</span>], strides=[<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">2</span>, <span class="hljs-number">1</span>], padding=<span class="hljs-string">'VALID'</span>)    print(pool_1.shape)    <span class="hljs-comment"># <span class="hljs-doctag">TODO:</span> 第二层卷积： 输入=14x14x6， 输出=10x10x16</span>    conv2_w = tf.Variable(tf.truncated_normal(shape=[<span class="hljs-number">5</span>, <span class="hljs-number">5</span>, <span class="hljs-number">6</span>, <span class="hljs-number">16</span>], mean=mu, stddev=sigma))    conv2_b = tf.Variable(tf.zeros(<span class="hljs-number">16</span>))    conv2 = tf.nn.conv2d(pool_1, conv2_w, strides=[<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>], padding=<span class="hljs-string">'VALID'</span>) + conv2_b    <span class="hljs-comment"># 激活函数</span>    conv2_out = tf.nn.relu(conv2)    <span class="hljs-comment"># 池化层， 输入=10x10x16, 输出=5x5x16</span>    pool_2 = tf.nn.max_pool(conv2_out, ksize=[<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">2</span>, <span class="hljs-number">1</span>], strides=[<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">2</span>, <span class="hljs-number">1</span>], padding=<span class="hljs-string">'VALID'</span>)    <span class="hljs-comment"># Flatten 输入=5x5x16， 输出=400</span>    pool_2_flat = tf.reshape(pool_2, [<span class="hljs-number">-1</span>, <span class="hljs-number">400</span>])    print(pool_2_flat.shape)    <span class="hljs-comment"># <span class="hljs-doctag">TODO:</span> 第三层全连接层， 输入=400， 输出=120</span>    fc1_w = tf.Variable(tf.truncated_normal(shape=[<span class="hljs-number">400</span>, <span class="hljs-number">120</span>], mean=mu, stddev=sigma))    fc1_b = tf.Variable(tf.zeros(<span class="hljs-number">120</span>))    fc1 = tf.matmul(pool_2_flat, fc1_w) + fc1_b    <span class="hljs-comment"># 激活函数</span>    fc1_out = tf.nn.relu(fc1)    print(fc1_out.shape)    <span class="hljs-comment"># <span class="hljs-doctag">TODO:</span> 第四层全连接层： 输入=120， 输出=84</span>    fc2_w = tf.Variable(tf.truncated_normal(shape=[<span class="hljs-number">120</span>, <span class="hljs-number">84</span>], mean=mu, stddev=sigma))    fc2_b = tf.Variable(tf.zeros(<span class="hljs-number">84</span>))    fc2 = tf.matmul(fc1_out, fc2_w) + fc2_b    <span class="hljs-comment"># 激活函数</span>    fc2_out = tf.nn.relu(fc2)    print(fc2_out.shape)    <span class="hljs-comment"># <span class="hljs-doctag">TODO:</span> 第五层全连接层： 输入=84， 输出=10</span>    fc3_w = tf.Variable(tf.truncated_normal(shape=[<span class="hljs-number">84</span>, <span class="hljs-number">10</span>], mean=mu, stddev=sigma))    fc3_b = tf.Variable(tf.zeros(<span class="hljs-number">10</span>))    fc3_out = tf.matmul(fc2_out, fc3_w) + fc3_b    print(fc3_out.shape)    <span class="hljs-keyword">return</span> fc3_out<span class="hljs-comment"># <span class="hljs-doctag">TODO:</span> 5.计算损失值并初始化optimizer</span>learning_rate = <span class="hljs-number">0.001</span>logits = LeNet(x)cross_entropy = tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=one_hot_y)loss_operation = tf.reduce_mean(cross_entropy)optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)training_operation = optimizer.minimize(loss_operation)print(<span class="hljs-string">"FUNCTION READY!!"</span>)<span class="hljs-comment"># <span class="hljs-doctag">TODO:</span> 6.初始化变量</span>init = tf.global_variables_initializer()<span class="hljs-comment"># <span class="hljs-doctag">TODO:</span> 7.模型评估</span>correct_prediction = tf.equal(tf.argmax(logits, <span class="hljs-number">1</span>), tf.argmax(one_hot_y, <span class="hljs-number">1</span>))accuracy_operation = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">evaluate</span><span class="hljs-params">(X_data, y_data)</span>:</span>    num_examples = len(X_data)    total_accuracy = <span class="hljs-number">0</span>    sess = tf.get_default_session()    <span class="hljs-keyword">for</span> offset <span class="hljs-keyword">in</span> range(<span class="hljs-number">0</span>, num_examples, batch_size):        batch_x, batch_y = X_data[offset:offset+batch_size], y_data[offset:offset+batch_size]        accuracy = sess.run(accuracy_operation, feed_dict=&#123;x: batch_x, y: batch_y&#125;)        total_accuracy += (accuracy * len(batch_x))    <span class="hljs-keyword">return</span> total_accuracy / num_examples<span class="hljs-comment"># <span class="hljs-doctag">TODO:</span> 保存模型</span>saver = tf.train.Saver()<span class="hljs-comment"># <span class="hljs-doctag">TODO:</span> 8.训练模型</span><span class="hljs-keyword">with</span> tf.Session() <span class="hljs-keyword">as</span> sess:    sess.run(init)    num_examples = len(x_train)    print(<span class="hljs-string">"Training....."</span>)    <span class="hljs-keyword">for</span> n <span class="hljs-keyword">in</span> range(epochs):        <span class="hljs-keyword">for</span> offset <span class="hljs-keyword">in</span> range(<span class="hljs-number">0</span>, num_examples, batch_size):            batch_x, batch_y = x_train[offset:offset+batch_size], y_train[offset:offset+batch_size]            sess.run(training_operation, feed_dict=&#123;x: batch_x, y: batch_y&#125;)        validation_accuracy = evaluate(x_validation, y_validation)        print(<span class="hljs-string">"EPOCH &#123;&#125; ..."</span>.format(n + <span class="hljs-number">1</span>))        print(<span class="hljs-string">"Validation Accuracy = &#123;:.3f&#125;"</span>.format(validation_accuracy))    saver.save(sess, <span class="hljs-string">'./model/LeNet.model'</span>)    print(<span class="hljs-string">"Model saved"</span>)    print(<span class="hljs-string">"Evaluate The Model"</span>)    <span class="hljs-comment"># <span class="hljs-doctag">TODO:</span> 读取模型</span>    saver.restore(sess, <span class="hljs-string">'./model/LeNet.model'</span>)    test_accuracy = evaluate(x_test, y_test)    print(<span class="hljs-string">"Test Accuracy = &#123;:.3f&#125;"</span>.format(test_accuracy))</code></pre><h3 id="执行结果"><a href="#执行结果" class="headerlink" title="执行结果"></a>执行结果</h3><pre><code class="hljs bash">FUNCTION READY!!Training.....EPOCH 1 ...Validation Accuracy = 0.969EPOCH 2 ...Validation Accuracy = 0.974EPOCH 3 ...Validation Accuracy = 0.979EPOCH 4 ...Validation Accuracy = 0.981EPOCH 5 ...Validation Accuracy = 0.984EPOCH 6 ...Validation Accuracy = 0.986EPOCH 7 ...Validation Accuracy = 0.986EPOCH 8 ...Validation Accuracy = 0.986EPOCH 9 ...Validation Accuracy = 0.986EPOCH 10 ...Validation Accuracy = 0.984Model savedEvaluate The ModelTest Accuracy = 0.988</code></pre>]]></content>
    
    
    <categories>
      
      <category>TensorFlow</category>
      
    </categories>
    
    
    <tags>
      
      <tag>深度学习</tag>
      
      <tag>神经网络</tag>
      
      <tag>卷积神经网络</tag>
      
      <tag>TensorFlow</tag>
      
      <tag>LeNet</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>TensorFlow实现简单的卷积神经网络-CNN</title>
    <link href="/2018/08/DL-CNN-python/"/>
    <url>/2018/08/DL-CNN-python/</url>
    
    <content type="html"><![CDATA[<h3 id="环境设定"><a href="#环境设定" class="headerlink" title="环境设定"></a>环境设定</h3><pre><code class="hljs python"><span class="hljs-keyword">import</span> tensorflow <span class="hljs-keyword">as</span> tf<span class="hljs-keyword">from</span> tensorflow.examples.tutorials.mnist <span class="hljs-keyword">import</span> input_data<span class="hljs-keyword">import</span> osos.environ[<span class="hljs-string">'TF_CPP_MIN_LOG_LEVEL'</span>] = <span class="hljs-string">'2'</span></code></pre><h3 id="数据准备"><a href="#数据准备" class="headerlink" title="数据准备"></a>数据准备</h3><pre><code class="hljs python"><span class="hljs-comment"># 使用tensorflow自带的工具加载MNIST手写数字集合</span>mnist = input_data.read_data_sets(<span class="hljs-string">'data/mnist'</span>, one_hot=<span class="hljs-literal">True</span>)<span class="hljs-comment"># 查看数据的维度和target的维度</span>print(mnist.train.images.shape)print(mnist.train.labels.shape)</code></pre><h3 id="准备好placeholder"><a href="#准备好placeholder" class="headerlink" title="准备好placeholder"></a>准备好placeholder</h3><pre><code class="hljs python">X = tf.placeholder(tf.float32, [<span class="hljs-literal">None</span>, <span class="hljs-number">784</span>])Y = tf.placeholder(tf.float32, [<span class="hljs-literal">None</span>, <span class="hljs-number">10</span>])learnRate = tf.placeholder(tf.float32)</code></pre><h3 id="构建网络计算图结构"><a href="#构建网络计算图结构" class="headerlink" title="构建网络计算图结构"></a>构建网络计算图结构</h3><pre><code class="hljs python"><span class="hljs-comment"># 把输入数据reshape--28x28=784, 单通道， -1表示None</span><span class="hljs-keyword">with</span> tf.name_scope(<span class="hljs-string">'reshape'</span>):    x_image = tf.reshape(x, [<span class="hljs-number">-1</span>, <span class="hljs-number">28</span>, <span class="hljs-number">28</span>, <span class="hljs-number">1</span>])<span class="hljs-comment"># 构建第一层卷积计算层--将一个灰度图像映射到32个feature maps, 卷积核为5x5</span><span class="hljs-keyword">with</span> tf.name_scope(<span class="hljs-string">'conv1'</span>):    shape = [<span class="hljs-number">5</span>, <span class="hljs-number">5</span>, <span class="hljs-number">1</span>, <span class="hljs-number">32</span>]    W_conv1 = tf.Variable(tf.truncated_normal(shape=shape, stddev=<span class="hljs-number">0.1</span>),                          collections=[tf.GraphKeys.GLOBAL_VARIABLES, <span class="hljs-string">'WEIGHTS'</span>])    shape = [<span class="hljs-number">32</span>]    b_conv1 = tf.Variable(tf.constant(<span class="hljs-number">0.1</span>, shape=shape))    net_conv1 = tf.nn.conv2d(x_image, W_conv1, strides=[<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>],                             padding=<span class="hljs-string">'SAME'</span>) + b_conv1    out_conv1 = tf.nn.relu(net_conv1)<span class="hljs-comment"># 构建池化层--采用最大池化</span><span class="hljs-keyword">with</span> tf.name_scope(<span class="hljs-string">'pool1'</span>):    h_pool1 = tf.nn.max_pool(out_conv1, ksize=[<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">2</span>, <span class="hljs-number">1</span>], strides=[<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">2</span>, <span class="hljs-number">1</span>],                             padding=<span class="hljs-string">'VALID'</span>)<span class="hljs-comment"># 构建第二层卷积计算层--maps 32 feature maps to 64.</span><span class="hljs-keyword">with</span> tf.name_scope(<span class="hljs-string">'conv2'</span>):    shape = [<span class="hljs-number">5</span>, <span class="hljs-number">5</span>, <span class="hljs-number">32</span>, <span class="hljs-number">64</span>]    W_conv2 = tf.Variable(tf.truncated_normal(shape=shape, stddev=<span class="hljs-number">0.1</span>),                          collections=[tf.GraphKeys.GLOBAL_VARIABLES, <span class="hljs-string">'WEIGHTS'</span>])    shape = [<span class="hljs-number">64</span>]    b_conv2 = tf.Variable(tf.constant(<span class="hljs-number">0.1</span>, shape=shape))    net_conv2 = tf.nn.conv2d(h_pool1, W_conv2, strides=[<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>],                             padding=<span class="hljs-string">'SAME'</span>) + b_conv2    out_conv2 = tf.nn.relu(net_conv2)<span class="hljs-comment"># 构建第二层池化层--采用最大池化</span><span class="hljs-keyword">with</span> tf.name_scope(<span class="hljs-string">'pool2'</span>):    h_pool2 = tf.nn.max_pool(out_conv2, ksize=[<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">2</span>, <span class="hljs-number">1</span>], strides=[<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">2</span>, <span class="hljs-number">1</span>],                             padding=<span class="hljs-string">'VALID'</span>)<span class="hljs-comment"># 构建全连接层--经过的两层的下采样（池化），28x28x1的图像--&gt;7x7x64，然后映射到1024个特征</span><span class="hljs-keyword">with</span> tf.name_scope(<span class="hljs-string">'fc1'</span>):    shape = [<span class="hljs-number">7</span>*<span class="hljs-number">7</span>*<span class="hljs-number">64</span>, <span class="hljs-number">1024</span>]    W_fc1 = tf.Variable(tf.truncated_normal(shape=shape, stddev=<span class="hljs-number">0.1</span>),                        collections=[tf.GraphKeys.GLOBAL_VARIABLES, <span class="hljs-string">'WEIGHTS'</span>])    shape = [<span class="hljs-number">1024</span>]    b_fc1 = tf.Variable(tf.constant(<span class="hljs-number">0.1</span>, shape=shape))    shape = [<span class="hljs-number">-1</span>, <span class="hljs-number">7</span>*<span class="hljs-number">7</span>*<span class="hljs-number">64</span>]    h_pool2_flat = tf.reshape(h_pool2, shape)    out_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, W_fc1) + b_fc1)<span class="hljs-comment"># Dropout--防止过拟合</span><span class="hljs-keyword">with</span> tf.name_scope(<span class="hljs-string">'dropout'</span>):    keep_prob = tf.placeholder(tf.float32)    out_fc1_drop = tf.nn.dropout(out_fc1, keep_prob=keep_prob)<span class="hljs-comment"># 构建第二层全连接层--将1024个特性映射到10个类，每个类对应一个数字</span><span class="hljs-keyword">with</span> tf.name_scope(<span class="hljs-string">'fc2'</span>):    shape = [<span class="hljs-number">1024</span>, <span class="hljs-number">10</span>]    W_fc2 = tf.Variable(tf.truncated_normal(shape=shape, stddev=<span class="hljs-number">0.1</span>),                        collections=[tf.GraphKeys.GLOBAL_VARIABLES, <span class="hljs-string">'WEIGHTS'</span>])    shape = [<span class="hljs-number">10</span>]    b_fc2 = tf.Variable(tf.constant(<span class="hljs-number">0.1</span>, shape=shape))    out = tf.matmul(out_fc1_drop, W_fc2) + b_fc2</code></pre><h3 id="计算损失函数并初始化optimizer"><a href="#计算损失函数并初始化optimizer" class="headerlink" title="计算损失函数并初始化optimizer"></a>计算损失函数并初始化optimizer</h3><pre><code class="hljs python">print(y.shape, out.shape)cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y, logits=out))l2_loss = tf.add_n([tf.nn.l2_loss(w) <span class="hljs-keyword">for</span> w <span class="hljs-keyword">in</span> tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES)])total_loss = cross_entropy + <span class="hljs-number">7e-5</span>*l2_losstrain_step = tf.train.AdamOptimizer(learnRate).minimize(total_loss)print(<span class="hljs-string">"FUNCTIONS READY!!"</span>)</code></pre><h3 id="初始化变量"><a href="#初始化变量" class="headerlink" title="初始化变量"></a>初始化变量</h3><pre><code class="hljs python">init = tf.global_variables_initializer()</code></pre><h3 id="在session中执行graph定义的运算"><a href="#在session中执行graph定义的运算" class="headerlink" title="在session中执行graph定义的运算"></a>在session中执行graph定义的运算</h3><pre><code class="hljs python"><span class="hljs-keyword">with</span> tf.Session() <span class="hljs-keyword">as</span> sess:    sess.run(init)    <span class="hljs-keyword">for</span> step <span class="hljs-keyword">in</span> range(<span class="hljs-number">3000</span>):        batch_xs, batch_ys = mnist.train.next_batch(<span class="hljs-number">100</span>)        lr = <span class="hljs-number">0.01</span>        _, loss, l2_loss_value, total_loss_value = sess.run(            [train_step, cross_entropy, l2_loss, total_loss],            feed_dict=&#123;x: batch_xs, y: batch_ys, learnRate: lr, keep_prob: <span class="hljs-number">0.5</span>&#125;)        <span class="hljs-keyword">if</span> (step + <span class="hljs-number">1</span>) % <span class="hljs-number">100</span> == <span class="hljs-number">0</span>:            print(<span class="hljs-string">"step %d, entropy loss: %f, l2_loss: %f, total loss: %f"</span> %                  (step + <span class="hljs-number">1</span>, loss, l2_loss_value, total_loss_value))            <span class="hljs-comment"># 验证训练的模型</span>            correct_prediction = tf.equal(tf.argmax(out, <span class="hljs-number">1</span>), tf.argmax(y, <span class="hljs-number">1</span>))            accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))            print(<span class="hljs-string">"Train accuracy:"</span>, sess.run(accuracy, feed_dict=&#123;x: batch_xs, y: batch_ys, keep_prob: <span class="hljs-number">0.5</span>&#125;))        <span class="hljs-keyword">if</span> (step + <span class="hljs-number">1</span>) % <span class="hljs-number">1000</span> == <span class="hljs-number">0</span>:            print(<span class="hljs-string">"Text accuracy:"</span>, sess.run(accuracy, feed_dict=&#123;x: batch_xs, y: batch_ys, keep_prob: <span class="hljs-number">0.5</span>&#125;))</code></pre><h3 id="运行结果"><a href="#运行结果" class="headerlink" title="运行结果"></a>运行结果</h3><pre><code class="hljs bash">step 2100, entropy loss: 0.127328, l2_loss: 1764.112305, total loss: 0.250816Train accuracy: 0.99step 2200, entropy loss: 0.275415, l2_loss: 1819.110107, total loss: 0.402753Train accuracy: 0.97step 2300, entropy loss: 0.147261, l2_loss: 1773.890869, total loss: 0.271434Train accuracy: 0.99step 2400, entropy loss: 0.100819, l2_loss: 1699.657349, total loss: 0.219795Train accuracy: 0.97step 2500, entropy loss: 0.148775, l2_loss: 1701.602661, total loss: 0.267887Train accuracy: 0.96step 2600, entropy loss: 0.142768, l2_loss: 1800.301880, total loss: 0.268789Train accuracy: 0.96step 2700, entropy loss: 0.134457, l2_loss: 2041.165161, total loss: 0.277339Train accuracy: 0.96step 2800, entropy loss: 0.071112, l2_loss: 2003.971924, total loss: 0.211390Train accuracy: 0.99step 2900, entropy loss: 0.217092, l2_loss: 1879.004272, total loss: 0.348622Train accuracy: 0.98step 3000, entropy loss: 0.207697, l2_loss: 1841.294556, total loss: 0.336588Train accuracy: 0.97Text accuracy: 0.95Optimization Finished</code></pre><h4 id="结论：简单写了个卷积神经网络，没有进行参数调试。"><a href="#结论：简单写了个卷积神经网络，没有进行参数调试。" class="headerlink" title="结论：简单写了个卷积神经网络，没有进行参数调试。"></a>结论：简单写了个卷积神经网络，没有进行参数调试。</h4>]]></content>
    
    
    <categories>
      
      <category>TensorFlow</category>
      
    </categories>
    
    
    <tags>
      
      <tag>深度学习</tag>
      
      <tag>神经网络</tag>
      
      <tag>卷积神经网络</tag>
      
      <tag>TensorFlow</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>神经网络之卷积神经网络反向传播算法</title>
    <link href="/2018/08/CNN-back-propagation/"/>
    <url>/2018/08/CNN-back-propagation/</url>
    
    <content type="html"><![CDATA[<h3 id="回顾DNN的反向传播算法"><a href="#回顾DNN的反向传播算法" class="headerlink" title="回顾DNN的反向传播算法"></a>回顾DNN的反向传播算法</h3><p>需要在理解深度神经网络的反向传播算来做预先理解，见<a href="https://0520.tech/2018/08/15/2018-08-15-DL-back-propagation/">神经网络入门-反向传播</a>所总结的反向传播四部曲：</p><h4 id="反向传播的四项基本原则："><a href="#反向传播的四项基本原则：" class="headerlink" title="反向传播的四项基本原则："></a><strong>反向传播的四项基本原则</strong>：</h4><h4 id="基本形式"><a href="#基本形式" class="headerlink" title="基本形式:"></a>基本形式:</h4><ul><li><h4 id="delta-i-L-bigtriangledown-out-E-total-times-sigma-prime-net-i-L"><a href="#delta-i-L-bigtriangledown-out-E-total-times-sigma-prime-net-i-L" class="headerlink" title="$\delta_i^{(L)}= \bigtriangledown_{out} E_{total} \times   \sigma^{\prime}(net_i^{(L)}) $"></a>$\delta_i^{(L)}= \bigtriangledown_{out} E_{total} \times   \sigma^{\prime}(net_i^{(L)}) $</h4></li><li><h4 id="delta-l-sum-j-delta-j-l-1-w-ji-l-1-sigma-prime-net-i-l"><a href="#delta-l-sum-j-delta-j-l-1-w-ji-l-1-sigma-prime-net-i-l" class="headerlink" title="$\delta^{(l)} = \sum_j \delta_j^{(l+1)} w_{ji}^{(l+1)}  \sigma^{\prime}(net_i^{(l)})$"></a>$\delta^{(l)} = \sum_j \delta_j^{(l+1)} w_{ji}^{(l+1)}  \sigma^{\prime}(net_i^{(l)})$</h4></li><li><h4 id="frac-partial-E-total-partial-bias-i-l-delta-i-l"><a href="#frac-partial-E-total-partial-bias-i-l-delta-i-l" class="headerlink" title="$\frac{\partial E_{total}}{\partial bias_i^{(l)}}=\delta_i^{(l)}$"></a>$\frac{\partial E_{total}}{\partial bias_i^{(l)}}=\delta_i^{(l)}$</h4></li><li><h4 id="frac-partial-E-total-partial-w-ij-l-outh-j-l-1-delta-i-l"><a href="#frac-partial-E-total-partial-w-ij-l-outh-j-l-1-delta-i-l" class="headerlink" title="$\frac{\partial E_{total}}{\partial w_{ij}^{(l)}}=outh_j^{(l-1)}\delta_i^{(l)}$"></a>$\frac{\partial E_{total}}{\partial w_{ij}^{(l)}}=outh_j^{(l-1)}\delta_i^{(l)}$</h4></li></ul><h4 id="矩阵形式："><a href="#矩阵形式：" class="headerlink" title="矩阵形式："></a>矩阵形式：</h4><ul><li><h4 id="delta-i-L-bigtriangledown-out-E-total-bigodot-sigma-prime-net-i-L-，-bigodot-是Hadamard乘积（对应位置相乘）"><a href="#delta-i-L-bigtriangledown-out-E-total-bigodot-sigma-prime-net-i-L-，-bigodot-是Hadamard乘积（对应位置相乘）" class="headerlink" title="$\delta_i^{(L)}= \bigtriangledown_{out} E_{total} \bigodot   \sigma^{\prime}(net_i^{(L)}) $ ， $\bigodot$是Hadamard乘积（对应位置相乘）"></a>$\delta_i^{(L)}= \bigtriangledown_{out} E_{total} \bigodot   \sigma^{\prime}(net_i^{(L)}) $ ， $\bigodot$是Hadamard乘积（对应位置相乘）</h4></li><li><h4 id="delta-l-w-l-1-T-delta-l-1-bigodot-sigma-prime-net-l"><a href="#delta-l-w-l-1-T-delta-l-1-bigodot-sigma-prime-net-l" class="headerlink" title="$\delta^{(l)} = (w^{(l+1)})^T \delta^{(l+1)} \bigodot  \sigma^{\prime}(net^{(l)})$"></a>$\delta^{(l)} = (w^{(l+1)})^T \delta^{(l+1)} \bigodot  \sigma^{\prime}(net^{(l)})$</h4></li><li><h4 id="frac-partial-E-total-partial-bias-l-delta-l"><a href="#frac-partial-E-total-partial-bias-l-delta-l" class="headerlink" title="$\frac{\partial E_{total}}{\partial bias^{(l)}}=\delta^{(l)}$"></a>$\frac{\partial E_{total}}{\partial bias^{(l)}}=\delta^{(l)}$</h4></li><li><h4 id="frac-partial-E-total-partial-w-l-delta-l-outh-l-1-T"><a href="#frac-partial-E-total-partial-w-l-delta-l-outh-l-1-T" class="headerlink" title="$\frac{\partial E_{total}}{\partial w^{(l)}}=\delta^{(l)}(outh^{(l-1)})^T$"></a>$\frac{\partial E_{total}}{\partial w^{(l)}}=\delta^{(l)}(outh^{(l-1)})^T$</h4></li></ul><p>现在我们想把同样的思想用到卷积神经网络中，但是很明显，CNN有很多不同的地方（卷积计算层，池化层），不能直接去套用DNN的反向传播公式。</p><h3 id="卷积神经网络的反向传播算法"><a href="#卷积神经网络的反向传播算法" class="headerlink" title="卷积神经网络的反向传播算法"></a>卷积神经网络的反向传播算法</h3><p>卷积神经网络相比于多层感知机，增加了两种新的层次——卷积层与池化层。由于反向传播链的存在，要求出这两种层结构的梯度，仅需要解决输出对权值的梯度即可。 接下来我们根据卷积神经网络的网络结构来更新反向传播四部曲。</p><p>我们在推导DNN的反向传播的时候，推导过程中使用了：</p><h4 id="output-l-1-sigma-W-cdot-output-l-b"><a href="#output-l-1-sigma-W-cdot-output-l-b" class="headerlink" title="$output^{(l+1)}=\sigma(W \cdot output^{(l)}+b)$"></a>$output^{(l+1)}=\sigma(W \cdot output^{(l)}+b)$</h4><p>而对于卷积神经网络，我们使用卷积和池化的公式进行重新推导：</p><h4 id="卷积层：-output-l-1-sigma-output-l-W-b"><a href="#卷积层：-output-l-1-sigma-output-l-W-b" class="headerlink" title="卷积层：$output^{(l+1)}=\sigma( output^{(l)}*W+b)$"></a>卷积层：$output^{(l+1)}=\sigma( output^{(l)}*W+b)$</h4><h4 id="池化层：-output-l-1-subsampling-output-l-b"><a href="#池化层：-output-l-1-subsampling-output-l-b" class="headerlink" title="池化层：$output^{(l+1)}=subsampling( output^{(l)}+b)$"></a>池化层：$output^{(l+1)}=subsampling( output^{(l)}+b)$</h4><h3 id="卷积层"><a href="#卷积层" class="headerlink" title="卷积层"></a>卷积层</h3><p>我们在回忆下：</p><h4 id="delta-l-frac-partial-E-total-partial-net-l"><a href="#delta-l-frac-partial-E-total-partial-net-l" class="headerlink" title="$\delta^{(l)}=\frac{\partial E_{total}}{\partial net^{(l)}} $"></a>$\delta^{(l)}=\frac{\partial E_{total}}{\partial net^{(l)}} $</h4><h4 id="frac-partial-E-total-partial-net-l-1-cdot-frac-partial-net-l-1-partial-net-l"><a href="#frac-partial-E-total-partial-net-l-1-cdot-frac-partial-net-l-1-partial-net-l" class="headerlink" title="$= \frac{\partial E_{total}}{\partial net^{(l+1)}} \cdot \frac{\partial net^{(l+1)}}{\partial net^{(l)}}$"></a>$= \frac{\partial E_{total}}{\partial net^{(l+1)}} \cdot \frac{\partial net^{(l+1)}}{\partial net^{(l)}}$</h4><h4 id="delta-l-1-times-frac-partial-（output-l-W-b）-partial-net-l"><a href="#delta-l-1-times-frac-partial-（output-l-W-b）-partial-net-l" class="headerlink" title="$= \delta^{(l+1)} \times  \frac{\partial （output^{(l)}*W+b）}{\partial net^{(l)}}$"></a>$= \delta^{(l+1)} \times  \frac{\partial （output^{(l)}*W+b）}{\partial net^{(l)}}$</h4><h4 id="delta-l-1-times-frac-partial-（output-l-W-b）-partial-out-l-times-frac-partial-out-l-partial-net-l"><a href="#delta-l-1-times-frac-partial-（output-l-W-b）-partial-out-l-times-frac-partial-out-l-partial-net-l" class="headerlink" title="$= \delta^{(l+1)} \times   \frac{\partial （output^{(l)}*W+b）}{\partial out^{(l)}} \times \frac{\partial out^{(l)}}{\partial net^{(l)}}$"></a>$= \delta^{(l+1)} \times   \frac{\partial （output^{(l)}*W+b）}{\partial out^{(l)}} \times \frac{\partial out^{(l)}}{\partial net^{(l)}}$</h4><h4 id="delta-l-1-cdot-frac-partial-（output-l-W-b）-partial-out-l-cdot-sigma-prime-net-i-L"><a href="#delta-l-1-cdot-frac-partial-（output-l-W-b）-partial-out-l-cdot-sigma-prime-net-i-L" class="headerlink" title="$= \delta^{(l+1)} \cdot  \frac{\partial （output^{(l)}*W+b）}{\partial out^{(l)}}  \cdot \sigma^{\prime}(net_i^{(L)})$"></a>$= \delta^{(l+1)} \cdot  \frac{\partial （output^{(l)}*W+b）}{\partial out^{(l)}}  \cdot \sigma^{\prime}(net_i^{(L)})$</h4><p>接下来，就是计算卷积运算的求导：</p><h4 id="frac-partial-（output-l-W-b）-partial-out-l"><a href="#frac-partial-（output-l-W-b）-partial-out-l" class="headerlink" title="$\frac{\partial （output^{(l)}*W+b）}{\partial out^{(l)}}$"></a>$\frac{\partial （output^{(l)}*W+b）}{\partial out^{(l)}}$</h4><p>假如我们要处理如下的卷积操作：</p><h4 id="left-begin-array-ccc-a-11-amp-a-12-amp-a-13-a-21-amp-a-22-amp-a-23-a-31-amp-a-32-amp-a-33-end-array-right-left-begin-array-ccc-w-11-amp-w-12-w-21-amp-w-22-end-array-right-left-begin-array-ccc-z-11-amp-z-12-z-21-amp-z-22-end-array-right"><a href="#left-begin-array-ccc-a-11-amp-a-12-amp-a-13-a-21-amp-a-22-amp-a-23-a-31-amp-a-32-amp-a-33-end-array-right-left-begin-array-ccc-w-11-amp-w-12-w-21-amp-w-22-end-array-right-left-begin-array-ccc-z-11-amp-z-12-z-21-amp-z-22-end-array-right" class="headerlink" title="$\left( \begin{array}{ccc} a_{11}&amp;a_{12}&amp;a_{13} \\ a_{21}&amp;a_{22}&amp;a_{23} \\ a_{31}&amp;a_{32}&amp;a_{33} \end{array} \right)    *  \left( \begin{array}{ccc} w_{11}&amp;w_{12} \\ w_{21}&amp;w_{22} \end{array} \right) = \left( \begin{array}{ccc} z_{11}&amp;z_{12} \\ z_{21}&amp;z_{22} \end{array} \right)$"></a>$\left( \begin{array}{ccc} a_{11}&amp;a_{12}&amp;a_{13} \\ a_{21}&amp;a_{22}&amp;a_{23} \\ a_{31}&amp;a_{32}&amp;a_{33} \end{array} \right)    *  \left( \begin{array}{ccc} w_{11}&amp;w_{12} \\ w_{21}&amp;w_{22} \end{array} \right) = \left( \begin{array}{ccc} z_{11}&amp;z_{12} \\ z_{21}&amp;z_{22} \end{array} \right)$</h4><p>从这步操作来看，套用DNN反向传播的思想是行不通的，所以我们需要把卷积操作表示成如下等式。</p><h4 id="z-11-a-11-w-11-a-12-w-12-a-21-w-21-a-22-w-22-z-12-a-12-w-11-a-13-w-12-a-22-w-21-a-23-w-22-z-21-a-21-w-11-a-22-w-12-a-31-w-21-a-32-w-22-z-22-a-22-w-11-a-23-w-12-a-32-w-21-a-33-w-22"><a href="#z-11-a-11-w-11-a-12-w-12-a-21-w-21-a-22-w-22-z-12-a-12-w-11-a-13-w-12-a-22-w-21-a-23-w-22-z-21-a-21-w-11-a-22-w-12-a-31-w-21-a-32-w-22-z-22-a-22-w-11-a-23-w-12-a-32-w-21-a-33-w-22" class="headerlink" title="$z_{11} = a_{11}w_{11} + a_{12}w_{12} + a_{21}w_{21} +   a_{22}w_{22} \\ z_{12} = a_{12}w_{11} + a_{13}w_{12} + a_{22}w_{21} +   a_{23}w_{22} \\ z_{21} = a_{21}w_{11} + a_{22}w_{12} + a_{31}w_{21} +   a_{32}w_{22} \\ z_{22} = a_{22}w_{11} + a_{23}w_{12} + a_{32}w_{21} +   a_{33}w_{22}$"></a>$z_{11} = a_{11}w_{11} + a_{12}w_{12} + a_{21}w_{21} +   a_{22}w_{22} \\ z_{12} = a_{12}w_{11} + a_{13}w_{12} + a_{22}w_{21} +   a_{23}w_{22} \\ z_{21} = a_{21}w_{11} + a_{22}w_{12} + a_{31}w_{21} +   a_{32}w_{22} \\ z_{22} = a_{22}w_{11} + a_{23}w_{12} + a_{32}w_{21} +   a_{33}w_{22}$</h4><p>对每个元素进行求导：</p><h4 id="begin-align-nabla-a-11-amp-frac-partial-C-partial-z-11-frac-partial-z-11-partial-a-11-frac-partial-C-partial-z-12-frac-partial-z-12-partial-a-11-frac-partial-C-partial-z-21-frac-partial-z-21-partial-a-11-frac-partial-C-partial-z-22-frac-partial-z-22-partial-a-11-notag-amp-w-11-notag-end-align"><a href="#begin-align-nabla-a-11-amp-frac-partial-C-partial-z-11-frac-partial-z-11-partial-a-11-frac-partial-C-partial-z-12-frac-partial-z-12-partial-a-11-frac-partial-C-partial-z-21-frac-partial-z-21-partial-a-11-frac-partial-C-partial-z-22-frac-partial-z-22-partial-a-11-notag-amp-w-11-notag-end-align" class="headerlink" title="$\begin{align} \nabla a_{11} = &amp; \frac{\partial C}{\partial z_{11}} \frac{\partial z_{11}}{\partial a_{11}}+  \frac{\partial C}{\partial z_{12}}\frac{\partial z_{12}}{\partial a_{11}}+ \frac{\partial C}{\partial z_{21}}\frac{\partial z_{21}}{\partial a_{11}} + \frac{\partial C}{\partial z_{22}}\frac{\partial z_{22}}{\partial a_{11}} \notag \\ =&amp;w_{11} \notag \end{align}$"></a>$\begin{align} \nabla a_{11} = &amp; \frac{\partial C}{\partial z_{11}} \frac{\partial z_{11}}{\partial a_{11}}+  \frac{\partial C}{\partial z_{12}}\frac{\partial z_{12}}{\partial a_{11}}+ \frac{\partial C}{\partial z_{21}}\frac{\partial z_{21}}{\partial a_{11}} + \frac{\partial C}{\partial z_{22}}\frac{\partial z_{22}}{\partial a_{11}} \notag \\ =&amp;w_{11} \notag \end{align}$</h4><h4 id="begin-align-nabla-a-12-amp-frac-partial-C-partial-z-11-frac-partial-z-11-partial-a-12-frac-partial-C-partial-z-12-frac-partial-z-12-partial-a-12-frac-partial-C-partial-z-21-frac-partial-z-21-partial-a-12-frac-partial-C-partial-z-22-frac-partial-z-22-partial-a-12-notag-amp-w-12-w-11-notag-end-align"><a href="#begin-align-nabla-a-12-amp-frac-partial-C-partial-z-11-frac-partial-z-11-partial-a-12-frac-partial-C-partial-z-12-frac-partial-z-12-partial-a-12-frac-partial-C-partial-z-21-frac-partial-z-21-partial-a-12-frac-partial-C-partial-z-22-frac-partial-z-22-partial-a-12-notag-amp-w-12-w-11-notag-end-align" class="headerlink" title="$\begin{align} \nabla a_{12} =&amp; \frac{\partial C}{\partial z_{11}}\frac{\partial z_{11}}{\partial a_{12}} + \frac{\partial C}{\partial z_{12}}\frac{\partial z_{12}}{\partial a_{12}} + \frac{\partial C}{\partial z_{21}}\frac{\partial z_{21}}{\partial a_{12}} + \frac{\partial C}{\partial z_{22}}\frac{\partial z_{22}}{\partial a_{12}} \notag \\ =&amp;w_{12} +w_{11} \notag \end{align}$"></a>$\begin{align} \nabla a_{12} =&amp; \frac{\partial C}{\partial z_{11}}\frac{\partial z_{11}}{\partial a_{12}} + \frac{\partial C}{\partial z_{12}}\frac{\partial z_{12}}{\partial a_{12}} + \frac{\partial C}{\partial z_{21}}\frac{\partial z_{21}}{\partial a_{12}} + \frac{\partial C}{\partial z_{22}}\frac{\partial z_{22}}{\partial a_{12}} \notag \\ =&amp;w_{12} +w_{11} \notag \end{align}$</h4><p>同理可得其他的$\nabla a_{ij}$，当你求出全部的结果后，你会发现我们可以用一个卷积运算来解决(实际就是把原来的卷积核做了180°的转换)：</p><h4 id="left-begin-array-ccc-0-amp-0-amp-0-amp-0-0-amp-1-amp-1-amp-0-0-amp-1-amp-1-amp-0-0-amp-0-amp-0-amp-0-end-array-right-left-begin-array-ccc-w-22-amp-w-21-w-12-amp-w-11-end-array-right-left-begin-array-ccc-nabla-a-11-amp-nabla-a-12-amp-nabla-a-13-nabla-a-21-amp-nabla-a-22-amp-nabla-a-23-nabla-a-31-amp-nabla-a-32-amp-nabla-a-33-end-array-right"><a href="#left-begin-array-ccc-0-amp-0-amp-0-amp-0-0-amp-1-amp-1-amp-0-0-amp-1-amp-1-amp-0-0-amp-0-amp-0-amp-0-end-array-right-left-begin-array-ccc-w-22-amp-w-21-w-12-amp-w-11-end-array-right-left-begin-array-ccc-nabla-a-11-amp-nabla-a-12-amp-nabla-a-13-nabla-a-21-amp-nabla-a-22-amp-nabla-a-23-nabla-a-31-amp-nabla-a-32-amp-nabla-a-33-end-array-right" class="headerlink" title="$\left( \begin{array}{ccc} 0&amp;0&amp;0&amp;0 \\ 0&amp;\ 1&amp; 1&amp;0 \\ 0&amp;1 &amp;1&amp;0 \\ 0&amp;0&amp;0&amp;0 \end{array} \right) * \left( \begin{array}{ccc} w_{22}&amp;w_{21}\\ w_{12}&amp;w_{11} \end{array} \right)  = \left( \begin{array}{ccc} \nabla a_{11}&amp;\nabla a_{12}&amp;\nabla a_{13} \\ \nabla a_{21}&amp;\nabla a_{22}&amp;\nabla a_{23}\\ \nabla a_{31}&amp;\nabla a_{32}&amp;\nabla a_{33} \end{array} \right)$"></a>$\left( \begin{array}{ccc} 0&amp;0&amp;0&amp;0 \\ 0&amp;\ 1&amp; 1&amp;0 \\ 0&amp;1 &amp;1&amp;0 \\ 0&amp;0&amp;0&amp;0 \end{array} \right) * \left( \begin{array}{ccc} w_{22}&amp;w_{21}\\ w_{12}&amp;w_{11} \end{array} \right)  = \left( \begin{array}{ccc} \nabla a_{11}&amp;\nabla a_{12}&amp;\nabla a_{13} \\ \nabla a_{21}&amp;\nabla a_{22}&amp;\nabla a_{23}\\ \nabla a_{31}&amp;\nabla a_{32}&amp;\nabla a_{33} \end{array} \right)$</h4><h4 id="最终我们就可以得到："><a href="#最终我们就可以得到：" class="headerlink" title="最终我们就可以得到："></a>最终我们就可以得到：</h4><h4 id="frac-partial-（output-l-W-b）-partial-out-l-left-begin-array-ccc-0-amp-0-amp-0-amp-0-0-amp-1-amp-1-amp-0-0-amp-1-amp-1-amp-0-0-amp-0-amp-0-amp-0-end-array-right-left-begin-array-ccc-w-22-amp-w-21-w-12-amp-w-11-end-array-right"><a href="#frac-partial-（output-l-W-b）-partial-out-l-left-begin-array-ccc-0-amp-0-amp-0-amp-0-0-amp-1-amp-1-amp-0-0-amp-1-amp-1-amp-0-0-amp-0-amp-0-amp-0-end-array-right-left-begin-array-ccc-w-22-amp-w-21-w-12-amp-w-11-end-array-right" class="headerlink" title="$\frac{\partial （output^{(l)}*W+b）}{\partial out^{(l)}} =\left( \begin{array}{ccc} 0&amp;0&amp;0&amp;0 \\ 0&amp;\ 1&amp; 1&amp;0 \\ 0&amp;1 &amp;1&amp;0 \\ 0&amp;0&amp;0&amp;0 \end{array} \right) * \left( \begin{array}{ccc} w_{22}&amp;w_{21}\\ w_{12}&amp;w_{11} \end{array} \right)  $"></a>$\frac{\partial （output^{(l)}*W+b）}{\partial out^{(l)}} =\left( \begin{array}{ccc} 0&amp;0&amp;0&amp;0 \\ 0&amp;\ 1&amp; 1&amp;0 \\ 0&amp;1 &amp;1&amp;0 \\ 0&amp;0&amp;0&amp;0 \end{array} \right) * \left( \begin{array}{ccc} w_{22}&amp;w_{21}\\ w_{12}&amp;w_{11} \end{array} \right)  $</h4><h4 id="所以原来的-delta-l-w-l-1-delta-l-1-bigodot-sigma-prime-net-l-就变为："><a href="#所以原来的-delta-l-w-l-1-delta-l-1-bigodot-sigma-prime-net-l-就变为：" class="headerlink" title="所以原来的$\delta^{(l)} = (w^{(l+1)}) \delta^{(l+1)} \bigodot  \sigma^{\prime}(net^{(l)})$就变为："></a>所以原来的$\delta^{(l)} = (w^{(l+1)}) \delta^{(l+1)} \bigodot  \sigma^{\prime}(net^{(l)})$就变为：</h4><h4 id="delta-l-w-l-1-rot180-W-bigodot-sigma-prime-net-l"><a href="#delta-l-w-l-1-rot180-W-bigodot-sigma-prime-net-l" class="headerlink" title="$\delta^{(l)} = (w^{(l+1)}) *rot180(W) \bigodot  \sigma^{\prime}(net^{(l)})$"></a>$\delta^{(l)} = (w^{(l+1)}) *rot180(W) \bigodot  \sigma^{\prime}(net^{(l)})$</h4><h4 id="对于偏置项："><a href="#对于偏置项：" class="headerlink" title="对于偏置项："></a>对于偏置项：</h4><h4 id="原来的是：-frac-partial-E-total-partial-bias-i-l-delta-i-l"><a href="#原来的是：-frac-partial-E-total-partial-bias-i-l-delta-i-l" class="headerlink" title="原来的是：$\frac{\partial E_{total}}{\partial bias_i^{(l)}}=\delta_i^{(l)}$"></a>原来的是：$\frac{\partial E_{total}}{\partial bias_i^{(l)}}=\delta_i^{(l)}$</h4><h4 id="因为对于全连接网络：-b-delta-l-in-R-n-out-l"><a href="#因为对于全连接网络：-b-delta-l-in-R-n-out-l" class="headerlink" title="因为对于全连接网络：$b, \delta^{(l)} \in R(n_{out}^{(l)})$"></a>因为对于全连接网络：$b, \delta^{(l)} \in R(n_{out}^{(l)})$</h4><h4 id="而对于卷积神经网络：-b-in-R-n-out-l-delta-l-in-R-H-out-l-W-out-l-n-out-l"><a href="#而对于卷积神经网络：-b-in-R-n-out-l-delta-l-in-R-H-out-l-W-out-l-n-out-l" class="headerlink" title="而对于卷积神经网络：$b \in R(n_{out}^{(l)}), \delta^{(l)} \in R(H_{out}^{(l)}, W_{out}^{(l)}, n_{out}^{(l)})$"></a>而对于卷积神经网络：$b \in R(n_{out}^{(l)}), \delta^{(l)} \in R(H_{out}^{(l)}, W_{out}^{(l)}, n_{out}^{(l)})$</h4><h4 id="我们对应的处理方式就是："><a href="#我们对应的处理方式就是：" class="headerlink" title="我们对应的处理方式就是："></a>我们对应的处理方式就是：</h4><h4 id="将-delta-l-的H和W维度求和"><a href="#将-delta-l-的H和W维度求和" class="headerlink" title="将$\delta^{(l)}$的H和W维度求和"></a>将$\delta^{(l)}$的H和W维度求和</h4><h4 id="所以："><a href="#所以：" class="headerlink" title="所以："></a>所以：</h4><h4 id="frac-partial-E-total-partial-bias-i-l-sum-H-sum-W-delta-H-W-i-l"><a href="#frac-partial-E-total-partial-bias-i-l-sum-H-sum-W-delta-H-W-i-l" class="headerlink" title="$\frac{\partial E_{total}}{\partial bias_i^{(l)}}=\sum_H \sum_W \delta_{(H,W,i)}^{(l)}$"></a>$\frac{\partial E_{total}}{\partial bias_i^{(l)}}=\sum_H \sum_W \delta_{(H,W,i)}^{(l)}$</h4><h3 id="卷积神经网络反向传播的四部曲"><a href="#卷积神经网络反向传播的四部曲" class="headerlink" title="卷积神经网络反向传播的四部曲"></a>卷积神经网络反向传播的四部曲</h3><ul><li><h4 id="delta-i-L-bigtriangledown-out-E-total-times-sigma-prime-net-i-L-1"><a href="#delta-i-L-bigtriangledown-out-E-total-times-sigma-prime-net-i-L-1" class="headerlink" title="$\delta_i^{(L)}= \bigtriangledown_{out} E_{total} \times   \sigma^{\prime}(net_i^{(L)}) $"></a>$\delta_i^{(L)}= \bigtriangledown_{out} E_{total} \times   \sigma^{\prime}(net_i^{(L)}) $</h4></li><li><h4 id="delta-l-w-l-1-rot180-W-bigodot-sigma-prime-net-l-1"><a href="#delta-l-w-l-1-rot180-W-bigodot-sigma-prime-net-l-1" class="headerlink" title="$\delta^{(l)} = (w^{(l+1)}) *rot180(W) \bigodot  \sigma^{\prime}(net^{(l)})$"></a>$\delta^{(l)} = (w^{(l+1)}) *rot180(W) \bigodot  \sigma^{\prime}(net^{(l)})$</h4></li><li><h4 id="frac-partial-E-total-partial-bias-i-l-sum-H-sum-W-delta-H-W-i-l-1"><a href="#frac-partial-E-total-partial-bias-i-l-sum-H-sum-W-delta-H-W-i-l-1" class="headerlink" title="$\frac{\partial E_{total}}{\partial bias_i^{(l)}}=\sum_H \sum_W \delta_{(H,W,i)}^{(l)}$"></a>$\frac{\partial E_{total}}{\partial bias_i^{(l)}}=\sum_H \sum_W \delta_{(H,W,i)}^{(l)}$</h4></li><li><h4 id="frac-partial-E-total-partial-w-l-rot180-delta-l-outh-l-1"><a href="#frac-partial-E-total-partial-w-l-rot180-delta-l-outh-l-1" class="headerlink" title="$\frac{\partial E_{total}}{\partial w^{(l)}}=rot180(\delta^{(l)}*(outh^{(l-1)}))$"></a>$\frac{\partial E_{total}}{\partial w^{(l)}}=rot180(\delta^{(l)}*(outh^{(l-1)}))$</h4></li></ul><h3 id="池化层"><a href="#池化层" class="headerlink" title="池化层"></a>池化层</h3><p>跟卷积层一样，我们先把 pooling 层也放回网络连接的形式中： </p><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20200601182249.png" srcset="/img/loading.gif" alt="images"></p><p>红色神经元是前一层的响应结果，一般是卷积后再用激活函数处理。绿色的神经元表示 pooling 层。</p><p>很明显，pooling 主要是起到<code>降维</code>的作用，而且，由于 pooling 时没有参数需要学习，</p><p>因此，当得到 pooling 层的误差项 $δ^l $后，我们只需要计算上一层的误差项 $δ^{l−1}$即可。</p><p>要注意的一点是，由于 pooling 一般会降维，因此传回去的误差矩阵要调整维度，即 upsample。这样，误差传播的公式原型大概是： </p><h4 id="delta-l-1-upsample-delta-l-odot-sigma’-z-l-1"><a href="#delta-l-1-upsample-delta-l-odot-sigma’-z-l-1" class="headerlink" title="$\delta^{l-1}=upsample(\delta^l) \odot \sigma’(z^{l-1})$"></a>$\delta^{l-1}=upsample(\delta^l) \odot \sigma’(z^{l-1})$</h4><p>下面以最常用的 <strong>average pooling</strong> 和 <strong>max pooling</strong> 为例，讲讲 $upsample(δ^l)$具体要怎么处理。 </p><p>假设 pooling 层的区域大小为 2×22×2，pooling 这一层的误差项为： </p><h4 id="delta-l-left-begin-array-ccc-2-amp-8-4-amp-6-end-array-right"><a href="#delta-l-left-begin-array-ccc-2-amp-8-4-amp-6-end-array-right" class="headerlink" title="$\delta^l= \left( \begin{array}{ccc} 2 &amp; 8 \\ 4 &amp; 6 \end{array} \right)$"></a>$\delta^l= \left( \begin{array}{ccc} 2 &amp; 8 \\ 4 &amp; 6 \end{array} \right)$</h4><p>首先，我们先把维度还原到上一层的维度： </p><h4 id="left-begin-array-ccc-0-amp-0-amp-0-amp-0-0-amp-2-amp-8-amp-0-0-amp-4-amp-6-amp-0-0-amp-0-amp-0-amp-0-end-array-right"><a href="#left-begin-array-ccc-0-amp-0-amp-0-amp-0-0-amp-2-amp-8-amp-0-0-amp-4-amp-6-amp-0-0-amp-0-amp-0-amp-0-end-array-right" class="headerlink" title="$\left( \begin{array}{ccc} 0 &amp; 0 &amp; 0 &amp; 0 \\ 0 &amp; 2 &amp; 8 &amp; 0  \\ 0 &amp; 4 &amp; 6 &amp; 0 \\ 0 &amp; 0 &amp; 0 &amp; 0  \end{array} \right)$"></a>$\left( \begin{array}{ccc} 0 &amp; 0 &amp; 0 &amp; 0 \\ 0 &amp; 2 &amp; 8 &amp; 0  \\ 0 &amp; 4 &amp; 6 &amp; 0 \\ 0 &amp; 0 &amp; 0 &amp; 0  \end{array} \right)$</h4><p>在 average pooling 中，我们是把一个范围内的响应值取平均后，作为一个 pooling unit 的结果。可以认为是经过一个 <strong>average()</strong> 函数，即 $average(x)=\frac{1}{m}\sum_{k=1}^m x_k$ 在本例中，$m=4$。则对每个 $x_k$ 的导数均为：</p><h4 id="frac-partial-average-x-partial-x-k-frac-1-m"><a href="#frac-partial-average-x-partial-x-k-frac-1-m" class="headerlink" title="$\frac{\partial average(x)}{\partial x_k}=\frac{1}{m}$"></a>$\frac{\partial average(x)}{\partial x_k}=\frac{1}{m}$</h4><p>因此，对 average pooling 来说，其误差项为： </p><h4 id="begin-align-delta-l-1-amp-delta-l-frac-partial-average-partial-x-odot-sigma’-z-l-1-notag-amp-upsample-delta-l-odot-sigma’-z-l-1-amp-left-begin-array-ccc-0-5-amp-0-5-amp-2-amp-2-0-5-amp-0-5-amp-2-amp-2-1-amp-1-amp-1-5-amp-1-5-1-amp-1-amp-1-5-amp-1-5-end-array-right-odot-sigma’-z-l-1-end-align"><a href="#begin-align-delta-l-1-amp-delta-l-frac-partial-average-partial-x-odot-sigma’-z-l-1-notag-amp-upsample-delta-l-odot-sigma’-z-l-1-amp-left-begin-array-ccc-0-5-amp-0-5-amp-2-amp-2-0-5-amp-0-5-amp-2-amp-2-1-amp-1-amp-1-5-amp-1-5-1-amp-1-amp-1-5-amp-1-5-end-array-right-odot-sigma’-z-l-1-end-align" class="headerlink" title="$\begin{align} \delta^{l-1}=&amp;\delta^l \frac{\partial average}{\partial x} \odot \sigma’(z^{l-1}) \notag \\ =&amp;upsample(\delta^l) \odot \sigma’(z^{l-1})  \\  =&amp;\left( \begin{array}{ccc} 0.5&amp;0.5&amp;2&amp;2 \\ 0.5&amp;0.5&amp;2&amp;2 \\ 1&amp;1&amp;1.5&amp;1.5 \\ 1&amp;1&amp;1.5&amp;1.5 \end{array} \right)\odot \sigma’(z^{l-1})   \end{align}$"></a>$\begin{align} \delta^{l-1}=&amp;\delta^l \frac{\partial average}{\partial x} \odot \sigma’(z^{l-1}) \notag \\ =&amp;upsample(\delta^l) \odot \sigma’(z^{l-1})  \\  =&amp;\left( \begin{array}{ccc} 0.5&amp;0.5&amp;2&amp;2 \\ 0.5&amp;0.5&amp;2&amp;2 \\ 1&amp;1&amp;1.5&amp;1.5 \\ 1&amp;1&amp;1.5&amp;1.5 \end{array} \right)\odot \sigma’(z^{l-1})   \end{align}$</h4><p>在 max pooling 中，则是经过一个 <strong>max()</strong> 函数，对应的导数为： </p><h4 id="frac-partial-max-x-partial-x-k-begin-cases-1-amp-if-x-k-max-x-0-amp-otherwise-end-cases"><a href="#frac-partial-max-x-partial-x-k-begin-cases-1-amp-if-x-k-max-x-0-amp-otherwise-end-cases" class="headerlink" title="$\frac{\partial \max(x)}{\partial x_k}=\begin{cases} 1 &amp; if\ x_k=max(x) \ 0 &amp; otherwise \end{cases}$"></a>$\frac{\partial \max(x)}{\partial x_k}=\begin{cases} 1 &amp; if\ x_k=max(x) \ 0 &amp; otherwise \end{cases}$</h4><p>假设前向传播时记录的最大值位置分别是左上、右下、右上、左下，则误差项为：  </p><h4 id="delta-l-1-left-begin-array-ccc-2-amp-0-amp-0-amp-0-0-amp-0-amp-0-amp-8-0-amp-4-amp-0-amp-0-0-amp-0-amp-6-amp-0-end-array-right-odot-sigma’-z-l-1"><a href="#delta-l-1-left-begin-array-ccc-2-amp-0-amp-0-amp-0-0-amp-0-amp-0-amp-8-0-amp-4-amp-0-amp-0-0-amp-0-amp-6-amp-0-end-array-right-odot-sigma’-z-l-1" class="headerlink" title="$\delta^{l-1}=\left( \begin{array}{ccc} 2&amp;0&amp;0&amp;0 \\ 0&amp;0&amp; 0&amp;8 \\ 0&amp;4&amp;0&amp;0 \\ 0&amp;0&amp;6&amp;0 \end{array} \right) \odot \sigma’(z^{l-1})  $"></a>$\delta^{l-1}=\left( \begin{array}{ccc} 2&amp;0&amp;0&amp;0 \\ 0&amp;0&amp; 0&amp;8 \\ 0&amp;4&amp;0&amp;0 \\ 0&amp;0&amp;6&amp;0 \end{array} \right) \odot \sigma’(z^{l-1})  $</h4><p>总结：在遇到池化层的时间就执行upsampling的反向操作。就这样实现了池化层的反向传播。</p><h4 id="这就是卷积神经网络的反向传播算法"><a href="#这就是卷积神经网络的反向传播算法" class="headerlink" title="这就是卷积神经网络的反向传播算法"></a>这就是卷积神经网络的反向传播算法</h4>]]></content>
    
    
    <categories>
      
      <category>深度学习</category>
      
    </categories>
    
    
    <tags>
      
      <tag>深度学习</tag>
      
      <tag>神经网络</tag>
      
      <tag>卷积神经网络</tag>
      
      <tag>反向传播</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>神经网络之卷积神经网络</title>
    <link href="/2018/08/DL-CNN/"/>
    <url>/2018/08/DL-CNN/</url>
    
    <content type="html"><![CDATA[<h3 id="卷积神经网络（Convolutional-Neural-Network-CNN）"><a href="#卷积神经网络（Convolutional-Neural-Network-CNN）" class="headerlink" title="卷积神经网络（Convolutional Neural Network, CNN）"></a>卷积神经网络（Convolutional Neural Network, CNN）</h3><p><code>卷积神经网络</code>是近年来广泛应用于模式识别、图像处理等领域的一种高效识别算法，它具有结构简单、训练参数少和适应性强等特点。</p><p><code>卷积神经网络</code>是一种前馈神经网络，它的人工神经元可以响应一部分覆盖范围内的周围单元，对于大型图像处理有出色表现。</p><p><code>卷积神经网络</code>由一个或多个卷积计算层和顶端的全连接层（经典神经网络）组成，同时也包括关联权重和池化层。这一结构是的卷积神经网络能够利用输入数据的二维结构。与其他神经网络结构相比，卷积神经网络在图像和语音识别方面能够给出更优的结果。相比较其他深度、前馈神经网络，卷积神经网络需要估计的参数更少，从而使之成为一种颇具吸引力的深度学习框架。</p><p>我们前面所接触到的神经网络的结构是这样的：</p><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20200601182215.png" srcset="/img/loading.gif" alt="images"></p><p>卷积神经网络依旧是层级网络，只是层的功能和形式做了变化，也可以是传统神经网络的一个改进，不同的层次有不同运算与功能，如下图中就多了很多传统神经网络中所没有的层次。</p><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20200601182216.png" srcset="/img/loading.gif" alt="images"></p><h3 id="卷积神经网络的核心思想"><a href="#卷积神经网络的核心思想" class="headerlink" title="卷积神经网络的核心思想"></a>卷积神经网络的核心思想</h3><ul><li><code>局部感受野</code>：普通的多层感知器中，隐层节点会全连接到一个图像的每个像素点上；而在卷积神经网络中，每个隐层的节点只连接到图像某个足够小的像素点上，从而大大减少需要训练的权重参数。</li><li><code>权值共享</code>：在卷积神经网中，同一个卷积核内，所有的神经元的权值是相同的，从而大大减少需要训练的参数。</li><li><code>池化</code>：在卷积神经网络中，没有必要一定就要对原图像做处理，而是可以使用某种“压缩”方法，这就是池化，也就是每次将原图像卷积后，都通过一个下采样的过程，来减小图像的规模。 </li></ul><h3 id="卷积神经网络的层级结构"><a href="#卷积神经网络的层级结构" class="headerlink" title="卷积神经网络的层级结构"></a>卷积神经网络的层级结构</h3><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20200601182217.png" srcset="/img/loading.gif" alt="images"></p><ul><li><h4 id="数据输入层（Input-layer）"><a href="#数据输入层（Input-layer）" class="headerlink" title="数据输入层（Input layer）"></a><strong>数据输入层（Input layer）</strong></h4></li><li><h4 id="卷积计算层（CONV-layer）"><a href="#卷积计算层（CONV-layer）" class="headerlink" title="卷积计算层（CONV layer）"></a><strong>卷积计算层（CONV layer）</strong></h4></li><li><h4 id="ReLU激活层（ReLU-layer）"><a href="#ReLU激活层（ReLU-layer）" class="headerlink" title="ReLU激活层（ReLU layer）"></a><strong>ReLU激活层（ReLU layer）</strong></h4></li><li><h4 id="池化层（POOling-layer）"><a href="#池化层（POOling-layer）" class="headerlink" title="池化层（POOling layer）"></a><strong>池化层（POOling layer）</strong></h4></li><li><h4 id="全连接层（FC-layer）"><a href="#全连接层（FC-layer）" class="headerlink" title="全连接层（FC layer）"></a><strong>全连接层（FC layer）</strong></h4></li></ul><h3 id="数据输入层（Input-layer）-1"><a href="#数据输入层（Input-layer）-1" class="headerlink" title="数据输入层（Input layer）"></a><strong>数据输入层（Input layer）</strong></h3><h4 id="任务：主要是对原始图像进行预处理"><a href="#任务：主要是对原始图像进行预处理" class="headerlink" title="任务：主要是对原始图像进行预处理"></a><code>任务</code>：主要是对原始图像进行预处理</h4><ul><li><h4 id="去均值：把输入数据的各个维度都中心化到0。"><a href="#去均值：把输入数据的各个维度都中心化到0。" class="headerlink" title="去均值：把输入数据的各个维度都中心化到0。"></a><code>去均值</code>：把输入数据的各个维度都中心化到0。</h4></li><li><h4 id="归一化：把数据的幅度归一化到同样的范围。"><a href="#归一化：把数据的幅度归一化到同样的范围。" class="headerlink" title="归一化：把数据的幅度归一化到同样的范围。"></a><code>归一化</code>：把数据的幅度归一化到同样的范围。</h4></li><li><h4 id="PCA-白化：用PCA降维、白化是对数据的每个特征轴上的幅度归一化。"><a href="#PCA-白化：用PCA降维、白化是对数据的每个特征轴上的幅度归一化。" class="headerlink" title="PCA/白化：用PCA降维、白化是对数据的每个特征轴上的幅度归一化。"></a><code>PCA/白化</code>：用PCA降维、白化是对数据的每个特征轴上的幅度归一化。</h4></li></ul><h4 id="举个栗子："><a href="#举个栗子：" class="headerlink" title="举个栗子："></a>举个栗子：</h4><h4 id="去均值与归一化"><a href="#去均值与归一化" class="headerlink" title="去均值与归一化"></a><code>去均值与归一化</code></h4><p>去均值的目的在于把样本的中心拉回到坐标系原点上；归一化的目的就是减少各维度数据取值范围的差异而带来的干扰。比如，我们有两个维度的特征A和B，A范围是0到10，而B范围是0到10000，如果直接使用这两个特征是有问题的，好的做法就是归一化，即A和B的数据都变为0到1的范围。 </p><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20200601182218.png" srcset="/img/loading.gif" alt="images"></p><h4 id="去相关与白化"><a href="#去相关与白化" class="headerlink" title="去相关与白化"></a><code>去相关与白化</code></h4><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20200601182219.gif" srcset="/img/loading.gif" alt="images"></p><h3 id="卷积计算层（CONV-layer）-1"><a href="#卷积计算层（CONV-layer）-1" class="headerlink" title="卷积计算层（CONV layer）"></a><strong>卷积计算层（CONV layer）</strong></h3><h4 id="任务：对输入的图像进行特征提取"><a href="#任务：对输入的图像进行特征提取" class="headerlink" title="任务：对输入的图像进行特征提取"></a><code>任务</code>：对输入的图像进行特征提取</h4><p>卷积计算层是卷积神经网络中最重要的一个层次，也是“卷积神经网络”的名字来源。</p><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20200601182220.png" srcset="/img/loading.gif" alt="images"></p><h4 id="举个栗子：-1"><a href="#举个栗子：-1" class="headerlink" title="举个栗子："></a>举个栗子：</h4><p>假设一张图像有 5x5 个像素，1 代表白，0 代表黑，这幅图像被视为 5x5 的单色图像。现在用一个由随机地 0 和 1 组成的 3x3 矩阵去和图像中的子区域做Hadamard乘积，每次迭代移动一个像素，这样该乘法会得到一个新的 3x3 的矩阵。下面的动图展示了这个过程。 </p><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20200601182221.png" srcset="/img/loading.gif" alt="images"></p><h4 id="直观上来理解："><a href="#直观上来理解：" class="headerlink" title="直观上来理解："></a><strong>直观上来理解</strong>：</h4><ul><li>用一个小的权重矩阵去覆盖输入数据，对应位置元素加权相乘，其和作为结果的一个像素点。</li><li>这个权重在输入数据上滑动，形成一张新的矩阵</li><li>这个权重矩阵就被称为<code>卷积核</code>（convolution kernel）</li><li>其覆盖的位置称为<code>感受野</code>（receptive fileld ）</li><li>生成的新矩阵叫做<code>特征图</code>（feature map）</li></ul><p>分解开来，就如下图所示：</p><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20200601182222.gif" srcset="/img/loading.gif" alt="images"></p><h4 id="其中："><a href="#其中：" class="headerlink" title="其中："></a>其中：</h4><ul><li><p>滑动的像素数量就叫做<code>步长</code>（stride），步长为1，表示跳过1个像素，步长为2，就表示跳过2个像素，以此类推</p><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20200601182223.gif" srcset="/img/loading.gif" alt="images"></p></li><li><p>以卷积核的边还是中心点作为开始/结束的依据，决定了卷积的<code>补齐</code>（padding）方式。前面我们所举的栗子是<code>valid</code>方式，而<code>same</code>方式则会在图像的边缘用0补齐，如将前面的<code>valid</code>改为<code>same</code>方式，如图所示：</p></li></ul><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20200601182224.png" srcset="/img/loading.gif" alt="images"></p><p>其采样方式对应变换为：</p><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20200601182225.gif" srcset="/img/loading.gif" alt="images"></p><ul><li>我们前面所提到的输入图像都是灰色的，只有一个通道，但是我们一般会遇到输入通道不只有一个，那么卷积核是三阶的，也就是说所有的通道的结果做累加。</li></ul><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20200601182226.gif" srcset="/img/loading.gif" alt="images"></p><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20200601182227.gif" srcset="/img/loading.gif" alt="images"></p><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20200601182228.png" srcset="/img/loading.gif" alt="images"></p><p>当然，最后，这里有一个术语：“<code>偏置</code>（bias）”，每个输出滤波器都有一个偏置项，偏置被添加到输出通道产生最终输出通道。 </p><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20200601182229.png" srcset="/img/loading.gif" alt="images"></p><h4 id="再举个栗子："><a href="#再举个栗子：" class="headerlink" title="再举个栗子："></a>再举个栗子：</h4><p>下面蓝色矩阵周围有一圈灰色的框，那些就是上面所说到的填充值 (padding=same)的方式。这里的蓝色矩阵就是输入的图像，粉色矩阵就是卷积层的神经元，这里表示了有两个神经元（w0,w1）。绿色矩阵就是经过卷积运算后的输出矩阵，这里的步长设置为2。 </p><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20200601182230.png" srcset="/img/loading.gif" alt="images"></p><h3 id="ReLU激活层（ReLU-layer）-1"><a href="#ReLU激活层（ReLU-layer）-1" class="headerlink" title="ReLU激活层（ReLU layer）"></a><strong>ReLU激活层（ReLU layer）</strong></h3><h4 id="任务：卷积后的结果压缩到某一个固定的范围做非线性映射，这样可以一直保持一层一层下去的数值范围是可控的。"><a href="#任务：卷积后的结果压缩到某一个固定的范围做非线性映射，这样可以一直保持一层一层下去的数值范围是可控的。" class="headerlink" title="任务：卷积后的结果压缩到某一个固定的范围做非线性映射，这样可以一直保持一层一层下去的数值范围是可控的。"></a><code>任务</code>：卷积后的结果压缩到某一个固定的范围做非线性映射，这样可以一直保持一层一层下去的数值范围是可控的。</h4><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20200601182231.png" srcset="/img/loading.gif" alt="images"></p><h4 id="激活函数："><a href="#激活函数：" class="headerlink" title="激活函数："></a>激活函数：</h4><ul><li><code>Sigmoid</code></li><li><code>Tanh</code>（双曲正切）</li><li><code>ReLU</code></li><li><code>Leaky ReLU</code></li><li><code>ELU</code></li><li><code>Maxout</code></li></ul><p>卷积神经网络一般采用的激活函数是ReLU(The Rectified Linear Unit/修正线性单元)，它的特点是收敛快，求梯度简单，但较脆弱，图像如下：</p><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20200601182232.gif" srcset="/img/loading.gif" alt="images"></p><h4 id="激励层的实践经验："><a href="#激励层的实践经验：" class="headerlink" title="激励层的实践经验： 　　"></a>激励层的实践经验： 　　</h4><ul><li>不要用sigmoid！不要用sigmoid！不要用sigmoid！ 　　</li><li>首先试RELU，因为快，但要小心点 　　、</li><li>如果2失效，请用Leaky ReLU或者Maxout 　　</li><li>某些情况下tanh倒是有不错的结果，但是很少 </li></ul><h3 id="池化层（POOling-layer）-1"><a href="#池化层（POOling-layer）-1" class="headerlink" title="池化层（POOling layer）"></a><strong>池化层（POOling layer）</strong></h3><h4 id="任务：对特征进行采样，即用一个数值替代一块区域，主要是为了降低网络训练参数及模型的过拟合程度。"><a href="#任务：对特征进行采样，即用一个数值替代一块区域，主要是为了降低网络训练参数及模型的过拟合程度。" class="headerlink" title="任务：对特征进行采样，即用一个数值替代一块区域，主要是为了降低网络训练参数及模型的过拟合程度。"></a><code>任务</code>：对特征进行采样，即用一个数值替代一块区域，主要是为了降低网络训练参数及模型的过拟合程度。</h4><p>通过卷积层获得了图像的特征之后，理论上我们可以直接使用这些特征训练分类器（如softmax），但是这样做将面临巨大的计算量的挑战，而且容易产生过拟合的现象。为了进一步降低网络训练参数及模型的过拟合程度，对卷积层进行池化/采样(Pooling)处理。池化/采样的方式通常有以下两种：</p><ol><li>最大池化（Max Pooling: 选择Pooling窗口中的最大值作为采样值；</li><li>均值池化（Mean Pooling）: 将Pooling窗口中的所有值相加取平均，以平均值作为采样值</li><li>高斯池化：借鉴高斯模糊的方法。不常用。</li><li>可训练池化：使用一个训练函数$y=f(x)$。不常用。</li></ol><p>主要使用不同的函数为输入降维。通常，最大池化层（max-pooling layer）出现在卷积层之后。池化层使用 2*2 的矩阵，以卷积层相同的方式处理图像，不过它是给图像本身降维。下面分别是使用「最大池化」和「平均池化」的示例。 </p><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20200601182233.png" srcset="/img/loading.gif" alt="images"></p><p>图像经过池化后，得到的是一系列的特征图，而多层感知器接受的输入是一个向量。因此需要将这些特征图中的像素依次取出，排列成一个向量（这个过程被称为光栅化）。 </p><h3 id="全连接层（FC-layer）-1"><a href="#全连接层（FC-layer）-1" class="headerlink" title="全连接层（FC layer）"></a><strong>全连接层（FC layer）</strong></h3><h4 id="任务：全连接层的每一个结点都与上一层的所有结点相连，用来把前边提取到的特征综合起来。由于其全相连的特性，一般全连接层的参数也是最多的。主要是为了分类或回归，当然也可以没有。"><a href="#任务：全连接层的每一个结点都与上一层的所有结点相连，用来把前边提取到的特征综合起来。由于其全相连的特性，一般全连接层的参数也是最多的。主要是为了分类或回归，当然也可以没有。" class="headerlink" title="任务：全连接层的每一个结点都与上一层的所有结点相连，用来把前边提取到的特征综合起来。由于其全相连的特性，一般全连接层的参数也是最多的。主要是为了分类或回归，当然也可以没有。"></a><code>任务</code>：全连接层的每一个结点都与上一层的所有结点相连，用来把前边提取到的特征综合起来。由于其全相连的特性，一般全连接层的参数也是最多的。主要是为了分类或回归，当然也可以没有。</h4><p>其原理和我们前面所推导的DNN是一样的。</p><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20200601182234.png" srcset="/img/loading.gif" alt="images"></p><h4 id="两层之间的所有神经元都有权重连接，通常全连接层在卷积神经网络的尾部。"><a href="#两层之间的所有神经元都有权重连接，通常全连接层在卷积神经网络的尾部。" class="headerlink" title="两层之间的所有神经元都有权重连接，通常全连接层在卷积神经网络的尾部。"></a>两层之间的所有神经元都有权重连接，通常全连接层在卷积神经网络的尾部。</h4><h3 id="Dropout层（Dropout-layer）"><a href="#Dropout层（Dropout-layer）" class="headerlink" title="Dropout层（Dropout layer）"></a><strong>Dropout层（Dropout layer）</strong></h3><h4 id="任务：在模型训练时随机让网络某些隐含层节点的权重不工作，不工作的那些节点可以暂时认为不是网络结构的一部分，但是它的权重得保留下来（只是暂时不更新而已），因为下次样本输入时它可能又得工作了。主要是为了防止过拟合。"><a href="#任务：在模型训练时随机让网络某些隐含层节点的权重不工作，不工作的那些节点可以暂时认为不是网络结构的一部分，但是它的权重得保留下来（只是暂时不更新而已），因为下次样本输入时它可能又得工作了。主要是为了防止过拟合。" class="headerlink" title="任务：在模型训练时随机让网络某些隐含层节点的权重不工作，不工作的那些节点可以暂时认为不是网络结构的一部分，但是它的权重得保留下来（只是暂时不更新而已），因为下次样本输入时它可能又得工作了。主要是为了防止过拟合。"></a><code>任务</code>：在模型训练时随机让网络某些隐含层节点的权重不工作，不工作的那些节点可以暂时认为不是网络结构的一部分，但是它的权重得保留下来（只是暂时不更新而已），因为下次样本输入时它可能又得工作了。主要是为了防止过拟合。</h4><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20200601182235.gif" srcset="/img/loading.gif" alt="images"></p><p>上图a是标准的一个全连接的神经网络，b是对a应用了dropout的结果，它会以一定的概率(dropout probability)随机的丢弃掉一些神经元。 </p><h3 id="卷积神经网络之优缺点"><a href="#卷积神经网络之优缺点" class="headerlink" title="卷积神经网络之优缺点"></a><strong>卷积神经网络之优缺点</strong></h3><h4 id="优点"><a href="#优点" class="headerlink" title="优点 　　"></a>优点 　　</h4><ul><li>共享卷积核，对高维数据处理无压力 　　</li><li>无需手动选取特征，训练好权重，即得特征分类效果好 </li></ul><h4 id="缺点"><a href="#缺点" class="headerlink" title="缺点 　　"></a>缺点 　　</h4><ul><li>需要调参，需要大样本量，训练最好要GPU 　　</li><li>物理含义不明确（也就说，我们并不知道每个卷积层到底提取到的是什么特征，而且神经网络本身就是一种难以解释的“黑箱模型”） </li></ul><h3 id="卷积神经网络之典型CNN"><a href="#卷积神经网络之典型CNN" class="headerlink" title="卷积神经网络之典型CNN"></a><strong>卷积神经网络之典型CNN</strong></h3><ul><li><code>LeNet</code>，这是最早用于数字识别的CNN 　　</li><li><code>AlexNet</code>， 2012 ILSVRC比赛远超第2名的CNN，比 LeNet更深，用多层小卷积层叠加替换单大卷积层。 </li><li><code>ZF Net</code>， 2013 ILSVRC比赛冠军 　　</li><li><code>GoogLeNet</code>， 2014 ILSVRC比赛冠军 　　</li><li><code>VGGNet</code>， 2014 ILSVRC比赛中的模型，图像识别略差于GoogLeNet，但是在很多图像转化学习问题(比如object detection)上效果奇好 </li></ul><h3 id="卷积神经网络的常用框架"><a href="#卷积神经网络的常用框架" class="headerlink" title="卷积神经网络的常用框架"></a><strong>卷积神经网络的常用框架</strong></h3><h4 id="Caffe"><a href="#Caffe" class="headerlink" title="Caffe"></a><strong>Caffe</strong></h4><ul><li><h4 id="源于Berkeley的主流CV工具包，支持C-python-matlab"><a href="#源于Berkeley的主流CV工具包，支持C-python-matlab" class="headerlink" title="源于Berkeley的主流CV工具包，支持C++,python,matlab"></a>源于Berkeley的主流CV工具包，支持C++,python,matlab</h4></li><li><h4 id="Model-Zoo中有大量预训练好的模型供使用"><a href="#Model-Zoo中有大量预训练好的模型供使用" class="headerlink" title="Model Zoo中有大量预训练好的模型供使用"></a>Model Zoo中有大量预训练好的模型供使用</h4></li></ul><h4 id="Torch"><a href="#Torch" class="headerlink" title="Torch"></a><strong>Torch</strong></h4><ul><li><h4 id="Facebook用的卷积神经网络工具包"><a href="#Facebook用的卷积神经网络工具包" class="headerlink" title="Facebook用的卷积神经网络工具包"></a>Facebook用的卷积神经网络工具包</h4></li><li><h4 id="通过时域卷积的本地接口，使用非常直观"><a href="#通过时域卷积的本地接口，使用非常直观" class="headerlink" title="通过时域卷积的本地接口，使用非常直观"></a>通过时域卷积的本地接口，使用非常直观</h4></li><li><h4 id="定义新网络层简单"><a href="#定义新网络层简单" class="headerlink" title="定义新网络层简单"></a>定义新网络层简单</h4></li></ul><h4 id="TensorFlow"><a href="#TensorFlow" class="headerlink" title="TensorFlow"></a><strong>TensorFlow</strong></h4><ul><li><h4 id="Google的深度学习框架"><a href="#Google的深度学习框架" class="headerlink" title="Google的深度学习框架"></a>Google的深度学习框架</h4></li><li><h4 id="TensorBoard可视化很方便"><a href="#TensorBoard可视化很方便" class="headerlink" title="TensorBoard可视化很方便"></a>TensorBoard可视化很方便</h4></li><li><h4 id="数据和模型并行化好，速度快"><a href="#数据和模型并行化好，速度快" class="headerlink" title="数据和模型并行化好，速度快"></a>数据和模型并行化好，速度快</h4></li></ul>]]></content>
    
    
    <categories>
      
      <category>深度学习</category>
      
    </categories>
    
    
    <tags>
      
      <tag>深度学习</tag>
      
      <tag>神经网络</tag>
      
      <tag>卷积神经网络</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>TensorFlow实现简单的深度神经网络-DNN</title>
    <link href="/2018/08/DL-DNN-python/"/>
    <url>/2018/08/DL-DNN-python/</url>
    
    <content type="html"><![CDATA[<h3 id="环境设定"><a href="#环境设定" class="headerlink" title="环境设定"></a>环境设定</h3><pre><code class="hljs python"><span class="hljs-keyword">import</span> tensorflow <span class="hljs-keyword">as</span> tf<span class="hljs-keyword">from</span> tensorflow.examples.tutorials.mnist <span class="hljs-keyword">import</span> input_data<span class="hljs-keyword">import</span> osos.environ[<span class="hljs-string">'TF_CPP_MIN_LOG_LEVEL'</span>] = <span class="hljs-string">'2'</span></code></pre><h3 id="数据准备"><a href="#数据准备" class="headerlink" title="数据准备"></a>数据准备</h3><pre><code class="hljs python"><span class="hljs-comment"># 使用tensorflow自带的工具加载MNIST手写数字集合</span>mnist = input_data.read_data_sets(<span class="hljs-string">'data/mnist'</span>, one_hot=<span class="hljs-literal">True</span>)<span class="hljs-comment"># 查看数据的维度和target的维度</span>print(mnist.train.images.shape)print(mnist.train.labels.shape)</code></pre><h3 id="准备好placeholder"><a href="#准备好placeholder" class="headerlink" title="准备好placeholder"></a>准备好placeholder</h3><pre><code class="hljs python">X = tf.placeholder(tf.float32, [<span class="hljs-literal">None</span>, <span class="hljs-number">784</span>])Y = tf.placeholder(tf.float32, [<span class="hljs-literal">None</span>, <span class="hljs-number">10</span>])</code></pre><h3 id="准备权重参数"><a href="#准备权重参数" class="headerlink" title="准备权重参数"></a>准备权重参数</h3><pre><code class="hljs python"><span class="hljs-comment"># 网格参数设置--三层网络结构</span>n_input = <span class="hljs-number">784</span>  <span class="hljs-comment"># MNIST 数据输入(28*28*1=784)</span>n_hidden_1 = <span class="hljs-number">512</span>  <span class="hljs-comment"># 第一个隐层</span>n_hidden_2 = <span class="hljs-number">256</span>  <span class="hljs-comment"># 第二个隐层</span>n_hidden_3 = <span class="hljs-number">128</span>  <span class="hljs-comment"># 第三个隐层</span>n_classes = <span class="hljs-number">10</span>  <span class="hljs-comment"># MNIST 总共10个手写数字类别</span><span class="hljs-comment"># 权重参数</span>weights = &#123;    <span class="hljs-string">'h1'</span>: tf.Variable(tf.random_normal([n_input, n_hidden_1])),    <span class="hljs-string">'h2'</span>: tf.Variable(tf.random_normal([n_hidden_1, n_hidden_2])),    <span class="hljs-string">'h3'</span>: tf.Variable(tf.random_normal([n_hidden_2, n_hidden_3])),    <span class="hljs-string">'out'</span>: tf.Variable(tf.random_normal([n_hidden_3, n_classes]))&#125;biases = &#123;    <span class="hljs-string">'b1'</span>: tf.Variable(tf.random_normal([n_hidden_1])),    <span class="hljs-string">'b2'</span>: tf.Variable(tf.random_normal([n_hidden_2])),    <span class="hljs-string">'b3'</span>: tf.Variable(tf.random_normal([n_hidden_3])),    <span class="hljs-string">'out'</span>: tf.Variable(tf.random_normal([n_classes]))&#125;</code></pre><h3 id="构建网络计算graph"><a href="#构建网络计算graph" class="headerlink" title="构建网络计算graph"></a>构建网络计算graph</h3><pre><code class="hljs python"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">multilayerPerceptron</span><span class="hljs-params">(x, weights, biases)</span>:</span>    <span class="hljs-string">"""</span><span class="hljs-string">    # 前向传播 y = wx + b</span><span class="hljs-string">    :param x: x</span><span class="hljs-string">    :param weights: w</span><span class="hljs-string">    :param biases: b</span><span class="hljs-string">    :return:</span><span class="hljs-string">    """</span>    <span class="hljs-comment"># 计算第一个隐层，使用激活函数</span>    layer1 = tf.nn.sigmoid(tf.add(tf.matmul(x, weights[<span class="hljs-string">'h1'</span>]), biases[<span class="hljs-string">'b1'</span>]))    <span class="hljs-comment"># 计算第二个隐层，使用激活函数</span>    layer2 = tf.nn.sigmoid(tf.add(tf.matmul(layer1, weights[<span class="hljs-string">'h2'</span>]), biases[<span class="hljs-string">'b2'</span>]))    <span class="hljs-comment"># 计算第三个隐层，使用激活函数</span>    layer3 = tf.nn.sigmoid(tf.add(tf.matmul(layer2, weights[<span class="hljs-string">'h3'</span>]), biases[<span class="hljs-string">'b3'</span>]))    <span class="hljs-comment"># 计算第输出层。</span>    outLayer = tf.add(tf.matmul(layer3, weights[<span class="hljs-string">'out'</span>]), biases[<span class="hljs-string">'out'</span>])    <span class="hljs-keyword">return</span> outLayer</code></pre><h3 id="获取预测值得score"><a href="#获取预测值得score" class="headerlink" title="获取预测值得score"></a>获取预测值得score</h3><pre><code class="hljs python">predictValue = multilayerPerceptron(X, weights, biases)</code></pre><h3 id="计算损失函数并初始化optimizer"><a href="#计算损失函数并初始化optimizer" class="headerlink" title="计算损失函数并初始化optimizer"></a>计算损失函数并初始化optimizer</h3><pre><code class="hljs python">learnRate = <span class="hljs-number">0.01</span>loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=predictValue, labels=Y))optimizer = tf.train.AdamOptimizer(learning_rate=learnRate).minimize(loss)<span class="hljs-comment"># 验证数据</span>correct_prediction = tf.equal(tf.argmax(predictValue, <span class="hljs-number">1</span>), tf.argmax(Y, <span class="hljs-number">1</span>))accuracy = tf.reduce_mean(tf.cast(correct_prediction, <span class="hljs-string">'float'</span>))print(<span class="hljs-string">"FUNCTIONS READY!!"</span>)</code></pre><h3 id="初始化变量"><a href="#初始化变量" class="headerlink" title="初始化变量"></a>初始化变量</h3><pre><code class="hljs python">init = tf.global_variables_initializer()</code></pre><h3 id="在session中执行graph定义的运算"><a href="#在session中执行graph定义的运算" class="headerlink" title="在session中执行graph定义的运算"></a>在session中执行graph定义的运算</h3><pre><code class="hljs python"><span class="hljs-comment"># 训练的总轮数</span>trainEpochs = <span class="hljs-number">20</span><span class="hljs-comment"># 每一批训练的数据大小</span>batchSize = <span class="hljs-number">128</span><span class="hljs-comment"># 信息显示的频数</span>displayStep = <span class="hljs-number">5</span><span class="hljs-keyword">with</span> tf.Session() <span class="hljs-keyword">as</span> sess:    <span class="hljs-comment"># 初始化变量</span>    sess.run(init)    <span class="hljs-comment"># 训练</span>    <span class="hljs-keyword">for</span> epoch <span class="hljs-keyword">in</span> range(trainEpochs):        avg_loss = <span class="hljs-number">0.</span>        totalBatch = int(mnist.train.num_examples/batchSize)        <span class="hljs-comment"># 遍历所有batch</span>        <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> range(totalBatch):            batchX, batchY = mnist.train.next_batch(batchSize)            <span class="hljs-comment"># 使用optimizer进行优化</span>            _, loss_value = sess.run([optimizer, loss], feed_dict=&#123;X: batchX, Y: batchY&#125;)            <span class="hljs-comment"># 求平均损失值</span>            avg_loss += loss_value/totalBatch        <span class="hljs-comment"># 显示信息</span>        <span class="hljs-keyword">if</span> (epoch+<span class="hljs-number">1</span>) % displayStep == <span class="hljs-number">0</span>:            print(<span class="hljs-string">"Epoch: %04d %04d Loss：%.9f"</span> % (epoch, trainEpochs, avg_loss))            train_acc = sess.run(accuracy, feed_dict=&#123;X: batchX, Y: batchY&#125;)            print(<span class="hljs-string">"Train Accuracy: %.3f"</span> % train_acc)            test_acc = sess.run(accuracy, feed_dict=&#123;X: mnist.test.images, Y: mnist.test.labels&#125;)            print(<span class="hljs-string">"Test Accuracy: %.3f"</span> % test_acc)    print(<span class="hljs-string">"Optimization Finished"</span>)</code></pre><h3 id="运行结果"><a href="#运行结果" class="headerlink" title="运行结果"></a>运行结果</h3><pre><code class="hljs bash">Extracting data/mnist\train-images-idx3-ubyte.gzExtracting data/mnist\train-labels-idx1-ubyte.gzExtracting data/mnist\t10k-images-idx3-ubyte.gzExtracting data/mnist\t10k-labels-idx1-ubyte.gz(55000, 784)(55000, 10)FUNCTIONS READY!!Epoch: 0004 0020 Loss：0.188512910Train Accuracy: 1.000Test Accuracy: 0.947Epoch: 0009 0020 Loss：0.142638438Train Accuracy: 0.969Test Accuracy: 0.954Epoch: 0014 0020 Loss：0.125044704Train Accuracy: 0.969Test Accuracy: 0.952Epoch: 0019 0020 Loss：0.114395266Train Accuracy: 0.984Test Accuracy: 0.957Optimization Finished</code></pre><h4 id="结论：简单写了个神经网络，没有进行参数调试。"><a href="#结论：简单写了个神经网络，没有进行参数调试。" class="headerlink" title="结论：简单写了个神经网络，没有进行参数调试。"></a>结论：简单写了个神经网络，没有进行参数调试。</h4>]]></content>
    
    
    <categories>
      
      <category>TensorFlow</category>
      
    </categories>
    
    
    <tags>
      
      <tag>深度学习</tag>
      
      <tag>神经网络</tag>
      
      <tag>TensorFlow</tag>
      
      <tag>深度神经网络</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>神经网络入门-反向传播python实现</title>
    <link href="/2018/08/DL-back-propagation-python/"/>
    <url>/2018/08/DL-back-propagation-python/</url>
    
    <content type="html"><![CDATA[<h3 id="反向传播（back-propagation）算法python实现"><a href="#反向传播（back-propagation）算法python实现" class="headerlink" title="反向传播（back propagation）算法python实现"></a>反向传播（back propagation）算法<code>python</code>实现</h3><h4 id="根据上一章所推导的反向传播（back-propagation）算法所示例代码"><a href="#根据上一章所推导的反向传播（back-propagation）算法所示例代码" class="headerlink" title="根据上一章所推导的反向传播（back propagation）算法所示例代码"></a>根据上一章所推导的<a href="https://0520.tech/2018/08/15/2018-08-15-DL-back-propagation/">反向传播（back propagation）算法</a>所示例代码</h4><pre><code class="hljs python"><span class="hljs-comment">#!/usr/bin/env python</span><span class="hljs-comment"># -*- coding: utf-8 -*-</span><span class="hljs-comment"># @Time    : 2018/8/15 21:32</span><span class="hljs-comment"># @Author  : Seven</span><span class="hljs-comment"># @Site    : </span><span class="hljs-comment"># @File    : bp.py</span><span class="hljs-comment"># @Software: PyCharm</span><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">sigmod</span><span class="hljs-params">(x)</span>:</span>    <span class="hljs-keyword">return</span> <span class="hljs-number">1</span> / (<span class="hljs-number">1</span> + np.exp(-x))<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">sigmodDerivative</span><span class="hljs-params">(x)</span>:</span>    <span class="hljs-keyword">return</span> np.multiply(x, np.subtract(<span class="hljs-number">1</span>, x))<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">forward</span><span class="hljs-params">(weightsA, weightsB, bias)</span>:</span>    <span class="hljs-comment"># 前向传播</span>    <span class="hljs-comment"># 隐层</span>    neth1 = inputX[<span class="hljs-number">0</span>] * weightsA[<span class="hljs-number">0</span>][<span class="hljs-number">0</span>] + inputX[<span class="hljs-number">1</span>] * weightsA[<span class="hljs-number">0</span>][<span class="hljs-number">1</span>] + bias[<span class="hljs-number">0</span>]    outh1 = sigmod(neth1)    print(<span class="hljs-string">"隐层第一个神经元"</span>, neth1, outh1)    neth2 = inputX[<span class="hljs-number">0</span>] * weightsA[<span class="hljs-number">1</span>][<span class="hljs-number">0</span>] + inputX[<span class="hljs-number">1</span>] * weightsA[<span class="hljs-number">1</span>][<span class="hljs-number">1</span>] + bias[<span class="hljs-number">1</span>]    outh2 = sigmod(neth2)    print(<span class="hljs-string">"隐层第二个神经元"</span>, neth2, outh2)    <span class="hljs-comment"># 输出层</span>    neto1 = outh1 * weightsB[<span class="hljs-number">0</span>][<span class="hljs-number">0</span>] + outh2 * weightsB[<span class="hljs-number">0</span>][<span class="hljs-number">1</span>] + bias[<span class="hljs-number">2</span>]    outo1 = sigmod(neto1)    print(<span class="hljs-string">"输出层第一个神经元"</span>, neto1, outo1)    neto2 = outh1 * weightsB[<span class="hljs-number">1</span>][<span class="hljs-number">0</span>] + outh2 * weightsB[<span class="hljs-number">1</span>][<span class="hljs-number">1</span>] + bias[<span class="hljs-number">3</span>]    outo2 = sigmod(neto2)    print(<span class="hljs-string">"输出层第二个神经元"</span>, neto2, outo2)    <span class="hljs-comment"># 向量化</span>    outA = np.array([outh1, outh2])    outB = np.array([outo1, outo2])    Etotal = <span class="hljs-number">1</span> / <span class="hljs-number">2</span> * np.subtract(y, outB) ** <span class="hljs-number">2</span>    print(<span class="hljs-string">"误差值："</span>, Etotal)    <span class="hljs-keyword">return</span> outA, outB<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">backpagration</span><span class="hljs-params">(outA, outB)</span>:</span>    <span class="hljs-comment"># 反向传播</span>    deltaB = np.multiply(np.subtract(outB, y), sigmodDerivative(outB))    print(<span class="hljs-string">"deltaB："</span>, deltaB)    deltaA = np.multiply(np.matmul(np.transpose(weightsB), deltaB), sigmodDerivative(outA))    print(<span class="hljs-string">"deltaA："</span>, deltaA)    deltaWB = np.matmul(deltaB.reshape(<span class="hljs-number">2</span>, <span class="hljs-number">1</span>), outA.reshape(<span class="hljs-number">1</span>, <span class="hljs-number">2</span>))    print(<span class="hljs-string">"deltaWB："</span>, deltaWB)    deltaWA = np.matmul(deltaA.reshape(<span class="hljs-number">2</span>, <span class="hljs-number">1</span>), inputX.reshape(<span class="hljs-number">1</span>, <span class="hljs-number">2</span>))    print(<span class="hljs-string">"deltaWA"</span>, deltaWA)    <span class="hljs-comment"># 权重参数更新</span>    weightsB_new = np.subtract(weightsB, deltaWB)    print(<span class="hljs-string">"weightsB_new"</span>, weightsB_new)    bias[<span class="hljs-number">3</span>] = np.subtract(bias[<span class="hljs-number">3</span>], deltaB[<span class="hljs-number">1</span>])    print(<span class="hljs-string">"biasB"</span>, bias[<span class="hljs-number">3</span>])    bias[<span class="hljs-number">2</span>] = np.subtract(bias[<span class="hljs-number">2</span>], deltaB[<span class="hljs-number">0</span>])    print(<span class="hljs-string">"biasB"</span>, bias[<span class="hljs-number">2</span>])    weightsA_new = np.subtract(weightsA, deltaWA)    print(<span class="hljs-string">"weightsA_new"</span>, weightsA_new)    bias[<span class="hljs-number">1</span>] = np.subtract(bias[<span class="hljs-number">1</span>], deltaA[<span class="hljs-number">1</span>])    print(<span class="hljs-string">"biasA"</span>, bias[<span class="hljs-number">1</span>])    bias[<span class="hljs-number">0</span>] = np.subtract(bias[<span class="hljs-number">0</span>], deltaA[<span class="hljs-number">0</span>])    print(<span class="hljs-string">"biasA"</span>, bias[<span class="hljs-number">0</span>])    print(<span class="hljs-string">"all bias"</span>, bias)    <span class="hljs-keyword">return</span> weightsA_new, weightsB_new, bias<span class="hljs-keyword">if</span> __name__==<span class="hljs-string">"__main__"</span>:    <span class="hljs-comment"># 初始化数据</span>    <span class="hljs-comment"># 权重参数</span>    bias = np.array([<span class="hljs-number">0.5</span>, <span class="hljs-number">0.5</span>, <span class="hljs-number">1.0</span>, <span class="hljs-number">1.0</span>])    weightsA = np.array([[<span class="hljs-number">0.1</span>, <span class="hljs-number">0.3</span>], [<span class="hljs-number">0.2</span>, <span class="hljs-number">0.4</span>]])    weightsB = np.array([[<span class="hljs-number">0.6</span>, <span class="hljs-number">0.8</span>], [<span class="hljs-number">0.7</span>, <span class="hljs-number">0.9</span>]])    <span class="hljs-comment"># 期望值</span>    y = np.array([<span class="hljs-number">1</span>, <span class="hljs-number">0</span>])    <span class="hljs-comment"># 输入层</span>    inputX = np.array([<span class="hljs-number">0.5</span>, <span class="hljs-number">1.0</span>])    print(<span class="hljs-string">"第一次前向传播"</span>)    outA, outB = forward(weightsA, weightsB, bias)    print(<span class="hljs-string">"反向传播-参数更新"</span>)    weightsA_new, weightsB_new, bias = backpagration(outA, outB)    <span class="hljs-comment"># 更新完毕</span>    <span class="hljs-comment"># 验证权重参数--第二次前向传播</span>    print(<span class="hljs-string">"第二次前向传播"</span>)    forward(weightsA_new, weightsB_new, bias)</code></pre><h4 id="程序执行结果："><a href="#程序执行结果：" class="headerlink" title="程序执行结果："></a>程序执行结果：</h4><pre><code class="hljs bash">第一次前向传播隐层第一个神经元 0.85 0.7005671424739729隐层第二个神经元 1.0 0.7310585786300049输出层第一个神经元 2.0051871483883876 0.8813406204337122输出层第二个神经元 2.1483497204987856 0.8955144637754225误差值： [0.00704002 0.40097308]反向传播-参数更新deltaB： [-0.01240932  0.08379177]deltaA： [0.01074218 0.01287516]deltaWB： [[-0.00869356 -0.00907194] [ 0.05870176  0.0612567 ]]deltaWA [[0.00537109 0.01074218] [0.00643758 0.01287516]]weightsB_new [[0.60869356 0.80907194] [0.64129824 0.8387433 ]]biasB 0.9162082259892468biasB 1.0124093185565075weightsA_new [[0.09462891 0.28925782] [0.19356242 0.38712484]]biasA 0.48712483967909465biasA 0.48925781687016445all bias [0.48925782 0.48712484 1.01240932 0.91620823]第二次前向传播隐层第一个神经元 0.8258300879578699 0.6954725026123048隐层第二个神经元 0.971030889277963 0.7253249281967498输出层第一个神经元 2.022578998544453 0.8831474198595102输出层第二个神经元 1.970574942645405 0.8776728542726611误差值： [0.00682726 0.38515482]</code></pre><h4 id="我们可以看出误差值明显下降了，说明我们的权重参数更新是正确的。"><a href="#我们可以看出误差值明显下降了，说明我们的权重参数更新是正确的。" class="headerlink" title="我们可以看出误差值明显下降了，说明我们的权重参数更新是正确的。"></a>我们可以看出误差值明显下降了，说明我们的权重参数更新是正确的。</h4>]]></content>
    
    
    <categories>
      
      <category>深度学习</category>
      
    </categories>
    
    
    <tags>
      
      <tag>深度学习</tag>
      
      <tag>神经网络</tag>
      
      <tag>反向传播</tag>
      
      <tag>python</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>神经网络入门-反向传播</title>
    <link href="/2018/08/DL-back-propagation/"/>
    <url>/2018/08/DL-back-propagation/</url>
    
    <content type="html"><![CDATA[<h3 id="反向传播（back-propagation）算法推导"><a href="#反向传播（back-propagation）算法推导" class="headerlink" title="反向传播（back propagation）算法推导"></a>反向传播（back propagation）算法推导</h3><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20200603141521.png" srcset="/img/loading.gif" alt="images"></p><h4 id="定义损失函数："><a href="#定义损失函数：" class="headerlink" title="定义损失函数："></a>定义损失函数：</h4><h4 id="E-total-frac12-y-outo-2"><a href="#E-total-frac12-y-outo-2" class="headerlink" title="$E_{total}=\frac12 (y-outo)^2$"></a>$E_{total}=\frac12 (y-outo)^2$</h4><h4 id="定义激活函数："><a href="#定义激活函数：" class="headerlink" title="定义激活函数："></a>定义激活函数：</h4><h4 id="sigma-x-sigmod-x"><a href="#sigma-x-sigmod-x" class="headerlink" title="$\sigma(x)=sigmod(x) $"></a>$\sigma(x)=sigmod(x) $</h4><h3 id="前向传播"><a href="#前向传播" class="headerlink" title="前向传播"></a>前向传播</h3><h4 id="第一层-输入层-："><a href="#第一层-输入层-：" class="headerlink" title="第一层(输入层)："></a>第一层(输入层)：</h4><ul><li><h4 id="x-1-x-2-b-1"><a href="#x-1-x-2-b-1" class="headerlink" title="$x_1\ \ \ x_2\ \ \ b_1$"></a>$x_1\ \ \ x_2\ \ \ b_1$</h4></li></ul><h4 id="加权和："><a href="#加权和：" class="headerlink" title="加权和："></a>加权和：</h4><ul><li><h4 id="net-h-1-x-1w-1-x-2w-2-b-1"><a href="#net-h-1-x-1w-1-x-2w-2-b-1" class="headerlink" title="$net h_1=x_1w_1+x_2w_2+b_1$"></a>$net h_1=x_1w_1+x_2w_2+b_1$</h4></li></ul><h4 id="第二层-隐层-："><a href="#第二层-隐层-：" class="headerlink" title="第二层(隐层)："></a>第二层(隐层)：</h4><ul><li><h4 id="outh-1-sigmod-neth-1"><a href="#outh-1-sigmod-neth-1" class="headerlink" title="$outh_1=sigmod(neth_1)$"></a>$outh_1=sigmod(neth_1)$</h4></li></ul><h4 id="加权和：-1"><a href="#加权和：-1" class="headerlink" title="加权和："></a>加权和：</h4><ul><li><h4 id="neto-1-outh-1w-3-outh-2w-4-b-2"><a href="#neto-1-outh-1w-3-outh-2w-4-b-2" class="headerlink" title="$neto_1=outh_1w_3+outh_2w_4+b_2$"></a>$neto_1=outh_1w_3+outh_2w_4+b_2$</h4></li></ul><h4 id="第三层-输出层-："><a href="#第三层-输出层-：" class="headerlink" title="第三层(输出层)："></a>第三层(输出层)：</h4><ul><li><h4 id="outo-1-sigmod-neto-1"><a href="#outo-1-sigmod-neto-1" class="headerlink" title="$outo_1=sigmod(neto_1)$"></a>$outo_1=sigmod(neto_1)$</h4></li></ul><h4 id="计算误差值："><a href="#计算误差值：" class="headerlink" title="计算误差值："></a>计算误差值：</h4><ul><li><h4 id="Eo-1-frac12-y-1-outo-1-2"><a href="#Eo-1-frac12-y-1-outo-1-2" class="headerlink" title="$Eo_1 = \frac12 (y_1-outo_1)^2$"></a>$Eo_1 = \frac12 (y_1-outo_1)^2$</h4></li><li><h4 id="Eo-2-frac12-y-2-outo-2-2"><a href="#Eo-2-frac12-y-2-outo-2-2" class="headerlink" title="$Eo_2 = \frac12 (y_2-outo_2)^2$"></a>$Eo_2 = \frac12 (y_2-outo_2)^2$</h4></li><li><h4 id="E-total-Eo-1-Eo-2"><a href="#E-total-Eo-1-Eo-2" class="headerlink" title="$E_{total}=Eo_1+Eo_2$"></a>$E_{total}=Eo_1+Eo_2$</h4></li></ul><p><code>总结</code>：要是使误差值最小，就需要<code>误差反向传播算法</code>，更新得到最小误差的权重参数<code>w和b</code>。</p><h3 id="反向传播"><a href="#反向传播" class="headerlink" title="反向传播"></a>反向传播</h3><p><code>须知</code>：我们需要反向传递回去更新每一层对应的权重参数<code>w和b</code>。我们使用<code>链式法则</code>来<code>反向模式求导</code>。</p><h4 id="更新第三层（输出层）的权重参数："><a href="#更新第三层（输出层）的权重参数：" class="headerlink" title="更新第三层（输出层）的权重参数："></a><strong>更新第三层（输出层）的权重参数：</strong></h4><h4 id="更新参数w："><a href="#更新参数w：" class="headerlink" title="更新参数w："></a>更新参数<code>w</code>：</h4><h4 id="frac-partial-E-total-partial-w-3-frac-partial-E-total-partial-outo-1-cdot-frac-partial-outo-1-partial-neto-1-cdot-frac-partial-neto-1-partial-w-3"><a href="#frac-partial-E-total-partial-w-3-frac-partial-E-total-partial-outo-1-cdot-frac-partial-outo-1-partial-neto-1-cdot-frac-partial-neto-1-partial-w-3" class="headerlink" title="$\frac{\partial E_{total}}{\partial w_3}=\frac{\partial E_{total}}{\partial outo_1} \cdot \frac{\partial outo_1}{\partial neto_1} \cdot \frac{\partial neto_1}{\partial w_3}$"></a>$\frac{\partial E_{total}}{\partial w_3}=\frac{\partial E_{total}}{\partial outo_1} \cdot \frac{\partial outo_1}{\partial neto_1} \cdot \frac{\partial neto_1}{\partial w_3}$</h4><h4 id="frac-partial-frac12-y-1-outo-1-2-partial-outo-1-cdot-frac-partial-sigmod-neto-1-partial-neto-1-cdot-frac-partial-neto-1-partial-w-3"><a href="#frac-partial-frac12-y-1-outo-1-2-partial-outo-1-cdot-frac-partial-sigmod-neto-1-partial-neto-1-cdot-frac-partial-neto-1-partial-w-3" class="headerlink" title="$= \frac{\partial \frac12(y_1-outo_1)^2}{\partial outo_1} \cdot \frac{\partial sigmod(neto_1)}{\partial neto_1} \cdot \frac{\partial neto_1}{\partial w_3}$"></a>$= \frac{\partial \frac12(y_1-outo_1)^2}{\partial outo_1} \cdot \frac{\partial sigmod(neto_1)}{\partial neto_1} \cdot \frac{\partial neto_1}{\partial w_3}$</h4><h4 id="outo-1-y-1-cdot-outo-1-1-outo-1-cdot-outh-1"><a href="#outo-1-y-1-cdot-outo-1-1-outo-1-cdot-outh-1" class="headerlink" title="$=(outo_1-y_1)\cdot outo_1(1-outo_1)\cdot outh_1$"></a>$=(outo_1-y_1)\cdot outo_1(1-outo_1)\cdot outh_1$</h4><p>$w_{3new}=w_{3old}-\eta \frac{\partial E_{total}}{\partial w_3}$， $\eta$是学习率</p><h4 id="更新参数b："><a href="#更新参数b：" class="headerlink" title="更新参数b："></a>更新参数<code>b</code>：</h4><h4 id="frac-partial-E-total-partial-b-2-frac-partial-E-total-partial-outo-1-cdot-frac-partial-outo-1-partial-neto-1-cdot-frac-partial-neto-1-partial-b-2"><a href="#frac-partial-E-total-partial-b-2-frac-partial-E-total-partial-outo-1-cdot-frac-partial-outo-1-partial-neto-1-cdot-frac-partial-neto-1-partial-b-2" class="headerlink" title="$\frac{\partial E_{total}}{\partial b_2}=\frac{\partial E_{total}}{\partial outo_1} \cdot \frac{\partial outo_1}{\partial neto_1} \cdot \frac{\partial neto_1}{\partial b_2}$"></a>$\frac{\partial E_{total}}{\partial b_2}=\frac{\partial E_{total}}{\partial outo_1} \cdot \frac{\partial outo_1}{\partial neto_1} \cdot \frac{\partial neto_1}{\partial b_2}$</h4><h4 id="frac-partial-frac12-y-1-outo-1-2-partial-outo-1-cdot-frac-partial-sigmod-neto-1-partial-neto-1-cdot-frac-partial-neto-1-partial-b-2"><a href="#frac-partial-frac12-y-1-outo-1-2-partial-outo-1-cdot-frac-partial-sigmod-neto-1-partial-neto-1-cdot-frac-partial-neto-1-partial-b-2" class="headerlink" title="$= \frac{\partial \frac12(y_1-outo_1)^2}{\partial outo_1} \cdot \frac{\partial sigmod(neto_1)}{\partial neto_1} \cdot \frac{\partial neto_1}{\partial b_2}$"></a>$= \frac{\partial \frac12(y_1-outo_1)^2}{\partial outo_1} \cdot \frac{\partial sigmod(neto_1)}{\partial neto_1} \cdot \frac{\partial neto_1}{\partial b_2}$</h4><h4 id="outo-1-y-1-cdot-outo-1-1-outo-1"><a href="#outo-1-y-1-cdot-outo-1-1-outo-1" class="headerlink" title="$=(outo_1-y_1)\cdot outo_1(1-outo_1)$"></a>$=(outo_1-y_1)\cdot outo_1(1-outo_1)$</h4><h4 id="b-2new-b-2old-eta-frac-partial-E-total-partial-b-2-，-eta-是学习率"><a href="#b-2new-b-2old-eta-frac-partial-E-total-partial-b-2-，-eta-是学习率" class="headerlink" title="$b_{2new}=b_{2old}-\eta \frac{\partial E_{total}}{\partial b_2}$， $\eta$是学习率"></a>$b_{2new}=b_{2old}-\eta \frac{\partial E_{total}}{\partial b_2}$， $\eta$是学习率</h4><h4 id="同理可得：w4：也就是同一层的w都可以用这种方式更新。"><a href="#同理可得：w4：也就是同一层的w都可以用这种方式更新。" class="headerlink" title="同理可得：w4：也就是同一层的w都可以用这种方式更新。"></a>同理可得：<code>w4</code>：也就是同一层的<code>w</code>都可以用这种方式更新。</h4><h4 id="更新上一层-隐层-的权重参数："><a href="#更新上一层-隐层-的权重参数：" class="headerlink" title="更新上一层(隐层)的权重参数："></a><strong>更新上一层(隐层)的权重参数</strong>：</h4><h4 id="更新权重参数w和b："><a href="#更新权重参数w和b：" class="headerlink" title="更新权重参数w和b："></a>更新权重参数<code>w和b</code>：</h4><h4 id="frac-partial-E-total-partial-w-1-frac-partial-E-total-partial-outh-1-cdot-frac-partial-outh-1-partial-neth-1-cdot-frac-partial-neth-1-partial-w-1"><a href="#frac-partial-E-total-partial-w-1-frac-partial-E-total-partial-outh-1-cdot-frac-partial-outh-1-partial-neth-1-cdot-frac-partial-neth-1-partial-w-1" class="headerlink" title="$\frac{\partial E_{total}}{\partial w_1}=\frac{\partial E_{total}}{\partial outh_1} \cdot \frac{\partial outh_1}{\partial neth_1} \cdot \frac{\partial neth_1}{\partial w_1}$"></a>$\frac{\partial E_{total}}{\partial w_1}=\frac{\partial E_{total}}{\partial outh_1} \cdot \frac{\partial outh_1}{\partial neth_1} \cdot \frac{\partial neth_1}{\partial w_1}$</h4><h4 id="frac-partial-E-total-partial-b-1-frac-partial-E-total-partial-outh-1-cdot-frac-partial-outh-1-partial-neth-1-cdot-frac-partial-neth-1-partial-b-1"><a href="#frac-partial-E-total-partial-b-1-frac-partial-E-total-partial-outh-1-cdot-frac-partial-outh-1-partial-neth-1-cdot-frac-partial-neth-1-partial-b-1" class="headerlink" title="$\frac{\partial E_{total}}{\partial b_1}=\frac{\partial E_{total}}{\partial outh_1} \cdot \frac{\partial outh_1}{\partial neth_1} \cdot \frac{\partial neth_1}{\partial b_1}$"></a>$\frac{\partial E_{total}}{\partial b_1}=\frac{\partial E_{total}}{\partial outh_1} \cdot \frac{\partial outh_1}{\partial neth_1} \cdot \frac{\partial neth_1}{\partial b_1}$</h4><h4 id="其中："><a href="#其中：" class="headerlink" title="其中："></a>其中：</h4><ul><li><h4 id="frac-partial-E-total-partial-outh-1-frac-partial-Eo-1-partial-outh-1-frac-partial-Eo-2-partial-outh-1"><a href="#frac-partial-E-total-partial-outh-1-frac-partial-Eo-1-partial-outh-1-frac-partial-Eo-2-partial-outh-1" class="headerlink" title="$\frac{\partial E_{total}}{\partial outh_1} = \frac{\partial Eo_1}{\partial outh_1}+ \frac{\partial Eo_2}{\partial outh_1}$"></a>$\frac{\partial E_{total}}{\partial outh_1} = \frac{\partial Eo_1}{\partial outh_1}+ \frac{\partial Eo_2}{\partial outh_1}$</h4></li><li><h4 id="frac-partial-Eo-1-partial-outh-1-frac-partial-Eo-1-partial-neto-1-cdot-frac-partial-neto-1-partial-outh-1"><a href="#frac-partial-Eo-1-partial-outh-1-frac-partial-Eo-1-partial-neto-1-cdot-frac-partial-neto-1-partial-outh-1" class="headerlink" title="$\frac{\partial Eo_1}{\partial outh_1} = \frac{\partial Eo_1}{\partial neto_1} \cdot \frac{\partial neto_1}{\partial outh_1}$"></a>$\frac{\partial Eo_1}{\partial outh_1} = \frac{\partial Eo_1}{\partial neto_1} \cdot \frac{\partial neto_1}{\partial outh_1}$</h4></li><li><h4 id="frac-partial-Eo-1-partial-neto-1-frac-partial-E-o-1-partial-outo-1-cdot-frac-partial-outo-1-partial-neto-1-outo-1-y-1-cdot-outo-1-1-outo-1"><a href="#frac-partial-Eo-1-partial-neto-1-frac-partial-E-o-1-partial-outo-1-cdot-frac-partial-outo-1-partial-neto-1-outo-1-y-1-cdot-outo-1-1-outo-1" class="headerlink" title="$ \frac{\partial Eo_1}{\partial neto_1} = \frac{\partial E_{o_1}}{\partial outo_1} \cdot \frac{\partial outo_1}{\partial neto_1} = (outo_1-y_1)\cdot outo_1(1-outo_1)$"></a>$ \frac{\partial Eo_1}{\partial neto_1} = \frac{\partial E_{o_1}}{\partial outo_1} \cdot \frac{\partial outo_1}{\partial neto_1} = (outo_1-y_1)\cdot outo_1(1-outo_1)$</h4></li><li><h4 id="frac-partial-neto-1-partial-outh-1-frac-partial-outh-1w-3-outo-2w-4-b-2-partial-outh-1-w-3"><a href="#frac-partial-neto-1-partial-outh-1-frac-partial-outh-1w-3-outo-2w-4-b-2-partial-outh-1-w-3" class="headerlink" title="$ \frac{\partial neto_1}{\partial outh_1} = \frac{\partial (outh_1w_3+outo_2w_4+b_2)}{\partial outh_1} = w_3$"></a>$ \frac{\partial neto_1}{\partial outh_1} = \frac{\partial (outh_1w_3+outo_2w_4+b_2)}{\partial outh_1} = w_3$</h4></li><li><h4 id="同理可得："><a href="#同理可得：" class="headerlink" title="同理可得："></a>同理可得：</h4><ul><li><h4 id="frac-partial-Eo-2-partial-outh-1-frac-partial-Eo-2-partial-neto-2-cdot-frac-partial-neto-2-partial-outh-1"><a href="#frac-partial-Eo-2-partial-outh-1-frac-partial-Eo-2-partial-neto-2-cdot-frac-partial-neto-2-partial-outh-1" class="headerlink" title="$\frac{\partial Eo_2}{\partial outh_1} = \frac{\partial Eo_2}{\partial neto_2} \cdot \frac{\partial neto_2}{\partial outh_1}$"></a>$\frac{\partial Eo_2}{\partial outh_1} = \frac{\partial Eo_2}{\partial neto_2} \cdot \frac{\partial neto_2}{\partial outh_1}$</h4></li><li><h4 id="frac-partial-Eo-2-partial-neto-2-frac-partial-E-o-2-partial-outo-2-cdot-frac-partial-outo-2-partial-neto-2-outo-2-y-2-cdot-outo-2-1-outo-2"><a href="#frac-partial-Eo-2-partial-neto-2-frac-partial-E-o-2-partial-outo-2-cdot-frac-partial-outo-2-partial-neto-2-outo-2-y-2-cdot-outo-2-1-outo-2" class="headerlink" title="$ \frac{\partial Eo_2}{\partial neto_2} = \frac{\partial E_{o_2}}{\partial outo_2} \cdot \frac{\partial outo_2}{\partial neto_2} = (outo_2-y_2)\cdot outo_2(1-outo_2)$"></a>$ \frac{\partial Eo_2}{\partial neto_2} = \frac{\partial E_{o_2}}{\partial outo_2} \cdot \frac{\partial outo_2}{\partial neto_2} = (outo_2-y_2)\cdot outo_2(1-outo_2)$</h4></li><li><h4 id="frac-partial-neto-2-partial-outh-1-w-5-outh1连接outo2的权重，暂定为w5-大家注意理解"><a href="#frac-partial-neto-2-partial-outh-1-w-5-outh1连接outo2的权重，暂定为w5-大家注意理解" class="headerlink" title="$ \frac{\partial neto_2}{\partial outh_1} = w_5$ (outh1连接outo2的权重，暂定为w5,大家注意理解)"></a>$ \frac{\partial neto_2}{\partial outh_1} = w_5$ (outh1连接outo2的权重，暂定为w5,大家注意理解)</h4></li></ul></li></ul><h4 id="综合上式："><a href="#综合上式：" class="headerlink" title="综合上式："></a>综合上式：</h4><h4 id="frac-partial-E-total-partial-w-1-w-3-outo-1-y-1-cdot-outo-1-1-outo-1-w-5-outo-2-y-2-cdot-outo-2-1-outo-2-cdot-outh-1-1-outh-1-cdot-x-1"><a href="#frac-partial-E-total-partial-w-1-w-3-outo-1-y-1-cdot-outo-1-1-outo-1-w-5-outo-2-y-2-cdot-outo-2-1-outo-2-cdot-outh-1-1-outh-1-cdot-x-1" class="headerlink" title="$\frac{\partial E_{total}}{\partial w_1}= [w_3 (outo_1-y_1)\cdot outo_1(1-outo_1) + w_5(outo_2-y_2)\cdot outo_2(1-outo_2)] \cdot outh_1(1-outh_1) \cdot x_1$"></a>$\frac{\partial E_{total}}{\partial w_1}= [w_3 (outo_1-y_1)\cdot outo_1(1-outo_1) + w_5(outo_2-y_2)\cdot outo_2(1-outo_2)] \cdot outh_1(1-outh_1) \cdot x_1$</h4><h4 id="frac-partial-E-total-partial-b-1-w-3-outo-1-y-1-cdot-outo-1-1-outo-1-w-5-outo-2-y-2-cdot-outo-2-1-outo-2-cdot-outh-1-1-outh-1"><a href="#frac-partial-E-total-partial-b-1-w-3-outo-1-y-1-cdot-outo-1-1-outo-1-w-5-outo-2-y-2-cdot-outo-2-1-outo-2-cdot-outh-1-1-outh-1" class="headerlink" title="$\frac{\partial E_{total}}{\partial b_1}= [w_3 (outo_1-y_1)\cdot outo_1(1-outo_1) +w_5(outo_2-y_2)\cdot outo_2(1-outo_2)] \cdot outh_1(1-outh_1) $"></a>$\frac{\partial E_{total}}{\partial b_1}= [w_3 (outo_1-y_1)\cdot outo_1(1-outo_1) +w_5(outo_2-y_2)\cdot outo_2(1-outo_2)] \cdot outh_1(1-outh_1) $</h4><h4 id="更新："><a href="#更新：" class="headerlink" title="更新："></a>更新：</h4><h4 id="w-1new-w-1old-eta-frac-partial-E-total-partial-w-1"><a href="#w-1new-w-1old-eta-frac-partial-E-total-partial-w-1" class="headerlink" title="$w_{1new}=w_{1old}-\eta \frac{\partial E_{total}}{\partial w_1}$"></a>$w_{1new}=w_{1old}-\eta \frac{\partial E_{total}}{\partial w_1}$</h4><h4 id="b-1new-b-1old-eta-frac-partial-E-total-partial-b-1"><a href="#b-1new-b-1old-eta-frac-partial-E-total-partial-b-1" class="headerlink" title="$b_{1new}=b_{1old}-\eta \frac{\partial E_{total}}{\partial b_1}$"></a>$b_{1new}=b_{1old}-\eta \frac{\partial E_{total}}{\partial b_1}$</h4><h4 id="同理可得：w2：也就是同一层的w都可以用这种方式更新。"><a href="#同理可得：w2：也就是同一层的w都可以用这种方式更新。" class="headerlink" title="同理可得：w2：也就是同一层的w都可以用这种方式更新。"></a>同理可得：<code>w2</code>：也就是同一层的<code>w</code>都可以用这种方式更新。</h4><h4 id="至此，我们所有的参数都更新完毕了，然后我们可以再次用新的参数进行前向传播，得到的误差就会是最小的"><a href="#至此，我们所有的参数都更新完毕了，然后我们可以再次用新的参数进行前向传播，得到的误差就会是最小的" class="headerlink" title="至此，我们所有的参数都更新完毕了，然后我们可以再次用新的参数进行前向传播，得到的误差就会是最小的"></a>至此，我们所有的参数都更新完毕了，然后我们可以再次用新的参数进行前向传播，得到的误差就会是最小的</h4><h3 id="推广"><a href="#推广" class="headerlink" title="推广:"></a>推广:</h3><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20200603141522.png" srcset="/img/loading.gif" alt="images"></p><h4 id="我们定义第L层的第i个神经元更新权重参数时-上标表示层数，下标表示神经元-："><a href="#我们定义第L层的第i个神经元更新权重参数时-上标表示层数，下标表示神经元-：" class="headerlink" title="我们定义第L层的第i个神经元更新权重参数时(上标表示层数，下标表示神经元)："></a>我们定义第<code>L</code>层的第<code>i</code>个神经元更新权重参数时(上标表示层数，下标表示神经元)：</h4><ul><li><h4 id="frac-partial-E-total-partial-net-i-L-delta-i-L"><a href="#frac-partial-E-total-partial-net-i-L-delta-i-L" class="headerlink" title="$\frac{\partial E_{total}}{\partial net_i^{(L)}} = \delta_i^{(L)}$"></a>$\frac{\partial E_{total}}{\partial net_i^{(L)}} = \delta_i^{(L)}$</h4></li><li><h4 id="frac-partial-E-total-partial-w-ij-l-outh-j-l-1-delta-i-l-，其中-w-ij-l-表示第-l-层的第-i-个神经元连接第-l-1-层的第-j-的神经元的相连的权重参数w。如下图所示："><a href="#frac-partial-E-total-partial-w-ij-l-outh-j-l-1-delta-i-l-，其中-w-ij-l-表示第-l-层的第-i-个神经元连接第-l-1-层的第-j-的神经元的相连的权重参数w。如下图所示：" class="headerlink" title="$\frac{\partial E_{total}}{\partial w_{ij}^{(l)}}=outh_j^{(l-1)}\delta_i^{(l)}$ ，其中$w_{ij}^{(l)}$表示第$l$层的第$i$个神经元连接第$l-1$层的第$j$的神经元的相连的权重参数w。如下图所示："></a>$\frac{\partial E_{total}}{\partial w_{ij}^{(l)}}=outh_j^{(l-1)}\delta_i^{(l)}$ ，其中$w_{ij}^{(l)}$表示第$l$层的第$i$个神经元连接第$l-1$层的第$j$的神经元的相连的权重参数<code>w</code>。如下图所示：</h4><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20200603141523.png" srcset="/img/loading.gif" alt="images"></p></li></ul><h3 id="推广总结："><a href="#推广总结：" class="headerlink" title="推广总结："></a>推广总结：</h3><h4 id="根据前面我们所定义的："><a href="#根据前面我们所定义的：" class="headerlink" title="根据前面我们所定义的："></a>根据前面我们所定义的：</h4><p>$E_{total}=\frac12 (y-outo)^2$ </p><h4 id="sigma-x-sigmod-x-1"><a href="#sigma-x-sigmod-x-1" class="headerlink" title="$\sigma(x)=sigmod(x) $"></a>$\sigma(x)=sigmod(x) $</h4><h4 id="frac-partial-E-total-partial-w-ij-l-outh-j-l-1-delta-i-l"><a href="#frac-partial-E-total-partial-w-ij-l-outh-j-l-1-delta-i-l" class="headerlink" title="$\frac{\partial E_{total}}{\partial w_{ij}^{(l)}}=outh_j^{(l-1)}\delta_i^{(l)}$"></a>$\frac{\partial E_{total}}{\partial w_{ij}^{(l)}}=outh_j^{(l-1)}\delta_i^{(l)}$</h4><h4 id="delta-i-L-frac-partial-E-total-partial-net-i-L"><a href="#delta-i-L-frac-partial-E-total-partial-net-i-L" class="headerlink" title="$\delta_i^{(L)}=\frac{\partial E_{total}}{\partial net_i^{(L)}}  $"></a>$\delta_i^{(L)}=\frac{\partial E_{total}}{\partial net_i^{(L)}}  $</h4><h4 id="frac-partial-E-total-partial-outh-i-cdot-frac-partial-outh-i-partial-net-i-L"><a href="#frac-partial-E-total-partial-outh-i-cdot-frac-partial-outh-i-partial-net-i-L" class="headerlink" title="$= \frac{\partial E_{total}}{\partial outh_i} \cdot \frac{\partial outh_i}{\partial net_i^{(L)}}$"></a>$= \frac{\partial E_{total}}{\partial outh_i} \cdot \frac{\partial outh_i}{\partial net_i^{(L)}}$</h4><h4 id="bigtriangledown-out-E-total-times-sigma-prime-net-i-L"><a href="#bigtriangledown-out-E-total-times-sigma-prime-net-i-L" class="headerlink" title="$= \bigtriangledown_{out} E_{total} \times   \sigma^{\prime}(net_i^{(L)})$"></a>$= \bigtriangledown_{out} E_{total} \times   \sigma^{\prime}(net_i^{(L)})$</h4><h4 id="对于第-l-层："><a href="#对于第-l-层：" class="headerlink" title="对于第$l$层："></a>对于第$l$层：</h4><h4 id="delta-l-frac-partial-E-total-partial-net-l"><a href="#delta-l-frac-partial-E-total-partial-net-l" class="headerlink" title="$\delta^{(l)}=\frac{\partial E_{total}}{\partial net^{(l)}} $"></a>$\delta^{(l)}=\frac{\partial E_{total}}{\partial net^{(l)}} $</h4><h4 id="frac-partial-E-total-partial-net-l-1-cdot-frac-partial-net-l-1-partial-net-l"><a href="#frac-partial-E-total-partial-net-l-1-cdot-frac-partial-net-l-1-partial-net-l" class="headerlink" title="$= \frac{\partial E_{total}}{\partial net^{(l+1)}} \cdot \frac{\partial net^{(l+1)}}{\partial net^{(l)}}$"></a>$= \frac{\partial E_{total}}{\partial net^{(l+1)}} \cdot \frac{\partial net^{(l+1)}}{\partial net^{(l)}}$</h4><h4 id="delta-l-1-times-frac-partial-net-l-1-partial-net-l"><a href="#delta-l-1-times-frac-partial-net-l-1-partial-net-l" class="headerlink" title="$= \delta^{(l+1)} \times  \frac{\partial net^{(l+1)}}{\partial net^{(l)}}$"></a>$= \delta^{(l+1)} \times  \frac{\partial net^{(l+1)}}{\partial net^{(l)}}$</h4><h4 id="delta-l-1-times-frac-partial-w-l-1-sigma-net-l-partial-net-l"><a href="#delta-l-1-times-frac-partial-w-l-1-sigma-net-l-partial-net-l" class="headerlink" title="$= \delta^{(l+1)} \times  \frac{\partial (w^{(l+1)}\sigma (net^{(l)}))}{\partial net^{(l)}}$"></a>$= \delta^{(l+1)} \times  \frac{\partial (w^{(l+1)}\sigma (net^{(l)}))}{\partial net^{(l)}}$</h4><h4 id="delta-l-1-w-l-1-sigma-prime-net-L"><a href="#delta-l-1-w-l-1-sigma-prime-net-L" class="headerlink" title="$= \delta^{(l+1)} w^{(l+1)}  \sigma^{\prime}(net^{(L)})$"></a>$= \delta^{(l+1)} w^{(l+1)}  \sigma^{\prime}(net^{(L)})$</h4><h4 id="对于偏置项bias："><a href="#对于偏置项bias：" class="headerlink" title="对于偏置项bias："></a>对于偏置项<code>bias</code>：</h4><h4 id="frac-partial-E-total-partial-bias-i-l-delta-i-l"><a href="#frac-partial-E-total-partial-bias-i-l-delta-i-l" class="headerlink" title="$\frac{\partial E_{total}}{\partial bias_i^{(l)}}=\delta_i^{(l)}$"></a>$\frac{\partial E_{total}}{\partial bias_i^{(l)}}=\delta_i^{(l)}$</h4><h3 id="反向传播的四项基本原则："><a href="#反向传播的四项基本原则：" class="headerlink" title="反向传播的四项基本原则："></a>反向传播的四项基本原则：</h3><h4 id="基本形式"><a href="#基本形式" class="headerlink" title="基本形式:"></a>基本形式:</h4><ul><li><h4 id="delta-i-L-bigtriangledown-out-E-total-times-sigma-prime-net-i-L"><a href="#delta-i-L-bigtriangledown-out-E-total-times-sigma-prime-net-i-L" class="headerlink" title="$\delta_i^{(L)}= \bigtriangledown_{out} E_{total} \times   \sigma^{\prime}(net_i^{(L)}) $"></a>$\delta_i^{(L)}= \bigtriangledown_{out} E_{total} \times   \sigma^{\prime}(net_i^{(L)}) $</h4></li><li><h4 id="delta-l-sum-j-delta-j-l-1-w-ji-l-1-sigma-prime-net-i-l"><a href="#delta-l-sum-j-delta-j-l-1-w-ji-l-1-sigma-prime-net-i-l" class="headerlink" title="$\delta^{(l)} = \sum_j \delta_j^{(l+1)} w_{ji}^{(l+1)}  \sigma^{\prime}(net_i^{(l)})$"></a>$\delta^{(l)} = \sum_j \delta_j^{(l+1)} w_{ji}^{(l+1)}  \sigma^{\prime}(net_i^{(l)})$</h4></li><li><h4 id="frac-partial-E-total-partial-bias-i-l-delta-i-l-1"><a href="#frac-partial-E-total-partial-bias-i-l-delta-i-l-1" class="headerlink" title="$\frac{\partial E_{total}}{\partial bias_i^{(l)}}=\delta_i^{(l)}$"></a>$\frac{\partial E_{total}}{\partial bias_i^{(l)}}=\delta_i^{(l)}$</h4></li><li><h4 id="frac-partial-E-total-partial-w-ij-l-outh-j-l-1-delta-i-l-1"><a href="#frac-partial-E-total-partial-w-ij-l-outh-j-l-1-delta-i-l-1" class="headerlink" title="$\frac{\partial E_{total}}{\partial w_{ij}^{(l)}}=outh_j^{(l-1)}\delta_i^{(l)}$"></a>$\frac{\partial E_{total}}{\partial w_{ij}^{(l)}}=outh_j^{(l-1)}\delta_i^{(l)}$</h4></li></ul><h4 id="矩阵形式："><a href="#矩阵形式：" class="headerlink" title="矩阵形式："></a>矩阵形式：</h4><ul><li><h4 id="delta-i-L-bigtriangledown-out-E-total-bigodot-sigma-prime-net-i-L-，-bigodot-是Hadamard乘积（对应位置相乘）"><a href="#delta-i-L-bigtriangledown-out-E-total-bigodot-sigma-prime-net-i-L-，-bigodot-是Hadamard乘积（对应位置相乘）" class="headerlink" title="$\delta_i^{(L)}= \bigtriangledown_{out} E_{total} \bigodot   \sigma^{\prime}(net_i^{(L)}) $ ， $\bigodot$是Hadamard乘积（对应位置相乘）"></a>$\delta_i^{(L)}= \bigtriangledown_{out} E_{total} \bigodot   \sigma^{\prime}(net_i^{(L)}) $ ， $\bigodot$是Hadamard乘积（对应位置相乘）</h4></li><li><h4 id="delta-l-w-l-1-T-delta-l-1-bigodot-sigma-prime-net-l"><a href="#delta-l-w-l-1-T-delta-l-1-bigodot-sigma-prime-net-l" class="headerlink" title="$\delta^{(l)} = (w^{(l+1)})^T \delta^{(l+1)} \bigodot  \sigma^{\prime}(net^{(l)})$"></a>$\delta^{(l)} = (w^{(l+1)})^T \delta^{(l+1)} \bigodot  \sigma^{\prime}(net^{(l)})$</h4></li><li><h4 id="frac-partial-E-total-partial-bias-l-delta-l"><a href="#frac-partial-E-total-partial-bias-l-delta-l" class="headerlink" title="$\frac{\partial E_{total}}{\partial bias^{(l)}}=\delta^{(l)}$"></a>$\frac{\partial E_{total}}{\partial bias^{(l)}}=\delta^{(l)}$</h4></li><li><h4 id="frac-partial-E-total-partial-w-l-delta-l-outh-l-1-T"><a href="#frac-partial-E-total-partial-w-l-delta-l-outh-l-1-T" class="headerlink" title="$\frac{\partial E_{total}}{\partial w^{(l)}}=\delta^{(l)}(outh^{(l-1)})^T$"></a>$\frac{\partial E_{total}}{\partial w^{(l)}}=\delta^{(l)}(outh^{(l-1)})^T$</h4></li></ul><p>当然如果你对具体推导不是很明白，你把这四项基本原则搞清楚，就可以直接使用了。</p><h3 id="举个栗子"><a href="#举个栗子" class="headerlink" title="举个栗子"></a>举个栗子</h3><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20200603141524.png" srcset="/img/loading.gif" alt="images"></p><ul><li><h4 id="因为：-delta-i-L-bigtriangledown-out-E-total-bigodot-sigma-prime-net-i-L"><a href="#因为：-delta-i-L-bigtriangledown-out-E-total-bigodot-sigma-prime-net-i-L" class="headerlink" title="因为：$\delta_i^{(L)}= \bigtriangledown_{out} E_{total} \bigodot   \sigma^{\prime}(net_i^{(L)}) $"></a>因为：$\delta_i^{(L)}= \bigtriangledown_{out} E_{total} \bigodot   \sigma^{\prime}(net_i^{(L)}) $</h4><h4 id="所以："><a href="#所以：" class="headerlink" title="所以："></a>所以：</h4><h4 id="delta-2-out-y-bigodot-out-1-out"><a href="#delta-2-out-y-bigodot-out-1-out" class="headerlink" title="$\delta^{(2)}= (out-y)\bigodot out(1-out)$"></a>$\delta^{(2)}= (out-y)\bigodot out(1-out)$</h4><h4 id="begin-bmatrix-0-88134-0-89551-end-bmatrix-begin-bmatrix-1-0-end-bmatrix-bigodot-begin-bmatrix-0-88134-0-89551-end-bmatrix-bigodot-begin-bmatrix-1-1-end-bmatrix-begin-bmatrix-0-88134-0-89551-end-bmatrix"><a href="#begin-bmatrix-0-88134-0-89551-end-bmatrix-begin-bmatrix-1-0-end-bmatrix-bigodot-begin-bmatrix-0-88134-0-89551-end-bmatrix-bigodot-begin-bmatrix-1-1-end-bmatrix-begin-bmatrix-0-88134-0-89551-end-bmatrix" class="headerlink" title="$=(\begin{bmatrix} 0.88134 \\ 0.89551 \end{bmatrix} -\begin{bmatrix} 1 \\ 0 \end{bmatrix}) \bigodot (\begin{bmatrix} 0.88134 \\ 0.89551 \end{bmatrix} \bigodot (\begin{bmatrix} 1 \\ 1 \end{bmatrix}-\begin{bmatrix} 0.88134 \\ 0.89551 \end{bmatrix})) $"></a>$=(\begin{bmatrix} 0.88134 \\ 0.89551 \end{bmatrix} -\begin{bmatrix} 1 \\ 0 \end{bmatrix}) \bigodot (\begin{bmatrix} 0.88134 \\ 0.89551 \end{bmatrix} \bigodot (\begin{bmatrix} 1 \\ 1 \end{bmatrix}-\begin{bmatrix} 0.88134 \\ 0.89551 \end{bmatrix})) $</h4><h4 id="begin-bmatrix-0-01240932-0-08379177-end-bmatrix"><a href="#begin-bmatrix-0-01240932-0-08379177-end-bmatrix" class="headerlink" title="$=\begin{bmatrix} -0.01240932\\ 0.08379177\end{bmatrix}$"></a>$=\begin{bmatrix} -0.01240932\\ 0.08379177\end{bmatrix}$</h4></li><li><h4 id="因为：-delta-l-w-l-1-T-delta-l-1-bigodot-sigma-prime-net-l"><a href="#因为：-delta-l-w-l-1-T-delta-l-1-bigodot-sigma-prime-net-l" class="headerlink" title="因为：$\delta^{(l)} = (w^{(l+1)})^T \delta^{(l+1)} \bigodot  \sigma^{\prime}(net^{(l)})$"></a>因为：$\delta^{(l)} = (w^{(l+1)})^T \delta^{(l+1)} \bigodot  \sigma^{\prime}(net^{(l)})$</h4><h4 id="所以：-1"><a href="#所以：-1" class="headerlink" title="所以："></a>所以：</h4><h4 id="delta-1-w-2-T-delta-2-bigodot-sigma-prime-net-1"><a href="#delta-1-w-2-T-delta-2-bigodot-sigma-prime-net-1" class="headerlink" title="$\delta^{(1)} = (w^{(2)})^T \delta^{(2)} \bigodot  \sigma^{\prime}(net^{(1)})$"></a>$\delta^{(1)} = (w^{(2)})^T \delta^{(2)} \bigodot  \sigma^{\prime}(net^{(1)})$</h4><h4 id="begin-bmatrix-0-6-amp-0-8-0-7-amp-0-9-end-bmatrix-T-cdot-begin-bmatrix-0-01240932-0-08379177-end-bmatrix-bigodot-begin-bmatrix-0-20977282-0-19661193-end-bmatrix"><a href="#begin-bmatrix-0-6-amp-0-8-0-7-amp-0-9-end-bmatrix-T-cdot-begin-bmatrix-0-01240932-0-08379177-end-bmatrix-bigodot-begin-bmatrix-0-20977282-0-19661193-end-bmatrix" class="headerlink" title="$=(\begin{bmatrix} 0.6 &amp; 0.8 \\ 0.7 &amp; 0.9\end{bmatrix}^T \cdot \begin{bmatrix} -0.01240932\\ 0.08379177\end{bmatrix}) \bigodot \begin{bmatrix} 0.20977282 \\ 0.19661193\end{bmatrix}$"></a>$=(\begin{bmatrix} 0.6 &amp; 0.8 \\ 0.7 &amp; 0.9\end{bmatrix}^T \cdot \begin{bmatrix} -0.01240932\\ 0.08379177\end{bmatrix}) \bigodot \begin{bmatrix} 0.20977282 \\ 0.19661193\end{bmatrix}$</h4><h4 id="begin-bmatrix-0-01074218-0-01287516-end-bmatrix"><a href="#begin-bmatrix-0-01074218-0-01287516-end-bmatrix" class="headerlink" title="$=\begin{bmatrix} 0.01074218\\ 0.01287516\end{bmatrix}$"></a>$=\begin{bmatrix} 0.01074218\\ 0.01287516\end{bmatrix}$</h4></li><li><h4 id="因为：-frac-partial-E-total-partial-w-l-delta-l-outh-l-1-T"><a href="#因为：-frac-partial-E-total-partial-w-l-delta-l-outh-l-1-T" class="headerlink" title="因为：$\frac{\partial E_{total}}{\partial w^{(l)}}=\delta^{(l)}(outh^{(l-1)})^T$"></a>因为：$\frac{\partial E_{total}}{\partial w^{(l)}}=\delta^{(l)}(outh^{(l-1)})^T$</h4><h4 id="所以：-2"><a href="#所以：-2" class="headerlink" title="所以："></a>所以：</h4><h4 id="Delta-w-2-delta-2-outh-1-T"><a href="#Delta-w-2-delta-2-outh-1-T" class="headerlink" title="$\Delta w^{(2)} = \delta^{(2)}(outh^{(1)})^T$"></a>$\Delta w^{(2)} = \delta^{(2)}(outh^{(1)})^T$</h4><h4 id="begin-bmatrix-0-01240932-0-08379177-end-bmatrix-cdot-begin-bmatrix-0-70056714-0-73105858-end-bmatrix-T"><a href="#begin-bmatrix-0-01240932-0-08379177-end-bmatrix-cdot-begin-bmatrix-0-70056714-0-73105858-end-bmatrix-T" class="headerlink" title="$=\begin{bmatrix} -0.01240932\\ 0.08379177\end{bmatrix} \cdot \begin{bmatrix} 0.70056714\\ 0.73105858 \end{bmatrix}^T$"></a>$=\begin{bmatrix} -0.01240932\\ 0.08379177\end{bmatrix} \cdot \begin{bmatrix} 0.70056714\\ 0.73105858 \end{bmatrix}^T$</h4><h4 id="begin-bmatrix-0-00869356-amp-0-00907194-0-5870176-amp-0-612567-end-bmatrix"><a href="#begin-bmatrix-0-00869356-amp-0-00907194-0-5870176-amp-0-612567-end-bmatrix" class="headerlink" title="$= \begin{bmatrix} -0.00869356 &amp; -0.00907194 \\ 0.5870176 &amp; 0.612567 \end{bmatrix}$"></a>$= \begin{bmatrix} -0.00869356 &amp; -0.00907194 \\ 0.5870176 &amp; 0.612567 \end{bmatrix}$</h4><h4 id="Delta-w-1-delta-1-x-T"><a href="#Delta-w-1-delta-1-x-T" class="headerlink" title="$\Delta w^{(1)} = \delta^{(1)}x^T$"></a>$\Delta w^{(1)} = \delta^{(1)}x^T$</h4><h4 id="begin-bmatrix-0-01074218-0-01287516-end-bmatrix-cdot-begin-bmatrix-0-5-1-end-bmatrix-T"><a href="#begin-bmatrix-0-01074218-0-01287516-end-bmatrix-cdot-begin-bmatrix-0-5-1-end-bmatrix-T" class="headerlink" title="$=\begin{bmatrix} 0.01074218\\ 0.01287516\end{bmatrix} \cdot \begin{bmatrix} 0.5\\ 1\end{bmatrix}^T$"></a>$=\begin{bmatrix} 0.01074218\\ 0.01287516\end{bmatrix} \cdot \begin{bmatrix} 0.5\\ 1\end{bmatrix}^T$</h4><h4 id="begin-bmatrix-0-00537109-amp-0-01074218-0-00643758-amp-0-01287516-end-bmatrix"><a href="#begin-bmatrix-0-00537109-amp-0-01074218-0-00643758-amp-0-01287516-end-bmatrix" class="headerlink" title="$= \begin{bmatrix} 0.00537109&amp; 0.01074218\\ 0.00643758 &amp; 0.01287516 \end{bmatrix}$"></a>$= \begin{bmatrix} 0.00537109&amp; 0.01074218\\ 0.00643758 &amp; 0.01287516 \end{bmatrix}$</h4></li><li><h4 id="权重更新"><a href="#权重更新" class="headerlink" title="权重更新"></a>权重更新</h4><h4 id="w-new-2-w-old-2-Delta-w-2"><a href="#w-new-2-w-old-2-Delta-w-2" class="headerlink" title="$w_{new}^2 = w_{old}^2-\Delta w^{(2)}$"></a>$w_{new}^2 = w_{old}^2-\Delta w^{(2)}$</h4><h4 id="begin-bmatrix-0-6-amp-0-8-0-7-amp-0-9-end-bmatrix-begin-bmatrix-0-00869356-amp-0-00907194-0-5870176-amp-0-612567-end-bmatrix"><a href="#begin-bmatrix-0-6-amp-0-8-0-7-amp-0-9-end-bmatrix-begin-bmatrix-0-00869356-amp-0-00907194-0-5870176-amp-0-612567-end-bmatrix" class="headerlink" title="$= {\begin{bmatrix} 0.6 &amp; 0.8 \\ 0.7 &amp; 0.9\end{bmatrix}}-\begin{bmatrix} -0.00869356 &amp; 0.00907194 \\ 0.5870176 &amp; 0.612567 \end{bmatrix}$"></a>$= {\begin{bmatrix} 0.6 &amp; 0.8 \\ 0.7 &amp; 0.9\end{bmatrix}}-\begin{bmatrix} -0.00869356 &amp; 0.00907194 \\ 0.5870176 &amp; 0.612567 \end{bmatrix}$</h4><h4 id="begin-bmatrix-0-60869356-amp-0-80907194-0-64129824-amp-0-8387433-end-bmatrix"><a href="#begin-bmatrix-0-60869356-amp-0-80907194-0-64129824-amp-0-8387433-end-bmatrix" class="headerlink" title="$= \begin{bmatrix} 0.60869356 &amp; 0.80907194 \\ 0.64129824&amp; 0.8387433 \end{bmatrix}$"></a>$= \begin{bmatrix} 0.60869356 &amp; 0.80907194 \\ 0.64129824&amp; 0.8387433 \end{bmatrix}$</h4><h4 id="b-new-2-b-old-2-Delta-b-2"><a href="#b-new-2-b-old-2-Delta-b-2" class="headerlink" title="$b_{new}^2=b_{old}^2-\Delta b^2$"></a>$b_{new}^2=b_{old}^2-\Delta b^2$</h4><h4 id="begin-bmatrix-1-1-end-bmatrix-begin-bmatrix-0-01240932-0-08379177-end-bmatrix"><a href="#begin-bmatrix-1-1-end-bmatrix-begin-bmatrix-0-01240932-0-08379177-end-bmatrix" class="headerlink" title="$= \begin{bmatrix} 1 \\ 1  \end{bmatrix}-\begin{bmatrix} -0.01240932\\ 0.08379177\end{bmatrix}$"></a>$= \begin{bmatrix} 1 \\ 1  \end{bmatrix}-\begin{bmatrix} -0.01240932\\ 0.08379177\end{bmatrix}$</h4><h4 id="begin-bmatrix-1-01240932-0-91620823-end-bmatrix"><a href="#begin-bmatrix-1-01240932-0-91620823-end-bmatrix" class="headerlink" title="$=\begin{bmatrix} 1.01240932\\ 0.91620823\end{bmatrix}$"></a>$=\begin{bmatrix} 1.01240932\\ 0.91620823\end{bmatrix}$</h4><h4 id="w-new-1-w-old-1-Delta-w-1"><a href="#w-new-1-w-old-1-Delta-w-1" class="headerlink" title="$w_{new}^1= w_{old}^1-\Delta w^{(1)}$"></a>$w_{new}^1= w_{old}^1-\Delta w^{(1)}$</h4><h4 id="begin-bmatrix-0-1-amp-0-3-0-2-amp-0-4-end-bmatrix-begin-bmatrix-0-00537109-amp-0-01074218-0-00643758-amp-0-01287516-end-bmatrix"><a href="#begin-bmatrix-0-1-amp-0-3-0-2-amp-0-4-end-bmatrix-begin-bmatrix-0-00537109-amp-0-01074218-0-00643758-amp-0-01287516-end-bmatrix" class="headerlink" title="$=\begin{bmatrix} 0.1 &amp; 0.3 \\ 0.2 &amp; 0.4\end{bmatrix} -  \begin{bmatrix} 0.00537109&amp; 0.01074218\\ 0.00643758 &amp; 0.01287516 \end{bmatrix}$"></a>$=\begin{bmatrix} 0.1 &amp; 0.3 \\ 0.2 &amp; 0.4\end{bmatrix} -  \begin{bmatrix} 0.00537109&amp; 0.01074218\\ 0.00643758 &amp; 0.01287516 \end{bmatrix}$</h4><h4 id="begin-bmatrix-0-09462891-amp-0-28925782-0-19356242-amp-0-38712484-end-bmatrix"><a href="#begin-bmatrix-0-09462891-amp-0-28925782-0-19356242-amp-0-38712484-end-bmatrix" class="headerlink" title="$= \begin{bmatrix} 0.09462891&amp; 0.28925782\\ 0.19356242&amp; 0.38712484\end{bmatrix}$"></a>$= \begin{bmatrix} 0.09462891&amp; 0.28925782\\ 0.19356242&amp; 0.38712484\end{bmatrix}$</h4><h4 id="b-new-1-b-old-1-Delta-b-1"><a href="#b-new-1-b-old-1-Delta-b-1" class="headerlink" title="$b_{new}^1=b_{old}^1-\Delta b^1$"></a>$b_{new}^1=b_{old}^1-\Delta b^1$</h4><h4 id="begin-bmatrix-0-5-0-5-end-bmatrix-begin-bmatrix-0-01074218-0-01287516-end-bmatrix"><a href="#begin-bmatrix-0-5-0-5-end-bmatrix-begin-bmatrix-0-01074218-0-01287516-end-bmatrix" class="headerlink" title="$=\begin{bmatrix} 0.5 \\ 0.5  \end{bmatrix} - \begin{bmatrix} 0.01074218\\ 0.01287516\end{bmatrix}$"></a>$=\begin{bmatrix} 0.5 \\ 0.5  \end{bmatrix} - \begin{bmatrix} 0.01074218\\ 0.01287516\end{bmatrix}$</h4><h4 id="begin-bmatrix-0-48925782-0-48712484-end-bmatrix"><a href="#begin-bmatrix-0-48925782-0-48712484-end-bmatrix" class="headerlink" title="$=\begin{bmatrix} 0.48925782\\ 0.48712484\end{bmatrix}$"></a>$=\begin{bmatrix} 0.48925782\\ 0.48712484\end{bmatrix}$</h4></li></ul><h4 id="就这样，我们就完成了一个简单神经网络的误差反向传播进行权重参数更新"><a href="#就这样，我们就完成了一个简单神经网络的误差反向传播进行权重参数更新" class="headerlink" title="就这样，我们就完成了一个简单神经网络的误差反向传播进行权重参数更新"></a>就这样，我们就完成了一个简单神经网络的误差反向传播进行权重参数更新</h4><h3 id="权重初始化"><a href="#权重初始化" class="headerlink" title="权重初始化"></a>权重初始化</h3><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20200603141525.png" srcset="/img/loading.gif" alt="images"></p>]]></content>
    
    
    <categories>
      
      <category>深度学习</category>
      
    </categories>
    
    
    <tags>
      
      <tag>深度学习</tag>
      
      <tag>神经网络</tag>
      
      <tag>反向传播</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>神经网络入门-感知器、激活函数</title>
    <link href="/2018/08/DL-method/"/>
    <url>/2018/08/DL-method/</url>
    
    <content type="html"><![CDATA[<h3 id="感知器–神经网络的起源"><a href="#感知器–神经网络的起源" class="headerlink" title="感知器–神经网络的起源"></a>感知器–神经网络的起源</h3><h4 id="神经网络是由人工神经元组成的。我们先来看看人的神经元是什么构造："><a href="#神经网络是由人工神经元组成的。我们先来看看人的神经元是什么构造：" class="headerlink" title="神经网络是由人工神经元组成的。我们先来看看人的神经元是什么构造："></a>神经网络是由人工神经元组成的。我们先来看看人的神经元是什么构造：</h4><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20200601182118.png" srcset="/img/loading.gif" alt="images"></p><h4 id="感知器是激活函数为阶跃函数的神经元。感知器的模型如下："><a href="#感知器是激活函数为阶跃函数的神经元。感知器的模型如下：" class="headerlink" title="感知器是激活函数为阶跃函数的神经元。感知器的模型如下："></a>感知器是激活函数为阶跃函数的神经元。感知器的模型如下：</h4><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20200601182119.png" srcset="/img/loading.gif" alt="images"></p><p>是不是感觉很一样啊 ，神经元也叫做<strong>感知器</strong>。感知器算法在上个世纪50-70年代很流行，也成功解决了很多问题。并且，感知器算法也是非常简单的。 </p><h3 id="感知器的定义"><a href="#感知器的定义" class="headerlink" title="感知器的定义"></a>感知器的定义</h3><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20200601182119.png" srcset="/img/loading.gif" alt="images"></p><p>我们再来分析下上图这个感知器，可以看到，一个感知器有如下几个组成部分：</p><ul><li><strong>输入(inputs)</strong>：一个感知器可以接收多个输入$(x_1,x_2,…,x_n \vert  x_i \in R)$</li><li><strong>权值(weights)</strong>：每一个输入上都有一个<code>权值</code>$w_i \in R$，此外还有一个<strong>偏置项</strong>$b \in R$，也就是上图的$w_0$。</li><li><strong>加权和(weighted sum)</strong>：就是<code>输入权值 x</code>    x    <code>权值 w</code>    +   <code>偏置项 b</code>的总和。</li><li><strong>激活函数(step function)</strong>：感知器的激活函数：$f(x)=\begin{cases} 0&amp;  x&gt;0 \ 1&amp; x  \le 0 \end{cases}$</li><li><strong>输出(output)</strong>：感知器的输出由<code>加权值</code>用<code>激活函数</code>做<code>非线性变换</code>。也就是这个公式：$y=f(w\cdot x +b )$</li></ul><h4 id="举个栗子："><a href="#举个栗子：" class="headerlink" title="举个栗子："></a>举个栗子：</h4><p>我们使用<code>unit激活函数</code>结合上图就有：</p><ul><li><h4 id="y-f-w-cdot-x-b-f-w-1x-1-w-2x-2-w-3x-3-bias"><a href="#y-f-w-cdot-x-b-f-w-1x-1-w-2x-2-w-3x-3-bias" class="headerlink" title="$y=f(w\cdot x +b )=f(w_1x_1+w_2x_2+w_3x_3+bias)$"></a>$y=f(w\cdot x +b )=f(w_1x_1+w_2x_2+w_3x_3+bias)$</h4></li><li><h5 id="其中-f-x-就是激活函数-f-x-begin-cases-1-amp-x-gt-0-0-amp-x-le-0-end-cases-，图像如下图所示。"><a href="#其中-f-x-就是激活函数-f-x-begin-cases-1-amp-x-gt-0-0-amp-x-le-0-end-cases-，图像如下图所示。" class="headerlink" title="其中$f(x)$就是激活函数 $f(x)=  \begin{cases} 1&amp;  x&gt;0 \ 0&amp; x  \le 0 \end{cases}$ ，图像如下图所示。"></a>其中$f(x)$就是激活函数 $f(x)=  \begin{cases} 1&amp;  x&gt;0 \ 0&amp; x  \le 0 \end{cases}$ ，图像如下图所示。</h5></li></ul><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20200601182120.png" srcset="/img/loading.gif" alt="images"></p><h3 id="感知器的前馈计算"><a href="#感知器的前馈计算" class="headerlink" title="感知器的前馈计算"></a>感知器的前馈计算</h3><p>再举个栗子：我们来计算下这个感知器：</p><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20200601182121.png" srcset="/img/loading.gif" alt="images"></p><p>其中<code>激活函数f</code>:$f(x)=  \begin{cases} 1&amp;  x&gt;0 \ 0&amp; x  \le 0 \end{cases}$</p><ul><li><p><code>加权和</code>：logits  =  1.0 * (-0.2) + 0.5 * (-0.4) + (-1.4) * 1.3 + 2.0 * 3.0  =  1.98</p></li><li><p><code>输出值</code>：output = f(logits) = f(1.98) = 1</p></li></ul><p>如果数据很多呢，我们就要把数据向量化：</p><p>例如：</p><h4 id="x-1-1-0-3-0-2-0-x-2-2-0-1-0-5-0-x-3-2-0-0-0-3-0-x-4-4-0-1-0-6-0-w-4-0-3-0-5-0-b-2-0"><a href="#x-1-1-0-3-0-2-0-x-2-2-0-1-0-5-0-x-3-2-0-0-0-3-0-x-4-4-0-1-0-6-0-w-4-0-3-0-5-0-b-2-0" class="headerlink" title="$x_1=[-1.0, 3.0, 2.0] \\ x_2=[2.0, -1.0, 5.0] \\ x_3=[-2.0, 0.0, 3.0 ] \\ x_4=[4.0, 1.0, 6.0] \\ w=[4.0, -3.0, 5.0 ] \\ b=2.0$"></a>$x_1=[-1.0, 3.0, 2.0] \\ x_2=[2.0, -1.0, 5.0] \\ x_3=[-2.0, 0.0, 3.0 ] \\ x_4=[4.0, 1.0, 6.0] \\ w=[4.0, -3.0, 5.0 ] \\ b=2.0$</h4><h4 id="则：-X-begin-bmatrix-1-0-amp-3-0-amp-2-0-2-0-amp-1-0-amp-5-0-2-0-amp-0-0-amp-3-0-4-0-amp-1-0-amp-6-0-end-bmatrix"><a href="#则：-X-begin-bmatrix-1-0-amp-3-0-amp-2-0-2-0-amp-1-0-amp-5-0-2-0-amp-0-0-amp-3-0-4-0-amp-1-0-amp-6-0-end-bmatrix" class="headerlink" title="则：$X=\begin{bmatrix}  -1.0 &amp; 3.0 &amp; 2.0 \\ 2.0 &amp; -1.0&amp; 5.0 \\ -2.0&amp; 0.0&amp; 3.0 \\ 4.0&amp; 1.0 &amp; 6.0  \end{bmatrix}$"></a>则：$X=\begin{bmatrix}  -1.0 &amp; 3.0 &amp; 2.0 \\ 2.0 &amp; -1.0&amp; 5.0 \\ -2.0&amp; 0.0&amp; 3.0 \\ 4.0&amp; 1.0 &amp; 6.0  \end{bmatrix}$</h4><h4 id="w-T-begin-bmatrix-4-0-3-0-5-0-end-bmatrix"><a href="#w-T-begin-bmatrix-4-0-3-0-5-0-end-bmatrix" class="headerlink" title="$w^T =\begin{bmatrix} 4.0 \\ -3.0 \\ 5.0 \end{bmatrix}$"></a>$w^T =\begin{bmatrix} 4.0 \\ -3.0 \\ 5.0 \end{bmatrix}$</h4><h4 id="所以：-logits-X-cdot-w-T-b-begin-bmatrix-1-0-amp-3-0-amp-2-0-2-0-amp-1-0-amp-5-0-2-0-amp-0-0-amp-3-0-4-0-amp-1-0-amp-6-0-end-bmatrix-cdot-begin-bmatrix-4-0-3-0-5-0-end-bmatrix-2-0-1-0-38-0-7-0-43-0"><a href="#所以：-logits-X-cdot-w-T-b-begin-bmatrix-1-0-amp-3-0-amp-2-0-2-0-amp-1-0-amp-5-0-2-0-amp-0-0-amp-3-0-4-0-amp-1-0-amp-6-0-end-bmatrix-cdot-begin-bmatrix-4-0-3-0-5-0-end-bmatrix-2-0-1-0-38-0-7-0-43-0" class="headerlink" title="所以：$logits =  X\cdot w^T + b= \begin{bmatrix}  -1.0 &amp; 3.0 &amp; 2.0 \\ 2.0 &amp; -1.0&amp; 5.0 \\ -2.0&amp; 0.0&amp; 3.0 \\ 4.0&amp; 1.0 &amp; 6.0  \end{bmatrix} \cdot \begin{bmatrix} 4.0 \\ -3.0 \\ 5.0 \end{bmatrix} + 2.0 \\ =[-1.0 \ \ \  38.0 \ \ \ 7.0 \ \ \ 43.0 ]$"></a>所以：$logits =  X\cdot w^T + b= \begin{bmatrix}  -1.0 &amp; 3.0 &amp; 2.0 \\ 2.0 &amp; -1.0&amp; 5.0 \\ -2.0&amp; 0.0&amp; 3.0 \\ 4.0&amp; 1.0 &amp; 6.0  \end{bmatrix} \cdot \begin{bmatrix} 4.0 \\ -3.0 \\ 5.0 \end{bmatrix} + 2.0 \\ =[-1.0 \ \ \  38.0 \ \ \ 7.0 \ \ \ 43.0 ]$</h4><h4 id="最后带入激活函数："><a href="#最后带入激活函数：" class="headerlink" title="最后带入激活函数："></a>最后带入激活函数：</h4><h4 id="则：-output-f-x-0-1-1-1"><a href="#则：-output-f-x-0-1-1-1" class="headerlink" title="则：$output = f(x)=[0\ \ \ 1 \ \ \ 1 \ \ \ 1 ]$"></a>则：$output = f(x)=[0\ \ \ 1 \ \ \ 1 \ \ \ 1 ]$</h4><h3 id="感知器的运用"><a href="#感知器的运用" class="headerlink" title="感知器的运用"></a>感知器的运用</h3><p>使用感知器可以完成一些基础的逻辑操作</p><p>例如<code>逻辑与</code></p><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20200601182122.png" srcset="/img/loading.gif" alt="images"></p><p>注意：激活函数是<code>unit激活函数</code>，再看看其他逻辑运算</p><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20200601182123.png" srcset="/img/loading.gif" alt="images"></p><h3 id="感知器的局限性"><a href="#感知器的局限性" class="headerlink" title="感知器的局限性"></a>感知器的局限性</h3><ul><li>仅能做0-1输出</li><li>仅能处理线性分类的问题（无法处理XOR问题）</li></ul><h3 id="多层感知机–现代神经网络的原型"><a href="#多层感知机–现代神经网络的原型" class="headerlink" title="多层感知机–现代神经网络的原型"></a>多层感知机–现代神经网络的原型</h3><p>对比于感知器，引入了隐层，改变了激活函数，加入反向传播算法，优化算法，也就是后面要讲的神经网络。</p><h3 id="隐层的定义"><a href="#隐层的定义" class="headerlink" title="隐层的定义"></a>隐层的定义</h3><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20200601182124.png" srcset="/img/loading.gif" alt="images"></p><p>从上图中可以看出：</p><ul><li>所有同一层的神经元都与上一层的每个输出相连</li><li>同一层的神经元之间不相互连接</li><li>各神经元的输出为数值</li></ul><h3 id="隐层的结构"><a href="#隐层的结构" class="headerlink" title="隐层的结构"></a>隐层的结构</h3><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20200601182125.png" srcset="/img/loading.gif" alt="images"></p><h4 id="举个栗子：-1"><a href="#举个栗子：-1" class="headerlink" title="举个栗子："></a>举个栗子：</h4><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20200601182126.png" srcset="/img/loading.gif" alt="images"></p><p>由上图可知：</p><h4 id="h-1-f-2x-1-2x-2-1-h-2-f-2x-1-2x-2-3-o-1-f-2h-1-2h-2-3"><a href="#h-1-f-2x-1-2x-2-1-h-2-f-2x-1-2x-2-3-o-1-f-2h-1-2h-2-3" class="headerlink" title="$h_1 = f(2x_1+2x_2-1) \\ h_2=f(-2x_1+-2x_2+3) \\ o_1 = f(2h_1+2h_2-3)$"></a>$h_1 = f(2x_1+2x_2-1) \\ h_2=f(-2x_1+-2x_2+3) \\ o_1 = f(2h_1+2h_2-3)$</h4><p>其中$f(x)$是激活函数</p><h3 id="激活函数"><a href="#激活函数" class="headerlink" title="激活函数"></a>激活函数</h3><p>新的激活函数：</p><ul><li><h4 id="unit激活函数：-f-x-unit-x-begin-cases-0-amp-x-gt-0-1-amp-x-le-0-end-cases"><a href="#unit激活函数：-f-x-unit-x-begin-cases-0-amp-x-gt-0-1-amp-x-le-0-end-cases" class="headerlink" title="unit激活函数：$f(x)=unit(x)=  \begin{cases} 0&amp;  x&gt;0 \ 1&amp; x  \le 0 \end{cases} $"></a><strong><code>unit激活函数</code></strong>：$f(x)=unit(x)=  \begin{cases} 0&amp;  x&gt;0 \ 1&amp; x  \le 0 \end{cases} $</h4><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20200601182120.png" srcset="/img/loading.gif" alt="images"></p></li><li><h4 id="sigmod激活函数：-f-x-sigmod-x-frac-1-1-e-x"><a href="#sigmod激活函数：-f-x-sigmod-x-frac-1-1-e-x" class="headerlink" title="sigmod激活函数：$f(x)=sigmod(x)=\frac{1}{1+e^{-x}}$"></a><strong><code>sigmod激活函数</code></strong>：$f(x)=sigmod(x)=\frac{1}{1+e^{-x}}$</h4><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20200601182127.png" srcset="/img/loading.gif" alt="images"></p></li><li><h5 id="tanh激活函数：-f-x-tanh-x-frac-e-x-e-x-e-x-e-x"><a href="#tanh激活函数：-f-x-tanh-x-frac-e-x-e-x-e-x-e-x" class="headerlink" title="tanh激活函数：$f(x)=tanh(x)=\frac{e^x-e^{-x}}{e^x+e^{-x}}$"></a><strong><code>tanh激活函数</code></strong>：$f(x)=tanh(x)=\frac{e^x-e^{-x}}{e^x+e^{-x}}$</h5><p>​                                <img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20200601182128.png" srcset="/img/loading.gif" alt="images">`</p></li><li><h5 id="ReLU激活函数：-f-x-ReLU-x-begin-cases-x-amp-x-gt-0-0-amp-x-le-0-end-cases"><a href="#ReLU激活函数：-f-x-ReLU-x-begin-cases-x-amp-x-gt-0-0-amp-x-le-0-end-cases" class="headerlink" title="ReLU激活函数：$f(x)=ReLU(x)=\begin{cases} x&amp; x&gt;0\ 0&amp; x \le 0 \end{cases} $"></a><code>ReLU激活函数</code>：$f(x)=ReLU(x)=\begin{cases} x&amp; x&gt;0\ 0&amp; x \le 0 \end{cases} $</h5><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20200601182129.png" srcset="/img/loading.gif" alt="images"></p></li></ul><h3 id="激活函数的作用"><a href="#激活函数的作用" class="headerlink" title="激活函数的作用"></a>激活函数的作用</h3><ul><li><p><strong>引入非线性因素。</strong> </p><p>在我们面对线性可分的数据集的时候，简单的用线性分类器即可解决分类问题。但是现实生活中的数据往往不是线性可分的，面对这样的数据，一般有两个方法：引入非线性函数、线性变换。 </p></li><li><h4 id="线性变换"><a href="#线性变换" class="headerlink" title="线性变换"></a>线性变换</h4><p>就是把当前特征空间通过一定的线性映射转换到另一个空间，让数据能够更好的被分类。 </p></li></ul><h3 id="激活函数的特点"><a href="#激活函数的特点" class="headerlink" title="激活函数的特点"></a>激活函数的特点</h3><ul><li><p><code>unit</code>：线性分界</p><p>– 几乎已经不用了</p></li><li><p><code>sigmoid</code>：非线性分界</p><p>– 两端软饱和，输出为 (0,1)区间</p><p>– 两端有梯度消失问题</p><p>– 因为输出恒正，可能有 zig现象</p></li><li><p><code>tanh</code>：非线性分界 ：非线性分界</p><p>– 两端软饱和，输出为 (-1, 1) 区间</p><p>– 仍然存在梯度消失问题</p><p>– 没有 zig，收敛更快 (LeCun 1989)</p></li><li><p><code>ReLU</code>：非线性分界<br>– 左侧硬饱和，右无输出为 [0,+∞)区间</p><p>– 左侧会出现梯度一直为 0的情况，导致神经元 不再更新（死亡）</p><p>– 改善了梯度弥散</p><p>– 同样存在 zig</p></li></ul><h3 id="一些新的激活函数"><a href="#一些新的激活函数" class="headerlink" title="一些新的激活函数"></a>一些新的激活函数</h3><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20200601182130.png" srcset="/img/loading.gif" alt="images"></p>]]></content>
    
    
    <categories>
      
      <category>深度学习</category>
      
    </categories>
    
    
    <tags>
      
      <tag>深度学习</tag>
      
      <tag>神经网络</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>TensorFlow-从入门到实践</title>
    <link href="/2018/08/TensorFlow-course/"/>
    <url>/2018/08/TensorFlow-course/</url>
    
    <content type="html"><![CDATA[<h3 id="TensorFlow-从入门到实践"><a href="#TensorFlow-从入门到实践" class="headerlink" title="TensorFlow 从入门到实践"></a>TensorFlow 从入门到实践</h3><p>导入TensorFlow模块，查看下当前模块的版本</p><pre><code class="hljs python"><span class="hljs-keyword">import</span> tensorflow <span class="hljs-keyword">as</span> tfprint(tf.__version__)</code></pre><pre><code class="hljs bash">1.4.0</code></pre><p>tensorflow是一种图计算框架，所有的计算操作被声明为图（graph）中的节点（Node）</p><p>即使只是声明一个变量或者常量，也并不执行实际的操作，而是向图中增加节点</p><p>我们来看一上前面安装TensorFlow的时候的测试代码</p><pre><code class="hljs python"><span class="hljs-comment"># 创建名为hello_constant的TensorFlow对象</span>hello_constant = tf.constant(<span class="hljs-string">'Hello World!'</span>)<span class="hljs-keyword">with</span> tf.Session() <span class="hljs-keyword">as</span> sess:        <span class="hljs-comment"># 在session中运行tf.constant 操作</span>    output = sess.run(hello_constant)        print(output)</code></pre><pre><code class="hljs 1c">b'Hello World!'</code></pre><h3 id="Tensor"><a href="#Tensor" class="headerlink" title="Tensor"></a>Tensor</h3><h4 id="在-TensorFlow-中，数据不是以整数，浮点数或者字符串形式存在的。"><a href="#在-TensorFlow-中，数据不是以整数，浮点数或者字符串形式存在的。" class="headerlink" title="在 TensorFlow 中，数据不是以整数，浮点数或者字符串形式存在的。"></a>在 TensorFlow 中，数据不是以整数，浮点数或者字符串形式存在的。</h4><h4 id="这些值被封装在一个叫做-tensor-的对象中。"><a href="#这些值被封装在一个叫做-tensor-的对象中。" class="headerlink" title="这些值被封装在一个叫做 tensor 的对象中。"></a>这些值被封装在一个叫做 tensor 的对象中。</h4><p>在 hello_constant = tf.constant(‘Hello World!’) 代码中，<br>hello_constant是一个 0 维度的字符串 tensor。</p><h4 id="tensors-还有很多不同大小："><a href="#tensors-还有很多不同大小：" class="headerlink" title="tensors 还有很多不同大小："></a>tensors 还有很多不同大小：</h4><pre><code class="hljs python"><span class="hljs-comment"># A 是一个0维的 int32 tensor</span>A = tf.constant(<span class="hljs-number">1234</span>) <span class="hljs-comment"># B 是一个1维的  int32 tensor</span>B = tf.constant([<span class="hljs-number">123</span>,<span class="hljs-number">456</span>,<span class="hljs-number">789</span>])  <span class="hljs-comment"># C 是一个2维的  int32 tensor</span>C = tf.constant([ [<span class="hljs-number">123</span>,<span class="hljs-number">456</span>,<span class="hljs-number">789</span>], [<span class="hljs-number">222</span>,<span class="hljs-number">333</span>,<span class="hljs-number">444</span>] ])<span class="hljs-keyword">with</span> tf.Session() <span class="hljs-keyword">as</span> sess:    out_A = sess.run(A)    out_B = sess.run(B)    out_C = sess.run(C)        print(<span class="hljs-string">'\n'</span>, out_A, <span class="hljs-string">'\n'</span>, <span class="hljs-string">'----------'</span>, <span class="hljs-string">'\n'</span>, out_B, <span class="hljs-string">'\n'</span>, <span class="hljs-string">'----------'</span>, <span class="hljs-string">'\n'</span>, out_C)</code></pre><pre><code class="hljs bash">1234 ---------- [123 456 789] ---------- [[123 456 789][222 333 444]]</code></pre><h3 id="Session"><a href="#Session" class="headerlink" title="Session"></a>Session</h3><p>TensorFlow 的 api 构建在 computational graph（计算图) 的概念上，它是一种对数学运算过程进行可视化的一种方法。让我们把你刚才运行的 TensorFlow 的代码变成一个图：</p><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20200601182049.png" srcset="/img/loading.gif" alt="44.png"></p><p>如上图所示，一个 “TensorFlow Session” 是用来运行图的环境。这个 session 负责分配 GPU(s) 和／或 CPU(s) 包括远程计算机的运算。让我们看看怎么使用它：</p><pre><code class="hljs python"><span class="hljs-keyword">with</span> tf.Session() <span class="hljs-keyword">as</span> sess:    output = sess.run(hello_constant)</code></pre><p>这段代码已经从上一行创建了一个 tensor hello_constant。下一行是在session里对 tensor 求值。</p><p>这段代码用 tf.Session 创建了一个sess的 session 实例。 sess.run() 函数对 tensor 求值，并返回结果。</p><h3 id="constant运算"><a href="#constant运算" class="headerlink" title="constant运算"></a>constant运算</h3><p>我们先计算一个线性函数，y=wx+b</p><pre><code class="hljs python">w = tf.Variable([[<span class="hljs-number">1.</span>, <span class="hljs-number">2.</span>]])x = tf.Variable([[<span class="hljs-number">3.</span>], [<span class="hljs-number">4.</span>]])b = tf.constant(<span class="hljs-number">.9</span>)y = tf.add(tf.matmul(w, x), b)<span class="hljs-comment"># 变量必须要初始化后才能使用</span>init = tf.global_variables_initializer()<span class="hljs-keyword">with</span> tf.Session() <span class="hljs-keyword">as</span> sess:    sess.run(init)    print(y.eval())</code></pre><pre><code class="hljs lua"><span class="hljs-string">[[11.9]]</span></code></pre><p>计算图中所有的数据均以tensor来存储和表达。<br>tensor是一个高阶张量，二阶张量为矩阵，一阶张量为向量，0阶张量为一个数（标量）。</p><pre><code class="hljs python">a = tf.constant(<span class="hljs-number">0</span>, name=<span class="hljs-string">'B'</span>)b = tf.constant(<span class="hljs-number">1</span>)</code></pre><pre><code class="hljs python">print(a)print(b)</code></pre><pre><code class="hljs bash">Tensor(<span class="hljs-string">"B:0"</span>, shape=(), dtype=int32)Tensor(<span class="hljs-string">"Const_5:0"</span>, shape=(), dtype=int32)</code></pre><p>其他计算操作也同样如此。 tensorflow中的大部分操作都需要通过tf.xxxxx的方式进行调用。</p><pre><code class="hljs python">c = tf.add(a,b)print(c)</code></pre><pre><code class="hljs bash">Tensor(<span class="hljs-string">"Add_1:0"</span>, shape=(), dtype=int32)</code></pre><p>从加法开始， tf.add() 完成的工作与你期望的一样。它把两个数字，两个 tensor，返回他们的和。减法和乘法同样也是直接调用对应的接口函数</p><pre><code class="hljs python">x = tf.subtract(<span class="hljs-number">10</span>, <span class="hljs-number">4</span>) <span class="hljs-comment"># 6</span>y = tf.multiply(<span class="hljs-number">2</span>, <span class="hljs-number">5</span>)  <span class="hljs-comment"># 10</span></code></pre><h3 id="还有很多关于数学的api-你可以自己去查阅文档"><a href="#还有很多关于数学的api-你可以自己去查阅文档" class="headerlink" title="还有很多关于数学的api,你可以自己去查阅文档"></a>还有很多关于数学的api,你可以自己去查阅<a href="https://www.tensorflow.org/api_guides/python/math_ops" target="_blank" rel="noopener">文档</a></h3><pre><code class="hljs python">mat_a = tf.constant([[<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>], [<span class="hljs-number">3</span>, <span class="hljs-number">3</span>, <span class="hljs-number">3</span>]])mat_b = tf.constant([[<span class="hljs-number">2</span>, <span class="hljs-number">2</span>, <span class="hljs-number">2</span>],[<span class="hljs-number">5</span>, <span class="hljs-number">5</span>, <span class="hljs-number">5</span>]], name=<span class="hljs-string">'mat_b'</span>)</code></pre><pre><code class="hljs python">mul_a_b = mat_a * mat_btf_mul_a_b = tf.multiply(mat_a, mat_b)tf_matmul_a_b = tf.matmul(mat_a, tf.transpose(mat_b), name=<span class="hljs-string">'matmul_with_name'</span>)</code></pre><pre><code class="hljs python">print(mul_a_b)print(tf_mul_a_b)print(tf_matmul_a_b)</code></pre><pre><code class="hljs bash">Tensor(<span class="hljs-string">"mul:0"</span>, shape=(2, 3), dtype=int32)Tensor(<span class="hljs-string">"Mul_1:0"</span>, shape=(2, 3), dtype=int32)Tensor(<span class="hljs-string">"matmul_with_name:0"</span>, shape=(2, 2), dtype=int32)</code></pre><pre><code class="hljs python"><span class="hljs-keyword">with</span> tf.Session() <span class="hljs-keyword">as</span> sess:    mul_value, tf_mul_value, tf_matmul_value = sess.run([mul_a_b, tf_mul_a_b, tf_matmul_a_b])</code></pre><pre><code class="hljs python">print(mul_value)print(tf_mul_value)print(tf_matmul_value)</code></pre><pre><code class="hljs bash">[[ 2  2  2] [15 15 15]][[ 2  2  2] [15 15 15]][[ 6 15] [18 45]]</code></pre><p>我们再举几个具体的实例来看看</p><pre><code class="hljs python">a = tf.constant(<span class="hljs-number">2</span>)b = tf.constant(<span class="hljs-number">3</span>)x = tf.add(a, b)<span class="hljs-keyword">with</span> tf.Session() <span class="hljs-keyword">as</span> sess:writer = tf.summary.FileWriter(<span class="hljs-string">'./graphs/const_add'</span>, sess.graph)print(sess.run(x))writer.close()</code></pre><pre><code class="hljs angelscript"><span class="hljs-number">5</span></code></pre><pre><code class="hljs python">a = tf.constant([<span class="hljs-number">2</span>, <span class="hljs-number">2</span>], name=<span class="hljs-string">'a'</span>)b = tf.constant([[<span class="hljs-number">0</span>, <span class="hljs-number">1</span>], [<span class="hljs-number">2</span>, <span class="hljs-number">3</span>]], name=<span class="hljs-string">'b'</span>)x = tf.multiply(a, b, name=<span class="hljs-string">'dot_product'</span>)<span class="hljs-keyword">with</span> tf.Session() <span class="hljs-keyword">as</span> sess:writer = tf.summary.FileWriter(<span class="hljs-string">'./graphs/const_mul'</span>, sess.graph) print(sess.run(x))writer.close()</code></pre><pre><code class="hljs json">[[<span class="hljs-number">0</span> <span class="hljs-number">2</span>] [<span class="hljs-number">4</span> <span class="hljs-number">6</span>]]</code></pre><pre><code class="hljs python">a = tf.constant([[<span class="hljs-number">2</span>, <span class="hljs-number">2</span>],[<span class="hljs-number">1</span>, <span class="hljs-number">4</span>]], name=<span class="hljs-string">'a'</span>)b = tf.constant([[<span class="hljs-number">0</span>, <span class="hljs-number">1</span>], [<span class="hljs-number">2</span>, <span class="hljs-number">3</span>]], name=<span class="hljs-string">'b'</span>)x = tf.multiply(a, b, name=<span class="hljs-string">'dot_product'</span>)y = tf.matmul(a, b, name=<span class="hljs-string">'mat_mul'</span>)<span class="hljs-keyword">with</span> tf.Session() <span class="hljs-keyword">as</span> sess:writer = tf.summary.FileWriter(<span class="hljs-string">'./graphs/const_mul_2'</span>, sess.graph) print(sess.run(x))print(sess.run(y))writer.close()</code></pre><pre><code class="hljs json">[[ <span class="hljs-number">0</span>  <span class="hljs-number">2</span>] [ <span class="hljs-number">2</span> <span class="hljs-number">12</span>]][[ <span class="hljs-number">4</span>  <span class="hljs-number">8</span>] [ <span class="hljs-number">8</span> <span class="hljs-number">13</span>]]</code></pre><h3 id="各式各样的常量"><a href="#各式各样的常量" class="headerlink" title="各式各样的常量"></a>各式各样的常量</h3><pre><code class="hljs python">x = tf.zeros([<span class="hljs-number">2</span>, <span class="hljs-number">3</span>], tf.int32)y = tf.zeros_like(x, optimize=<span class="hljs-literal">True</span>)</code></pre><pre><code class="hljs python">print(y)</code></pre><pre><code class="hljs reasonml"><span class="hljs-constructor">Tensor(<span class="hljs-string">"zeros_like:0"</span>, <span class="hljs-params">shape</span>=(2, 3)</span>, dtype=<span class="hljs-built_in">int32</span>)</code></pre><pre><code class="hljs python"><span class="hljs-keyword">with</span> tf.Session() <span class="hljs-keyword">as</span> sess:    print(sess.run(y))</code></pre><pre><code class="hljs angelscript">[[<span class="hljs-number">0</span> <span class="hljs-number">0</span> <span class="hljs-number">0</span>] [<span class="hljs-number">0</span> <span class="hljs-number">0</span> <span class="hljs-number">0</span>]]</code></pre><pre><code class="hljs python">t_0 = <span class="hljs-number">19</span> x = tf.zeros_like(t_0) <span class="hljs-comment"># ==&gt; 0</span>y = tf.ones_like(t_0) <span class="hljs-comment"># ==&gt; 1</span><span class="hljs-keyword">with</span> tf.Session() <span class="hljs-keyword">as</span> sess:    print(sess.run([x]))    print(sess.run([y]))</code></pre><pre><code class="hljs json">[<span class="hljs-number">0</span>][<span class="hljs-number">1</span>]</code></pre><h3 id="变量"><a href="#变量" class="headerlink" title="变量"></a>变量</h3><pre><code class="hljs python"><span class="hljs-keyword">with</span> tf.variable_scope(<span class="hljs-string">'meh'</span>) <span class="hljs-keyword">as</span> scope:    a = tf.get_variable(<span class="hljs-string">'a'</span>, [<span class="hljs-number">10</span>])    b = tf.get_variable(<span class="hljs-string">'b'</span>, [<span class="hljs-number">100</span>])writer = tf.summary.FileWriter(<span class="hljs-string">'./graphs/test'</span>, tf.get_default_graph())writer.close()</code></pre><p>例如：求导也是一个运算</p><pre><code class="hljs python">x = tf.Variable(<span class="hljs-number">2.0</span>)y = <span class="hljs-number">2.0</span> * (x ** <span class="hljs-number">3</span>)z = <span class="hljs-number">3.0</span> + y ** <span class="hljs-number">2</span>grad_z = tf.gradients(z, y)<span class="hljs-keyword">with</span> tf.Session() <span class="hljs-keyword">as</span> sess:    sess.run(x.initializer)    print(sess.run(grad_z))</code></pre><pre><code class="hljs json">[<span class="hljs-number">32.0</span>]</code></pre><h4 id="一次性初始化所有变量"><a href="#一次性初始化所有变量" class="headerlink" title="一次性初始化所有变量"></a>一次性初始化所有变量</h4><pre><code class="hljs python">my_var = tf.Variable(<span class="hljs-number">2</span>, name=<span class="hljs-string">"my_var"</span>) my_var_times_two = my_var.assign(<span class="hljs-number">2</span> * my_var)<span class="hljs-keyword">with</span> tf.Session() <span class="hljs-keyword">as</span> sess:    sess.run(tf.global_variables_initializer())    print(sess.run(my_var_times_two))    print(sess.run(my_var_times_two))    print(sess.run(my_var_times_two))</code></pre><pre><code class="hljs angelscript"><span class="hljs-number">4</span><span class="hljs-number">8</span><span class="hljs-number">16</span></code></pre><hr><p>在前面我们都是采用，先赋值再计算的方法，但是在我们的工作过程中，我会遇到先定义变量，后赋值的情况。<br>如果你想用一个非常量 non-constant 该怎么办？这就是 tf.placeholder() 和 feed_dict 派上用场的时候了。我们来看看如何向 TensorFlow 传输数据的基本知识。</p><h3 id="tf-placeholder"><a href="#tf-placeholder" class="headerlink" title="tf.placeholder()"></a>tf.placeholder()</h3><p>当你你不能把数据赋值到 x 在把它传给 TensorFlow。因为后面你需要你的 TensorFlow 模型对不同的数据集采取不同的参数。这时你需要 tf.placeholder()！</p><p>数据经过 tf.session.run() 函数得到的值，由 tf.placeholder() 返回成一个 tensor，这样你可以在 session 开始跑之前，设置输入。</p><h4 id="Session’s-feed-dict-功能"><a href="#Session’s-feed-dict-功能" class="headerlink" title="Session’s feed_dict 功能"></a>Session’s feed_dict 功能</h4><pre><code class="hljs python">x = tf.placeholder(tf.string)<span class="hljs-keyword">with</span> tf.Session() <span class="hljs-keyword">as</span> sess:    out = sess.run(x, feed_dict=&#123;x: <span class="hljs-string">'Hello World'</span>&#125;)    print(out)</code></pre><pre><code class="hljs ebnf"><span class="hljs-attribute">Hello World</span></code></pre><p>用 tf.session.run() 里 feed_dict 参数设置占位 tensor。上面的例子显示 tensor x 被设置成字符串 “Hello, world”。当然你也可以用 feed_dict 设置多个 tensor。</p><p>placeholder是一个占位符，它通常代表着从外界输入的值。 其中None代表着尚不确定的维度。</p><pre><code class="hljs python">input1 = tf.placeholder(tf.float32)input2 = tf.placeholder(tf.float32)output = tf.multiply(input1, input2)<span class="hljs-keyword">with</span> tf.Session() <span class="hljs-keyword">as</span> sess:    print(sess.run([output], feed_dict=&#123;input1:[<span class="hljs-number">7.</span>], input2:[<span class="hljs-number">2.</span>]&#125;))</code></pre><pre><code class="hljs json">[array([<span class="hljs-number">14.</span>], dtype=float32)]</code></pre><h3 id="把numpy转换成Tensor"><a href="#把numpy转换成Tensor" class="headerlink" title="把numpy转换成Tensor"></a>把numpy转换成Tensor</h3><pre><code class="hljs python"><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> npa = np.zeros((<span class="hljs-number">3</span>,<span class="hljs-number">3</span>))print(a)print(<span class="hljs-string">'----------------'</span>)ta = tf.convert_to_tensor(a)<span class="hljs-keyword">with</span> tf.Session() <span class="hljs-keyword">as</span> sess:     print(sess.run(ta))</code></pre><pre><code class="hljs angelscript">[[<span class="hljs-number">0.</span> <span class="hljs-number">0.</span> <span class="hljs-number">0.</span>] [<span class="hljs-number">0.</span> <span class="hljs-number">0.</span> <span class="hljs-number">0.</span>] [<span class="hljs-number">0.</span> <span class="hljs-number">0.</span> <span class="hljs-number">0.</span>]]----------------[[<span class="hljs-number">0.</span> <span class="hljs-number">0.</span> <span class="hljs-number">0.</span>] [<span class="hljs-number">0.</span> <span class="hljs-number">0.</span> <span class="hljs-number">0.</span>] [<span class="hljs-number">0.</span> <span class="hljs-number">0.</span> <span class="hljs-number">0.</span>]]</code></pre><h3 id="类型转换"><a href="#类型转换" class="headerlink" title="类型转换"></a>类型转换</h3><p>为了让特定运算能运行，有时会对类型进行转换。例如，你尝试下列代码，会报错：</p><pre><code class="hljs python">tf.subtract(tf.constant(<span class="hljs-number">2.0</span>),tf.constant(<span class="hljs-number">1</span>))</code></pre><pre><code class="hljs routeros">TypeError: Input <span class="hljs-string">'y'</span> of <span class="hljs-string">'Sub'</span> Op has<span class="hljs-built_in"> type </span>int32 that does <span class="hljs-keyword">not</span> match<span class="hljs-built_in"> type </span>float32 of argument <span class="hljs-string">'x'</span>.</code></pre><p>只是因为常量 1 是整数，但是常量 2.0 是浮点数 subtract 需要他们能相符。</p><p>在这种情况下，你可以让数据都是同一类型，或者强制转换一个值到另一个类型。这里，我们可以把 2.0 转换成整数再相减，这样就能得出正确的结果：</p><pre><code class="hljs python">number = tf.subtract(tf.cast(tf.constant(<span class="hljs-number">2.0</span>), tf.int32), tf.constant(<span class="hljs-number">1</span>))   <span class="hljs-keyword">with</span> tf.Session() <span class="hljs-keyword">as</span> sess:    print(sess.run(number))</code></pre><pre><code class="hljs angelscript"><span class="hljs-number">1</span></code></pre><h3 id="tensorboard-了解图结构-可视化利器"><a href="#tensorboard-了解图结构-可视化利器" class="headerlink" title="tensorboard 了解图结构/可视化利器"></a>tensorboard 了解图结构/可视化利器</h3><pre><code class="hljs python"><span class="hljs-keyword">import</span> tensorflow <span class="hljs-keyword">as</span> tfa = tf.constant(<span class="hljs-number">2</span>, name=<span class="hljs-string">'A'</span>)b = tf.constant(<span class="hljs-number">3</span>, name=<span class="hljs-string">'B'</span>)x = tf.add(a,b, name=<span class="hljs-string">'addAB'</span>)<span class="hljs-keyword">with</span> tf.Session() <span class="hljs-keyword">as</span> sess:    print(sess.run(x))    <span class="hljs-comment">#写到日志文件里</span>    writer = tf.summary.FileWriter(<span class="hljs-string">'./graphs/add_test'</span>, sess.graph)    <span class="hljs-comment">#关闭writer</span>    writer.close()</code></pre><pre><code class="hljs angelscript"><span class="hljs-number">5</span></code></pre><h3 id="启动-TensorBoard"><a href="#启动-TensorBoard" class="headerlink" title="启动 TensorBoard"></a>启动 TensorBoard</h3><p>在命令端运行：tensorboard –logdir=”./graphs” –port 7007</p><p>然后打开Google浏览器访问：<a href="http://localhost:7007/" target="_blank" rel="noopener">http://localhost:7007/</a></p><h3 id="实践"><a href="#实践" class="headerlink" title="实践"></a>实践</h3><h4 id="TensorFlow实现线性回归"><a href="#TensorFlow实现线性回归" class="headerlink" title="TensorFlow实现线性回归"></a>TensorFlow实现线性回归</h4><pre><code class="hljs python"><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<span class="hljs-keyword">import</span> os<span class="hljs-keyword">import</span> tensorflow <span class="hljs-keyword">as</span> tf<span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt</code></pre><p>设置生成的图像尺寸和去除警告</p><pre><code class="hljs python">os.environ[<span class="hljs-string">'TF_CPP_MIN_LOG_LEVEL'</span>] = <span class="hljs-string">'2'</span>plt.rcParams[<span class="hljs-string">"figure.figsize"</span>] = (<span class="hljs-number">14</span>, <span class="hljs-number">8</span>)  <span class="hljs-comment"># 生成的图像尺寸</span></code></pre><p>随机生成一个线性的数据，当然你可以换成读取对应的数据集</p><pre><code class="hljs python">n_observations = <span class="hljs-number">100</span>xs = np.linspace(<span class="hljs-number">-3</span>, <span class="hljs-number">3</span>, n_observations)ys = <span class="hljs-number">0.8</span>*xs + <span class="hljs-number">0.1</span> + np.random.uniform(<span class="hljs-number">-0.5</span>, <span class="hljs-number">0.5</span>, n_observations)plt.scatter(xs, ys)plt.show()</code></pre><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20200601182050.png" srcset="/img/loading.gif" alt="images"></p><h3 id="准备好placeholder"><a href="#准备好placeholder" class="headerlink" title="准备好placeholder"></a>准备好placeholder</h3><pre><code class="hljs python">X = tf.placeholder(tf.float32, name=<span class="hljs-string">'X'</span>)Y = tf.placeholder(tf.float32, name=<span class="hljs-string">'Y'</span>)</code></pre><h3 id="初始化参数-权重"><a href="#初始化参数-权重" class="headerlink" title="初始化参数/权重"></a>初始化参数/权重</h3><pre><code class="hljs python">W = tf.Variable(tf.random_normal([<span class="hljs-number">1</span>]), name=<span class="hljs-string">'weight'</span>)b = tf.Variable(tf.random_normal([<span class="hljs-number">1</span>]), name=<span class="hljs-string">'bias'</span>)</code></pre><h3 id="计算预测结果"><a href="#计算预测结果" class="headerlink" title="计算预测结果"></a>计算预测结果</h3><pre><code class="hljs python">Y_pred = tf.add(tf.multiply(X, W), b)</code></pre><h3 id="计算损失值函数"><a href="#计算损失值函数" class="headerlink" title="计算损失值函数"></a>计算损失值函数</h3><pre><code class="hljs python">loss = tf.square(Y - Y_pred, name=<span class="hljs-string">'loss'</span>)</code></pre><h3 id="初始化optimizer"><a href="#初始化optimizer" class="headerlink" title="初始化optimizer"></a>初始化optimizer</h3><pre><code class="hljs python">learning_rate = <span class="hljs-number">0.01</span>optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss)</code></pre><h3 id="指定迭代次数，并在session里执行graph"><a href="#指定迭代次数，并在session里执行graph" class="headerlink" title="指定迭代次数，并在session里执行graph"></a>指定迭代次数，并在session里执行graph</h3><pre><code class="hljs python">n_samples = xs.shape[<span class="hljs-number">0</span>]init = tf.global_variables_initializer()<span class="hljs-keyword">with</span> tf.Session() <span class="hljs-keyword">as</span> sess:    <span class="hljs-comment"># 记得初始化所有变量</span>    sess.run(init)    writer = tf.summary.FileWriter(<span class="hljs-string">'./graphs/linear_regression'</span>, sess.graph)    <span class="hljs-comment"># 训练模型</span>    <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> range(<span class="hljs-number">50</span>):        total_loss = <span class="hljs-number">0</span>        <span class="hljs-keyword">for</span> x, y <span class="hljs-keyword">in</span> zip(xs, ys):            <span class="hljs-comment"># 通过feed_dic把数据灌进去</span>            _, loss_value = sess.run([optimizer, loss], feed_dict=&#123;X: x, Y: y&#125;)            total_loss += loss_value        <span class="hljs-keyword">if</span> i % <span class="hljs-number">5</span> == <span class="hljs-number">0</span>:            print(<span class="hljs-string">'Epoch &#123;0&#125;: &#123;1&#125;'</span>.format(i, total_loss / n_samples))    <span class="hljs-comment"># 关闭writer</span>    writer.close()    <span class="hljs-comment"># 取出w和b的值</span>    W, b = sess.run([W, b])</code></pre><pre><code class="hljs angelscript">Epoch <span class="hljs-number">0</span>: [<span class="hljs-number">0.7908481</span>]Epoch <span class="hljs-number">5</span>: [<span class="hljs-number">0.07929672</span>]Epoch <span class="hljs-number">10</span>: [<span class="hljs-number">0.07929661</span>]Epoch <span class="hljs-number">15</span>: [<span class="hljs-number">0.07929661</span>]Epoch <span class="hljs-number">20</span>: [<span class="hljs-number">0.07929661</span>]Epoch <span class="hljs-number">25</span>: [<span class="hljs-number">0.07929661</span>]Epoch <span class="hljs-number">30</span>: [<span class="hljs-number">0.07929661</span>]Epoch <span class="hljs-number">35</span>: [<span class="hljs-number">0.07929661</span>]Epoch <span class="hljs-number">40</span>: [<span class="hljs-number">0.07929661</span>]Epoch <span class="hljs-number">45</span>: [<span class="hljs-number">0.07929661</span>]</code></pre><h3 id="打印最后更新的w、b的值"><a href="#打印最后更新的w、b的值" class="headerlink" title="打印最后更新的w、b的值"></a>打印最后更新的w、b的值</h3><pre><code class="hljs python">print(W, b)print(<span class="hljs-string">"W:"</span>+str(W[<span class="hljs-number">0</span>]))print(<span class="hljs-string">"b:"</span>+str(b[<span class="hljs-number">0</span>]))</code></pre><pre><code class="hljs angelscript">[<span class="hljs-number">0.82705235</span>] [<span class="hljs-number">0.16835527</span>]W:<span class="hljs-number">0.82705235</span>b:<span class="hljs-number">0.16835527</span></code></pre><h3 id="画出线性回归线"><a href="#画出线性回归线" class="headerlink" title="画出线性回归线"></a>画出线性回归线</h3><pre><code class="hljs python">plt.plot(xs, ys, <span class="hljs-string">'bo'</span>, label=<span class="hljs-string">'Real data'</span>)plt.plot(xs, xs * W + b, <span class="hljs-string">'r'</span>, label=<span class="hljs-string">'Predicted data'</span>)plt.legend()plt.show()</code></pre><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20200601182051.png" srcset="/img/loading.gif" alt="44.png"></p>]]></content>
    
    
    <categories>
      
      <category>TensorFlow</category>
      
    </categories>
    
    
    <tags>
      
      <tag>深度学习</tag>
      
      <tag>TensorFlow</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>环境搭建-使用 Virtualenv 进行安装TensorFlow</title>
    <link href="/2018/08/TensorFlow-Virtualenv/"/>
    <url>/2018/08/TensorFlow-Virtualenv/</url>
    
    <content type="html"><![CDATA[<h3 id="使用-Virtualenv-进行安装TensorFlow"><a href="#使用-Virtualenv-进行安装TensorFlow" class="headerlink" title="使用 Virtualenv 进行安装TensorFlow"></a>使用 Virtualenv 进行安装TensorFlow</h3><p>环境：<code>ubuntu 16.04</code></p><ul><li>安装 pip 和 Virtualenv </li></ul><pre><code class="hljs shell"><span class="hljs-meta">$</span><span class="bash">  sudo apt-get install python-pip python-dev python-virtualenv <span class="hljs-comment"># for Python 2.7</span></span><span class="hljs-meta">$</span><span class="bash">  sudo apt-get install python3-pip python3-dev python-virtualenv <span class="hljs-comment"># for Python 3.n</span></span></code></pre><ul><li>创建 Virtualenv 环境</li></ul><pre><code class="hljs shell"><span class="hljs-meta">$</span><span class="bash">  virtualenv --system-site-packages targetDirectory <span class="hljs-comment"># for Python 2.7</span></span><span class="hljs-meta">$</span><span class="bash">  virtualenv --system-site-packages -p python3 targetDirectory <span class="hljs-comment"># for Python 3.n</span></span></code></pre><p><code>targetDirectory</code> 用于指定 Virtualenv 目录。我们假定创建的虚拟环境为<code>tensorflow</code>，即<code>targetDirectory</code>为<code>tensorflow</code>。 </p><ul><li>激活 Virtualenv 环境 </li></ul><pre><code class="hljs elixir"><span class="hljs-variable">$ </span> source ~<span class="hljs-regexp">/tensorflow/bin</span><span class="hljs-regexp">/activate</span></code></pre><ul><li>执行上述 <code>source</code> 命令后，您的提示符应该会变成如下内容： </li></ul><pre><code class="hljs gams">(tensorflow)<span class="hljs-symbol">$</span></code></pre><ul><li>安装或更新<code>pip</code></li></ul><pre><code class="hljs elixir">(tensorflow)<span class="hljs-variable">$ </span>easy_install -U pip</code></pre><ul><li>安装 <code>TensorFlow</code></li></ul><pre><code class="hljs cmake">(tensorflow)$ pip <span class="hljs-keyword">install</span> --upgrade tensorflow      <span class="hljs-comment"># for Python 2.7</span>(tensorflow)$ pip3 <span class="hljs-keyword">install</span> --upgrade tensorflow     <span class="hljs-comment"># for Python 3.n</span>(tensorflow)$ pip <span class="hljs-keyword">install</span> --upgrade tensorflow-gpu  <span class="hljs-comment"># for Python 2.7 and GPU</span>(tensorflow)$ pip3 <span class="hljs-keyword">install</span> --upgrade tensorflow-gpu <span class="hljs-comment"># for Python 3.n and GPU</span></code></pre><ul><li>备注：如果安装失败或者网速慢，可参考<a href="https://sevenold.github.io/2018/08/TensorFlow-ubuntu/" target="_blank" rel="noopener">TensorFlow-CPU/GPU安装ubuntu16.04版</a> ,离线安装<code>tensorflow</code>。</li><li>退出 Virtualenv 环境 </li></ul><pre><code class="hljs elixir">(tensorflow)<span class="hljs-variable">$ </span>deactivate</code></pre><h3 id="验证环境"><a href="#验证环境" class="headerlink" title="验证环境"></a>验证环境</h3><ul><li>激活 Virtualenv 环境 </li></ul><pre><code class="hljs elixir"><span class="hljs-variable">$ </span>source ~<span class="hljs-regexp">/tensorflow/bin</span><span class="hljs-regexp">/activate</span></code></pre><p>  在 Virtualenv 环境激活后，您就可以从这个 shell 运行 TensorFlow 程序。您的提示符将变成如下所示，这表示您的 Tensorflow 环境已处于活动状态： </p><pre><code class="hljs gams">(tensorflow)<span class="hljs-symbol">$</span></code></pre><p>  输入代码验证</p><pre><code class="hljs vim">import tensorflow <span class="hljs-keyword">as</span> <span class="hljs-keyword">tf</span>hello = <span class="hljs-keyword">tf</span>.constant(<span class="hljs-string">'Hello, TensorFlow!'</span>)sess = <span class="hljs-keyword">tf</span>.Session()<span class="hljs-keyword">print</span>(sess.run(hello))</code></pre><ul><li>看到如下输出，表示安装正确</li></ul><pre><code class="hljs autohotkey"><span class="hljs-built_in">Hello,</span> TensorFlow!</code></pre><p>  用完 TensorFlow 后，可以通过发出以下命令调用 <code>deactivate</code> 函数来停用环境： </p><pre><code class="hljs elixir">(tensorflow)<span class="hljs-variable">$ </span>deactivate</code></pre><ul><li>提示符将恢复为您的默认提示符 ，就表示退出了。</li></ul><pre><code class="hljs gams"><span class="hljs-symbol">$</span></code></pre><h3 id="卸载-Virtualenv-环境-targetDirectory"><a href="#卸载-Virtualenv-环境-targetDirectory" class="headerlink" title="卸载 Virtualenv 环境 targetDirectory"></a>卸载 Virtualenv 环境 <code>targetDirectory</code></h3><p>要卸载你刚刚新建的Virtualenv 环境 <code>TensorFlow</code>的话，只需要删除该文件夹就可以了。</p><pre><code class="hljs elixir"><span class="hljs-variable">$ </span>rm -r targetDirectory  <span class="hljs-comment"># 在这里 targetDirectory改为你创建的环境名字</span></code></pre>]]></content>
    
    
    <categories>
      
      <category>TensorFlow</category>
      
    </categories>
    
    
    <tags>
      
      <tag>深度学习</tag>
      
      <tag>TensorFlow</tag>
      
      <tag>环境搭建</tag>
      
      <tag>Virtualenv</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>环境搭建-TensorFlow-CPU/GPU安装ubuntu16.04版</title>
    <link href="/2018/08/TensorFlow-ubuntu/"/>
    <url>/2018/08/TensorFlow-ubuntu/</url>
    
    <content type="html"><![CDATA[<p><code>系统：Ubuntu 16.04。显卡：GTX 1050，独显无集成显卡。</code></p><h3 id="Ubuntu-TensorFlow-CPU安装"><a href="#Ubuntu-TensorFlow-CPU安装" class="headerlink" title="Ubuntu TensorFlow-CPU安装"></a>Ubuntu TensorFlow-CPU安装</h3><p>最简单的方法使用 pip 来安装 </p><pre><code class="hljs shell"><span class="hljs-meta">#</span><span class="bash"> Python 2.7</span>pip install --upgrade tensorflow<span class="hljs-meta">#</span><span class="bash"> Python 3.x</span>pip3 install --upgrade tensorflow</code></pre><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20200601181857.png" srcset="/img/loading.gif" alt="image"></p><p>安装出错，或者下载速度慢，可以采用离线安装的方式</p><p>离线安装包下载地址：<a href="https://pypi.org/project/tensorflow/" target="_blank" rel="noopener">https://pypi.org/project/tensorflow/</a></p><p>然后进入安装包路径：</p><pre><code class="hljs shell"><span class="hljs-meta">#</span><span class="bash"> Python 2.7</span>pip install tensorflow-1.10.0-cp27-cp27mu-manylinux1_x86_64.whl <span class="hljs-meta">#</span><span class="bash"> Python 3.x</span>pip3 install tensorflow-1.10.0-cp35-cp35m-manylinux1_x86_64.whl</code></pre><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20200601181858.png" srcset="/img/loading.gif" alt="image"></p><p>然后等待，安装成功。</p><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20200601181859.png" srcset="/img/loading.gif" alt="image"></p><h3 id="验证安装"><a href="#验证安装" class="headerlink" title="验证安装"></a>验证安装</h3><pre><code class="hljs python"><span class="hljs-keyword">import</span> tensorflow <span class="hljs-keyword">as</span> tfhello = tf.constant(<span class="hljs-string">'Hello, TensorFlow!'</span>)sess = tf.Session()print(sess.run(hello))</code></pre><p>看到如下的输出，表示安装正确。 </p><pre><code class="hljs python">Hello, TensorFlow!</code></pre><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20200601181900.png" srcset="/img/loading.gif" alt="image"></p><h3 id="Ubuntu-TensorFlow-GPU安装"><a href="#Ubuntu-TensorFlow-GPU安装" class="headerlink" title="Ubuntu TensorFlow-GPU安装"></a>Ubuntu TensorFlow-GPU安装</h3><p>如果你的电脑的GPU支持CUDA，那么你就可以使用GPU加速了 </p><h3 id="检查自己的GPU是否是CUDA-capable"><a href="#检查自己的GPU是否是CUDA-capable" class="headerlink" title="检查自己的GPU是否是CUDA-capable"></a>检查自己的GPU是否是CUDA-capable</h3><pre><code class="hljs shell">lspci | grep -i nvidia</code></pre><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20200601181901.png" srcset="/img/loading.gif" alt="image"></p><p>查看你的电脑GPU是否支持CUDA:<a href="https://developer.nvidia.com/cuda-gpus" target="_blank" rel="noopener">https://developer.nvidia.com/cuda-gpus</a> </p><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20200601181902.png" srcset="/img/loading.gif" alt="image"></p><h3 id="安装NVIDIA驱动"><a href="#安装NVIDIA驱动" class="headerlink" title="安装NVIDIA驱动"></a>安装NVIDIA驱动</h3><ul><li><p><code>System Setting</code>–&gt;<code>software&amp;Updates</code>–&gt;<code>Additional Drives</code>，然后选择<code>NVIDIA</code>驱动</p><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20200601181903.png" srcset="/img/loading.gif" alt="image"></p></li></ul><ul><li>安装成功–重启电脑</li></ul><h4 id="查看是否安装成功"><a href="#查看是否安装成功" class="headerlink" title="查看是否安装成功"></a>查看是否安装成功</h4><pre><code class="hljs shell">nvidia-smi</code></pre><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20200601181904.png" srcset="/img/loading.gif" alt="image"></p><p>出现这个页面就表示安装成功了。</p><h3 id="安装CUDA"><a href="#安装CUDA" class="headerlink" title="安装CUDA"></a>安装CUDA</h3><p>查询电脑的版本号：</p><pre><code class="hljs shell">nvidia-smi</code></pre><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20200601181905.png" srcset="/img/loading.gif" alt="image"></p><p>对应版本号去下载对应的CUDA安装包 </p><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20200601181906.png" srcset="/img/loading.gif" alt="image"></p><p>由上图可以看出，我这台演示电脑的</p><ul><li>版本驱动号：<code>384.130</code></li><li>对应版本：<code>CUDA：8.0</code></li></ul><p>所以我们对应的CUDA的下载版本就是8.0，注意我下载的是<code>runfile</code>，下载网站：<a href="https://developer.nvidia.com/cuda-toolkit-archive" target="_blank" rel="noopener">https://developer.nvidia.com/cuda-toolkit-archive</a> </p><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20200601181907.png" srcset="/img/loading.gif" alt="image"></p><h3 id="安装说明"><a href="#安装说明" class="headerlink" title="安装说明"></a>安装说明</h3><p>进入安装包的路径执行以下命令（注意版本号）</p><pre><code class="hljs bash">sudo sh cuda_8.0.61_375.26_linux.run</code></pre><p>按照命令行提示 在安装过程中会询问是否安装显卡驱动，由于我们在第一步中已经安装，所以我们选择否（不安装） </p><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20200601181908.png" srcset="/img/loading.gif" alt="image"></p><p>然后等待，安装完成。</p><p>安装完成后可能会有警告，提示samplees缺少必要的包： </p><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20200601181909.png" srcset="/img/loading.gif" alt="image"></p><p>原因是缺少相关的依赖库,安装相应库就解决了： </p><pre><code class="hljs bash">sudo apt-get install freeglut3-dev build-essential libx11-dev libxmu-dev libxi-dev libgl1-mesa-glx libglu1-mesa libglu1-mesa-dev</code></pre><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20200601181910.png" srcset="/img/loading.gif" alt="image"></p><h3 id="配置环境变量"><a href="#配置环境变量" class="headerlink" title="配置环境变量"></a>配置环境变量</h3><p>打开shell运行： <code>gedit ~/.bashrc</code></p><p>加入如下内容： </p><pre><code class="hljs bash"><span class="hljs-built_in">export</span> PATH=/usr/<span class="hljs-built_in">local</span>/cuda-8.0/bin:<span class="hljs-variable">$PATH</span><span class="hljs-built_in">export</span> LD_LIBRARY_PATH=/usr/<span class="hljs-built_in">local</span>/cuda-8.0/lib64:<span class="hljs-variable">$LD_LIBRARY_PATH</span></code></pre><p>立即生效，运行<code>source ~/.bashrc</code> </p><h3 id="测试是否安装成功"><a href="#测试是否安装成功" class="headerlink" title="测试是否安装成功"></a>测试是否安装成功</h3><ul><li>查看CUDA版本:<code>nvcc -V</code></li></ul><pre><code class="hljs bash">seven@seven:~$ nvcc -Vnvcc: NVIDIA (R) Cuda compiler driverCopyright (c) 2005-2016 NVIDIA CorporationBuilt on Tue_Jan_10_13:22:03_CST_2017Cuda compilation tools, release 8.0, V8.0.61</code></pre><ol><li>编译 CUDA Samples<br>进入samples的安装目录<br>我们选择其中一个进行编译验证下如：</li></ol><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20200601181911.png" srcset="/img/loading.gif" alt="image"></p><p>如果没有报错，则安装完成 </p><h3 id="安装cuDNN"><a href="#安装cuDNN" class="headerlink" title="安装cuDNN"></a>安装cuDNN</h3><p>同样，根据我们的驱动程序版本号：我们下载的对应版本： </p><ul><li>版本驱动号：<code>384.130</code></li><li>对应版本：<code>CUDA：8.0</code></li><li>对应版本：<code>cuDNN: v6.0</code></li></ul><p>下载网址：<a href="https://developer.nvidia.com/rdp/cudnn-archive" target="_blank" rel="noopener">https://developer.nvidia.com/rdp/cudnn-archive</a> </p><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20200601181912.png" srcset="/img/loading.gif" alt="image"></p><h3 id="安装说明-1"><a href="#安装说明-1" class="headerlink" title="安装说明"></a>安装说明</h3><ul><li>解压下载好的安装包以后会出现cuda的目录，进入该目录 执行以下命令</li></ul><pre><code class="hljs bash"><span class="hljs-built_in">cd</span> ~tar -zxf cudnn-8.0-linux-x64-v5.1.tgz<span class="hljs-built_in">cd</span> cudasudo cp lib64/* /usr/<span class="hljs-built_in">local</span>/cuda/lib64/sudo cp include/* /usr/<span class="hljs-built_in">local</span>/cuda/include/</code></pre><h3 id="安装完成"><a href="#安装完成" class="headerlink" title="安装完成"></a>安装完成</h3><p>至此，CUDA与cuDNN已经安装完成</p><h3 id="安装-TensorFlow-GPU"><a href="#安装-TensorFlow-GPU" class="headerlink" title="安装 TensorFlow-GPU"></a>安装 TensorFlow-GPU</h3><p>备注：我用的是cuda 8.0和cudnn6.0 所以TensorFlow的版本应该是1.4。</p><p>最简单的方式是使用pip安装：</p><pre><code class="hljs shell"><span class="hljs-meta">#</span><span class="bash"> Python 2.7</span>pip install --upgrade tensorflow-gpu==1.4<span class="hljs-meta">#</span><span class="bash"> Python 3.x</span>pip3 install --upgrade tensorflow-gpu==1.4</code></pre><p>如果你网速很慢的话，你可以选择离线安装。</p><p>下载所需的离线包：<a href="https://github.com/tensorflow/tensorflow/tags" target="_blank" rel="noopener">https://github.com/tensorflow/tensorflow/tags</a></p><p>打开终端，进入你保存文件的目录，使用命令</p><pre><code class="hljs shell">pip3 install tensorflow_gpu-1.4.0-cp35-cp35m-manylinux1_x86_64.whl</code></pre><p>然后等待，直至安装成功。 </p><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20200601181913.png" srcset="/img/loading.gif" alt="image"></p><h3 id="验证安装-1"><a href="#验证安装-1" class="headerlink" title="验证安装"></a>验证安装</h3><pre><code class="hljs python"><span class="hljs-keyword">import</span> tensorflow <span class="hljs-keyword">as</span> tfhello = tf.constant(<span class="hljs-string">'Hello, TensorFlow!'</span>)sess = tf.Session()print(sess.run(hello))</code></pre><p>看到如下的输出，表示安装正确。 </p><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20200601181914.png" srcset="/img/loading.gif" alt="image"></p><pre><code class="hljs shell">Hello, TensorFlow!</code></pre>]]></content>
    
    
    <categories>
      
      <category>TensorFlow</category>
      
    </categories>
    
    
    <tags>
      
      <tag>深度学习</tag>
      
      <tag>TensorFlow</tag>
      
      <tag>环境搭建</tag>
      
      <tag>Ubuntu</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>环境搭建-TensorFlow-CPU/GPU安装windows10版</title>
    <link href="/2018/08/TensorFlow-windows/"/>
    <url>/2018/08/TensorFlow-windows/</url>
    
    <content type="html"><![CDATA[<h3 id="Windows-TensorFlow-CPU安装"><a href="#Windows-TensorFlow-CPU安装" class="headerlink" title="Windows TensorFlow-CPU安装"></a>Windows TensorFlow-CPU安装</h3><p>备注：CPU和GPU是不能同时安装的，如果需要的话可以再配置一个环境或者建个虚拟环境。</p><p>环境：window7或以上</p><p>python版本要求：<code>3.5.x</code>以上</p><p>打开<strong>window cmd</strong>，直接使用CPU-only命令安装，如下： </p><pre><code class="hljs bash">pip3 install --upgrade tensorflow</code></pre><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20200601181933.png" srcset="/img/loading.gif" alt="image"></p><p>然后等待，直至安装成功。</p><blockquote><blockquote><blockquote></blockquote></blockquote></blockquote><p>如果你网速很慢的话，你可以选择离线安装。</p><p>下载所需的离线包：<a href="https://pypi.org/project/tensorflow/#files" target="_blank" rel="noopener">https://pypi.org/project/tensorflow/#files</a></p><p>打开<strong>window cmd</strong>，进入你保存文件的目录，使用命令</p><pre><code class="hljs bash">pip install tensorflow-1.10.0-cp36-cp36m-win_amd64.whl</code></pre><p>然后等待，直至安装成功。</p><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20200601181934.png" srcset="/img/loading.gif" alt="image"></p><h3 id="验证安装"><a href="#验证安装" class="headerlink" title="验证安装"></a>验证安装</h3><pre><code class="hljs python"><span class="hljs-keyword">import</span> tensorflow <span class="hljs-keyword">as</span> tfhello = tf.constant(<span class="hljs-string">'Hello, TensorFlow!'</span>)sess = tf.Session()print(sess.run(hello))</code></pre><p>看到如下的输出，表示安装正确。 </p><pre><code class="hljs bash">Hello, TensorFlow!</code></pre><h3 id="Windows-TensorFlow-GPU安装"><a href="#Windows-TensorFlow-GPU安装" class="headerlink" title="Windows TensorFlow-GPU安装"></a>Windows TensorFlow-GPU安装</h3><p>如果你的电脑的GPU支持CUDA，那么你就可以使用GPU加速了</p><p>查看你的电脑GPU是否支持CUDA:<a href="https://developer.nvidia.com/cuda-gpus" target="_blank" rel="noopener">https://developer.nvidia.com/cuda-gpus</a></p><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20200601181935.png" srcset="/img/loading.gif" alt="image"></p><h3 id="安装CUDA"><a href="#安装CUDA" class="headerlink" title="安装CUDA"></a>安装CUDA</h3><p>当然你的先安装显卡的驱动，这个就自己查询了。我们默认你已经装好了。</p><p>查看驱动程序版本号</p><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20200601181936.png" srcset="/img/loading.gif" alt="image"></p><p>对应版本号去下载对应的CUDA安装包</p><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20200601181937.png" srcset="/img/loading.gif" alt="image"></p><p>由上图可以看出，我这台演示电脑的</p><ul><li>版本驱动号：391.24</li><li>对应版本：CUDA：9.0</li></ul><p>所以我们对应的CUDA的下载版本就是9.0，下载网站：<a href="https://developer.nvidia.com/cuda-toolkit-archive" target="_blank" rel="noopener">https://developer.nvidia.com/cuda-toolkit-archive</a></p><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20200601181938.png" srcset="/img/loading.gif" alt="image"></p><h3 id="安装说明"><a href="#安装说明" class="headerlink" title="安装说明"></a>安装说明</h3><ul><li>双击cuda_9.0.176_windows.exe</li><li>按照屏幕上的提示操作</li><li>如果出现下面这个界面–说明你还需要安装<code>[ Visual Studio 2012或以上版本]</code><br><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20200601181939.png" srcset="/img/loading.gif" alt="image"></li></ul><h3 id="安装cuDNN"><a href="#安装cuDNN" class="headerlink" title="安装cuDNN"></a>安装cuDNN</h3><p>同样，根据我们的驱动程序版本号：我们下载的对应版本：</p><ul><li>版本驱动号：391.24</li><li>CUDA：9.0</li><li>cuDNN：v7.1.4</li></ul><p>下载网址：<a href="https://developer.nvidia.com/rdp/cudnn-archive" target="_blank" rel="noopener">https://developer.nvidia.com/rdp/cudnn-archive</a></p><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20200601181940.png" srcset="/img/loading.gif" alt="image"></p><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20200601181941.png" srcset="/img/loading.gif" alt="image"></p><h3 id="安装说明-1"><a href="#安装说明-1" class="headerlink" title="安装说明"></a>安装说明</h3><p>说明：把cuDNN的文件复制到CUDA Toolkit 安装目录</p><p>解压cudnn5.0</p><ul><li>生成cuda/include、cuda/lib、cuda/bin三个目录； </li><li>分别将<code>cuda/include、cuda/lib、cuda/bin</code>三个目录中的内容拷贝到<code>C:\Program Files\NVIDIAGPU Computing Toolkit\CUDA\v8.0</code>对应的<code>include、lib、bin</code>目录下即可 </li><li>如果你是自定义的安装路径，需要自己搜索一下NVIDIA GPU Computing Toolkit的目录 </li></ul><h3 id="安装-TensorFlow-GPU"><a href="#安装-TensorFlow-GPU" class="headerlink" title="安装 TensorFlow-GPU"></a>安装 TensorFlow-GPU</h3><p>打开<strong>window cmd</strong>，直接使用命令安装，如下： </p><pre><code class="hljs bash">pip install --upgrade tensorflow-gpu</code></pre><p>然后等待，直至安装成功。</p><blockquote><blockquote><blockquote></blockquote></blockquote></blockquote><p>如果你网速很慢的话，你可以选择离线安装。</p><p>下载所需的离线包：<a href="https://pypi.org/project/tensorflow-gpu/#files" target="_blank" rel="noopener">https://pypi.org/project/tensorflow-gpu/#files</a></p><p>打开<strong>window cmd</strong>，进入你保存文件的目录，使用命令</p><pre><code class="hljs css"><span class="hljs-selector-tag">pip3</span> <span class="hljs-selector-tag">install</span> <span class="hljs-selector-tag">tensorflow_gpu-1</span><span class="hljs-selector-class">.10</span><span class="hljs-selector-class">.0-cp36-cp36m-win_amd64</span><span class="hljs-selector-class">.whl</span></code></pre><p>然后等待，直至安装成功。</p><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20200601181942.png" srcset="/img/loading.gif" alt="image"></p><h3 id="验证安装-1"><a href="#验证安装-1" class="headerlink" title="验证安装"></a>验证安装</h3><pre><code class="hljs python"><span class="hljs-keyword">import</span> tensorflow <span class="hljs-keyword">as</span> tfhello = tf.constant(<span class="hljs-string">'Hello, TensorFlow!'</span>)sess = tf.Session()print(sess.run(hello))</code></pre><p>看到如下的输出，表示安装正确。 </p><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20200601181943.png" srcset="/img/loading.gif" alt="image"></p>]]></content>
    
    
    <categories>
      
      <category>TensorFlow</category>
      
    </categories>
    
    
    <tags>
      
      <tag>深度学习</tag>
      
      <tag>TensorFlow</tag>
      
      <tag>环境搭建</tag>
      
      <tag>windows</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>深度学习-什么是深度学习？</title>
    <link href="/2018/08/DL-introduction/"/>
    <url>/2018/08/DL-introduction/</url>
    
    <content type="html"><![CDATA[<h3 id="人工智能、机器学习、神经网络、深度学习关系"><a href="#人工智能、机器学习、神经网络、深度学习关系" class="headerlink" title="人工智能、机器学习、神经网络、深度学习关系"></a><strong>人工智能、机器学习、神经网络、深度学习关系</strong></h3><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20200601181445.png" srcset="/img/loading.gif" alt="image"></p><h3 id="理解深度学习和传统算法区别"><a href="#理解深度学习和传统算法区别" class="headerlink" title="理解深度学习和传统算法区别"></a><strong>理解深度学习和传统算法区别</strong></h3><p><code>机器学习与人类思维</code></p><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20200601181446.png" srcset="/img/loading.gif" alt="image"></p><h3 id="深度学习与应用"><a href="#深度学习与应用" class="headerlink" title="深度学习与应用"></a>深度学习与应用</h3><p><code>图像识别</code><br>图像识别，是指利用计算机对图像进行处理、分析和理解，以识别各种不同模式的目标和对像的技术。</p><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20200601181447.png" srcset="/img/loading.gif" alt="image"></p><p><code>目标识别</code><br>目标识别是指一个特殊目标（或一种类型的目标）从其它目标（或其它类型的目标）中被区分出来的过程。它既包括两个非常相似目标的识别，也包括一种类型的目标同其他类型目标的识别。</p><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20200601181448.png" srcset="/img/loading.gif" alt="image"></p><p><code>人脸识别</code><br>人脸识别，是基于人的脸部特征信息进行身份识别的一种生物识别技术。用摄像机或摄像头采集含有人脸的图像或视频流，并自动在图像中检测和跟踪人脸，进而对检测到的人脸进行脸部识别的一系列相关技术，通常也叫做人像识别、面部识别。</p><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20200601181449.png" srcset="/img/loading.gif" alt="image"></p><p><code>图片描述</code><br>根据识别出的内容，组织成一段内容，用于描述图片信息。</p><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20200601181450.png" srcset="/img/loading.gif" alt="image"></p><p><code>图像风格变换</code><br>把一张图片，变换成不同的风格的图片。</p><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20200601181451.png" srcset="/img/loading.gif" alt="image"></p><p><code>语音识别</code><br>把语音进行处理，识别出语音内容，感情等等。</p><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20200601181452.png" srcset="/img/loading.gif" alt="image"></p><p><code>文本分类</code><br>文本分类用电脑对文本集(或其他实体或物件)按照一定的分类体系或标准进行自动分类标记</p><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20200601181453.png" srcset="/img/loading.gif" alt="image"></p><p><code>图像生成</code><br>根据用户所描述的特征来生成对应的图像。</p><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20200601181454.png" srcset="/img/loading.gif" alt="image"></p><h3 id="神经网络的分类"><a href="#神经网络的分类" class="headerlink" title="神经网络的分类"></a>神经网络的分类</h3><h4 id="按数据流向"><a href="#按数据流向" class="headerlink" title="按数据流向"></a><strong>按数据流向</strong></h4><ul><li>前馈神经网络</li><li>递归神经网络</li><li>反馈神经网络</li></ul><h4 id="按网络中神经元组织形式"><a href="#按网络中神经元组织形式" class="headerlink" title="按网络中神经元组织形式"></a><strong>按网络中神经元组织形式</strong></h4><ul><li>全连接神经网络</li><li>部分连接神经网络</li></ul><h4 id="按网络中神经元的行为和连接方式"><a href="#按网络中神经元的行为和连接方式" class="headerlink" title="按网络中神经元的行为和连接方式"></a><strong>按网络中神经元的行为和连接方式</strong></h4><ul><li>全连接神经网络</li><li>卷积神经网络</li><li>循环神经网络</li></ul><h4 id="按训练方法"><a href="#按训练方法" class="headerlink" title="按训练方法"></a><strong>按训练方法</strong></h4><ul><li>监督学习</li><li>无监督学习</li><li>强化学习</li></ul><h3 id="应用领域"><a href="#应用领域" class="headerlink" title="应用领域"></a>应用领域</h3><ul><li>全连接神经网络<ul><li>用作数据分析</li><li>可作为其他网络的组成部分</li></ul></li><li>卷积神经网络<ul><li>计算机视觉、图像处理</li><li>具有局部相关性的数据</li></ul></li><li>循环神经网络<ul><li>自然语言处理</li><li>语音识别</li><li>具有顺序及前后相关性的数据</li></ul></li></ul><h3 id="演示"><a href="#演示" class="headerlink" title="演示"></a>演示</h3><p>图像生成<br><a href="https://make.girls.moe/#/" target="_blank" rel="noopener">用AI生成二次元角色</a></p><p>神经网络演示<br><a href="http://scs.ryerson.ca/~aharley/vis/conv/" target="_blank" rel="noopener">卷积神经网络的三维可视化</a></p>]]></content>
    
    
    <categories>
      
      <category>深度学习</category>
      
    </categories>
    
    
    <tags>
      
      <tag>深度学习</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>机器学习-贝叶斯网络</title>
    <link href="/2018/08/ml-NB-network/"/>
    <url>/2018/08/ml-NB-network/</url>
    
    <content type="html"><![CDATA[<h3 id="贝叶斯网络的概念"><a href="#贝叶斯网络的概念" class="headerlink" title="贝叶斯网络的概念"></a>贝叶斯网络的概念</h3><p>把某个研究系统中涉及的<strong>随机变量</strong>，根据是否条件独立绘制在一个<strong>有向图</strong>中，就形成了<strong><code>贝叶斯网络</code></strong>。</p><p>贝叶斯网络(Bayesian network)，又称信念网络(Belief Network)，或<strong>有向无环图模型</strong>。是一种概率图模型，根据概率图的拓扑结构，考察一组随机变量${ X_1,X_2…X_n}$及其n组<strong>条件概率分布</strong>的性质。也就是说它用网络结构代表领域的基本因果知识。  </p><h3 id="贝叶斯网络的形式化定义"><a href="#贝叶斯网络的形式化定义" class="headerlink" title="贝叶斯网络的形式化定义"></a>贝叶斯网络的形式化定义</h3><ul><li><p>$BN(G,Θ)$: 贝叶斯网络(Bayesian Network)</p><ul><li><p>$G$:有向无环图 (Directed Acyclic Graphical model, DAG)</p></li><li><p>$G$的结点:随机变量$X_1,X_2…X_n$</p></li><li><p>$G$的边:结点间的有向依赖</p></li><li><p>$Θ$:所有条件概率分布的参数集合</p></li><li><p>结点X的条件概率: $P(X |parent(X))$</p><p>$P(S,C,B,X,D)=P(S)P(C \mid S)P(B \mid S)P(X \mid C,S)P(D \mid C,B)$</p></li></ul></li></ul><p>每个结点所需参数的个数:</p><p>若结点的$parent$数目是$M$,结点和$parent$的可取值数目都是$K:K^M∗(K−1)$</p><h3 id="一个简单的贝叶斯网络"><a href="#一个简单的贝叶斯网络" class="headerlink" title="一个简单的贝叶斯网络"></a>一个简单的贝叶斯网络</h3><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20200530230251.png" srcset="/img/loading.gif" alt="image"></p><h4 id="P-a-b-c-P-c-mid-a-b-P-a-b-P-c-mid-a-b-P-b-mid-a-P-a"><a href="#P-a-b-c-P-c-mid-a-b-P-a-b-P-c-mid-a-b-P-b-mid-a-P-a" class="headerlink" title="$P(a,b,c)=P(c \mid a,b)P(a,b)=P(c \mid a,b)P(b \mid a)P(a)$"></a>$P(a,b,c)=P(c \mid a,b)P(a,b)=P(c \mid a,b)P(b \mid a)P(a)$</h4><h3 id="全连接贝叶斯网络"><a href="#全连接贝叶斯网络" class="headerlink" title="全连接贝叶斯网络"></a>全连接贝叶斯网络</h3><p><strong>每一对结点之间都有边连接</strong></p><h4 id="p-x-1-…x-n-p-x-K-mid-x-1-…-x-K-1-…p-x-2-mid-x-1-p-x-1"><a href="#p-x-1-…x-n-p-x-K-mid-x-1-…-x-K-1-…p-x-2-mid-x-1-p-x-1" class="headerlink" title="$p(x_1,…x_n)=p(x_K \mid x_1,…,x_{K-1})…p(x_2 \mid x_1)p(x_1)$"></a>$p(x_1,…x_n)=p(x_K \mid x_1,…,x_{K-1})…p(x_2 \mid x_1)p(x_1)$</h4><h4 id="P-X-1-x-1-…-X-n-x-n-prod-i-1-nP-X-i-x-i-mid-X-i-1-…-X-n-x-n"><a href="#P-X-1-x-1-…-X-n-x-n-prod-i-1-nP-X-i-x-i-mid-X-i-1-…-X-n-x-n" class="headerlink" title="$P(X_1=x_1,…,X_n=x_n)=\prod_{i=1}^nP(X_i=x_i \mid X_{i+1},…,X_n=x_n)$"></a>$P(X_1=x_1,…,X_n=x_n)=\prod_{i=1}^nP(X_i=x_i \mid X_{i+1},…,X_n=x_n)$</h4><h3 id="一个”正常“的贝叶斯网络"><a href="#一个”正常“的贝叶斯网络" class="headerlink" title="一个”正常“的贝叶斯网络"></a>一个”正常“的贝叶斯网络</h3><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20200530230252.png" srcset="/img/loading.gif" alt="image"></p><p>从图中我们可以看出：</p><ul><li><p>有些边是缺失的</p></li><li><p>直观上来看：$x_1,x_2$是相互独立的</p></li><li><p>直观上来看：$x_6,x_7$在$x_4$给定的条件下独立</p></li><li><p>$x_1,x_2,…x_7$的联合分布：</p><h4 id="P-x-1-P-x-2-P-x-3-P-x-4-mid-x-1-x-2-x-3-P-x-5-mid-x-1-x-3-P-x-6-mid-x-4-P-x-7-mid-x-4-x-5"><a href="#P-x-1-P-x-2-P-x-3-P-x-4-mid-x-1-x-2-x-3-P-x-5-mid-x-1-x-3-P-x-6-mid-x-4-P-x-7-mid-x-4-x-5" class="headerlink" title="$P(x_1)P(x_2)P(x_3)P(x_4\mid x_1, x_2, x_3)P(x_5\mid x_1, x_3)P(x_6\mid x_4)P(x_7\mid x_4, x_5)$"></a>$P(x_1)P(x_2)P(x_3)P(x_4\mid x_1, x_2, x_3)P(x_5\mid x_1, x_3)P(x_6\mid x_4)P(x_7\mid x_4, x_5)$</h4></li></ul><h3 id="贝叶斯网络的条件独立判定"><a href="#贝叶斯网络的条件独立判定" class="headerlink" title="贝叶斯网络的条件独立判定"></a>贝叶斯网络的条件独立判定</h3><p>我们来看一下贝叶斯网络的条件是如何判定的：</p><ol><li><h4 id="条件独立：tail-to-tail"><a href="#条件独立：tail-to-tail" class="headerlink" title="条件独立：tail-to-tail"></a>条件独立：<strong>tail-to-tail</strong></h4><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20200530230253.png" srcset="/img/loading.gif" alt="image"></p><p>根据图模型，得：$P(a,b,c)=P(c)P(a \mid c)P(b \mid c)$</p><p>从而：$P(a,b,c)/P(c)=P(a \mid c)P(b \mid c)$</p><p>因为$P(a,b \mid c)=P(a,b,c)/P(c)$</p><p>得：$P(a,b\mid c)=P(a\mid c)P(b\mid c)$</p><p>解释：在<code>c</code>给定的条件下，因为<code>a,b</code>被阻断（blocked），因此是独立的：$P(a,b\mid c)=P(a\mid c)P(b\mid c)$</p></li></ol><ol start="2"><li><h4 id="条件独立：head-to-tail"><a href="#条件独立：head-to-tail" class="headerlink" title="条件独立：head-to-tail"></a>条件独立：head-to-tail</h4><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20200530230254.png" srcset="/img/loading.gif" alt="image"></p><p>根据图模型，得：$P(a,b,c)=P(a)P(c \mid a)P(b \mid c)$</p><p>$P(a,b \mid c) \\ =P(a,b,c)/P(c) \\ =P(a)P(c \mid a)P(b \mid c) / P(c) \\ =P(a,c)P(b \mid c) / P(c) \\ = P(a\mid c)P(b\mid c)$</p><p>解释：在<code>c</code>给定的条件下，因为<code>a,b</code>被阻断（blocked），因此是独立的：$P(a,b\mid c)=P(a\mid c)P(b\mid c)$</p></li></ol><ol start="3"><li><h4 id="条件独立：head-to-head"><a href="#条件独立：head-to-head" class="headerlink" title="条件独立：head-to-head"></a>条件独立：head-to-head</h4><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20200530230255.png" srcset="/img/loading.gif" alt="image"></p><p>根据图模型，得：$P(a,b,c)=P(a)P(b)P(c \mid a,b) $</p><p>由：$\sum_c P(a,b,c)= \sum_n P(a)P(b)P(c \mid a,b)$</p><p>得：$P(a,b)=P(a)P(b)$</p><p>解释：在<code>c</code>给定的条件下，因为<code>a,b</code>被阻断（blocked），因此是独立的：$P(a,b)=P(a)P(b)$</p></li></ol><h3 id="有向分离"><a href="#有向分离" class="headerlink" title="有向分离"></a>有向分离</h3><p>对于任意的结点集,<code>有向分离</code>(D-separation): 对于任意的结点集<code>A,B,C</code>,考察所有通过A中任意结点到B中任意结点的路径,若要求<code>A,B</code>条件独立,则需要所有的路径都被阻断(blocked),即满足下列两个前提之一:  </p><ol><li>A和B的<code>head-to-tail型</code>和<code>tail-to-tail型</code>路径都通过C; </li><li>A和B的<code>head-to-head型</code>路径不通过C以及C的子孙结点; </li></ol><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20200530230256.png" srcset="/img/loading.gif" alt="image"></p><p>图(a), 在<code>tail-to-tail</code>中, <code>f</code>没有阻断; 在<code>head-to-head</code>中, <code>e</code>阻断, 然而它的子结点<code>c</code>没有阻断, 即<code>e</code>所在的结点集没有阻断; 因此, 结点<code>a, b</code>关于<code>c</code>不独立.</p><p> 图(b), 在<code>tail-to-tail</code>中, <code>f</code>阻断; 因此, 结点<code>a,b</code>关于<code>f</code> 独立. 在<code>head-to-head</code>中, <code>e</code>和它的子孙结点<code>c</code>都阻断; 因此, 结点<code>a,b</code>关于<code>e</code>独立.</p><h3 id="特殊的贝叶斯网络"><a href="#特殊的贝叶斯网络" class="headerlink" title="特殊的贝叶斯网络"></a>特殊的贝叶斯网络</h3><ol><li><h4 id="马尔科夫模型"><a href="#马尔科夫模型" class="headerlink" title="马尔科夫模型"></a>马尔科夫模型</h4><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20200530230257.png" srcset="/img/loading.gif" alt="image"></p><p>结点形成一条链式网络，这种按顺次演变的随机过程模型就称作<strong>马尔科夫模型</strong></p><p>$A_{i+1}$只与$A_i$有关，与$A_1,…,A_{i-1}$无关。</p></li><li><h4 id="隐马尔科夫模型"><a href="#隐马尔科夫模型" class="headerlink" title="隐马尔科夫模型"></a><strong>隐马尔科夫模型</strong></h4><h4 id="Hidden-Markov-Model"><a href="#Hidden-Markov-Model" class="headerlink" title="Hidden Markov Model"></a><code>Hidden Markov Model</code></h4><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20200530230258.png" srcset="/img/loading.gif" alt="image"></p><ul><li>隐马尔科夫模型（HMM）可用标注问题，在语音识别、NLP、生物信息、模式识别等领域别实践证明的有效算法。</li><li>HMM是关于时序的概率模型，描述由一个隐藏的马尔可夫链随机生成不可观测的状态随机序列，再由各个状态生成一个观测而产生观测随机序列的过程。</li><li>HMM随机生成的状态的序列，成为<code>状态序列</code>，每个状态生成一个观测，由此产生的观测随机序列，称为<code>观测序列</code><ul><li>序列的每一个位置可看做是一个时刻。</li><li>空间序列也可以使用该模型.</li></ul></li></ul></li></ol><ol start="3"><li><h4 id="马尔科夫毯"><a href="#马尔科夫毯" class="headerlink" title="马尔科夫毯"></a><strong>马尔科夫毯</strong></h4><p>一个结点的*<em><code>Markov Blanket</code> *</em>是一个集合，在这个集合中的结点都给定条件下，该结点条件独立于其他结点。</p><p>*<em><code>Markov Blanket</code> *</em>: 一个结点的<code>Markov Blanket</code>是它的<code>parents,children</code>以及<code>spouses</code></p><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20200530230259.png" srcset="/img/loading.gif" alt="image"></p></li></ol><pre><code>深色的结点集合，就是“马尔科夫毯”（**`Markov Blanket` **）</code></pre>]]></content>
    
    
    <categories>
      
      <category>机器学习</category>
      
    </categories>
    
    
    <tags>
      
      <tag>机器学习</tag>
      
      <tag>贝叶斯网络</tag>
      
      <tag>信念网络，有向无环模型</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>机器学习-朴素贝叶斯</title>
    <link href="/2018/08/ml-NB/"/>
    <url>/2018/08/ml-NB/</url>
    
    <content type="html"><![CDATA[<h3 id="朴素贝叶斯的概念"><a href="#朴素贝叶斯的概念" class="headerlink" title="朴素贝叶斯的概念"></a>朴素贝叶斯的概念</h3><p><strong>朴素贝叶斯分类（Naive Bayes Classifier）</strong>是一种简单而容易理解的分类方法，看起来很Naive，但用起来却很有效。其原理就是<strong>贝叶斯定理</strong>，从数据中得到新的信息，然后对<strong>先验概率</strong>进行更新，从而得到<strong>后验概率</strong>。</p><p>就好比说我们判断一个人的品质好坏，对于陌生人我们对他的判断是五五开，如果说他做了一件好事，那么这个新的信息使我们判断他是好人的概率增加了。<strong>朴素贝叶斯分类的优势</strong>在于不怕噪声和无关变量，其Naive之处在于它假设各特征属性是无关的。而<strong>贝叶斯网络（Bayesian Network）</strong>则放宽了变量无关的假设，将贝叶斯原理和图论相结合，建立起一种基于概率推理的数学模型,对于解决复杂的不确定性和关联性问题有很强的优势。</p><h3 id="条件概率"><a href="#条件概率" class="headerlink" title="条件概率"></a>条件概率</h3><p>条件概率（Conditional Probability）是指在事件B发生的情况下，事件A发生的概率，用$P(A |B)$表示，读作在B条件下的A的概率。 </p><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20200530230241.png" srcset="/img/loading.gif" alt="image"></p><p>在上方的文氏图中，描述了两个事件A和B，与它们的交集<code>A ∩ B</code>，代入条件概率公式，可推出事件A发生的概率为</p><h4 id="P-A-B-frac-P-A⋂B-P-B-。"><a href="#P-A-B-frac-P-A⋂B-P-B-。" class="headerlink" title="$P(A |B)=\frac{P(A⋂B)}{P(B)}$。"></a>$P(A |B)=\frac{P(A⋂B)}{P(B)}$。</h4><p> 对该公式做一下变换可推得$P(A⋂B)=P(A | B)P(B)与P(A⋂B)=P(B | A)P(A)$,（<code>P(B|A)</code>为在A条件下的B的概率）。</p><p>同理可得$P(A |B)P(B)=P(B |A)P(A)$。</p><h3 id="全概率公式"><a href="#全概率公式" class="headerlink" title="全概率公式"></a>全概率公式</h3><p>全概率公式是将边缘概率与条件概率关联起来的基本规则，它表示了一个结果的总概率，可以通过几个不同的事件来实现。</p><p>全概率公式将对一复杂事件的概率求解问题转化为了在不同情况下发生的简单事件的概率的求和问题，公式为</p><h4 id="P-B-sum-i-1-n-P-A-i-P-B-A-i"><a href="#P-B-sum-i-1-n-P-A-i-P-B-A-i" class="headerlink" title="$P(B) = {\sum_{i=1}^n}P(A_i)P(B |A_i)$"></a>$P(B) = {\sum_{i=1}^n}P(A_i)P(B |A_i)$</h4><p>假定一个样本空间S，它是两个事件A与C之和，同时事件B与它们两个都有交集，如下图所示： </p><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20200530230242.png" srcset="/img/loading.gif" alt="image"></p><p>那么事件B的概率可以表示为$P(B)=P(B⋂A)+P(B⋂C)$</p><p>通过条件概率，可以推断出$P(B⋂A)=P(B |A)P(A)$，所以$P(B)=P(B |A)P(A)+P(B |C)P(C)$</p><p>这就是全概率公式，即事件B的概率等于事件A与事件C的概率分别乘以B对这两个事件的条件概率之和。</p><h3 id="贝叶斯定理"><a href="#贝叶斯定理" class="headerlink" title="贝叶斯定理"></a>贝叶斯定理</h3><h4 id="贝叶斯公式："><a href="#贝叶斯公式：" class="headerlink" title="贝叶斯公式："></a><code>贝叶斯公式</code>：</h4><h4 id="P-A-B-frac-P-B-A-P-A-P-B"><a href="#P-A-B-frac-P-B-A-P-A-P-B" class="headerlink" title="$P(A|B) = \frac{P(B|A)P(A)}{P(B)}$"></a>$P(A|B) = \frac{P(B|A)P(A)}{P(B)}$</h4><ul><li>$P(A |B)$：在B条件下的事件A的概率，在贝叶斯定理中，条件概率也被称为<code>后验概率</code>，即在事件B发生之后，我们对事件A概率的重新评估。</li><li>$P(B |A)$：在A条件下的事件B的概率，其实就是和上一条是一样的意思。</li><li>$P(A)$与$P(B)$被称为<code>先验概率</code>（也被称为边缘概率），即在事件B发生之前，我们对事件A概率的一个推断（不考虑任何事件B方面的因素），后面同理。</li><li>$P(B |A)P(B)$被称为<code>标准相似度</code>，它是一个调整因子，主要是为了保证预测概率更接近真实概率。</li><li>根据这些术语，贝叶斯定理表述为： <strong>后验概率 = 标准相似度 * 先验概率</strong>。</li></ul><h3 id="朴素贝叶斯分类的原理"><a href="#朴素贝叶斯分类的原理" class="headerlink" title="朴素贝叶斯分类的原理"></a>朴素贝叶斯分类的原理</h3><p>朴素贝叶斯分类是一种十分简单的分类算法，叫它朴素贝叶斯分类是因为这种方法的思想真的很朴素，朴素贝叶斯的思想基础是这样的：对于给出的待分类项，求解在此项出现的条件下各个类别出现的概率，哪个最大，就认为此待分类项属于哪个类别。 </p><p>我们设一个待分类项$X=f_1,f_2,⋯,f_n$，其中每个<code>f</code>为<code>X</code>的一个特征属性，然后设一个类别集合$C_1,C_2,⋯,C_m$。</p><p>然后需要计算$P(C_1 |X),P(C_2 |X),⋯,P(C_m |X)$，然后我们就可以根据一个训练样本集合（已知分类的待分类项集合），然后统计得到在各类别下各个特征属性的条件概率：</p><p>$P(f_1 |C_1),P(f_2 |C_1),⋯,P(f_n |C_1),\, P(f_1 |C_2),P(f_2 |C_2),⋯  P(f_n |C_2), \\ P(f_1 |C_m),P(f_2 |C_m),⋯,P(f_n |C_m)$</p><p>如果$P(C_k |X)=MAX(P(C_1 |X),P(C_2 |X),⋯,P(C_m |X))$，则$X∈C_k$（贝叶斯分类其实就是取<strong>概率最大</strong>的那一个）。</p><p>朴素贝叶斯会假设每个特征都是独立的，根据贝叶斯定理可推得：$P(C_i |X)=\frac {P(X |C_i)P(C_i)}{P(X)}$，由于分母对于所有类别为常数，因此只需要将分子最大化即可，又因为各特征是互相独立的，所以最终推得：</p><h4 id="P-X-C-i-P-C-i-P-f-1-C-i-P-f-2-C-i-…-P-f-n-C-i-P-C-i-P-C-i-prod-i-1-nP-f-i-C-i"><a href="#P-X-C-i-P-C-i-P-f-1-C-i-P-f-2-C-i-…-P-f-n-C-i-P-C-i-P-C-i-prod-i-1-nP-f-i-C-i" class="headerlink" title="$P(X | C_i)P(C_i)=P(f_1 |C_i)P(f_2 |C_i)…,P(f_n | C_i)P(C_i) \\ =P(C_i)\prod_{i=1}^nP(f_i |C_i)$"></a>$P(X | C_i)P(C_i)=P(f_1 |C_i)P(f_2 |C_i)…,P(f_n | C_i)P(C_i) \\ =P(C_i)\prod_{i=1}^nP(f_i |C_i)$</h4><p>根据上述的公式推导，朴素贝叶斯的流程可如下图所示： </p><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20200530230243.png" srcset="/img/loading.gif" alt="image"></p><h3 id="朴素贝叶斯的算法模型"><a href="#朴素贝叶斯的算法模型" class="headerlink" title="朴素贝叶斯的算法模型"></a>朴素贝叶斯的算法模型</h3><p>在朴素贝叶斯中含有以下三种算法模型：</p><ul><li><p><strong><code>Gaussian Naive Bayes</code></strong>：适合在特征变量具有连续性的时候使用，同时它还假设特征遵从于高斯分布（正态分布）。</p><p> 假设我们有一组人体特征的统计资料，该数据中的特征：身高、体重和脚掌长度等都为连续变量，很明显我们不能采用离散变量的方法来计算概率，由于样本太少，也无法分成区间计算，那么要怎么办呢？解决方法是假设特征项都是正态分布，然后通过样本计算出均值与标准差，这样就得到了正态分布的密度函数，有了密度函数，就可以代入值，进而算出某一点的密度函数的值。</p></li><li><p><strong><code>MultiNomial Naive Bayes</code></strong>：与Gaussian Naive Bayes相反，多项式模型更适合处理特征是离散变量的情况，该模型会在计算先验概率$P(C_m)$和条件概率$P(F_n |C_m)$时会做一些平滑处理。具体公式为</p><h4 id="P-C-m-frac-T-cm-a-T-ma"><a href="#P-C-m-frac-T-cm-a-T-ma" class="headerlink" title="$P(C_m)=\frac{T_{cm}+a}{T+ma}$"></a>$P(C_m)=\frac{T_{cm}+a}{T+ma}$</h4><p>其中T为总的样本数，m为总类别数，$T*{cm}即类别为即类别为C_m$的样本个数，<code>a</code>是一个平滑值。条件概率的公式为</p><h4 id="P-F-n-C-m-frac-T-cm-f-n-a-T-cm-an"><a href="#P-F-n-C-m-frac-T-cm-f-n-a-T-cm-an" class="headerlink" title="$P(F_n |C_m) = \frac{T_{cm}f_n+a}{T_{cm}+an}$"></a>$P(F_n |C_m) = \frac{T_{cm}f_n+a}{T_{cm}+an}$</h4><p><code>n</code>为特征的个数，$$T_{cm}f_n$$为类别为$$C_m$$特征为$$F_n$$的样本个数。</p><p>当平滑值<code>a = 1</code>，被称作为<code>Laplace</code>平滑，</p><p>当平滑值<code>a &lt; 1</code>，被称为<code>Lidstone</code>平滑。</p><p>它的思想其实就是对每类别下所有划分的计数加1，这样如果训练样本数量足够大时，就不会对结果产生影响，并且解决了$P(F |C)$的频率为0的现象（某个类别下的某个特征划分没有出现，这会严重影响分类器的质量）。</p></li><li><p><strong><code>Bernoulli Naive Bayes</code></strong>：Bernoulli适用于在特征属性为二进制的场景下，它对每个特征的取值是基于布尔值的，一个典型例子就是判断单词有没有在文本中出现。</p></li></ul>]]></content>
    
    
    <categories>
      
      <category>机器学习</category>
      
    </categories>
    
    
    <tags>
      
      <tag>机器学习</tag>
      
      <tag>贝叶斯分类器</tag>
      
      <tag>贝叶斯定理</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>机器学习-EM</title>
    <link href="/2018/07/ml-em/"/>
    <url>/2018/07/ml-em/</url>
    
    <content type="html"><![CDATA[<h3 id="EM算法简介"><a href="#EM算法简介" class="headerlink" title="EM算法简介"></a>EM算法简介</h3><p><strong>最大期望演算法</strong>（<strong>Expectation-maximization algorithm</strong>，又译<strong>期望最大化算法</strong>）在统计中被用于寻找，依赖于不可观察的隐性变量的概率模型中，参数的最大似然估计。</p><p>在<a href="https://zh.wikipedia.org/wiki/%E7%BB%9F%E8%AE%A1" target="_blank" rel="noopener">统计</a><a href="https://zh.wikipedia.org/wiki/%E8%AE%A1%E7%AE%97" target="_blank" rel="noopener">计算</a>中，<strong>最大期望（EM）算法</strong>是在<a href="https://zh.wikipedia.org/wiki/%E6%A6%82%E7%8E%87%E6%A8%A1%E5%9E%8B" target="_blank" rel="noopener">概率模型</a>中寻找<a href="https://zh.wikipedia.org/wiki/%E5%8F%82%E6%95%B0" target="_blank" rel="noopener">参数</a><a href="https://zh.wikipedia.org/wiki/%E6%9C%80%E5%A4%A7%E4%BC%BC%E7%84%B6%E4%BC%B0%E8%AE%A1" target="_blank" rel="noopener">最大似然估计</a>或者<a href="https://zh.wikipedia.org/w/index.php?title=%E6%9C%80%E5%A4%A7%E5%90%8E%E9%AA%8C%E4%BC%B0%E8%AE%A1&action=edit&redlink=1" target="_blank" rel="noopener">最大后验估计</a>的<a href="https://zh.wikipedia.org/wiki/%E7%AE%97%E6%B3%95" target="_blank" rel="noopener">算法</a>，其中概率模型依赖于无法观测的<a href="https://zh.wikipedia.org/w/index.php?title=%E9%9A%90%E6%80%A7%E5%8F%98%E9%87%8F&action=edit&redlink=1" target="_blank" rel="noopener">隐性变量</a>。最大期望算法经常用在<a href="https://zh.wikipedia.org/wiki/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0" target="_blank" rel="noopener">机器学习</a>和<a href="https://zh.wikipedia.org/wiki/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89" target="_blank" rel="noopener">计算机视觉</a>的<a href="https://zh.wikipedia.org/wiki/%E6%95%B0%E6%8D%AE%E8%81%9A%E7%B1%BB" target="_blank" rel="noopener">数据聚类</a>（Data Clustering）领域。最大期望算法经过两个步骤交替进行计算，第一步是计算期望（E），利用对隐藏变量的现有估计值，计算其最大似然估计值；第二步是最大化（M），最大化在E步上求得的<a href="https://zh.wikipedia.org/wiki/%E6%9C%80%E5%A4%A7%E4%BC%BC%E7%84%B6%E4%BC%B0%E8%AE%A1" target="_blank" rel="noopener">最大似然值</a>来计算参数的值。M步上找到的参数估计值被用于下一个E步计算中，这个过程不断交替进行。</p><h3 id="最大似然估计"><a href="#最大似然估计" class="headerlink" title="最大似然估计"></a>最大似然估计</h3><p><strong>注意</strong>：EM算法主要用于从不完整数据中计算最大似然估计，本身可以看成是特殊情况下计算极大似然的一种方法。 </p><p><strong>极大似然估计</strong>所要解决的问题是：给定一组数据和一个参数待定的模型，如何确定模型的参数。使得这个确定的参数后的模型在所有模型中产生已知数据的概率最大。（<strong>模型已定，参数未知</strong>）。</p><h4 id="简单举个例子："><a href="#简单举个例子：" class="headerlink" title="简单举个例子："></a><strong>简单举个例子</strong>：</h4><ul><li>10次抛硬币的结果是：正正反正正正反反正正</li><li>假设P是每次抛硬币的结果为正的概率。</li><li>得到这样实验结果的概率我们就可以算出来：<ul><li>$P=pp(1-p)ppp(1-p)(1-p)pp \\ = p^7(1-p)^3$</li></ul></li><li>然后很愉快的最优解就算出来了：$p=0.7$</li></ul><p>我们引申到<strong>二项分布的最大似然估计</strong></p><ul><li>在抛硬币的试验中，进行N次独立试验，n次朝上，N-n次朝下。</li><li>然后假定朝上的概率为p，使用对数似然函数作为目标函数：<ul><li>$f(n | p)=log(p^n(1-p^{N-n}))$</li><li>$\frac{\partial(f(n | p))}{\partial p}=\frac nP-\frac{N-n}{1-p}=0$</li><li>然后就可以得到概率：$p=\frac nN$</li></ul></li></ul><p>升级一下，<strong>进一步考察</strong></p><ul><li><p>若给定有一组样本$x_1,x_2,…x_n$，已知它们来自于高斯分布$N(u,\sigma)$，试估计参数$u, \sigma$.</p></li><li><p>高斯分布的概率密度函数：</p><ul><li><h4 id="f-x-frac-1-sqrt-2-pi-sigma-e-frac-x-i-u-2-2-sigma-2"><a href="#f-x-frac-1-sqrt-2-pi-sigma-e-frac-x-i-u-2-2-sigma-2" class="headerlink" title="$f(x)=\frac{1}{\sqrt{2\pi}\sigma}e^{-\frac{(x_i-u)^2}{2\sigma^2}}$"></a>$f(x)=\frac{1}{\sqrt{2\pi}\sigma}e^{-\frac{(x_i-u)^2}{2\sigma^2}}$</h4></li></ul></li><li><p>将$X_i$的样本值$x_i$带入，得：</p><ul><li><h4 id="L-x-prod-i-1-n-frac-1-sqrt-2-pi-sigma-e-frac-x-i-u-2-2-sigma-2"><a href="#L-x-prod-i-1-n-frac-1-sqrt-2-pi-sigma-e-frac-x-i-u-2-2-sigma-2" class="headerlink" title="$L(x)=\prod_{i=1}^n\frac{1}{\sqrt{2\pi}\sigma}e^{-\frac{(x_i-u)^2}{2\sigma^2}}$"></a>$L(x)=\prod_{i=1}^n\frac{1}{\sqrt{2\pi}\sigma}e^{-\frac{(x_i-u)^2}{2\sigma^2}}$</h4></li></ul></li><li><p>化简对数似然函数：</p><ul><li><h4 id="l-x-log-prod-i-1-n-frac-1-sqrt-2-pi-sigma-e-frac-x-i-u-2-2-sigma-2-sum-ilog-frac-1-sqrt-2-pi-sigma-e-frac-x-i-u-2-2-sigma-2-sum-ilog-frac-1-sqrt-2-pi-sigma-sum-i-frac-x-i-u-2-2-sigma-2-frac-n2log-2-pi-sigma-2-frac-1-2-sigma-2-sum-i-x-i-u-2"><a href="#l-x-log-prod-i-1-n-frac-1-sqrt-2-pi-sigma-e-frac-x-i-u-2-2-sigma-2-sum-ilog-frac-1-sqrt-2-pi-sigma-e-frac-x-i-u-2-2-sigma-2-sum-ilog-frac-1-sqrt-2-pi-sigma-sum-i-frac-x-i-u-2-2-sigma-2-frac-n2log-2-pi-sigma-2-frac-1-2-sigma-2-sum-i-x-i-u-2" class="headerlink" title="$l(x)=log\prod_{i=1}^n\frac{1}{\sqrt{2\pi}\sigma}e^{-\frac{(x_i-u)^2}{2\sigma^2}} \\ = \sum_ilog\frac{1}{\sqrt{2\pi}\sigma}e^{-\frac{(x_i-u)^2}{2\sigma^2}} \\ =\sum_ilog\frac{1}{\sqrt{2\pi}\sigma} + \sum_i {-\frac{(x_i-u)^2}{2\sigma^2}} \\ =-\frac n2log(2\pi\sigma^2)-\frac{1}{2\sigma^2}\sum_i(x_i-u)^2 $"></a>$l(x)=log\prod_{i=1}^n\frac{1}{\sqrt{2\pi}\sigma}e^{-\frac{(x_i-u)^2}{2\sigma^2}} \\ = \sum_ilog\frac{1}{\sqrt{2\pi}\sigma}e^{-\frac{(x_i-u)^2}{2\sigma^2}} \\ =\sum_ilog\frac{1}{\sqrt{2\pi}\sigma} + \sum_i {-\frac{(x_i-u)^2}{2\sigma^2}} \\ =-\frac n2log(2\pi\sigma^2)-\frac{1}{2\sigma^2}\sum_i(x_i-u)^2 $</h4></li></ul></li><li><p>参数估计</p><ul><li><p>目标函数：</p><ul><li><h4 id="l-x-frac-n2log-2-pi-sigma-2-frac-1-2-sigma-2-sum-i-x-i-u-2"><a href="#l-x-frac-n2log-2-pi-sigma-2-frac-1-2-sigma-2-sum-i-x-i-u-2" class="headerlink" title="$l(x)=-\frac n2log(2\pi\sigma^2)-\frac{1}{2\sigma^2}\sum_i(x_i-u)^2 $"></a>$l(x)=-\frac n2log(2\pi\sigma^2)-\frac{1}{2\sigma^2}\sum_i(x_i-u)^2 $</h4></li></ul></li><li><p>将目标函数对参数$u,、\sigma$分别<strong>求偏导</strong>，然后<strong>令偏导等于0</strong>，就可以得到$u,\sigma$的式子：</p><ul><li><h4 id="u-frac-1n-sum-ix-i"><a href="#u-frac-1n-sum-ix-i" class="headerlink" title="$u=\frac 1n \sum_ix_i$"></a>$u=\frac 1n \sum_ix_i$</h4></li><li><h4 id="sigma-2-frac-1n-sum-i-x-i-u-2"><a href="#sigma-2-frac-1n-sum-i-x-i-u-2" class="headerlink" title="$\sigma^2=\frac 1n \sum_i(x_i-u)^2$"></a>$\sigma^2=\frac 1n \sum_i(x_i-u)^2$</h4></li></ul></li></ul></li><li><p>到现在，我们就可以很直观的看出，这个结果是和矩估计的结果是一致的，也就是说样本的均值就是高斯分布的均值，样本的伪方差就是高斯分布的方差。</p></li></ul><p>这就是我们所熟知的极大似然估计。</p><h3 id="Jensen-不等式"><a href="#Jensen-不等式" class="headerlink" title="Jensen 不等式"></a>Jensen 不等式</h3><p>设$f$是定义域为实数的函数，如果对于所有的实数$x$。如果对于所有的实数$x$，$f(x)$的二次导数大于等于0，那么$f$是凸函数。当$x$是向量时，如果其$hessian$矩阵$H$是半正定的，那么$f$是凸函数。如果只大于0，不等于0，那么称f是严格凸函数。<br> Jensen不等式表述如下：<br> 如果$f$是凸函数，X是随机变量，那么：$E[f(X)]&gt;=f(E[X])$<br> 特别地，如果$f$是严格凸函数，当且仅当X是常量时，上式取等号。<br> 如果用图表示会很清晰：</p><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20200530230233.png" srcset="/img/loading.gif" alt="images"></p><p>图中，实线$f$是凸函数，$X$是随机变量，有0.5的概率是a，有0.5的概率是b。（就像掷硬币一样）。X的期望值就是a和b的中值了，图中可以看到$E[f(X)]&gt;=f(E[X])$成立。<br> 当f是（严格）凹函数当且仅当-f是（严格）凸函数。<br> Jensen不等式应用于凹函数时，不等号方向反向。</p><h3 id="如何理解EM算法？"><a href="#如何理解EM算法？" class="headerlink" title="如何理解EM算法？"></a>如何理解EM算法？</h3><p>在前面我们所遇到的，一堆由已知分布得到的数据，如果模型中的变量都是可以观测到了，为了求其中的参数，这时就可以直接使用极大似然估计。</p><p>但是，有这么一种情况，就是当模型中含有<strong>隐变量</strong>（数据观测不到了）的时候，再使用极大似然估计的时候，计算过程就会显得极为复杂甚至不可解。当我们正一筹莫展的时候，<strong>EM算法</strong>就出现了。</p><h3 id="EM算法的思路"><a href="#EM算法的思路" class="headerlink" title="EM算法的思路"></a>EM算法的思路</h3><p>  <strong>输入</strong>：观测变量数据X，隐变量数据Z，联合分布$P(X,Z | \theta)$，条件分布$P(Z | X,\theta)$</p><p><strong>输出</strong>：模型参数$\theta$</p><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20200530230234.png" srcset="/img/loading.gif" alt="images"></p>]]></content>
    
    
    <categories>
      
      <category>机器学习</category>
      
    </categories>
    
    
    <tags>
      
      <tag>机器学习</tag>
      
      <tag>无监督学习</tag>
      
      <tag>聚类</tag>
      
      <tag>EM</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>机器学习-聚类</title>
    <link href="/2018/07/ml-clustering/"/>
    <url>/2018/07/ml-clustering/</url>
    
    <content type="html"><![CDATA[<h3 id="无监督学习"><a href="#无监督学习" class="headerlink" title="无监督学习"></a>无监督学习</h3><p>无监督学习是一种机器学习方法，用于<strong>发现数据中的模式</strong>。输入无监督算法的数据都没有标签，也就是只为算法提供了输入变量$(X)$而没有对应的输出变量。在无监督学习中，算法需要<strong>自行寻找数据中的结构</strong>。</p><p>无监督学习问题可以有以下三种类型：</p><ul><li><strong>关联</strong>：发现目录中项目共现的概率。其广泛应用于“购物车分析”。例如，如果一个顾客购买了火腿，他会有80% 的概率也购买鸡蛋。</li><li><strong>聚类</strong>：将样本分组，这样，同一聚类中的物体与来自另一聚类的物体相比，相互之间会更加类似。</li><li><strong>降维</strong>：<strong>降维指减少一个数据集的变量数量，同时保证还能传达重要信息</strong>。降维可以通过特征抽取方法和特征选择方法完成。特征选择方法会选择初始变量的子集。特征抽取方法执行从高维度空间到低维度空间的数据转换。例如，PCA算法就是一种特征抽取方式。</li></ul><h3 id="聚类的概念"><a href="#聚类的概念" class="headerlink" title="聚类的概念"></a>聚类的概念</h3><p>聚类是一种无监督机器学习方法，它基于数据的内部结构寻找观察样本的自然族群（即集群），常用于新闻分类、推荐系统等。聚类的特点是训练数据没有标注，通常使用数据可视化评价结果。 </p><p>聚类分析仅根据在数据中发现的描述对象及其关系的信息，将数据对象分组。其目标是，组内的对象相互之间是相似的（相关的），而不同组中的对象是不同的（不相关的）。组内的相似性（同质性）越大，组间差别越大，聚类就越好。</p><h3 id="聚类的方法"><a href="#聚类的方法" class="headerlink" title="聚类的方法"></a>聚类的方法</h3><p>聚类的常用方法包括</p><ul><li><strong>划分聚类法</strong>，<strong>K均值</strong>:是基于原型的、划分的聚类技术。它试图发现用户指定个数K的簇（由质心代表）。</li><li><strong>层次聚类。凝聚的层次聚类：</strong>开始，每个点作为一个单点簇；然后，重复地合并两个最靠近的簇，直到产生单个的、包含所有点的簇。</li><li><strong>基于密度的聚类，</strong> <strong>DBSCAN</strong>是一种产生划分聚类的基于密度的聚类算法，簇的个数由算法自动地确定。低密度区域中的点被视为噪声而忽略，因此DBSCAN不产生完全聚类。</li></ul><h3 id="常用的聚类数据集"><a href="#常用的聚类数据集" class="headerlink" title="常用的聚类数据集"></a>常用的聚类数据集</h3><p>常用的聚类数据集包括</p><ul><li>scikit-learn blob: 简单聚类</li><li>scikit-learn circle: 非线性可分数据集</li><li>scikit-learn moon: 更复杂的数据集</li></ul><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20200530230220.png" srcset="/img/loading.gif" alt="image"></p><h3 id="聚类的性能度量"><a href="#聚类的性能度量" class="headerlink" title="聚类的性能度量"></a>聚类的性能度量</h3><p>我们希望聚类结果的<strong>“簇内相似度”高且“簇间相似度”低</strong>。</p><p>其性能度量大致有两类：</p><h4 id="一类是将聚类结果与某个“参考模型”进行比较。称为“外部指标”。"><a href="#一类是将聚类结果与某个“参考模型”进行比较。称为“外部指标”。" class="headerlink" title="一类是将聚类结果与某个“参考模型”进行比较。称为“外部指标”。"></a>一类是将聚类结果与某个“参考模型”进行比较。称为“外部指标”。</h4><h4 id="一类是直接考查聚类结果而不利于任何参考模型。称为“内部指标”。"><a href="#一类是直接考查聚类结果而不利于任何参考模型。称为“内部指标”。" class="headerlink" title="一类是直接考查聚类结果而不利于任何参考模型。称为“内部指标”。"></a>一类是直接考查聚类结果而不利于任何参考模型。称为“内部指标”。</h4><h3 id="外部指标"><a href="#外部指标" class="headerlink" title="外部指标"></a>外部指标</h3><p>对数据集$D={x_1,x_2,…,x_m}$,假定通过聚类给出额簇划分为$C=C_1,C_2,…,C_k$,参考模型给出的簇划分为$C’=C_1^T,C_2^T,…,C_s^T$。相应的，令λ与$λ^T$分别表示与C和$C^T$对应的簇标记向量。注意的是，参考模型给出的划分类别数量不一定等于通过聚类得到的数量。 </p><p>样本两两配对： </p><ol><li>$a=\mid SS \mid ,SS={(x_i,x_j)\mid \lambda_i = \lambda_j,\lambda_i^T=\lambda_j^T,i&lt;j}$</li><li>$b=\mid SS \mid ,SD={(x_i,x_j)\mid \lambda_i = \lambda_j,\lambda_i^T\neq \lambda_j^T,i&lt;j}$</li><li>$c=\mid SS \mid ,DS={(x_i,x_j)\mid \lambda_i \neq \lambda_j,\lambda_i^T=\lambda_j^T,i&lt;j}$</li><li>$d=\mid SS \mid ,DD={(x_i,x_j)\mid \lambda_i \neq \lambda_j,\lambda_i^T \neq \lambda_j^T,i&lt;j}$</li></ol><p>集合SS包含了C中隶属于相同簇且在$C’$中也隶属于相同簇的样本对，集合SD包含了在C中隶属于相同簇但在$C^T$中隶属于不同簇的样本对 .</p><ol><li>Jaccard系数：$JC=\frac{a}{a+b+c}$  </li><li>FM指数：  $FMI=\sqrt{\frac{a}{a+b}\frac{a}{a+c}}$</li><li>Rand指数：  $RI=\frac{2(a+d)}{m(m-1)}$</li></ol><p>上述性能度量的结果值均在[0,1]区间，值越大越好。 </p><h3 id="内部指标"><a href="#内部指标" class="headerlink" title="内部指标"></a>内部指标</h3><p>考虑聚类结果的簇划分$C=C_1,C_2,…,C_k$，定义</p><ol><li>$avg(C)=\frac{2}{\mid C \mid(\mid C \mid -1)}\sum_{1 \leq i &lt; j \leq \mid C \mid}dist(x_i,x_j)$</li><li>$diam(C)=\max_{1 \leq i &lt;j \leq \mid C \mid}dist(x_i,x_j)$</li><li>$d_\min(C_i,C_j)=\min_{x_i \in C_i , x_j \in C_j} dist(x_i,x_j)$</li><li>$d_cen(C_i,C_j)=dist(\mu_i,\mu_j)$</li></ol><p>我们在上面的式子中，dist是计算两个样本之间的距离，$u$代表簇的中心点$\mu=\frac{\sum_{1 \leq i \leq \mid C \mid x_i}}{\mid C \mid}$ ，avg(C)与簇内样本间的平均距离，diam(C)对应与簇C内样本间的最远距离，$d_min(C_i,C<em>j)对应与簇i和簇j最近样本间的距离；对应与簇i和簇j最近样本间的距离；d</em>{cen}(C_i,C_j)$对应与簇i和j中心点间的距离。 </p><p>基于上面的指标，推出下面几个内部指标：</p><ol><li>$DBI=\frac{1}{k}\sum\limits_{i=1}^k\max\limits_{j \neq i}(\frac{avg(C_i)+avg(C_j)}{d_{cen}(\mu_i,\mu_j)})$</li><li>$DI=\min\limits_{1 \leq i \leq k}{ \min\limits_{j \neq i}(\frac{d_{min}(C_i,C_j)}{\max_{1\leq l \leq k diam(C_l)}}) }$</li></ol><p>显然，DBI的值越小越好，DI值越大越好</p><h3 id="相似度-距离度量"><a href="#相似度-距离度量" class="headerlink" title="相似度/距离度量"></a>相似度/距离度量</h3><p>“距离度量”需满足一些基本性质，如<strong>非负性、同一性、对称性和直递性</strong>。最常用的是闵可夫斯基距离、欧氏距离和曼哈顿距离（后两者其实都是闵可夫斯基距离的特例）。</p><p>闵可夫斯基距离只可用于有序属性，对无序属性可采用VDM（Value Difference Metric）。</p><h4 id="计算方法总结："><a href="#计算方法总结：" class="headerlink" title="计算方法总结："></a>计算方法总结：</h4><ul><li><h5 id="闵可夫斯基距离Minkowski-欧式距离：-dist-X-Y-sum-i-1-n-x-i-y-i-p-frac1p"><a href="#闵可夫斯基距离Minkowski-欧式距离：-dist-X-Y-sum-i-1-n-x-i-y-i-p-frac1p" class="headerlink" title="闵可夫斯基距离Minkowski/欧式距离：$dist(X,Y)=(\sum_{i=1}^n|x_i-y_i|^p)^\frac1p$"></a>闵可夫斯基距离Minkowski/欧式距离：$dist(X,Y)=(\sum_{i=1}^n|x_i-y_i|^p)^\frac1p$</h5></li><li><h5 id="杰卡德相似系数-Jaccard-J-A-B-frac-A-cap-B-A-cup-B"><a href="#杰卡德相似系数-Jaccard-J-A-B-frac-A-cap-B-A-cup-B" class="headerlink" title="杰卡德相似系数(Jaccard): $J(A,B)=\frac{|A\cap B|}{|A\cup B|}$"></a>杰卡德相似系数(Jaccard): $J(A,B)=\frac{|A\cap B|}{|A\cup B|}$</h5></li><li><h5 id="余弦相似度-cosine-similarity-cos-theta-frac-a-Tb-a-cdot-b"><a href="#余弦相似度-cosine-similarity-cos-theta-frac-a-Tb-a-cdot-b" class="headerlink" title="余弦相似度(cosine similarity):$cos(\theta)=\frac{a^Tb}{|a|\cdot |b|}$"></a>余弦相似度(cosine similarity):$cos(\theta)=\frac{a^Tb}{|a|\cdot |b|}$</h5></li><li><h5 id="Pearson相似系数：-rho-xy-frac-cov-X-Y-sigma-X-sigma-Y-frac-E-X-u-X-Y-u-Y-sigma-X-sigma-Y-frac-sum-i-1-n-X-i-u-X-Y-i-u-Y-sqrt-sum-i-1-n-X-i-u-X-2-sqrt-sum-i-1-n-Y-i-u-Y-2"><a href="#Pearson相似系数：-rho-xy-frac-cov-X-Y-sigma-X-sigma-Y-frac-E-X-u-X-Y-u-Y-sigma-X-sigma-Y-frac-sum-i-1-n-X-i-u-X-Y-i-u-Y-sqrt-sum-i-1-n-X-i-u-X-2-sqrt-sum-i-1-n-Y-i-u-Y-2" class="headerlink" title="Pearson相似系数：$\rho_{xy}=\frac{cov(X,Y)}{\sigma_X\sigma_Y}=\frac{E[(X-u_X)(Y-u_Y)]}{\sigma_X\sigma_Y}=\frac{\sum_{i=1}^n (X_i-u_X)(Y_i-u_Y) }{\sqrt{\sum_{i=1}^n(X_i-u_X)^2} \sqrt{\sum_{i=1}^n(Y_i-u_Y)^2}}$"></a>Pearson相似系数：$\rho_{xy}=\frac{cov(X,Y)}{\sigma_X\sigma_Y}=\frac{E[(X-u_X)(Y-u_Y)]}{\sigma_X\sigma_Y}=\frac{\sum_{i=1}^n (X_i-u_X)(Y_i-u_Y) }{\sqrt{\sum_{i=1}^n(X_i-u_X)^2} \sqrt{\sum_{i=1}^n(Y_i-u_Y)^2}}$</h5></li><li><h5 id="相对熵（K-L距离）：-D-P-q-sum-xp-x-log-frac-p-x-q-x-E-p-x-log-frac-p-x-q-x"><a href="#相对熵（K-L距离）：-D-P-q-sum-xp-x-log-frac-p-x-q-x-E-p-x-log-frac-p-x-q-x" class="headerlink" title="相对熵（K-L距离）：$D(P||q)=\sum_xp(x)log\frac{p(x)}{q(x)}=E_{p(x)}log\frac{p(x)}{q(x)}$"></a>相对熵（K-L距离）：$D(P||q)=\sum_xp(x)log\frac{p(x)}{q(x)}=E_{p(x)}log\frac{p(x)}{q(x)}$</h5></li><li><h5 id="Hellinger距离：-D-a-p-q-frac-2-1-a-2-1-int-p-x-frac-1-a-2-cdot-q-x-frac-1-a-2-dx"><a href="#Hellinger距离：-D-a-p-q-frac-2-1-a-2-1-int-p-x-frac-1-a-2-cdot-q-x-frac-1-a-2-dx" class="headerlink" title="Hellinger距离：$D_a(p||q)=\frac{2}{1-a^2}(1-\int (p(x)^\frac{1+a}{2}\cdot q(x)^\frac{1-a}{2}dx$"></a>Hellinger距离：$D_a(p||q)=\frac{2}{1-a^2}(1-\int (p(x)^\frac{1+a}{2}\cdot q(x)^\frac{1-a}{2}dx$</h5></li></ul><h3 id="聚类的基本思想"><a href="#聚类的基本思想" class="headerlink" title="聚类的基本思想"></a>聚类的基本思想</h3><p>给定一个有N个对象的数据集，构造数据的K个簇，$k\le n$,满足下列条件：</p><ul><li>每一个簇至少包含一个对象</li><li>每一个对象属于且仅属于一个簇</li><li>将满足上述条件的K个簇称作一个合理的划分</li></ul><p><strong>基本思想</strong>：对于给定的类别数目K，首先给出初始划分，通过迭代改变样本和簇的隶属关系，使得每一次改进之后的划分方案都比前一次好。</p><h3 id="原型聚类"><a href="#原型聚类" class="headerlink" title="原型聚类"></a>原型聚类</h3><p>原型聚类亦称“基于原型的聚类”，假设聚类结构能通过一组原型（原型是指样本空间中具有代表性的点）刻画。</p><p>常用的方法包括</p><ul><li>k均值算法</li><li>学习向量化</li><li>高斯混合聚类（基于概率模型）</li></ul><h3 id="K-Means-聚类"><a href="#K-Means-聚类" class="headerlink" title="K-Means 聚类"></a>K-Means 聚类</h3><p><strong>K-Means</strong>算法的基本思想是初始随机给定K个簇中心，按照最邻近原则把待分类样本点分到各个簇。然后按平均法重新计算各个簇的质心(这个点可以不是样本点)，从而确定新的簇心。一直迭代，直到簇心的移动距离小于某个给定的值。 </p><h3 id="K-Means聚类算法步骤："><a href="#K-Means聚类算法步骤：" class="headerlink" title="K-Means聚类算法步骤："></a><strong>K-Means聚类算法步骤</strong>：</h3><p>(1)选择K个初始质心，其中K是用户指定的参数，即所期望的簇的个数。</p><p>(2)每个点指派到最近的质心，而指派到一个质心的点集为一个簇。</p><p>(3)根据指派到簇的点，更新每个簇的质心。</p><p>(4)重复指派和更新步骤，直到簇不发生变化，或等价地，直到质心不发生变化。</p><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20200530230221.png" srcset="/img/loading.gif" alt="image"></p><h3 id="K均值算法"><a href="#K均值算法" class="headerlink" title="K均值算法"></a>K均值算法</h3><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20200530230222.png" srcset="/img/loading.gif" alt="image"></p><h3 id="k均值常用的邻近度，质心和目标函数的选择："><a href="#k均值常用的邻近度，质心和目标函数的选择：" class="headerlink" title="k均值常用的邻近度，质心和目标函数的选择："></a><strong>k均值常用的邻近度，质心和目标函数</strong>的选择：</h3><p>邻近度函数：曼哈顿距离。质心：中位数。目标函数：最小化对象到其簇质心的距离和。</p><p>邻近度函数：平方欧几里德距离。质心：均值。目标函数：最小化对象到其簇质心的距离的平方和。</p><p>邻近度函数：余弦。质心：均值。最大化对象与其质心的余弦相似度和。</p><p>邻近度函数：Bregman散度。质心：均值。目标函数：最小化对象到其簇质心的Bregman散度和。</p><p>由于基本K均值算法采取随机地选取初始质心的办法，导致最后形成的簇的质量常常很糟糕。在此基础上引出了基本K均值算法的扩充：<strong>二分K均值算法</strong>。二分K均值算法不太受初始化问题的影响。 </p><h3 id="二分K均值算法"><a href="#二分K均值算法" class="headerlink" title="二分K均值算法"></a>二分K均值算法</h3><ol><li><p>把所有数据作为一个cluster加入cluster list</p></li><li><p>repeat</p></li><li><p>​       从cluster list中挑选出一个SSE最大的cluster来进行划分</p></li><li><p>​       for i=1 to预设的循环次数</p></li><li><p>​                       用基本K均值算法把挑选出来的cluster划分成两个子cluster</p></li><li><p>​                       计算两个子cluster的SSE和。</p></li><li><p>​       end for</p></li><li><p>​      把for循环中SSE和最小的那两个子cluster加入cluster list</p></li><li><p><strong>until</strong>  cluster list拥有K个cluster</p></li></ol><p>除此以外，每次划分不止执行一次基本K均值算法，而是预先设置一个ITER值，然后对这个cluster进行ITER次执行基本K均值运算。因为基本K均值每次一开始都是随机选K个质心来执行，所以i一般来说ITER次执行基本K均值，每次都会得到不同的两个cluster。那么应该选哪对cluster来作为划分以后的cluster呢？答案就是在每次循环中，每次都计算当次基本K均值划分出来的两个cluster的SSE和，最后就选SSE和最小的那对cluster作为划分以后的cluster。 </p><h3 id="学习向量化"><a href="#学习向量化" class="headerlink" title="学习向量化"></a>学习向量化</h3><p>与k均值算法类似，“学习向量量化”（Learning Vector Quantization，简称LVQ）也是试图找到一组原型向量来刻画聚类结构，但与一般的聚类算法不同的是，LVQ假设数据样本带有类别标记，学习过程用样本的这些监督信息来辅助聚类。 </p><h3 id="学习向量化算法"><a href="#学习向量化算法" class="headerlink" title="学习向量化算法"></a>学习向量化算法</h3><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20200530230223.png" srcset="/img/loading.gif" alt="image"></p><h3 id="高斯混合模型"><a href="#高斯混合模型" class="headerlink" title="高斯混合模型"></a>高斯混合模型</h3><p>高斯混合聚类使用了一个很流行的算法：GMM(Gaussian Mixture Model)。高斯混合聚类与k均值聚类类似，但是采用了<strong>概率模型</strong>来表达聚类原型。每个高斯模型（Gaussian Model）就代表了一个簇（类）。GMM是单一高斯概率密度函数的延伸，能够平滑地近似任意形状的密度分布。在高斯混合聚类中，每个GMM会由k个高斯模型分布组成，每个高斯模型被称为一个component，这些component线性加乘在一起就组成了GMM的。</p><p>简单地来说，k-Means的结果是每个数据点没分配到其中某一个cluster,而GMM则给出的是这个数据点被分配到每个cluster的概率，又称为soft assignment。 </p><h3 id="高斯混合模型聚类算法"><a href="#高斯混合模型聚类算法" class="headerlink" title="高斯混合模型聚类算法"></a>高斯混合模型聚类算法</h3><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20200530230224.png" srcset="/img/loading.gif" alt="image"></p><h3 id="层次聚类"><a href="#层次聚类" class="headerlink" title="层次聚类"></a>层次聚类</h3><p>层次聚类（hierarchical clustering）试图在不同层次对数据集进行划分，从而形成树形的聚类结构。数据集的划分可采用“自底向上”的聚合策略，也可采用“自顶向下”的分拆策略。 典型的<strong>AGNES</strong>是一种采用自底向上聚合策略的层次聚类算法。它先将数据集中的每个样本看作一个初始聚类簇，然后在算法运行的每一步中找出距离最近的两个聚类簇进行合并，该过程不断重复，直至达到预设的聚类簇个数。 </p><h4 id="有两种产生层次聚类的基本方法："><a href="#有两种产生层次聚类的基本方法：" class="headerlink" title="有两种产生层次聚类的基本方法："></a>有两种产生层次聚类的基本方法：</h4><ol><li>凝聚的。从点作为个体簇开始，每一步合并两个最接近的簇。这需要定义簇的临近性概念。凝聚层次聚类技术最常见。</li><li>分裂的。从包含所有点的某个簇开始，每一步分裂一个簇，直到仅剩下单点簇。在这种情况下，我们需要确定每一步分裂哪个簇，以及如何分裂。</li><li></li></ol><h3 id="层次聚类算法"><a href="#层次聚类算法" class="headerlink" title="层次聚类算法"></a>层次聚类算法</h3><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20200530230225.png" srcset="/img/loading.gif" alt="image"></p><h4 id="基本凝聚层次聚类算法："><a href="#基本凝聚层次聚类算法：" class="headerlink" title="基本凝聚层次聚类算法："></a>基本凝聚层次聚类算法：</h4><ol><li>如果需要，计算临近度矩阵</li><li>repeat</li><li>​        合并最接近的两个簇</li><li>​        更新临近度矩阵，以反映新的簇与原来的簇之间的临近性。</li><li>until 仅剩下一个簇</li></ol><h4 id="簇之间的临近性有3种定义方式："><a href="#簇之间的临近性有3种定义方式：" class="headerlink" title="簇之间的临近性有3种定义方式："></a>簇之间的临近性有3种定义方式：</h4><ol><li>MIN（单链）。不同簇中的两个最近的点之间的距离作为临近度。</li><li>MAX（全链）。不同簇中的两个最远的点之间的距离作为临近度。</li><li>GROUP（组平均）。取自不同簇的所有点对距离的平均值作为临近度。</li></ol><h4 id="注意："><a href="#注意：" class="headerlink" title="注意："></a>注意：</h4><ol><li>簇与簇合并的原则永远是dist最小。</li><li>但在计算dist值的时候，可以采用MIN, MAX, GROUP AVG 3中方式得出dist的值。</li></ol><h3 id="密度聚类"><a href="#密度聚类" class="headerlink" title="密度聚类"></a>密度聚类</h3><p>密度聚类亦称“基于密度的聚类”，假设聚类结构能通过样本分布的紧密程度确定。典型的代表算法为<strong>DBSCAN算法</strong>，它基于一组“领域”（neighborhood）参数来刻画样本分布的紧密程度。</p><p>DBSCAN将“簇”定义为：由密度可达关系导出的最大的密度相连样本集合。</p><h3 id="DBSCAN算法"><a href="#DBSCAN算法" class="headerlink" title="DBSCAN算法"></a>DBSCAN算法</h3><p>基于密度的聚类寻找被低密度区域分离的高密度区域。DBSCAN是一种简单、有效的基于密度的聚类算法。</p><h4 id="DBSCAN算法："><a href="#DBSCAN算法：" class="headerlink" title="DBSCAN算法："></a>DBSCAN算法：</h4><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20200530230226.png" srcset="/img/loading.gif" alt="image"></p><ol><li>将所有点标记为核心点、边界点或噪声点。</li><li>删除噪声点。</li><li>为距离在Eps之内的所有核心点之间连线。</li><li>每组连通的核心点形成一个簇。</li><li>将每个边界点指派到一个与之关联的核心点的簇中。</li></ol><h4 id="DBSCAN算法阐释："><a href="#DBSCAN算法阐释：" class="headerlink" title="DBSCAN算法阐释："></a>DBSCAN算法阐释：</h4><ol><li>算法需要用户输入2个参数： 半径Eps; 最小（少）点值MinPts。</li><li>确定Eps和MinPts需要用到K-距离的概念。K-距离就是“到第K近的点的距离”，按经验一般取值为4。并且，一般取K的值为MinPts参数的值。</li><li>首先计算每个点到所有其余点的欧式距离，升序排序后，选出每个点的“K距离”。</li><li>所有点的K距离形成一个集合D。对D进行升序排序，依此可以形成一个样本数据的K距离图。</li><li>图中急剧变化处的值，即为Eps。</li><li>根据Eps和MinPts，计算出所有的核心点。</li><li>给核心点到小于Eps的另一个核心点赋予一个连线，到核心点的距离等于Eps的点被识别为边界点。最后，核心点、边界点之外的点都是噪声点。</li><li>将能够连线的点和与之关联的边界点都放到一起，形成了一个簇。</li></ol><h3 id="几种聚类的优缺点"><a href="#几种聚类的优缺点" class="headerlink" title="几种聚类的优缺点"></a>几种聚类的优缺点</h3><h4 id="层次聚类的优缺点："><a href="#层次聚类的优缺点：" class="headerlink" title="层次聚类的优缺点："></a><strong>层次聚类的优缺点</strong>：</h4><p>优点：</p><ol><li>距离和规则的相似度容易定义，限制少；</li><li>不需要预先指定聚类数；</li><li>可以发现类的层次关系；</li><li>可以聚类成其他形状。</li></ol><p>缺点：</p><ol><li>计算复杂度太高；</li><li>奇异值也能产生很大影响；</li><li>算法很可能聚类成链状。</li></ol><h4 id="DBSCAN的优缺点："><a href="#DBSCAN的优缺点：" class="headerlink" title="DBSCAN的优缺点："></a><strong>DBSCAN的优缺点</strong>：</h4><p>优点：</p><ol><li>不需要事先知道要形成的簇的数量。</li><li>可以发现任意形状的簇类。</li><li>对噪声点不敏感。</li><li>对样本点的顺序不敏感。</li></ol><p>缺点：</p><ol><li>簇的密度变化太大时，DBSCAN会有麻烦。</li><li>对于高维数据，密度定义困难，DBSCAN也有问题。</li></ol><p>注意：</p><ol><li>K均值对于圆形区域聚类的效果很好，DBSCAN基于密度，对于集中区域效果很好。</li><li>对于不规则形状，K均值完全无法使用。DBSCAN可以起到很好的效果。</li></ol><h4 id="K均值的优缺点："><a href="#K均值的优缺点：" class="headerlink" title="K均值的优缺点："></a><strong>K均值的优缺点</strong>：</h4><p>优点：</p><ol><li>简单，易于理解和实现。</li><li>时间复杂度低。</li></ol><p>缺点：</p><ol><li>要手工输入K值，对初始值的设置很敏感。</li><li>对噪声和离群点很敏感。</li><li>只用于数值型数据，不适用于categorical类型的数据。</li><li>不能解决非凸数据。</li><li>主要发现圆形或者球形簇，不能识别非球形的簇。</li></ol>]]></content>
    
    
    <categories>
      
      <category>机器学习</category>
      
    </categories>
    
    
    <tags>
      
      <tag>机器学习</tag>
      
      <tag>无监督学习</tag>
      
      <tag>聚类</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>机器学习-支持向量机软间隔和正则化</title>
    <link href="/2018/07/ml-svm-L1/"/>
    <url>/2018/07/ml-svm-L1/</url>
    
    <content type="html"><![CDATA[<h3 id="软间隔和正则化"><a href="#软间隔和正则化" class="headerlink" title="软间隔和正则化"></a>软间隔和正则化</h3><p>我们在最开始讨论支持向量机的时候，我们就假定数据在样本空间是线性可分的，也就是我们可以找到一个可行的超平面将数据完全分开。后来为了处理非线性数据，我们又推出使用 Kernel 方法对原来的线性 SVM 进行了推广，使得非线性的的情况也能处理。虽然通过映射$\Phi(x)$将原始数据映射到高维空间之后，使得数据集在特征空间中线性可分，但是也很难断定这个貌似线性可分的结果是不是由于<strong>过拟合</strong>造成的。</p><h3 id="软间隔的概念"><a href="#软间隔的概念" class="headerlink" title="软间隔的概念"></a>软间隔的概念</h3><p>为了缓解该问题，我们允许支持向量机在一些样本上出错。如图所示：</p><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20200530230212.png" srcset="/img/loading.gif" alt="image"></p><p>前面我们所介绍的支持向量机的形式是每个样本必须分正确，这是<strong>“硬间隔”</strong>，而<strong>软间隔</strong>就是允许一些样本不满足约束.</p><h3 id="软间隔分类器"><a href="#软间隔分类器" class="headerlink" title="软间隔分类器"></a>软间隔分类器</h3><h5 id="软间隔分类器-soft-margin-classifier-可以解决两种情况"><a href="#软间隔分类器-soft-margin-classifier-可以解决两种情况" class="headerlink" title="软间隔分类器(soft margin classifier)可以解决两种情况."></a>软间隔分类器(soft margin classifier)可以解决两种情况.</h5><p>前面我们都假定数据是线性可分的, 但实际上数据即使映射到了高维也不一定是线性可分. 这个时候就要对超平面进行一个调整, 即这里所说的软间隔. </p><p>另一种情况是即使数据是线性可分的, 但数据中可能存在噪点. 而如果按照前面那种常规处理的话, 这些噪点会对我们的结果造成很大的影响.这个时候也是需要使用软间隔来尽可能减少噪点对我们的影响. </p><p>如下图所示, 如果数据是线性可分并且不存在噪点的话, 我们可以找到一个完美的分类超平面: </p><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20200530230213.png" srcset="/img/loading.gif" alt="image"></p><p>但是, 如果数据中出现了一个噪点并且仍然是线性可分, 如果我们还是按照之前的办法处理, 那么我们就会得到如下的分类超平面, 这明显是不合理的，这也就出现了过拟合的状况. </p><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20200530230214.png" srcset="/img/loading.gif" alt="image"></p><h3 id="软间隔支持向量机："><a href="#软间隔支持向量机：" class="headerlink" title="软间隔支持向量机："></a>软间隔支持向量机：</h3><p>在硬间隔最大化的目标函数中添加<strong>松弛变量</strong>和<strong>惩罚参数</strong>，在约束条件中添加<strong>松弛变量</strong>，即 </p><h4 id="min-w-b-frac-1-2-w-2-C-sum-i-1-n-xi-i"><a href="#min-w-b-frac-1-2-w-2-C-sum-i-1-n-xi-i" class="headerlink" title="$min_{w,b}\frac{1}{2}{||w||}^2+C\sum_{i=1}^n \xi_i$"></a>$min_{w,b}\frac{1}{2}{||w||}^2+C\sum_{i=1}^n \xi_i$</h4><h4 id="s-t-y-i-w-Tx-b-ge1-xi-i-xi-i-ge0-i-1-2-cdot-cdot-cdot-m"><a href="#s-t-y-i-w-Tx-b-ge1-xi-i-xi-i-ge0-i-1-2-cdot-cdot-cdot-m" class="headerlink" title="$s.t.y_i(w^Tx+b)\ge1-\xi_i,\xi_i\ge0, i=1,2,\cdot \cdot \cdot m$"></a>$s.t.y_i(w^Tx+b)\ge1-\xi_i,\xi_i\ge0, i=1,2,\cdot \cdot \cdot m$</h4><h3 id="注意："><a href="#注意：" class="headerlink" title="注意："></a>注意：</h3><ul><li><h4 id="要使间隔尽量大，那-frac-1-2-w-2-尽量小"><a href="#要使间隔尽量大，那-frac-1-2-w-2-尽量小" class="headerlink" title="要使间隔尽量大，那$\frac{1}{2}{||w||}^2$尽量小"></a>要使间隔尽量大，那$\frac{1}{2}{||w||}^2$尽量小</h4></li><li><h4 id="误分类点的个数尽量小，即松弛变量-xi-i-尽量小，-C-为调和两者的系数"><a href="#误分类点的个数尽量小，即松弛变量-xi-i-尽量小，-C-为调和两者的系数" class="headerlink" title="误分类点的个数尽量小，即松弛变量$\xi_i$尽量小，$C$为调和两者的系数"></a>误分类点的个数尽量小，即松弛变量$\xi_i$尽量小，$C$为调和两者的系数</h4></li></ul><h3 id="构建拉格朗日函数"><a href="#构建拉格朗日函数" class="headerlink" title="构建拉格朗日函数"></a>构建拉格朗日函数</h3><p>在<strong>软间隔支持向量机</strong>中每个样本都有一个对应的松弛变量，用来表示该样本不满足约束条件:$y_i(w^Tx+b)\ge1$</p><p>但是这个函数同样是一个二次规划问题，我们就可以通过拉格朗日乘子法就可以得到拉格朗日函数。</p><h4 id="L-w-b-a-xi-u-frac-1-2-w-2-C-sum-i-1-n-xi-i-sum-i-1-na-i-1-xi-i-y-i-w-Tx-b-sum-i-1-nu-i-xi-i"><a href="#L-w-b-a-xi-u-frac-1-2-w-2-C-sum-i-1-n-xi-i-sum-i-1-na-i-1-xi-i-y-i-w-Tx-b-sum-i-1-nu-i-xi-i" class="headerlink" title="$L(w,b,a,\xi,u)=\frac{1}{2}{||w||}^2+C\sum_{i=1}^n \xi_i+\sum_{i=1}^na_i(1-\xi_i- y_i(w^Tx+b))-\sum_{i=1}^nu_i\xi_i$"></a>$L(w,b,a,\xi,u)=\frac{1}{2}{||w||}^2+C\sum_{i=1}^n \xi_i+\sum_{i=1}^na_i(1-\xi_i- y_i(w^Tx+b))-\sum_{i=1}^nu_i\xi_i$</h4><p>其中$a_i\ge0, u_i\ge0$是拉格朗日的乘子。</p><p>令$L(w,b,a,\xi,u)$对$w,b,\xi_i$的偏导为零可得：</p><p>求导方法就按照<a href="https://sevenold.github.io/2018/07/ml-svm/" target="_blank" rel="noopener">支持向量机的算法推导</a>来进行：</p><h4 id="w-sum-i-1-na-ix-iy-i"><a href="#w-sum-i-1-na-ix-iy-i" class="headerlink" title="$w=\sum_{i=1}^na_ix_iy_i$"></a>$w=\sum_{i=1}^na_ix_iy_i$</h4><h4 id="sum-i-1-na-iy-i-0"><a href="#sum-i-1-na-iy-i-0" class="headerlink" title="$\sum_{i=1}^na_iy_i=0$"></a>$\sum_{i=1}^na_iy_i=0$</h4><h4 id="C-a-i-u-i"><a href="#C-a-i-u-i" class="headerlink" title="$C=a_i+u_i$"></a>$C=a_i+u_i$</h4><h3 id="对偶问题"><a href="#对偶问题" class="headerlink" title="对偶问题"></a>对偶问题</h3><p>类似线性可分支持向量机中做法，引入拉格朗日乘子并构建拉格朗日函数，利用拉格朗日对偶性，问题转化为求解对偶问题 </p><h4 id="max-a-sum-i-1-na-i-frac12-sum-i-1-j-1-na-ia-j-x-i-Tx-jy-iy-j"><a href="#max-a-sum-i-1-na-i-frac12-sum-i-1-j-1-na-ia-j-x-i-Tx-jy-iy-j" class="headerlink" title="$max_a \sum_{i=1}^na_i-\frac12\sum_{i=1,j=1}^na_ia_j{x_i}^Tx_jy_iy_j$"></a>$max_a \sum_{i=1}^na_i-\frac12\sum_{i=1,j=1}^na_ia_j{x_i}^Tx_jy_iy_j$</h4><h4 id="s-t-0-le-a-i-le-C-i-1-cdot-cdot-cdot-n"><a href="#s-t-0-le-a-i-le-C-i-1-cdot-cdot-cdot-n" class="headerlink" title="$s.t. ,0\le a_i\le C, i=1,\cdot \cdot \cdot n$"></a>$s.t. ,0\le a_i\le C, i=1,\cdot \cdot \cdot n$</h4><h4 id="sum-i-1-na-iy-i-0-1"><a href="#sum-i-1-na-iy-i-0-1" class="headerlink" title="$\sum_{i=1}^na_iy_i=0$"></a>$\sum_{i=1}^na_iy_i=0$</h4><p>与前面所推导的<strong>硬间隔</strong>下的对偶问题，两者的差别就在于对偶变量的约束不同，前者是$0\le a_i$，后者是$0\le a_i\le C$。后续同样可以引入核函数得到对应的支持向量展开式。</p><h3 id="软间隔支持向量机KKT条件"><a href="#软间隔支持向量机KKT条件" class="headerlink" title="软间隔支持向量机KKT条件"></a>软间隔支持向量机KKT条件</h3><h4 id="left-lbrace-begin-aligned-a-i-ge0-u-i-ge0-y-i-w-Tx-i-b-1-xi-i-ge-0-a-i-y-i-w-Tx-i-b-1-xi-i-0-xi-i-ge0-u-i-xi-i-0-end-aligned-right"><a href="#left-lbrace-begin-aligned-a-i-ge0-u-i-ge0-y-i-w-Tx-i-b-1-xi-i-ge-0-a-i-y-i-w-Tx-i-b-1-xi-i-0-xi-i-ge0-u-i-xi-i-0-end-aligned-right" class="headerlink" title="$ \left\lbrace\ \begin{aligned} a_i \ge0 , u_i\ge0\\ y_i(w^Tx_i+b) -1+\xi_i\ge 0 \\ a_i(y_i(w^Tx_i+b)-1+\xi_i)=0 \\ \xi_i\ge0, u_i\xi_i=0 \end{aligned} \right. $"></a>$ \left\lbrace\ \begin{aligned} a_i \ge0 , u_i\ge0\\ y_i(w^Tx_i+b) -1+\xi_i\ge 0 \\ a_i(y_i(w^Tx_i+b)-1+\xi_i)=0 \\ \xi_i\ge0, u_i\xi_i=0 \end{aligned} \right. $</h4><h3 id="合页损失函数"><a href="#合页损失函数" class="headerlink" title="合页损失函数"></a>合页损失函数</h3><p>间隔/线性支持向量机的原始问题可以等价于添加了<strong>正则化项的合页损失函数</strong>，即最小化以下目标函数 </p><h4 id="min-w-b-sum-i-1-n-1-y-i-w-Tx-b-lambda-w-2"><a href="#min-w-b-sum-i-1-n-1-y-i-w-Tx-b-lambda-w-2" class="headerlink" title="$min_{w,b}\sum_{i=1}^n[1- y_i(w^Tx+b)]_++\lambda||w||^2$"></a>$min_{w,b}\sum_{i=1}^n[1- y_i(w^Tx+b)]_++\lambda||w||^2$</h4><ul><li>第一项为合页损失函数 $L(y(w^Tx+b))=[1- y_i(w^Tx+b)]<em>+$，我们再结合前面我在进行数据分割的约束条件，一般对于函数$[z]</em>+$就有：</li></ul><h4 id="z-z-如果z-gt-0"><a href="#z-z-如果z-gt-0" class="headerlink" title="$[z]_+=z, 如果z&gt;0$"></a>$[z]_+=z, 如果z&gt;0$</h4><h4 id="z-0-如果z-lt-0"><a href="#z-0-如果z-lt-0" class="headerlink" title="$[z]_+=0, 如果z&lt;0$"></a>$[z]_+=0, 如果z&lt;0$</h4><p>​           这样原式就能表明当样本点$x_i,y_i$被正确分类且<strong>函数间隔</strong>$y_i(w^Tx+b)&gt;1$时损失为0，</p><p>​                                                                                                                                         否则损失就为$1-y_i(w^Tx+b)$</p><ul><li>第二项为正则化项，是系数为λ的w的L2范数</li></ul><h4 id="线性支持向量机的原始最优化问题与上述目标函数的最优化问题是等价的"><a href="#线性支持向量机的原始最优化问题与上述目标函数的最优化问题是等价的" class="headerlink" title="线性支持向量机的原始最优化问题与上述目标函数的最优化问题是等价的"></a><strong>线性支持向量机的原始最优化问题与上述目标函数的最优化问题是等价的</strong></h4><h4 id="我们把一些损失函数做一个对比："><a href="#我们把一些损失函数做一个对比：" class="headerlink" title="我们把一些损失函数做一个对比："></a>我们把一些损失函数做一个对比：</h4><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20200530230215.png" srcset="/img/loading.gif" alt="image"></p><p>如图所示为常用的一些损失函数，可以看到，各个图中损失函数的曲线基本位于0-1损失函数的上方，所以可以作为0-1损失函数的上界；</p><ul><li><p>由于0-1损失函数不是连续可导的，直接优化由其构成的目标损失函数比较困难，所以对于svm而言，可以认为是在优化由0-1损失函数的上界(合页损失函数)构成的目标函数，又称为<strong>代理损失函数</strong></p></li><li><p>合页损失函数对学习有更高的要求</p></li></ul><h3 id="常用替代损失函数"><a href="#常用替代损失函数" class="headerlink" title="常用替代损失函数"></a>常用替代损失函数</h3><p>  通常具有较好的数学性质，比如凸的连续函数且是0/1损失函数的上界</p><ul><li><h4 id="hinge损失-：-l-hinge-z-max-0-1-z"><a href="#hinge损失-：-l-hinge-z-max-0-1-z" class="headerlink" title="hinge损失  ： $l_{hinge}(z)=max(0,1-z)$"></a>hinge损失  ： $l_{hinge}(z)=max(0,1-z)$</h4></li><li><h4 id="指数损失：-l-exp-z-exp-z"><a href="#指数损失：-l-exp-z-exp-z" class="headerlink" title="指数损失： $l_{exp}(z)=exp(-z)$"></a>指数损失： $l_{exp}(z)=exp(-z)$</h4></li><li><h4 id="对率损失：-l-log-z-log-1-exp-z"><a href="#对率损失：-l-log-z-log-1-exp-z" class="headerlink" title="对率损失： $l_{log}(z)=log(1+exp(-z))$"></a>对率损失： $l_{log}(z)=log(1+exp(-z))$</h4></li></ul><h3 id="SVM和Logistic-Regression对比"><a href="#SVM和Logistic-Regression对比" class="headerlink" title="SVM和Logistic Regression对比"></a>SVM和Logistic Regression对比</h3><p><strong>相同点</strong>：优化目标相近，通常情形下性能也相当</p><p><strong>不同点</strong>：</p><ul><li>LR的优势在于其输出具有自然的概率意义，给出预测标记的同时也给出了概率，而SVM要想得到概率输出需特殊处理</li><li>LR能直接用于多分类任务，而SVM则需要进行推广</li><li>SVM的解具有稀疏性（仅依赖于支持向量），LR对应的对率损失则是光滑的单调递减函数，因此LR的解依赖于更多的训练样本，其预测开销更大</li></ul>]]></content>
    
    
    <categories>
      
      <category>机器学习</category>
      
    </categories>
    
    
    <tags>
      
      <tag>机器学习</tag>
      
      <tag>支持向量机</tag>
      
      <tag>软间隔和正则化</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>机器学习-支持向量机核函数</title>
    <link href="/2018/07/ml-svm-kernel/"/>
    <url>/2018/07/ml-svm-kernel/</url>
    
    <content type="html"><![CDATA[<h3 id="SVM回顾"><a href="#SVM回顾" class="headerlink" title="SVM回顾"></a>SVM回顾</h3><p>上文<a href="https://sevenold.github.io/2018/07/ml-svm/" target="_blank" rel="noopener">支持向量机SVM</a> ，简单总结了对于<strong>线性可分</strong>数据的SVM的算法原理，现在我们对于<strong>非线性可分</strong>以及有噪声存在的时候我们需要对基本SVM算法的改进进行下总结其中包括: </p><ul><li><p>核函数在SVN算法中的使用</p></li><li><p>引入松弛变量和惩罚函数的软间隔分类器</p><p>我们再回顾一下我们上次推导最终的对偶优化问题，我们后面的改进和优化都是在对偶问题形式上展开的。</p></li></ul><h3 id="SVM标准形式"><a href="#SVM标准形式" class="headerlink" title="SVM标准形式"></a>SVM标准形式</h3><h4 id="min-w-b-frac-1-2-w-2"><a href="#min-w-b-frac-1-2-w-2" class="headerlink" title="$min_{w,b}\frac{1}{2}{||w||}^2$"></a>$min_{w,b}\frac{1}{2}{||w||}^2$</h4><h4 id="s-t-y-i-w-Tx-b-ge1-i-1-2-cdot-cdot-cdot-m"><a href="#s-t-y-i-w-Tx-b-ge1-i-1-2-cdot-cdot-cdot-m" class="headerlink" title="$s.t.y_i(w^Tx+b)\ge1, i=1,2,\cdot \cdot \cdot m$"></a>$s.t.y_i(w^Tx+b)\ge1, i=1,2,\cdot \cdot \cdot m$</h4><h3 id="对偶形式"><a href="#对偶形式" class="headerlink" title="对偶形式"></a>对偶形式</h3><h4 id="max-a-sum-i-1-na-i-frac12-sum-i-1-j-1-na-ia-j-x-i-Tx-jy-iy-j"><a href="#max-a-sum-i-1-na-i-frac12-sum-i-1-j-1-na-ia-j-x-i-Tx-jy-iy-j" class="headerlink" title="$max_a \sum_{i=1}^na_i-\frac12\sum_{i=1,j=1}^na_ia_j{x_i}^Tx_jy_iy_j$"></a>$max_a \sum_{i=1}^na_i-\frac12\sum_{i=1,j=1}^na_ia_j{x_i}^Tx_jy_iy_j$</h4><h4 id="s-t-a-i-ge0-i-1-cdot-cdot-cdot-n"><a href="#s-t-a-i-ge0-i-1-cdot-cdot-cdot-n" class="headerlink" title="$s.t. ,a_i\ge0, i=1,\cdot \cdot \cdot n$"></a>$s.t. ,a_i\ge0, i=1,\cdot \cdot \cdot n$</h4><h4 id="sum-i-1-na-iy-i-0"><a href="#sum-i-1-na-iy-i-0" class="headerlink" title="$\sum_{i=1}^na_iy_i=0$"></a>$\sum_{i=1}^na_iy_i=0$</h4><h3 id="SVM预测模型"><a href="#SVM预测模型" class="headerlink" title="SVM预测模型"></a>SVM预测模型</h3><p>SVM通过分割超平面$w^Tx+b$来获取未知数据的类型，将上述的$w和b$替换就可以得到：</p><h4 id="h-w-b-x-g-w-Tx-b-g-sum-i-1-n-a-iy-i-x-i-Tx-b"><a href="#h-w-b-x-g-w-Tx-b-g-sum-i-1-n-a-iy-i-x-i-Tx-b" class="headerlink" title="$h_{w,b}(x)=g(w^Tx+b)=g(\sum_{i=1}^n a_iy_i{x_i}^Tx+b)$"></a>$h_{w,b}(x)=g(w^Tx+b)=g(\sum_{i=1}^n a_iy_i{x_i}^Tx+b)$</h4><p>通过$g(x)来输出+1还是-1$来获取未知数据的类型</p><h3 id="核函数"><a href="#核函数" class="headerlink" title="核函数"></a>核函数</h3><p> 前面我在推导SVM算法的时候解决的一般是线性可分的数据，而对于非线性可分的数据的时候（如图），我们就需要引出核函数</p><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20200530230157.png" srcset="/img/loading.gif" alt="image"></p><p>所以对于这类问题，SVM的处理方法就是选择一个核函数，其通过将数据映射到更高维的特征空间（非线性映射），使得样本在这个特征空间内线性可分，从而解决原始空间中线性不可分的问题。</p><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20200530230158.png" srcset="/img/loading.gif" alt="image"></p><p>从数据上来看就是把数据映射到多维：例如从一维映射到四维：</p><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20200530230159.png" srcset="/img/loading.gif" alt="image"></p><p>这里是通过$\Phi(x)$把数据映射到高维空间，所以对应的对偶问题就改写为：</p><h4 id="max-a-sum-i-1-na-i-frac12-sum-i-1-j-1-na-ia-j-Phi-x-i-T-Phi-x-j）y-iy-j"><a href="#max-a-sum-i-1-na-i-frac12-sum-i-1-j-1-na-ia-j-Phi-x-i-T-Phi-x-j）y-iy-j" class="headerlink" title="$max_a \sum_{i=1}^na_i-\frac12\sum_{i=1,j=1}^na_ia_j{\Phi(x_i)}^T\Phi(x_j）y_iy_j$"></a>$max_a \sum_{i=1}^na_i-\frac12\sum_{i=1,j=1}^na_ia_j{\Phi(x_i)}^T\Phi(x_j）y_iy_j$</h4><h4 id="s-t-a-i-ge0-i-1-cdot-cdot-cdot-n-1"><a href="#s-t-a-i-ge0-i-1-cdot-cdot-cdot-n-1" class="headerlink" title="$s.t. ,a_i\ge0, i=1,\cdot \cdot \cdot n$"></a>$s.t. ,a_i\ge0, i=1,\cdot \cdot \cdot n$</h4><h4 id="sum-i-1-na-iy-i-0-1"><a href="#sum-i-1-na-iy-i-0-1" class="headerlink" title="$\sum_{i=1}^na_iy_i=0$"></a>$\sum_{i=1}^na_iy_i=0$</h4><p>${\Phi(x_i)}^T\Phi(x_j$,是我们把样本$x_i和 x_j$映射到特征空间之后的内积，但是由于特征空间的维数可能很高，甚至是无穷维，所以直接计算${\Phi(x_i)}^T\Phi(x_j）$是非常困难的，但是我们有又必须要计算他，所以为了避开这个障碍，我们就设一个函数：</p><h4 id="k-x-i-x-j-Phi-x-i-T-Phi-x-j）"><a href="#k-x-i-x-j-Phi-x-i-T-Phi-x-j）" class="headerlink" title="$k(x_i,x_j)={\Phi(x_i)}^T\Phi(x_j）$"></a>$k(x_i,x_j)={\Phi(x_i)}^T\Phi(x_j）$</h4><p>我们就直接通过函数$h(\cdot, \cdot)$计算获得${\Phi(x_i)}^T\Phi(x_j）$的结果，就不必直接去计算高维甚至无穷维的特征空间中的内积。于是对偶问题和预测模型就改写为：</p><h4 id="max-a-sum-i-1-na-i-frac12-sum-i-1-j-1-na-ia-jk-x-i-x-j-y-iy-j"><a href="#max-a-sum-i-1-na-i-frac12-sum-i-1-j-1-na-ia-jk-x-i-x-j-y-iy-j" class="headerlink" title="$max_a \sum_{i=1}^na_i-\frac12\sum_{i=1,j=1}^na_ia_jk(x_i,x_j)y_iy_j$"></a>$max_a \sum_{i=1}^na_i-\frac12\sum_{i=1,j=1}^na_ia_jk(x_i,x_j)y_iy_j$</h4><h4 id="s-t-a-i-ge0-i-1-cdot-cdot-cdot-n-2"><a href="#s-t-a-i-ge0-i-1-cdot-cdot-cdot-n-2" class="headerlink" title="$s.t. ,a_i\ge0, i=1,\cdot \cdot \cdot n$"></a>$s.t. ,a_i\ge0, i=1,\cdot \cdot \cdot n$</h4><h4 id="sum-i-1-na-iy-i-0-2"><a href="#sum-i-1-na-iy-i-0-2" class="headerlink" title="$\sum_{i=1}^na_iy_i=0$"></a>$\sum_{i=1}^na_iy_i=0$</h4><h4 id="f-x-sum-i-1-n-a-iy-i-x-i-Tx-b-sum-i-1-n-a-iy-ik-x-x-i-b"><a href="#f-x-sum-i-1-n-a-iy-i-x-i-Tx-b-sum-i-1-n-a-iy-ik-x-x-i-b" class="headerlink" title="$f(x)=\sum_{i=1}^n a_iy_i{x_i}^Tx+b=\sum_{i=1}^n a_iy_ik(x,x_i)+b$"></a>$f(x)=\sum_{i=1}^n a_iy_i{x_i}^Tx+b=\sum_{i=1}^n a_iy_ik(x,x_i)+b$</h4><p>而这里的$k(\cdot, \cdot )$就是<strong>核函数</strong>（kernel function）.上述的$f(x)$显示出模型最优解可通过训练样本的核函数展开，这也就是<strong>支持向量展式</strong>。</p><h3 id="核函数定理"><a href="#核函数定理" class="headerlink" title="核函数定理"></a>核函数定理</h3><p>令$X$为输入空间，$k(\cdot , \cdot )$是定义在$X \times X$上的对称函数，则$k$是核函数当且仅当对于数据$D={x_1,x_2 ,\cdot \cdot \cdot x_n}$，“核矩阵”K就是总是一个半正定矩阵：</p><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20200530230200.png" srcset="/img/loading.gif" alt="image"></p><p>通俗来讲：只要一个对称函数所对应的核矩阵是半正定矩阵，它就能作为核函数。</p><h3 id="常用的核函数"><a href="#常用的核函数" class="headerlink" title="常用的核函数"></a>常用的核函数</h3><ul><li><h4 id="线性核：-k-x-i-x-j-x-i-Tx-j"><a href="#线性核：-k-x-i-x-j-x-i-Tx-j" class="headerlink" title="线性核：$k(x_i,x_j)={x_i}^Tx_j$"></a>线性核：$k(x_i,x_j)={x_i}^Tx_j$</h4></li><li><h4 id="多项式核：-k-x-i-x-j-x-i-Tx-j-n-n-ge1-为多项式的次数"><a href="#多项式核：-k-x-i-x-j-x-i-Tx-j-n-n-ge1-为多项式的次数" class="headerlink" title="多项式核：$k(x_i,x_j)=({x_i}^Tx_j)^n, n\ge1$为多项式的次数"></a>多项式核：$k(x_i,x_j)=({x_i}^Tx_j)^n, n\ge1$为多项式的次数</h4></li><li><h4 id="高斯核：-k-x-i-x-j-e-frac-x-i-x-j-2-2-sigma-2-sigma-gt-0-为高斯核的带宽（width）"><a href="#高斯核：-k-x-i-x-j-e-frac-x-i-x-j-2-2-sigma-2-sigma-gt-0-为高斯核的带宽（width）" class="headerlink" title="高斯核：$k(x_i,x_j)=e^{-\frac{||x_i-x_j||^2}{2\sigma^2}}, \sigma&gt;0$为高斯核的带宽（width）"></a>高斯核：$k(x_i,x_j)=e^{-\frac{||x_i-x_j||^2}{2\sigma^2}}, \sigma&gt;0$为高斯核的带宽（width）</h4></li><li><h4 id="拉普拉斯核：-k-x-i-x-j-e-frac-x-i-x-j-2-2-sigma-sigma-gt-0"><a href="#拉普拉斯核：-k-x-i-x-j-e-frac-x-i-x-j-2-2-sigma-sigma-gt-0" class="headerlink" title="拉普拉斯核：$k(x_i,x_j)=e^{-\frac{||x_i-x_j||^2}{2\sigma}}, \sigma&gt;0$"></a>拉普拉斯核：$k(x_i,x_j)=e^{-\frac{||x_i-x_j||^2}{2\sigma}}, \sigma&gt;0$</h4></li><li><h4 id="Sigmoid核：-k-x-i-x-j-tanh-beta-x-i-Tx-j-theta-，-tanh为双曲正切函数，-beta-gt-0-theta-gt-0"><a href="#Sigmoid核：-k-x-i-x-j-tanh-beta-x-i-Tx-j-theta-，-tanh为双曲正切函数，-beta-gt-0-theta-gt-0" class="headerlink" title="Sigmoid核：$k(x_i,x_j)=tanh(\beta{x_i}^Tx_j+\theta)  $，$tanh为双曲正切函数，$$\beta&gt;0, \theta&gt;0$"></a>Sigmoid核：$k(x_i,x_j)=tanh(\beta{x_i}^Tx_j+\theta)  $，$tanh为双曲正切函数，$$\beta&gt;0, \theta&gt;0$</h4></li></ul><h4 id="通过函数组合："><a href="#通过函数组合：" class="headerlink" title="通过函数组合："></a>通过函数组合：</h4><ul><li><h4 id="若-k-1-和-k-2-为核函数，则对任意正数-lambda-1-lambda-2-，其线性组合：-lambda-1k-1-lambda-2k-2"><a href="#若-k-1-和-k-2-为核函数，则对任意正数-lambda-1-lambda-2-，其线性组合：-lambda-1k-1-lambda-2k-2" class="headerlink" title="若$k_1$和$k_2$为核函数，则对任意正数$\lambda_1, \lambda_2$，其线性组合：$\lambda_1k_1+\lambda_2k_2$"></a>若$k_1$和$k_2$为核函数，则对任意正数$\lambda_1, \lambda_2$，其线性组合：$\lambda_1k_1+\lambda_2k_2$</h4></li><li><h4 id="若-k-1-和-k-2-为核函数，则核函数的直积：-k-1-otimes-k-2-x-z-k-1-x-z-k-2-x-z"><a href="#若-k-1-和-k-2-为核函数，则核函数的直积：-k-1-otimes-k-2-x-z-k-1-x-z-k-2-x-z" class="headerlink" title="若$k_1$和$k_2$为核函数，则核函数的直积：$k_1 \otimes k_2 (x,z)=k_1(x,z)k_2(x,z)$"></a>若$k_1$和$k_2$为核函数，则核函数的直积：$k_1 \otimes k_2 (x,z)=k_1(x,z)k_2(x,z)$</h4></li><li><h4 id="若-k-1-为核函数，则对于任意函数-g-x-k-x-z-g-x-k-1-x-z-g-z"><a href="#若-k-1-为核函数，则对于任意函数-g-x-k-x-z-g-x-k-1-x-z-g-z" class="headerlink" title="若$k_1$为核函数，则对于任意函数$g(x)$ :$k(x,z)=g(x)k_1(x,z)g(z)$"></a>若$k_1$为核函数，则对于任意函数$g(x)$ :$k(x,z)=g(x)k_1(x,z)g(z)$</h4></li></ul>]]></content>
    
    
    <categories>
      
      <category>机器学习</category>
      
    </categories>
    
    
    <tags>
      
      <tag>机器学习</tag>
      
      <tag>支持向量机</tag>
      
      <tag>核函数</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>机器学习-支持向量机</title>
    <link href="/2018/07/ml-svm/"/>
    <url>/2018/07/ml-svm/</url>
    
    <content type="html"><![CDATA[<h2 id="支持向量机SVM的概念及起源"><a href="#支持向量机SVM的概念及起源" class="headerlink" title="支持向量机SVM的概念及起源"></a>支持向量机SVM的概念及起源</h2><h3 id="什么是支持向量机SVM"><a href="#什么是支持向量机SVM" class="headerlink" title="什么是支持向量机SVM"></a>什么是支持向量机SVM</h3><p>支持向量机，因其英文名为support vector machine，故一般简称SVM，通俗来讲，它是一种二类分类模型，其基本模型定义为特征空间上的间隔最大的线性分类器，其学习策略便是间隔最大化，最终可转化为一个凸二次规划问题的求解。</p><h3 id="分类标准的起源：Logistic回归"><a href="#分类标准的起源：Logistic回归" class="headerlink" title="分类标准的起源：Logistic回归"></a>分类标准的起源：Logistic回归</h3><h4 id="我们先看看什么是线性分类器"><a href="#我们先看看什么是线性分类器" class="headerlink" title="我们先看看什么是线性分类器"></a>我们先看看什么是线性分类器</h4><p>给定一些数据点，它们分别属于两个不同的类，现在要找到一个线性分类器把这些数据分成两类。如果用$x$表示数据点，用$y$表示类别（$y=1$或者$y=-1$，分别代表两个不同的类），一个线性分类器的学习目标便是要在$n$维的数据空间中找到一个超平面（hyper plane），这个超平面的方程可以表示为（$w^T$中的T代表转置）：</p><h2 id="w-Tx-b-0"><a href="#w-Tx-b-0" class="headerlink" title="$w^Tx+b=0$"></a>$w^Tx+b=0$</h2><p><strong>Logistic回归</strong>目的是从特征学习出一个0/1分类模型，而这个模型是将特性的线性组合作为自变量，由于自变量的取值范围是负无穷到正无穷。因此，使用logistic函数（或称作sigmoid函数）将自变量映射到(0,1)上，映射后的值被认为是属于$y=1$的概率。</p><p>根据我们前面<a href="https://0520.tech/2018/07/23/2018-07-23-ml-logisticRegression/">Logistic回归的推导</a>  ，假设函数：</p><h2 id="h-theta-x-g-theta-Tx-frac1-1-e-theta-Tx"><a href="#h-theta-x-g-theta-Tx-frac1-1-e-theta-Tx" class="headerlink" title="$h_\theta(x)=g(\theta^Tx)=\frac1{1+e^{-\theta^Tx}}$"></a>$h_\theta(x)=g(\theta^Tx)=\frac1{1+e^{-\theta^Tx}}$</h2><p>其中x是n维特征向量，函数g就是Logistic函数。</p><h4 id="同样-g-z-frac-1-1-e-z-的图像："><a href="#同样-g-z-frac-1-1-e-z-的图像：" class="headerlink" title="同样$g(z)=\frac{1}{1+e^{-z}}$的图像："></a>同样$g(z)=\frac{1}{1+e^{-z}}$的图像：</h4><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20200530230137.png" srcset="/img/loading.gif" alt="images"></p><p>从图像中，我们就可以看出将无穷映射到了（0,1）。</p><p>而假设函数就是特征属于y=1的概率。 </p><h2 id="P-y-1-x-theta-h-theta-x-P-y-0-x-theta-1-h-theta-x"><a href="#P-y-1-x-theta-h-theta-x-P-y-0-x-theta-1-h-theta-x" class="headerlink" title="$P(y=1|x;\theta)=h_\theta(x) \\ P(y=0|x;\theta)=1-h_\theta(x)$"></a>$P(y=1|x;\theta)=h_\theta(x) \\ P(y=0|x;\theta)=1-h_\theta(x)$</h2><p>所以，我们在判别一个新的特征属于哪个类别的时候，就只需要求$h_\theta(x)$就可以了，由上面的图中可以看出如果$h_\theta(x)$ 大于0.5就是y=1的类，否则就是属于y=1的类。</p><h4 id="然后我们尝试给Logistic回归做一个变形。首先，将使用的结果标签y-0和y-1替换为-y-1-y-1-然后把-theta-0-替换成b，把-w-替换-theta-i-所以就变换为："><a href="#然后我们尝试给Logistic回归做一个变形。首先，将使用的结果标签y-0和y-1替换为-y-1-y-1-然后把-theta-0-替换成b，把-w-替换-theta-i-所以就变换为：" class="headerlink" title="然后我们尝试给Logistic回归做一个变形。首先，将使用的结果标签y=0和y=1替换为$y=-1,y=1 $,然后把$\theta_0$替换成b，把$w$替换$\theta_i$,所以就变换为："></a>然后我们尝试给Logistic回归做一个变形。首先，将使用的结果标签y=0和y=1替换为$y=-1,y=1 $,然后把$\theta_0$替换成b，把$w$替换$\theta_i$,所以就变换为：</h4><h2 id="h-theta-x-g-w-Tx-b"><a href="#h-theta-x-g-w-Tx-b" class="headerlink" title="$h_\theta(x)=g(w^Tx+b)$"></a>$h_\theta(x)=g(w^Tx+b)$</h2><h4 id="所以我们就可以假设函数：-h-w-b-x-g-w-Tx-b-中的g-z-做一个简化，将其简单映射到y-1和y-1上。映射关系如下："><a href="#所以我们就可以假设函数：-h-w-b-x-g-w-Tx-b-中的g-z-做一个简化，将其简单映射到y-1和y-1上。映射关系如下：" class="headerlink" title="所以我们就可以假设函数：$h_{w,b}(x)=g(w^Tx+b)$,中的g(z)做一个简化，将其简单映射到y=-1和y=1上。映射关系如下："></a>所以我们就可以假设函数：$h_{w,b}(x)=g(w^Tx+b)$,中的g(z)做一个简化，将其简单映射到y=-1和y=1上。映射关系如下：</h4><h2 id="g-z-left-lbrace-begin-aligned-1-amp-amp-z-ge0-1-amp-amp-z-lt-0-end-aligned-right"><a href="#g-z-left-lbrace-begin-aligned-1-amp-amp-z-ge0-1-amp-amp-z-lt-0-end-aligned-right" class="headerlink" title="$ g(z)=\left\lbrace  \begin{aligned} 1 &amp;&amp; z \ge0 \\ -1 &amp;&amp; z&lt;0 \end{aligned} \right. $"></a>$ g(z)=\left\lbrace  \begin{aligned} 1 &amp;&amp; z \ge0 \\ -1 &amp;&amp; z&lt;0 \end{aligned} \right. $</h2><h4 id="然后我们举个线性分类的例子来看看"><a href="#然后我们举个线性分类的例子来看看" class="headerlink" title="然后我们举个线性分类的例子来看看"></a>然后我们举个线性分类的例子来看看</h4><p>如下图所示，现在有一个二维平面，平面上有两种不同的数据，分别用圈和叉表示。由于这些数据是线性可分的，所以可以用一条直线将这两类数据分开，这条直线就相当于一个超平面，超平面一边的数据点所对应的y全是-1，另一边所对应的y全是1。 </p><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20200530230138.png" srcset="/img/loading.gif" alt="images"></p><p>这个超平面可以用分类函数$f(x)=w^Tx+b$来表示，当f(x)等于0时候，x便是位于超平面上的点，而f(x)大于0的点对应y=1的数据点，f(x)小于0的点对应y=-1的点，如下图所示： </p><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20200530230139.png" srcset="/img/loading.gif" alt="images"></p><p>接下来的问题是，如何确定这个超平面呢？从直观上而言，这个超平面应该是最适合分开两类数据的直线。而判定“最适合”的标准就是这条直线离直线两边的数据的间隔最大。所以，得寻找有着最大间隔的超平面。 </p><h3 id="间隔与支持向量"><a href="#间隔与支持向量" class="headerlink" title="间隔与支持向量"></a>间隔与支持向量</h3><p>SVM支持向量机（英文全称：support vector machine）是一个分类算法， 通过找到一个分类平面， 将数据分隔在平面两侧， 从而达到分类的目的。如下图所示， 直线表示的是训练出的一个分类平面， 将数据有效的分隔开。 </p><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20200530230140.png" srcset="/img/loading.gif" alt="images"></p><p>根据上面的逻辑，我们在做数据分隔的时候，有很多个分类平面，这时我们就需要找出“最优”的那一个平面模型，根据【超平面】【数据点】【分开】这几个词，我们可以想到最优的模型必然是最大程度地将数据点划分开的模型，不能靠近负样本也不能靠近正样本，要不偏不倚，并且与所有Support Vector的距离尽量大才可以。 </p><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20200530230141.png" srcset="/img/loading.gif" alt="images"></p><h4 id="上图中-x-0-是-x-在超平面上的投影，-ω-是超平面的法向量"><a href="#上图中-x-0-是-x-在超平面上的投影，-ω-是超平面的法向量" class="headerlink" title="上图中 $x_0$ 是 $x $在超平面上的投影，$ω$是超平面的法向量."></a>上图中 $x_0$ 是 $x $在超平面上的投影，$ω$是超平面的法向量.</h4><h4 id="超平面用线性方程来描述："><a href="#超平面用线性方程来描述：" class="headerlink" title="超平面用线性方程来描述："></a>超平面用线性方程来描述：</h4><h2 id="w-T-b-0"><a href="#w-T-b-0" class="headerlink" title="$w^T+b=0$"></a>$w^T+b=0$</h2><h3 id="函数间隔"><a href="#函数间隔" class="headerlink" title="函数间隔"></a>函数间隔</h3><p>在超平面$w ^T x + b = 0$确定的情况下，$|w^Tx+b|$表示点距离超平面的距离，而超平面作为二分类器，如果$w^Tx+b&gt;0$， 判断类别y为1, 否则判定为-1。从而引出函数间隔的定义：</p><h2 id="r-y-w-Tx-b-yf-x"><a href="#r-y-w-Tx-b-yf-x" class="headerlink" title="$r=y(w^Tx+b)=yf(x)$"></a>$r=y(w^Tx+b)=yf(x)$</h2><p>其中y是训练数据的类标记值， 如果$y(w^T x + b) &gt;0$说明，预测的值和标记的值相同， 分类正确，而且值越大，说明点离平面越远，分类的可靠程度更高。这是对单个样本的函数定义， 对整个样本集来说，要找到所有样本中间隔值最小的作为整个集合的函数间隔：</p><h2 id="r-min-r-i-i-1-2-cdot-cdot-cdot-n"><a href="#r-min-r-i-i-1-2-cdot-cdot-cdot-n" class="headerlink" title="$r=min \   r_i  , i=1,2 \cdot \cdot \cdot n$"></a>$r=min \   r_i  , i=1,2 \cdot \cdot \cdot n$</h2><p>即w和b同时缩小或放大M倍后，超平面并没有变化，但是函数间隔跟着w和b变化。所以，需要加入约束条件使得函数间隔固定, 也就是几何间隔。</p><h3 id="几何间隔"><a href="#几何间隔" class="headerlink" title="几何间隔"></a>几何间隔</h3><h4 id="样本空间-x-到超平面-x-0-的距离："><a href="#样本空间-x-到超平面-x-0-的距离：" class="headerlink" title="样本空间$x$到超平面$x_0$的距离："></a>样本空间$x$到超平面$x_0$的距离：</h4><h2 id="r-frac-w-Tx-b-w"><a href="#r-frac-w-Tx-b-w" class="headerlink" title="$r=\frac{|w^Tx+b|}{||w||}$"></a>$r=\frac{|w^Tx+b|}{||w||}$</h2><h4 id="如果超平面将样本成功分类，若-y-i-1-则有-w-Tx-b-gt-0-若-y-i-1-则有-w-Tx-b-lt-0-则下式成立"><a href="#如果超平面将样本成功分类，若-y-i-1-则有-w-Tx-b-gt-0-若-y-i-1-则有-w-Tx-b-lt-0-则下式成立" class="headerlink" title="如果超平面将样本成功分类，若$y_i=+1$,则有$w^Tx+b&gt;0$;若$y_i=-1$,则有$w^Tx+b&lt;0$;则下式成立"></a>如果超平面将样本成功分类，若$y_i=+1$,则有$w^Tx+b&gt;0$;若$y_i=-1$,则有$w^Tx+b&lt;0$;则下式成立</h4><h2 id="left-lbrace-begin-aligned-w-Tx-b-ge-1-amp-amp-y-i-1-w-Tx-b-le-1-amp-amp-y-i-1-end-aligned-right"><a href="#left-lbrace-begin-aligned-w-Tx-b-ge-1-amp-amp-y-i-1-w-Tx-b-le-1-amp-amp-y-i-1-end-aligned-right" class="headerlink" title="$ \left \lbrace \begin{aligned} w^Tx+b \ge+1&amp;&amp; y_i=+1 \\ w^Tx+b\le-1 &amp;&amp; y_i=-1 \end{aligned} \right.$"></a>$ \left \lbrace \begin{aligned} w^Tx+b \ge+1&amp;&amp; y_i=+1 \\ w^Tx+b\le-1 &amp;&amp; y_i=-1 \end{aligned} \right.$</h2><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20200530230142.png" srcset="/img/loading.gif" alt="images"></p><h4 id="从上图中可以看出，距离超平面最近的这几个训练样本点使上述公式的等号成立，这几个训练样本就称为“支持向量”（support-vector）"><a href="#从上图中可以看出，距离超平面最近的这几个训练样本点使上述公式的等号成立，这几个训练样本就称为“支持向量”（support-vector）" class="headerlink" title="从上图中可以看出，距离超平面最近的这几个训练样本点使上述公式的等号成立，这几个训练样本就称为“支持向量”（support vector）"></a>从上图中可以看出，距离超平面最近的这几个训练样本点使上述公式的等号成立，这几个训练样本就称为<strong>“支持向量”</strong>（support vector）</h4><h4 id="两个异类支持向量到超平面的距离之和（被称为间隔）："><a href="#两个异类支持向量到超平面的距离之和（被称为间隔）：" class="headerlink" title="两个异类支持向量到超平面的距离之和（被称为间隔）："></a>两个异类支持向量到超平面的距离之和（被称为间隔）：</h4><h2 id="r-frac-2-w"><a href="#r-frac-2-w" class="headerlink" title="$r=\frac{2}{||w||}$"></a>$r=\frac{2}{||w||}$</h2><h3 id="”最大间隔“的超平面"><a href="#”最大间隔“的超平面" class="headerlink" title="”最大间隔“的超平面"></a>”最大间隔“的超平面</h3><h4 id="我们要找的”最大间隔“的超平面，即："><a href="#我们要找的”最大间隔“的超平面，即：" class="headerlink" title="我们要找的”最大间隔“的超平面，即："></a>我们要找的”最大间隔“的超平面，即：</h4><h2 id="max-w-b-frac-2-w"><a href="#max-w-b-frac-2-w" class="headerlink" title="$max_{w,b}\frac{2}{||w||}$"></a>$max_{w,b}\frac{2}{||w||}$</h2><h2 id="s-t-y-i-w-Tx-b-ge1-i-1-2-cdot-cdot-cdot-m"><a href="#s-t-y-i-w-Tx-b-ge1-i-1-2-cdot-cdot-cdot-m" class="headerlink" title="$s.t.y_i(w^Tx+b)\ge1, i=1,2,\cdot \cdot \cdot m$"></a>$s.t.y_i(w^Tx+b)\ge1, i=1,2,\cdot \cdot \cdot m$</h2><h3 id="SVM的二次凸函数和约束条件"><a href="#SVM的二次凸函数和约束条件" class="headerlink" title="SVM的二次凸函数和约束条件"></a>SVM的二次凸函数和约束条件</h3><p>最大间隔分类器的求解， 可以转换为上面的一个最优化问题， 即在满足约束条件：  </p><h2 id="y-i-w-Tx-b-ge1-i-1-2-cdot-cdot-cdot-m"><a href="#y-i-w-Tx-b-ge1-i-1-2-cdot-cdot-cdot-m" class="headerlink" title="$y_i(w^Tx+b)\ge1, i=1,2,\cdot \cdot \cdot m$"></a>$y_i(w^Tx+b)\ge1, i=1,2,\cdot \cdot \cdot m$</h2><h4 id="求出就最大的-frac1-w-。"><a href="#求出就最大的-frac1-w-。" class="headerlink" title="求出就最大的$\frac1{||w||}$。"></a>求出就最大的$\frac1{||w||}$。</h4><h4 id="为更好的利用现有的理论和计算方法，-可以将求解-frac1-w-最大值，-转换为一个二次凸函数优化问题：求解-min-w-b-frac-1-2-w-2-，-两者问题是等价的。原来的问题转换为二次凸函数优化问题。在一定的约束条件下，目标最优，损失最小。"><a href="#为更好的利用现有的理论和计算方法，-可以将求解-frac1-w-最大值，-转换为一个二次凸函数优化问题：求解-min-w-b-frac-1-2-w-2-，-两者问题是等价的。原来的问题转换为二次凸函数优化问题。在一定的约束条件下，目标最优，损失最小。" class="headerlink" title="为更好的利用现有的理论和计算方法， 可以将求解$\frac1{||w||}$最大值， 转换为一个二次凸函数优化问题：求解 $min_{w,b}\frac{1}{2}{||w||}^2$， 两者问题是等价的。原来的问题转换为二次凸函数优化问题。在一定的约束条件下，目标最优，损失最小。"></a>为更好的利用现有的理论和计算方法， 可以将求解$\frac1{||w||}$最大值， 转换为一个二次凸函数优化问题：求解 $min_{w,b}\frac{1}{2}{||w||}^2$， 两者问题是等价的。原来的问题转换为二次凸函数优化问题。在一定的约束条件下，目标最优，损失最小。</h4><h3 id="SVM的基本型"><a href="#SVM的基本型" class="headerlink" title="SVM的基本型"></a>SVM的基本型</h3><h4 id="我们前面已经知道，最大化-w-1-等价于最小化-w-2-所以："><a href="#我们前面已经知道，最大化-w-1-等价于最小化-w-2-所以：" class="headerlink" title="我们前面已经知道，最大化$||w||^{-1}$等价于最小化$||w||^2$,所以："></a>我们前面已经知道，最大化$||w||^{-1}$等价于最小化$||w||^2$,所以：</h4><h2 id="min-w-b-frac-1-2-w-2"><a href="#min-w-b-frac-1-2-w-2" class="headerlink" title="$min_{w,b}\frac{1}{2}{||w||}^2$"></a>$min_{w,b}\frac{1}{2}{||w||}^2$</h2><h2 id="s-t-y-i-w-Tx-b-ge1-i-1-2-cdot-cdot-cdot-m-1"><a href="#s-t-y-i-w-Tx-b-ge1-i-1-2-cdot-cdot-cdot-m-1" class="headerlink" title="$s.t.y_i(w^Tx+b)\ge1, i=1,2,\cdot \cdot \cdot m$"></a>$s.t.y_i(w^Tx+b)\ge1, i=1,2,\cdot \cdot \cdot m$</h2><h3 id="拉格朗日构建方程"><a href="#拉格朗日构建方程" class="headerlink" title="拉格朗日构建方程"></a>拉格朗日构建方程</h3><p>由于这个问题的特殊结构，还可以通过拉格朗日对偶性（Lagrange Duality）变换到对偶变量(dual variable)的优化问题，即通过求解与原问题等价的对偶问题（dual problem）得到原始问题的最优解，这就是线性可分条件下支持向量机的对偶算法.</p><h4 id="这样做的优点在于：一者对偶问题往往更容易求解；二者可以自然的引入核函数，"><a href="#这样做的优点在于：一者对偶问题往往更容易求解；二者可以自然的引入核函数，" class="headerlink" title="这样做的优点在于：一者对偶问题往往更容易求解；二者可以自然的引入核函数，"></a>这样做的优点在于：一者对偶问题往往更容易求解；二者可以自然的引入核函数，</h4><p>进而推广到非线性分类问题。 具体来说就是对svm基本型的每条约束添加拉格朗日乘子$a_i\ge0$,则该问题的拉格朗日函数可写为：</p><h4 id="L-w-b-a-frac12-w-2-sum-i-1-na-i-1-y-i-w-Tx-b-a-i-a-1-a-2-cdot-cdot-cdot-a-m"><a href="#L-w-b-a-frac12-w-2-sum-i-1-na-i-1-y-i-w-Tx-b-a-i-a-1-a-2-cdot-cdot-cdot-a-m" class="headerlink" title="$L(w,b,a)= \frac12 ||w||^2+\sum_{i=1}^na_i(1-y_i(w^Tx+b))$                   $a_i=(a_1;a_2; \cdot \cdot \cdot a_m)$."></a>$L(w,b,a)= \frac12 ||w||^2+\sum_{i=1}^na_i(1-y_i(w^Tx+b))$                   $a_i=(a_1;a_2; \cdot \cdot \cdot a_m)$.</h4><p>我们的目标是让拉格朗如函数$ L(ω,b,α)$ 针对 $α$ 达到最大值。为什么能够这么写呢，我们可以这样想，哪怕有一个 $y_i(ω^Tx_i+b)⩾1$不满足，只要让对应的 $α_i$ 是正无穷就好了。所以，如果$L(ω,b,α)$有有限的最大值，那么那些不等式条件是自然满足的。 之后，我们再让 $L(ω,b,α)$ 针对 $ω,b$ 达到最小值，就可以了。 从而，我们的目标函数变成： </p><h4 id="原问题是极小极大的问题"><a href="#原问题是极小极大的问题" class="headerlink" title="原问题是极小极大的问题"></a>原问题是极小极大的问题</h4><h2 id="min-w-b-max-aL-w-b-a-p"><a href="#min-w-b-max-aL-w-b-a-p" class="headerlink" title="$min_{w,b}max_aL(w,b,a)=p^*$"></a>$min_{w,b}max_aL(w,b,a)=p^*$</h2><h4 id="原始问题的对偶问题，是极大极小问题"><a href="#原始问题的对偶问题，是极大极小问题" class="headerlink" title="原始问题的对偶问题，是极大极小问题"></a>原始问题的对偶问题，是极大极小问题</h4><h2 id="max-amin-w-b-L-w-b-a-b"><a href="#max-amin-w-b-L-w-b-a-b" class="headerlink" title="$max_amin_{w,b}L(w,b,a)=b^*$"></a>$max_amin_{w,b}L(w,b,a)=b^*$</h2><p>交换以后的新问题是原始问题的对偶问题，这个新问题的最优值用$d^*$来表示。而且有$d^∗⩽p^∗$，在满足某些条件的情况下，这两者相等，这个时候就可以通过求解对偶问题来间接地求解原始问题。</p><p>这所谓的“满足某些条件”就是要满足KKT条件。 </p><h2 id="left-lbrace-begin-aligned-a-i-ge0-y-i-w-Tx-i-b-1-ge-0-a-i-y-i-w-Tx-i-b-1-0-end-aligned-right"><a href="#left-lbrace-begin-aligned-a-i-ge0-y-i-w-Tx-i-b-1-ge-0-a-i-y-i-w-Tx-i-b-1-0-end-aligned-right" class="headerlink" title="$ \left\lbrace\ \begin{aligned} a_i \ge0 \\ y_i(w^Tx_i+b) -1\ge 0 \\ a_i(y_i(w^Tx_i+b)-1)=0 \end{aligned} \right. $"></a>$ \left\lbrace\ \begin{aligned} a_i \ge0 \\ y_i(w^Tx_i+b) -1\ge 0 \\ a_i(y_i(w^Tx_i+b)-1)=0 \end{aligned} \right. $</h2><h3 id="KKT条件的意义"><a href="#KKT条件的意义" class="headerlink" title="KKT条件的意义"></a>KKT条件的意义</h3><h5 id="一般地，一个最优化数学模型能够表示成下列标准形式："><a href="#一般地，一个最优化数学模型能够表示成下列标准形式：" class="headerlink" title="一般地，一个最优化数学模型能够表示成下列标准形式："></a>一般地，一个最优化数学模型能够表示成下列标准形式：</h5><h2 id="min-f-x"><a href="#min-f-x" class="headerlink" title="$min.f(x)$"></a>$min.f(x)$</h2><h2 id="s-t-h-j-x-0-j-1-cdot-cdot-cdot-n"><a href="#s-t-h-j-x-0-j-1-cdot-cdot-cdot-n" class="headerlink" title="$s.t.  h_j(x)=0,j=1,\cdot \cdot \cdot n$"></a>$s.t.  h_j(x)=0,j=1,\cdot \cdot \cdot n$</h2><h2 id="g-k-x-le0-k-1-cdot-cdot-cdot-m"><a href="#g-k-x-le0-k-1-cdot-cdot-cdot-m" class="headerlink" title="$g_k(x)\le0,k=1,\cdot \cdot \cdot m$"></a>$g_k(x)\le0,k=1,\cdot \cdot \cdot m$</h2><h2 id="x-in-X-subset-R-n"><a href="#x-in-X-subset-R-n" class="headerlink" title="$x\in X \subset R^n$"></a>$x\in X \subset R^n$</h2><h5 id="其中，f-x-是需要最小化的函数，h-x-是等式约束，g-x-是不等式约束，p和q分别为等式约束和不等式约束的数量。"><a href="#其中，f-x-是需要最小化的函数，h-x-是等式约束，g-x-是不等式约束，p和q分别为等式约束和不等式约束的数量。" class="headerlink" title="其中，f(x)是需要最小化的函数，h(x)是等式约束，g(x)是不等式约束，p和q分别为等式约束和不等式约束的数量。"></a>其中，f(x)是需要最小化的函数，h(x)是等式约束，g(x)是不等式约束，p和q分别为等式约束和不等式约束的数量。</h5><ul><li><h4 id="凸优化的概念：-x-subset-R-n-为一凸集，-f-x-to-R-为一凸函数，凸优化就是要找出一点-x-in-X-使得每一-x-in-X-满足-f-x-le-f-x"><a href="#凸优化的概念：-x-subset-R-n-为一凸集，-f-x-to-R-为一凸函数，凸优化就是要找出一点-x-in-X-使得每一-x-in-X-满足-f-x-le-f-x" class="headerlink" title="凸优化的概念：$x\subset R^n$为一凸集，$f:x\to R$为一凸函数，凸优化就是要找出一点$x^\in X$,使得每一$x\in X$满足$f(x^)\le f(x)$"></a>凸优化的概念：$x\subset R^n$为一凸集，$f:x\to R$为一凸函数，凸优化就是要找出一点$x^<em>\in X$,使得每一$x\in X$满足$f(x^</em>)\le f(x)$</h4></li><li><h4 id="KKT条件的意义：它是一个非线性规划（Nonlinear-Programming）问题能有最优化解法的必要和充分条件。"><a href="#KKT条件的意义：它是一个非线性规划（Nonlinear-Programming）问题能有最优化解法的必要和充分条件。" class="headerlink" title="KKT条件的意义：它是一个非线性规划（Nonlinear Programming）问题能有最优化解法的必要和充分条件。"></a>KKT条件的意义：它是一个非线性规划（Nonlinear Programming）问题能有最优化解法的必要和充分条件。</h4></li></ul><p>而KKT条件就是指上面最优化数学模型的标准形式中的最小点$ x*$ 必须满足下面的条件： </p><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20200530230143.png" srcset="/img/loading.gif" alt="images"></p><p>经过论证，我们这里的问题是满足KKT条件的（首先已经满足Slater condition，再者$f(x)$和$g(x)$也都是可微的，即$L$对$w$和$b$都可导），因此现在我们便转化为求解第二个问题。</p><p>也就是说，原始问题通过满足KKT条件，已经转化成了对偶问题。而求解这个对偶学习问题，分为3个步骤：首先要让$L(w，b，a)$关于$w$和$b$最小化，然后求对$a$的极大，最后利用SMO算法求解对偶问题中的拉格朗日乘子。</p><h3 id="对偶问题求解"><a href="#对偶问题求解" class="headerlink" title="对偶问题求解"></a>对偶问题求解</h3><h4 id="首先固定-a-先求出-min-w-b-L-w-b-a-所以分别对-w-b-进行求偏导并令其等于0。"><a href="#首先固定-a-先求出-min-w-b-L-w-b-a-所以分别对-w-b-进行求偏导并令其等于0。" class="headerlink" title="首先固定$a$,先求出$min_{w,b}L(w,b,a)$,所以分别对$w,b$进行求偏导并令其等于0。"></a>首先固定$a$,先求出$min_{w,b}L(w,b,a)$,所以分别对$w,b$进行求偏导并令其等于0。</h4><h2 id="frac-partial-L-w-b-a-partial-w-frac-partial-frac12w-Tw-sum-i-1-na-i-sum-i-1-n-a-iby-i-sum-i-1-n-a-iy-iw-Tx-partial-w-0"><a href="#frac-partial-L-w-b-a-partial-w-frac-partial-frac12w-Tw-sum-i-1-na-i-sum-i-1-n-a-iby-i-sum-i-1-n-a-iy-iw-Tx-partial-w-0" class="headerlink" title="$\frac{\partial L(w,b,a)}{\partial w}=\frac{\partial(\frac12w^Tw+\sum_{i=1}^na_i-\sum_{i=1}^n a_iby_i-\sum_{i=1}^n a_iy_iw^Tx)}{\partial w}  =0$"></a>$\frac{\partial L(w,b,a)}{\partial w}=\frac{\partial(\frac12w^Tw+\sum_{i=1}^na_i-\sum_{i=1}^n a_iby_i-\sum_{i=1}^n a_iy_iw^Tx)}{\partial w}  =0$</h2><h2 id="frac-partial-L-w-b-a-partial-b-frac-partial-frac12w-Tw-sum-i-1-na-i-sum-i-1-n-a-iby-i-sum-i-1-n-a-iy-iw-Tx-partial-b-0"><a href="#frac-partial-L-w-b-a-partial-b-frac-partial-frac12w-Tw-sum-i-1-na-i-sum-i-1-n-a-iby-i-sum-i-1-n-a-iy-iw-Tx-partial-b-0" class="headerlink" title="$\frac{\partial L(w,b,a)}{\partial b}=\frac{\partial(\frac12w^Tw+\sum_{i=1}^na_i-\sum_{i=1}^n a_iby_i-\sum_{i=1}^n a_iy_iw^Tx)}{\partial b}  =0$"></a>$\frac{\partial L(w,b,a)}{\partial b}=\frac{\partial(\frac12w^Tw+\sum_{i=1}^na_i-\sum_{i=1}^n a_iby_i-\sum_{i=1}^n a_iy_iw^Tx)}{\partial b}  =0$</h2><h2 id="w-sum-i-1-na-ix-iy-i"><a href="#w-sum-i-1-na-ix-iy-i" class="headerlink" title="$w=\sum_{i=1}^na_ix_iy_i$"></a>$w=\sum_{i=1}^na_ix_iy_i$</h2><h2 id="sum-i-1-na-iy-i-0"><a href="#sum-i-1-na-iy-i-0" class="headerlink" title="$\sum_{i=1}^na_iy_i=0$"></a>$\sum_{i=1}^na_iy_i=0$</h2><p>然后我们将以上结果带入原式<strong>$L(w,b,a)$</strong>:</p><h2 id="L-w-b-a-frac12-w-2-sum-i-1-na-i-1-y-i-w-Tx-b"><a href="#L-w-b-a-frac12-w-2-sum-i-1-na-i-1-y-i-w-Tx-b" class="headerlink" title="$L(w,b,a)= \frac12 ||w||^2+\sum_{i=1}^na_i(1-y_i(w^Tx+b))$"></a>$L(w,b,a)= \frac12 ||w||^2+\sum_{i=1}^na_i(1-y_i(w^Tx+b))$</h2><h2 id="frac12w-Tw-sum-i-1-na-i-sum-i-1-n-a-iby-i-sum-i-1-n-a-iy-iw-Tx"><a href="#frac12w-Tw-sum-i-1-na-i-sum-i-1-n-a-iby-i-sum-i-1-n-a-iy-iw-Tx" class="headerlink" title="$=\frac12w^Tw+\sum_{i=1}^na_i-\sum_{i=1}^n a_iby_i-\sum_{i=1}^n a_iy_iw^Tx$"></a>$=\frac12w^Tw+\sum_{i=1}^na_i-\sum_{i=1}^n a_iby_i-\sum_{i=1}^n a_iy_iw^Tx$</h2><h4 id="导入-w-sum-i-1-na-ix-iy-i"><a href="#导入-w-sum-i-1-na-ix-iy-i" class="headerlink" title="导入$w=\sum_{i=1}^na_ix_iy_i$:"></a><strong>导入$w=\sum_{i=1}^na_ix_iy_i$:</strong></h4><h2 id="frac12w-T-sum-i-1-na-ix-iy-i-sum-i-1-na-i-sum-i-1-n-a-iby-i-w-T-sum-i-1-n-a-iy-ix"><a href="#frac12w-T-sum-i-1-na-ix-iy-i-sum-i-1-na-i-sum-i-1-n-a-iby-i-w-T-sum-i-1-n-a-iy-ix" class="headerlink" title="$=\frac12w^T(\sum_{i=1}^na_ix_iy_i)+\sum_{i=1}^na_i-\sum_{i=1}^n a_iby_i-w^T(\sum_{i=1}^n a_iy_ix)$"></a>$=\frac12w^T(\sum_{i=1}^na_ix_iy_i)+\sum_{i=1}^na_i-\sum_{i=1}^n a_iby_i-w^T(\sum_{i=1}^n a_iy_ix)$</h2><h4 id="导入-sum-i-1-na-iy-i-0"><a href="#导入-sum-i-1-na-iy-i-0" class="headerlink" title="导入$\sum_{i=1}^na_iy_i=0$:"></a><strong>导入$\sum_{i=1}^na_iy_i=0$:</strong></h4><h2 id="sum-i-1-na-i-frac12w-T-sum-i-1-na-ix-iy-i"><a href="#sum-i-1-na-i-frac12w-T-sum-i-1-na-ix-iy-i" class="headerlink" title="$=\sum_{i=1}^na_i-\frac12w^T(\sum_{i=1}^na_ix_iy_i)$"></a>$=\sum_{i=1}^na_i-\frac12w^T(\sum_{i=1}^na_ix_iy_i)$</h2><h2 id="sum-i-1-na-i-frac12-sum-i-1-na-ix-iy-i-T-sum-i-1-na-ix-iy-i"><a href="#sum-i-1-na-i-frac12-sum-i-1-na-ix-iy-i-T-sum-i-1-na-ix-iy-i" class="headerlink" title="$=\sum_{i=1}^na_i-\frac12{(\sum_{i=1}^na_ix_iy_i)}^T(\sum_{i=1}^na_ix_iy_i)$"></a>$=\sum_{i=1}^na_i-\frac12{(\sum_{i=1}^na_ix_iy_i)}^T(\sum_{i=1}^na_ix_iy_i)$</h2><h2 id="sum-i-1-na-i-frac12-sum-i-1-na-i-x-i-Ty-i-sum-i-1-na-ix-iy-i"><a href="#sum-i-1-na-i-frac12-sum-i-1-na-i-x-i-Ty-i-sum-i-1-na-ix-iy-i" class="headerlink" title="$=\sum_{i=1}^na_i-\frac12(\sum_{i=1}^na_i{x_i}^Ty_i)(\sum_{i=1}^na_ix_iy_i)$"></a>$=\sum_{i=1}^na_i-\frac12(\sum_{i=1}^na_i{x_i}^Ty_i)(\sum_{i=1}^na_ix_iy_i)$</h2><h2 id="sum-i-1-na-i-frac12-sum-i-1-j-1-na-ia-j-x-i-Tx-jy-iy-j"><a href="#sum-i-1-na-i-frac12-sum-i-1-j-1-na-ia-j-x-i-Tx-jy-iy-j" class="headerlink" title="$=\sum_{i=1}^na_i-\frac12\sum_{i=1,j=1}^na_ia_j{x_i}^Tx_jy_iy_j$"></a>$=\sum_{i=1}^na_i-\frac12\sum_{i=1,j=1}^na_ia_j{x_i}^Tx_jy_iy_j$</h2><p>从上面的最后一个式子，我们可以看出，此时的拉格朗日函数只包含了一个变量，那就是$a_i$(求出了$a_i$便能求出$w,b$,然后我们的分类函数$f(x)=w^Tx+b$就非常容易的求出来了)。</p><h4 id="然后求对-a-的极大值："><a href="#然后求对-a-的极大值：" class="headerlink" title="然后求对$a$的极大值："></a>然后求对$a$的极大值：</h4><p>即是关于对偶问题的最优化问题。经过上面第一个步骤的求w和b，得到的拉格朗日函数式子已经没有了变量w，b，只有从上面的式子得到： </p><h2 id="max-a-sum-i-1-na-i-frac12-sum-i-1-j-1-na-ia-j-x-i-Tx-jy-iy-j"><a href="#max-a-sum-i-1-na-i-frac12-sum-i-1-j-1-na-ia-j-x-i-Tx-jy-iy-j" class="headerlink" title="$max_a \sum_{i=1}^na_i-\frac12\sum_{i=1,j=1}^na_ia_j{x_i}^Tx_jy_iy_j$"></a>$max_a \sum_{i=1}^na_i-\frac12\sum_{i=1,j=1}^na_ia_j{x_i}^Tx_jy_iy_j$</h2><h2 id="s-t-a-i-ge0-i-1-cdot-cdot-cdot-n"><a href="#s-t-a-i-ge0-i-1-cdot-cdot-cdot-n" class="headerlink" title="$s.t. ,a_i\ge0, i=1,\cdot \cdot \cdot n$"></a>$s.t. ,a_i\ge0, i=1,\cdot \cdot \cdot n$</h2><h2 id="sum-i-1-na-iy-i-0-1"><a href="#sum-i-1-na-iy-i-0-1" class="headerlink" title="$\sum_{i=1}^na_iy_i=0$"></a>$\sum_{i=1}^na_iy_i=0$</h2><p>我们一般用SMO算法来求解$a$</p><h3 id="SMO优化算法"><a href="#SMO优化算法" class="headerlink" title="SMO优化算法"></a>SMO优化算法</h3><p>SMO算法由Microsoft Research的John C. Platt在1998年提出，并成为最快的二次规划优化算法，特别针对线性SVM和数据稀疏时性能更优。 </p><p>SMO的基本思路是先固定$a_i$之外的所有参数，然后求$a_i$上的极值。由于存在约束$\sum_{i=1}^na_iy_i=0$，若固定$a_i$之外的其他变量，则$a_i$可由其他变量导出。于是，SMO每次选择两个变量$a_i和a_j$,并固定其他参数。这样，在参数初始化后，SMO不断执行如下两个步骤直至收敛：</p><ul><li><h4 id="选取一对需更新的变量-a-i和-a-j"><a href="#选取一对需更新的变量-a-i和-a-j" class="headerlink" title="选取一对需更新的变量$a_i和 a_j$."></a>选取一对需更新的变量$a_i和 a_j$.</h4></li><li><h4 id="固定-a-i和-a-j-以外的参数，求解-max-a-sum-i-1-na-i-frac12-sum-i-1-j-1-na-ia-j-x-i-Tx-jy-iy-j-获得更新后的-a-i和-a-j"><a href="#固定-a-i和-a-j-以外的参数，求解-max-a-sum-i-1-na-i-frac12-sum-i-1-j-1-na-ia-j-x-i-Tx-jy-iy-j-获得更新后的-a-i和-a-j" class="headerlink" title="固定$a_i和 a_j$以外的参数，求解$max_a \sum_{i=1}^na_i-\frac12\sum_{i=1,j=1}^na_ia_j{x_i}^Tx_jy_iy_j$,获得更新后的$a_i和 a_j$"></a>固定$a_i和 a_j$以外的参数，求解$max_a \sum_{i=1}^na_i-\frac12\sum_{i=1,j=1}^na_ia_j{x_i}^Tx_jy_iy_j$,获得更新后的$a_i和 a_j$</h4></li></ul><h4 id="那如何做才能做到不断收敛呢？"><a href="#那如何做才能做到不断收敛呢？" class="headerlink" title="那如何做才能做到不断收敛呢？"></a>那如何做才能做到不断收敛呢？</h4><p>注意只需选取的$a_i和 a_j$ 中有一个不满足KKT的条件，目标函数就会在不断迭代后减小。直观来说<strong>KKT条件的违背的程度越大，则变量更新后可能导致的目标函数值减幅越大</strong>，</p><h4 id="那如何选取变量呢？"><a href="#那如何选取变量呢？" class="headerlink" title="那如何选取变量呢？"></a>那如何选取变量呢？</h4><p>第一个变量SMO 先选取违背KKT条件程度最大的变量。</p><p>第二个变量应该选择一个使目标函数值减小最快的变量。</p><p><strong>但是</strong>由于比较各变量所对应的目标函数值减幅的复杂度过高，因此SMO就采用了一个启发式：<strong>使选取的变量所对应的样本之间的间隔最大</strong>。</p><p> <strong>总结</strong>：这样选取的两个变量有很大的差别，与对两个相似的变量进行更新相比，对它们进行更新会带给目标函数值更大的变化。SMO之所以高效，就是在于固定其他参数后，只优化两个参数的过程能做到非常高效。</p><h4 id="所以：只考虑-a-i和-a-j-时，约束条件就改变为："><a href="#所以：只考虑-a-i和-a-j-时，约束条件就改变为：" class="headerlink" title="所以：只考虑$a_i和 a_j$时，约束条件就改变为："></a>所以：只考虑$a_i和 a_j$时，约束条件就改变为：</h4><h4 id="max-a-sum-i-1-na-i-frac12-sum-i-1-j-1-na-ia-j-x-i-Tx-jy-iy-j-1"><a href="#max-a-sum-i-1-na-i-frac12-sum-i-1-j-1-na-ia-j-x-i-Tx-jy-iy-j-1" class="headerlink" title="$max_a \sum_{i=1}^na_i-\frac12\sum_{i=1,j=1}^na_ia_j{x_i}^Tx_jy_iy_j$"></a>$max_a \sum_{i=1}^na_i-\frac12\sum_{i=1,j=1}^na_ia_j{x_i}^Tx_jy_iy_j$</h4><h4 id="s-t-a-iy-i-a-jy-j-c-a-i-ge-0-a-j-ge-0"><a href="#s-t-a-iy-i-a-jy-j-c-a-i-ge-0-a-j-ge-0" class="headerlink" title="$s.t. ,a_iy_i+a_jy_j=c, a_i \ge 0, a_j \ge 0$"></a>$s.t. ,a_iy_i+a_jy_j=c, a_i \ge 0, a_j \ge 0$</h4><h4 id="sum-i-1-na-iy-i-0-2"><a href="#sum-i-1-na-iy-i-0-2" class="headerlink" title="$\sum_{i=1}^na_iy_i=0$"></a>$\sum_{i=1}^na_iy_i=0$</h4><h4 id="其中：-c-sum-k-ne-i-j-a-ky-k-是使-sum-i-1-na-iy-i-0-成立的常数。"><a href="#其中：-c-sum-k-ne-i-j-a-ky-k-是使-sum-i-1-na-iy-i-0-成立的常数。" class="headerlink" title="其中：$c= -\sum_{k \ne i,j}a_ky_k$是使$\sum_{i=1}^na_iy_i=0$ 成立的常数。"></a>其中：$c= -\sum_{k \ne i,j}a_ky_k$是使$\sum_{i=1}^na_iy_i=0$ 成立的常数。</h4><h4 id="然后用-a-iy-i-a-jy-j-c-消去变量-a-j-，就可以得到一个关于-a-j-的单变量的二次规划问题，通过约束条件-a-i-ge0-就可以解出这个二次规划问题的闭式解，于是不必调用数值优化算法就可以很快的算出更新后的-a-i-和-a-j-。"><a href="#然后用-a-iy-i-a-jy-j-c-消去变量-a-j-，就可以得到一个关于-a-j-的单变量的二次规划问题，通过约束条件-a-i-ge0-就可以解出这个二次规划问题的闭式解，于是不必调用数值优化算法就可以很快的算出更新后的-a-i-和-a-j-。" class="headerlink" title="然后用$a_iy_i+a_jy_j=c$ 消去变量$a_j$ ，就可以得到一个关于$a_j$ 的单变量的二次规划问题，通过约束条件$a_i \ge0$,就可以解出这个二次规划问题的闭式解，于是不必调用数值优化算法就可以很快的算出更新后的$a_i 和 a_j$。"></a>然后用$a_iy_i+a_jy_j=c$ 消去变量$a_j$ ，就可以得到一个关于$a_j$ 的单变量的二次规划问题，通过约束条件$a_i \ge0$,就可以解出这个二次规划问题的闭式解，于是不必调用数值优化算法就可以很快的算出更新后的$a_i 和 a_j$。</h4><h3 id="线性模型"><a href="#线性模型" class="headerlink" title="线性模型"></a>线性模型</h3><h4 id="用SMO-解出-a-后，我们前面以前算出-w-的值，所以我们还需要在前面的基础上算出b的值"><a href="#用SMO-解出-a-后，我们前面以前算出-w-的值，所以我们还需要在前面的基础上算出b的值" class="headerlink" title="用SMO 解出$a$后，我们前面以前算出$w$的值，所以我们还需要在前面的基础上算出b的值"></a>用SMO 解出$a$后，我们前面以前算出$w$的值，所以我们还需要在前面的基础上算出b的值</h4><h2 id="w-sum-i-1-na-ix-iy-i-1"><a href="#w-sum-i-1-na-ix-iy-i-1" class="headerlink" title="$w=\sum_{i=1}^na_ix_iy_i$"></a>$w=\sum_{i=1}^na_ix_iy_i$</h2><h2 id="b-y-j-sum-i-1-n-a-iy-i-x-i-Tx-j"><a href="#b-y-j-sum-i-1-n-a-iy-i-x-i-Tx-j" class="headerlink" title="$b=y_j-\sum_{i=1}^n a_iy_i{x_i}^Tx_j$"></a>$b=y_j-\sum_{i=1}^n a_iy_i{x_i}^Tx_j$</h2><h4 id="所以就可以得到模型"><a href="#所以就可以得到模型" class="headerlink" title="所以就可以得到模型"></a>所以就可以得到模型</h4><h2 id="f-x-w-Tx-b-sum-i-1-n-a-iy-i-x-i-Tx-b"><a href="#f-x-w-Tx-b-sum-i-1-n-a-iy-i-x-i-Tx-b" class="headerlink" title="$f(x)=w^Tx+b=\sum_{i=1}^n a_iy_i{x_i}^Tx+b$"></a>$f(x)=w^Tx+b=\sum_{i=1}^n a_iy_i{x_i}^Tx+b$</h2><h3 id="实战一下"><a href="#实战一下" class="headerlink" title="实战一下"></a>实战一下</h3><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20200530230144.png" srcset="/img/loading.gif" alt="image"></p><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20200530230145.png" srcset="/img/loading.gif" alt="image"></p><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20200530230146.png" srcset="/img/loading.gif" alt="image"></p><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20200530230147.png" srcset="/img/loading.gif" alt="image"></p><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20200530230148.png" srcset="/img/loading.gif" alt="image"></p><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20200530230149.png" srcset="/img/loading.gif" alt="image"></p>]]></content>
    
    
    <categories>
      
      <category>机器学习</category>
      
    </categories>
    
    
    <tags>
      
      <tag>机器学习</tag>
      
      <tag>支持向量机</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>机器学习-梯度下降</title>
    <link href="/2018/07/arithmetic-gradientDescent/"/>
    <url>/2018/07/arithmetic-gradientDescent/</url>
    
    <content type="html"><![CDATA[<h3 id="优化与机器学习"><a href="#优化与机器学习" class="headerlink" title="优化与机器学习"></a>优化与机器学习</h3><h4 id="机器学习的主要任务之一就是通过训练，学习获得一组最优的参数，我们经常以成本函数来作为参数估计的函数。所以机器学习的任务也就是最小成本函数。"><a href="#机器学习的主要任务之一就是通过训练，学习获得一组最优的参数，我们经常以成本函数来作为参数估计的函数。所以机器学习的任务也就是最小成本函数。" class="headerlink" title="机器学习的主要任务之一就是通过训练，学习获得一组最优的参数，我们经常以成本函数来作为参数估计的函数。所以机器学习的任务也就是最小成本函数。"></a>机器学习的主要任务之一就是通过训练，学习获得一组最优的参数，我们经常以<strong>成本函数</strong>来作为参数估计的函数。所以机器学习的任务也就是最小成本函数。</h4><h4 id="优化也是机器学习算法的非常重要的组成部分，基本上每一个机器学习算法都有一个优化算法"><a href="#优化也是机器学习算法的非常重要的组成部分，基本上每一个机器学习算法都有一个优化算法" class="headerlink" title="优化也是机器学习算法的非常重要的组成部分，基本上每一个机器学习算法都有一个优化算法"></a>优化也是机器学习算法的非常重要的组成部分，基本上每一个机器学习算法都有一个优化算法</h4><h3 id="梯度下降方法"><a href="#梯度下降方法" class="headerlink" title="梯度下降方法"></a>梯度下降方法</h3><h4 id="用负梯度作搜索方向，即令-bigtriangleup-x-bigtriangledown-f-x-是一种自然的选择。相应的方法就称梯度方法或者梯度下降方法。"><a href="#用负梯度作搜索方向，即令-bigtriangleup-x-bigtriangledown-f-x-是一种自然的选择。相应的方法就称梯度方法或者梯度下降方法。" class="headerlink" title="用负梯度作搜索方向，即令$\bigtriangleup x=-\bigtriangledown f(x)$, 是一种自然的选择。相应的方法就称梯度方法或者梯度下降方法。"></a>用负梯度作搜索方向，即令<strong>$\bigtriangleup x=-\bigtriangledown f(x)$</strong>, 是一种自然的选择。相应的方法就称梯度方法或者梯度下降方法。</h4><h3 id="梯度下降算法的概念"><a href="#梯度下降算法的概念" class="headerlink" title="梯度下降算法的概念"></a>梯度下降算法的概念</h3><p><strong>梯度下降算法</strong>就是一个被广泛使用的优化算法, 它可以用于寻找<strong>最小化成本函数</strong>的参数值. 也就是说: <em>当函数 $$J(\theta)$$</em> 取得最小值时, 求所对应的自变量<strong>$\theta$</strong>的过程， 此处<strong>$\theta$</strong>就是机器要学习的参数，$$J(\theta)$$ 就是用于参数估计的成本函数, 是关于$$\theta$$ 的函数. </p><h3 id="梯度下降的基本步骤"><a href="#梯度下降的基本步骤" class="headerlink" title="梯度下降的基本步骤"></a>梯度下降的基本步骤</h3><p>梯度下降法的计算过程就是沿梯度下降的方向求解极小值（也可以沿梯度上升方向求解极大值） </p><hr><h5 id="给定-初始点-x-in-dom-f"><a href="#给定-初始点-x-in-dom-f" class="headerlink" title="给定 初始点 $x \in dom f $"></a>给定 初始点 $x \in dom f $</h5><h5 id="重复进行："><a href="#重复进行：" class="headerlink" title="重复进行："></a>重复进行：</h5><ol><li><h5 id="bigtriangleup-x-bigtriangledown-f-x"><a href="#bigtriangleup-x-bigtriangledown-f-x" class="headerlink" title="$\bigtriangleup x :=-\bigtriangledown f(x)$"></a>$\bigtriangleup x :=-\bigtriangledown f(x)$</h5></li><li><h5 id="直线搜索。通过精确或回溯直线搜索方法确实步长-t"><a href="#直线搜索。通过精确或回溯直线搜索方法确实步长-t" class="headerlink" title="直线搜索。通过精确或回溯直线搜索方法确实步长$t$."></a>直线搜索。通过精确或回溯直线搜索方法确实步长$t$.</h5></li><li><h5 id="修改-x-x-t-bigtriangleup-x"><a href="#修改-x-x-t-bigtriangleup-x" class="headerlink" title="修改 :$x :=x+t\bigtriangleup x$."></a>修改 :$x :=x+t\bigtriangleup x$.</h5><h4 id="直到：满足停止准则。"><a href="#直到：满足停止准则。" class="headerlink" title="直到：满足停止准则。"></a>直到：满足停止准则。</h4></li></ol><hr><h4 id="换种方式："><a href="#换种方式：" class="headerlink" title="换种方式："></a>换种方式：</h4><ol><li>对成本函数进行微分, 得到其在给定点的梯度. 梯度的正负指示了成本函数值的上升或下降:$ Δ(\theta)=\frac{∂J(\theta)}{∂\theta}$  </li><li>选择使成本函数值减小的方向, 即梯度负方向, 乘以学习率为 α 计算得参数的更新量, 并更新参数:<strong>$\theta=\theta−αΔ(\theta)  $</strong></li><li>重复以上步骤, 直到取得最小的成本</li></ol><hr><h3 id="批量梯度下降法（Batch-Gradient-Descent）"><a href="#批量梯度下降法（Batch-Gradient-Descent）" class="headerlink" title="批量梯度下降法（Batch Gradient Descent）"></a>批量梯度下降法（Batch Gradient Descent）</h3><p>　批量梯度下降法，是梯度下降法最常用的形式，具体做法也就是在更新参数时使用所有的样本来进行更新，这个方法对应于<a href="https://0520.tech/2018/07/20/2018-07-20-ml-linearRegression/">线性回归的梯度下降算法</a>，也就是说线性回归的梯度下降算法就是批量梯度下降法。　　 </p><h4 id="具体实现过程："><a href="#具体实现过程：" class="headerlink" title="具体实现过程："></a>具体实现过程：</h4><hr><ol><li><h5 id="假设函数：-h-theta-sum-i-1-n-theta-ix-i"><a href="#假设函数：-h-theta-sum-i-1-n-theta-ix-i" class="headerlink" title="假设函数：$h_\theta = \sum_{i=1}^n\theta_ix_i$"></a>假设函数：$h_\theta = \sum_{i=1}^n\theta_ix_i$</h5></li><li><h5 id="成本函数：-J-theta-frac-1-2m-sum-i-1-n-h-theta-x-i-y-i-2"><a href="#成本函数：-J-theta-frac-1-2m-sum-i-1-n-h-theta-x-i-y-i-2" class="headerlink" title="成本函数：$J(\theta)=\frac{1}{2m} \sum_{i=1}^n(h_\theta(x_i)-y_i)^2$"></a>成本函数：$J(\theta)=\frac{1}{2m} \sum_{i=1}^n(h_\theta(x_i)-y_i)^2$</h5></li><li><h5 id="对成本函数进行求偏导：对每一个参数-theta-j-进行分别求偏导，得出各自的梯度。"><a href="#对成本函数进行求偏导：对每一个参数-theta-j-进行分别求偏导，得出各自的梯度。" class="headerlink" title="对成本函数进行求偏导：对每一个参数$\theta_j$进行分别求偏导，得出各自的梯度。"></a>对成本函数进行求偏导：对每一个参数$\theta_j$进行分别求偏导，得出各自的梯度。</h5><h4 id="frac-partial-J-theta-partial-theta-frac1-m-sum-i-1-n-y-i-h-theta-x-i-x-j-i"><a href="#frac-partial-J-theta-partial-theta-frac1-m-sum-i-1-n-y-i-h-theta-x-i-x-j-i" class="headerlink" title="$\frac{\partial J(\theta)}{\partial  \theta}=-\frac1 m  \sum_{i=1}^n(y_i-h_\theta(x_i))x_j^i$"></a>$\frac{\partial J(\theta)}{\partial  \theta}=-\frac1 m  \sum_{i=1}^n(y_i-h_\theta(x_i))x_j^i$</h4></li><li><h5 id="每个参数都按照梯度的负方向进行更新："><a href="#每个参数都按照梯度的负方向进行更新：" class="headerlink" title="每个参数都按照梯度的负方向进行更新："></a>每个参数都按照梯度的负方向进行更新：</h5><h4 id="theta-j-theta-j-frac-a-m-sum-i-1-n-y-i-h-theta-x-i-x-j-i"><a href="#theta-j-theta-j-frac-a-m-sum-i-1-n-y-i-h-theta-x-i-x-j-i" class="headerlink" title="$\theta_j=\theta_j+\frac a m \sum_{i=1}^n(y_i-h_\theta(x_i))x_j^i$"></a>$\theta_j=\theta_j+\frac a m \sum_{i=1}^n(y_i-h_\theta(x_i))x_j^i$</h4></li></ol><hr><h4 id="BGD伪代码："><a href="#BGD伪代码：" class="headerlink" title="BGD伪代码："></a>BGD伪代码：</h4><hr><h4 id="repeat"><a href="#repeat" class="headerlink" title="repeat{"></a>repeat{</h4><h4 id="theta-j-theta-j-frac-a-m-sum-i-1-n-y-i-h-theta-x-i-x-j-i-1"><a href="#theta-j-theta-j-frac-a-m-sum-i-1-n-y-i-h-theta-x-i-x-j-i-1" class="headerlink" title="$\theta_j=\theta_j+\frac a m \sum_{i=1}^n(y_i-h_\theta(x_i))x_j^i$"></a>$\theta_j=\theta_j+\frac a m \sum_{i=1}^n(y_i-h_\theta(x_i))x_j^i$</h4><h4 id="for-every-j-0-1-n"><a href="#for-every-j-0-1-n" class="headerlink" title="(for every j = 0, 1, .. n)"></a>(for every j = 0, 1, .. n)</h4><h4 id=""><a href="#" class="headerlink" title="}"></a>}</h4><hr><h4 id="总结："><a href="#总结：" class="headerlink" title="总结："></a>总结：</h4><p>优点：BGD 得到的是全局最优解, 因为它总是以整个训练集来计算梯度, </p><p>缺点：因此带来了巨大的计算量, 计算迭代速度很很慢. </p><h3 id="随机梯度下降法（Stochastic-Gradient-Descent）"><a href="#随机梯度下降法（Stochastic-Gradient-Descent）" class="headerlink" title="随机梯度下降法（Stochastic Gradient Descent）"></a>随机梯度下降法（Stochastic Gradient Descent）</h3><p>随机梯度下降法，其实和批量梯度下降法原理类似，区别在与求梯度时没有用所有的m个样本的数据，而是仅仅选取一个样本j来求梯度。 </p><h4 id="具体实现过程：-1"><a href="#具体实现过程：-1" class="headerlink" title="具体实现过程："></a>具体实现过程：</h4><p>SGD 每次以一个样本, 而不是整个数据集来计算梯度. 因此, SGD 从成本函数开始, 就不必再求和了, 针对单个样例的成本函数可以写成:  </p><h4 id="J-theta-frac-1-2-h-theta-x-i-y-i-2"><a href="#J-theta-frac-1-2-h-theta-x-i-y-i-2" class="headerlink" title="$J(\theta)=\frac{1}{2} (h_\theta(x_i)-y_i)^2$"></a>$J(\theta)=\frac{1}{2} (h_\theta(x_i)-y_i)^2$</h4><p>于是, SGD 的参数更新规则就可以写成 ：</p><h4 id="theta-j-theta-j-a-y-i-h-theta-x-i-x-j-i"><a href="#theta-j-theta-j-a-y-i-h-theta-x-i-x-j-i" class="headerlink" title="$\theta_j=\theta_j+a (y_i-h_\theta(x_i))x_j^i$"></a>$\theta_j=\theta_j+a (y_i-h_\theta(x_i))x_j^i$</h4><h4 id="SGD伪代码："><a href="#SGD伪代码：" class="headerlink" title="SGD伪代码："></a>SGD伪代码：</h4><hr><h4 id="repeat-1"><a href="#repeat-1" class="headerlink" title="repeat {     "></a>repeat {     </h4><h4 id="for-i-1-m"><a href="#for-i-1-m" class="headerlink" title="for i = 1, .., m{    "></a>for i = 1, .., m{    </h4><h4 id="theta-j-theta-j-a-y-i-h-theta-x-i-x-j-i-1"><a href="#theta-j-theta-j-a-y-i-h-theta-x-i-x-j-i-1" class="headerlink" title="               $\theta_j=\theta_j+a (y_i-h_\theta(x_i))x_j^i$"></a>               $\theta_j=\theta_j+a (y_i-h_\theta(x_i))x_j^i$</h4><h4 id="for-every-j-0-1-n-1"><a href="#for-every-j-0-1-n-1" class="headerlink" title="                     (for every j = 0, 1, .. n)    "></a>                     (for every j = 0, 1, .. n)    </h4><h4 id="-1"><a href="#-1" class="headerlink" title="}"></a>}</h4><h4 id="-2"><a href="#-2" class="headerlink" title="}"></a>}</h4><hr><h4 id="总结：-1"><a href="#总结：-1" class="headerlink" title="总结："></a>总结：</h4><p>SGD 的关键点在于以随机顺序选取样本. 因为 SGD 存在局部最优困境, 若每次都以相同的顺序选取样本, 其有很大的可能会在相同的地方陷入局部最优解困境, 或者收敛减缓. 因此, 欲使 SGD 发挥更好的效果, 应充分利用<strong>随机化</strong> 带来的优势: 可以*<em>在每次迭代之前 (伪代码中最外围循环), 对训练集进行随机排列. *</em></p><p>缺点：因为每次只取一个样本来进行梯度下降, SGD 的训练<strong>速度很快</strong>, 但会引入噪声, 使准确度下降</p><p>优点：.  可以使用<strong>在线学习</strong>. 也就是说, 在模型训练好之后, 只要有新的数据到来, 模型都可以利用新的数据进行再学习, 更新参数,以适应新的变化. </p><h3 id="对比"><a href="#对比" class="headerlink" title="对比"></a>对比</h3><p><strong>随机梯度下降法和批量梯度下降法</strong>是两个极端，一个采用所有数据来梯度下降，一个用一个样本来梯度下降。自然各自的优缺点都非常突出。对于训练速度来说，随机梯度下降法由于每次仅仅采用一个样本来迭代，训练速度很快，而批量梯度下降法在样本量很大的时候，训练速度不能让人满意。对于准确度来说，随机梯度下降法用于仅仅用一个样本决定梯度方向，导致解很有可能不是最优。对于收敛速度来说，由于随机梯度下降法一次迭代一个样本，导致迭代方向变化很大，不能很快的收敛到局部最优解。 </p><h4 id="MBGD就综合了这两种方法的优点。"><a href="#MBGD就综合了这两种方法的优点。" class="headerlink" title="MBGD就综合了这两种方法的优点。"></a>MBGD就综合了这两种方法的优点。</h4><h3 id="小批量梯度下降法（Mini-batch-Gradient-Descent）"><a href="#小批量梯度下降法（Mini-batch-Gradient-Descent）" class="headerlink" title="小批量梯度下降法（Mini-batch Gradient Descent）"></a>小批量梯度下降法（Mini-batch Gradient Descent）</h3><p>MBGD 是为解决 BGD 与 SGD 各自缺点而发明的折中算法, 或者说它利用了 BGD 和 SGD 各自优点. 其基本思想是: <em>每次更新参数时, 使用 n 个样本, 既不是全部, 也不是 1.</em> (SGD 可以看成是 n=1 的 MBGD 的一个特例) </p><h4 id="MBGD-的成本函数或其求导公式或参数更新规则公式基本同-BGD-。"><a href="#MBGD-的成本函数或其求导公式或参数更新规则公式基本同-BGD-。" class="headerlink" title="MBGD 的成本函数或其求导公式或参数更新规则公式基本同 BGD 。"></a>MBGD 的成本函数或其求导公式或参数更新规则公式基本同 BGD 。</h4><h4 id="MBGD-的伪代码："><a href="#MBGD-的伪代码：" class="headerlink" title="MBGD 的伪代码："></a>MBGD 的伪代码：</h4><hr><h4 id="say-b-10-m-1000"><a href="#say-b-10-m-1000" class="headerlink" title="say b=10, m=1000,"></a>say b=10, m=1000,</h4><h4 id="repeat-2"><a href="#repeat-2" class="headerlink" title="repeat {     "></a>repeat {     </h4><h4 id="for-i-1-11-21-991"><a href="#for-i-1-11-21-991" class="headerlink" title="for i = 1, 11, 21, .., 991 {"></a>for i = 1, 11, 21, .., 991 {</h4><h4 id="theta-j-theta-j-frac-a-10-sum-i-1-i-9-y-i-h-theta-x-i-x-j-i"><a href="#theta-j-theta-j-frac-a-10-sum-i-1-i-9-y-i-h-theta-x-i-x-j-i" class="headerlink" title="$\theta_j=\theta_j+\frac a {10} \sum_{i=1}^{i+9}(y_i-h_\theta(x_i))x_j^i$"></a>$\theta_j=\theta_j+\frac a {10} \sum_{i=1}^{i+9}(y_i-h_\theta(x_i))x_j^i$</h4><h4 id="for-every-j-0-1-n-2"><a href="#for-every-j-0-1-n-2" class="headerlink" title=" (for every j = 0, 1, .. n)    "></a> (for every j = 0, 1, .. n)    </h4><h4 id="-3"><a href="#-3" class="headerlink" title=" }"></a> }</h4><h4 id="-4"><a href="#-4" class="headerlink" title="}"></a>}</h4><hr><h3 id="梯度下降算法总结"><a href="#梯度下降算法总结" class="headerlink" title="梯度下降算法总结"></a>梯度下降算法总结</h3><table><thead><tr><th>梯度下降算法</th><th>优点</th><th>缺点</th></tr></thead><tbody><tr><td>BGD</td><td>全局最优解</td><td>计算量大, 迭代速度慢, 训练速度慢</td></tr><tr><td>SGD</td><td>1.训练速度快 ,对于很大的数据集，也能以较快的速度收敛                                                                        2. 支持在线学习</td><td>准确度下降, 有噪声, 非全局最优解</td></tr><tr><td>MBGD</td><td>1. 训练速度较快, 取决于小批量的数目                                            2. 支持在线学习</td><td>准确度不如 BGD, 速度比SGD慢，仍然有噪声, 非全局最优解</td></tr></tbody></table>]]></content>
    
    
    <categories>
      
      <category>机器学习</category>
      
    </categories>
    
    
    <tags>
      
      <tag>机器学习</tag>
      
      <tag>梯度下降，优化算法</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>机器学习-决策树</title>
    <link href="/2018/07/ml-DecisionTree/"/>
    <url>/2018/07/ml-DecisionTree/</url>
    
    <content type="html"><![CDATA[<h2 id="决策树"><a href="#决策树" class="headerlink" title="决策树"></a>决策树</h2><p><strong>决策树（decision tree）</strong>是一种<strong>分类</strong>与<strong>回归</strong>方法，本文主要讨论用于<strong>分类</strong>的决策树，决策树的结构呈<strong>树形</strong>结构，在分类问题中，其代表基于特征对数据进行分类的过程，通常可以认为是if-then规则的集合，也可以认为是定义在<strong>特征空间与类空间上的条件概率分布</strong>。其主要优点是模型可读性好并且分类速度快。训练的时候，利用训练数据根据损失函数最小化的原则建立决策树模型。</p><p>预测时对于新的数据，利用决策树进行分类。决策树的学习通常包括三个步骤：<strong>特征选择，生成决策树，对决策树进行剪枝</strong>。这些决策树的思想主要来自Quinlan在1986年提出的ID3算法和1993年提出的C4.5算法，以及Breiman等人在1984年提出的CART算法。 </p><p>用于分类的决策树是一种对数据进行分类的树形结构。决策树主要由节点（node）和有向边（directed edge）组成。节点有两种类型：内部节点（internal node）以及叶节点（leaf node）。内部节点表示一个特征或者属性，叶节点表示一个类。其结构如图所示：  </p><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20200530230056.png" srcset="/img/loading.gif" alt="image"></p><p>决策树学习采用的是<strong>自顶向下</strong>的递归方法, 其基本思想是以<strong>信息熵</strong>为度量构造一棵<strong>熵值 下降最快</strong>的树,到叶子节点处的熵值为零, 此时每个叶节点中的实例都属于同一类。</p><ul><li>最大优点: 可以自学习。在学习的过程中,不需要使用者了解过多背景知识,只需要对训练实例进行较好的标注,就能够进行学习。</li><li>显然,属于<strong>有监督学习</strong>。</li></ul><h3 id="决策树三种生成算法"><a href="#决策树三种生成算法" class="headerlink" title="决策树三种生成算法"></a>决策树三种生成算法</h3><ol><li><h4 id="ID3-—-信息增益-最大的准则"><a href="#ID3-—-信息增益-最大的准则" class="headerlink" title="ID3 — 信息增益 最大的准则"></a>ID3 — <strong>信息增益</strong> <strong>最大</strong>的准则</h4><p>​    ID3算法的核心是在决策树各个节点上使用<strong>信息增益</strong>作为特征选择的依据，递归的构建决策树。  从根节点开始，对节点计算所有可能的特征的信息增益，选择信息增益最大的特征作为节点的特征，由该特征的不同取值建立子节点；再对子节点递归的调用以上方法，构建决策树；知道所有特征的信息增益均很小或没有特征可以选择为止，得到最后一个决策树。ID3相当于用最大似然法进行概率模型的选择。 </p></li><li><h4 id="C4-5-—-信息增益比-最大的准则"><a href="#C4-5-—-信息增益比-最大的准则" class="headerlink" title="C4.5 — 信息增益比 最大的准则"></a>C4.5 — <strong>信息增益比</strong> 最大的准则</h4><p>​    C4.5算法使用<strong>信息增益率</strong>作为特征选择的依据，算法的流程与ID3相似。 </p></li><li><p>CART</p><ul><li>回归树: <strong>平方误差</strong> <strong>最小</strong> 的准则</li><li>分类树: <strong>基尼系数</strong> <strong>最小</strong>的准则</li></ul><p>CART树的名字其实非常有意思，Classification And Regression Tree（分类回归树），它使用基尼系数(Gini Index)作为特征的划分依据。顾名思义，CART既可以用来做分类也可以用来做回归。它是在给定输入随机变量X条件下输出随机变量Y的条件概率分布的学习方法。  CART树假设我么的决策树是二叉树，内部节点特征的取值为是或否。</p><p><strong>CART算法</strong>为：  </p><pre><code>1. 决策树的生成：基于训练数据集生成决策树，生成的决策树要尽量大；</code></pre><p>​               2.决策树剪枝：用验证数据集对已经生成的巨额额数进行剪枝并选择最优子树。 </p></li></ol><h3 id="信息熵"><a href="#信息熵" class="headerlink" title="信息熵"></a>信息熵</h3><h4 id="信息熵（information-entropy）-是度量样本集合纯度的最常用的一种指标。"><a href="#信息熵（information-entropy）-是度量样本集合纯度的最常用的一种指标。" class="headerlink" title="信息熵（information entropy） 是度量样本集合纯度的最常用的一种指标。"></a><strong>信息熵（information entropy）</strong> 是度量样本集合纯度的最常用的一种指标。</h4><ul><li><h4 id="熵：-H-X-sum-x-in-X-p-x-y-logp-x"><a href="#熵：-H-X-sum-x-in-X-p-x-y-logp-x" class="headerlink" title="熵：$H(X)=-\sum_{x \in X} p(x,y)logp(x)$"></a>熵：$H(X)=-\sum_{x \in X} p(x,y)logp(x)$</h4></li><li><h4 id="联合熵：-H-X，Y-sum-x-in-X-y-in-Y-p-x-y-logp-x-y"><a href="#联合熵：-H-X，Y-sum-x-in-X-y-in-Y-p-x-y-logp-x-y" class="headerlink" title="联合熵：$H(X，Y)=-\sum_{x \in X,y \in Y} p(x,y)logp(x,y)$"></a>联合熵：$H(X，Y)=-\sum_{x \in X,y \in Y} p(x,y)logp(x,y)$</h4></li><li><h4 id="条件熵：-H-X-Y-sum-x-in-X-y-in-Y-p-x-y-logp-x-y"><a href="#条件熵：-H-X-Y-sum-x-in-X-y-in-Y-p-x-y-logp-x-y" class="headerlink" title="条件熵：$H(X|Y)=-\sum_{x \in X,y \in Y} p(x,y)logp(x|y)$"></a>条件熵：$H(X|Y)=-\sum_{x \in X,y \in Y} p(x,y)logp(x|y)$</h4></li><li><h4 id="相对熵：-D-p-q-sum-x-p-x-log-frac-p-x-q-x"><a href="#相对熵：-D-p-q-sum-x-p-x-log-frac-p-x-q-x" class="headerlink" title="相对熵：$D(p||q)=\sum_x p(x)log\frac{p(x)}{q(x)}$"></a>相对熵：$D(p||q)=\sum_x p(x)log\frac{p(x)}{q(x)}$</h4></li><li><h4 id="互信息：-I-x-y-sum-x-in-X-y-in-Y-p-x-y-log-frac-p-x-y-p-x-p-y"><a href="#互信息：-I-x-y-sum-x-in-X-y-in-Y-p-x-y-log-frac-p-x-y-p-x-p-y" class="headerlink" title="互信息：$I(x,y)=\sum_{x\in X, y \in Y} p(x,y )log\frac{p(x,y)}{p(x)p(y)}$"></a>互信息：$I(x,y)=\sum_{x\in X, y \in Y} p(x,y )log\frac{p(x,y)}{p(x)p(y)}$</h4></li></ul><h3 id="决策树学习基本算法"><a href="#决策树学习基本算法" class="headerlink" title="决策树学习基本算法"></a>决策树学习基本算法</h3><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20200530230057.png" srcset="/img/loading.gif" alt="image"></p><h3 id="信息增益—-g-D-A"><a href="#信息增益—-g-D-A" class="headerlink" title="信息增益—$g(D,A)$"></a>信息增益—$g(D,A)$</h3><p><strong>信息增益(Information Gain)</strong>：表示得知特征A的信息而使得类X的信息的不确定性减少的程度。<br>定义：特征A对训练数据集D的<strong>信息增益</strong>g(D, A)，定义为集合D的<strong>经验熵</strong>H(D)与<strong>经验条件熵</strong>H(D|A)的差值。 </p><h2 id="g-D-A-H-D-−H-D-A"><a href="#g-D-A-H-D-−H-D-A" class="headerlink" title="$g(D,A)=H(D)−H(D|A)$"></a>$g(D,A)=H(D)−H(D|A)$</h2><p>而这又是互信息的定义。所以<strong>决策树中的信息增益等价于训练集中类与特征的互信息。</strong></p><h4 id="总结：一般而言，信息增益g-D-A-越大，就意味着使用特征A来进行划分所获得的“纯度提升”就越大。著名的ID3决策树学习算法就是以信息增益为准则来学习划分属性的。"><a href="#总结：一般而言，信息增益g-D-A-越大，就意味着使用特征A来进行划分所获得的“纯度提升”就越大。著名的ID3决策树学习算法就是以信息增益为准则来学习划分属性的。" class="headerlink" title="总结：一般而言，信息增益g(D, A)越大，就意味着使用特征A来进行划分所获得的“纯度提升”就越大。著名的ID3决策树学习算法就是以信息增益为准则来学习划分属性的。"></a>总结：一般而言，<strong>信息增益</strong>g(D, A)越大，就意味着使用特征A来进行划分所获得的“纯度提升”就越大。著名的ID3决策树学习算法就是以信息增益为准则来学习划分属性的。</h4><h3 id="增益率"><a href="#增益率" class="headerlink" title="增益率"></a>增益率</h3><p><strong>信息率(Information Gain Ratio)</strong>：  用信息增益作为划分特征的依据时，会存在一些问题。例如，如果使用数据的ID作为特征，这样，每个数据点相当于均匀分布，所以得到的信息增益一定是最大的，但是我们都知道ID是不能作为特征的。这样如果单纯的使用信息增益作为划分特征的依据的话，会导致我们生成的树比较矮胖，容易<strong>过拟合</strong>。 </p><p>定义：特征A对训练数据集D的<strong>信息增益率</strong>$$gR(D,A)$$定义为其<strong>信息增益</strong>$$g(D,A)$$与训练数据集D关于特征A的值得<strong>信息熵</strong>$$IV(D)$$之比：</p><h2 id="gR-D-A-frac-g-D-A-IV-D"><a href="#gR-D-A-frac-g-D-A-IV-D" class="headerlink" title="$ gR(D,A)=\frac {g(D,A)}{IV(D)} $"></a>$ gR(D,A)=\frac {g(D,A)}{IV(D)} $</h2><h4 id="总结：信息增益准则对可取值数目较多的属性有所偏好，为减少这种偏好可能带来的不利影响，著名的C4-5决策树算法就不直接使用信息增益而是使用“增益率”-。"><a href="#总结：信息增益准则对可取值数目较多的属性有所偏好，为减少这种偏好可能带来的不利影响，著名的C4-5决策树算法就不直接使用信息增益而是使用“增益率”-。" class="headerlink" title="总结：信息增益准则对可取值数目较多的属性有所偏好，为减少这种偏好可能带来的不利影响，著名的C4.5决策树算法就不直接使用信息增益而是使用“增益率” 。"></a>总结：信息增益准则对可取值数目较多的属性有所偏好，为减少这种偏好可能带来的不利影响，著名的C4.5决策树算法就不直接使用信息增益而是使用“增益率” 。</h4><h3 id="Gini系数："><a href="#Gini系数：" class="headerlink" title="Gini系数："></a>Gini系数：</h3><h4 id="数据集P的纯度可以用Gini值来度量。"><a href="#数据集P的纯度可以用Gini值来度量。" class="headerlink" title="数据集P的纯度可以用Gini值来度量。"></a>数据集P的纯度可以用Gini值来度量。</h4><h2 id="Gini-P-sum-k-1-K-p-k-1-p-k-1-sum-k-1-Kp-k-2"><a href="#Gini-P-sum-k-1-K-p-k-1-p-k-1-sum-k-1-Kp-k-2" class="headerlink" title="$Gini(P)=\sum_{k=1}^K p_k(1-p_k)=1-\sum_{k=1}^Kp_k^2$"></a>$Gini(P)=\sum_{k=1}^K p_k(1-p_k)=1-\sum_{k=1}^Kp_k^2$</h2><h4 id="总结：CART决策树使用Gini指数来选择划分属性。-Gini-P-反映了从数据集D中随机抽取了两个样本，其类别标记不一致的概率，因此，-Gini（D）-越小，则数据集P的纯度就越高。"><a href="#总结：CART决策树使用Gini指数来选择划分属性。-Gini-P-反映了从数据集D中随机抽取了两个样本，其类别标记不一致的概率，因此，-Gini（D）-越小，则数据集P的纯度就越高。" class="headerlink" title="总结：CART决策树使用Gini指数来选择划分属性。$Gini(P)$反映了从数据集D中随机抽取了两个样本，其类别标记不一致的概率，因此，$Gini（D）$越小，则数据集P的纯度就越高。"></a>总结：CART决策树使用Gini指数来选择划分属性。$Gini(P)$反映了从数据集D中随机抽取了两个样本，其类别标记不一致的概率，因此，$Gini（D）$越小，则数据集P的纯度就越高。</h4><h3 id="决策树的评价-——-loss-function："><a href="#决策树的评价-——-loss-function：" class="headerlink" title="决策树的评价 —— loss function："></a>决策树的评价 —— loss function：</h3><p>  假定样本的总类别数为$K$个；树T的叶节点个数为$\mid T \mid$，t是树T的叶节点，叶节点有$N_t$个样本点，其中$k$类的样本点有$N_{ik}$个，$H_t(T)$为叶节点t上的<strong>经验熵</strong>，则决策树的loss function可以定义为：  </p><h2 id="C-a-T-sum-t-in-Leaf-N-tH-t-T-a-T"><a href="#C-a-T-sum-t-in-Leaf-N-tH-t-T-a-T" class="headerlink" title="$C_a(T)=\sum_{t\in Leaf}N_tH_t(T)+a|T|$"></a>$C_a(T)=\sum_{t\in Leaf}N_tH_t(T)+a|T|$</h2><h3 id="决策树种避免过拟合的方法——剪枝"><a href="#决策树种避免过拟合的方法——剪枝" class="headerlink" title="决策树种避免过拟合的方法——剪枝"></a>决策树种避免过拟合的方法——剪枝</h3><ul><li><strong>预剪枝</strong>：在决策树生成过程中，对每个结点在划分前先进行估计，如果当前的结点划分不能带来决策树泛化性能提升，则停止划分并将当前的结点标记为叶节点。</li><li><strong>后剪枝</strong>：先从训练集生成一个完整的决策树，然后自底向上的对非叶节点进行考察，若将该结点对应的子树替换为叶节点能带来决策树泛化能力性能提升，则就把该子树替换为叶结点。</li></ul><h4 id="在上面决策树的评价指标loss-function-：-C-a-T-sum-t-in-Leaf-N-tH-t-T-a-T-中，"><a href="#在上面决策树的评价指标loss-function-：-C-a-T-sum-t-in-Leaf-N-tH-t-T-a-T-中，" class="headerlink" title="在上面决策树的评价指标loss function ：$C_a(T)=\sum_{t\in Leaf}N_tH_t(T)+a|T|$中，"></a>在上面决策树的评价指标loss function ：$C_a(T)=\sum_{t\in Leaf}N_tH_t(T)+a|T|$中，</h4><ul><li><p>C(T)表示模型对训练数据集的预测误差，即模型与训练数据的拟合程度，</p></li><li><p>$\mid T \mid$表示模型复杂度，由参数α控制两者之间的影响。 </p></li></ul><h4 id="当α确定时："><a href="#当α确定时：" class="headerlink" title="当α确定时："></a>当α确定时：</h4><ul><li>子树越大，与训练集的拟合越好，但模型的复杂度就越高；</li><li>子树越小，模型简单，但与训练集的拟合度不好。</li></ul><h4 id="决策树生成学习局部的模型，而剪枝学习整体的模型"><a href="#决策树生成学习局部的模型，而剪枝学习整体的模型" class="headerlink" title="决策树生成学习局部的模型，而剪枝学习整体的模型!"></a>决策树生成学习局部的模型，而剪枝学习整体的模型!</h4><h4 id="剪枝的过程为："><a href="#剪枝的过程为：" class="headerlink" title="剪枝的过程为："></a>剪枝的过程为：</h4><ul><li>计算每个节点的经验熵；</li><li>递归的从树的叶节点向上回缩，设一组叶节点回缩到其父节点之前和之后的整体树分别为$T_B$和$T_A$，对应的损失函数值分别为$C_α(T_B)$和$C_α(T_A),$如果$C_α(T_A)&lt;=C_α(T_B)$，则进行剪枝。</li></ul><h3 id="决策树的优缺点"><a href="#决策树的优缺点" class="headerlink" title="决策树的优缺点"></a>决策树的优缺点</h3><ul><li>优点: 决策树对训练属于有很好的分类能力</li><li>缺点: 但对未知的测试数据未必有好的分类能力,泛化 能力弱,即可能发生过拟合现象。 </li></ul><h3 id="Bagging（套袋法）"><a href="#Bagging（套袋法）" class="headerlink" title="Bagging（套袋法）"></a>Bagging（套袋法）</h3><p>bagging的算法过程如下：</p><ol><li>从原始样本集中使用Bootstraping方法随机抽取n个训练样本，共进行k轮抽取，得到k个训练集。（k个训练集之间相互独立，元素可以有重复）</li><li>对于k个训练集，我们训练k个模型（这k个模型可以根据具体问题而定，比如决策树，knn等）</li><li>对于分类问题：由投票表决产生分类结果；对于回归问题：由k个模型预测结果的均值作为最后预测结果。（所有模型的重要性相同）</li></ol><h3 id="Boosting（提升法）"><a href="#Boosting（提升法）" class="headerlink" title="Boosting（提升法）"></a>Boosting（提升法）</h3><p>boosting的算法过程如下：</p><ol><li>对于训练集中的每个样本建立权值wi，表示对每个样本的关注度。当某个样本被误分类的概率很高时，需要加大对该样本的权值。</li><li>进行迭代的过程中，每一步迭代都是一个弱分类器。我们需要用某种策略将其组合，作为最终模型。（例如AdaBoost给每个弱分类器一个权值，将其线性组合最为最终分类器。误差越小的弱分类器，权值越大）</li></ol><h3 id="Bagging，Boosting的主要区别"><a href="#Bagging，Boosting的主要区别" class="headerlink" title="Bagging，Boosting的主要区别"></a>Bagging，Boosting的主要区别</h3><ol><li><p>样本选择上：</p><p>Bagging采用的是Bootstrap随机有放回抽样；</p><p>Boosting每一轮的训练集是不变的，改变的只是每一个样本的权重。</p></li><li><p>样本权重：</p><p>Bagging使用的是均匀取样，每个样本权重相等；</p><p>Boosting根据错误率调整样本权重，错误率越大的样本权重越大。</p></li><li><p>预测函数：</p><p>Bagging所有的预测函数的权重相等；</p><p>Boosting中误差越小的预测函数其权重越大。</p></li><li><p>并行计算：</p><p>Bagging各个预测函数可以并行生成；</p><p>Boosting各个预测函数必须按顺序迭代生成。</p></li></ol><h3 id="决策树与这些算法框架进行结合所得到的新的算法："><a href="#决策树与这些算法框架进行结合所得到的新的算法：" class="headerlink" title="决策树与这些算法框架进行结合所得到的新的算法："></a>决策树与这些算法框架进行结合所得到的新的算法：</h3><p>1）Bagging + 决策树 = 随机森林</p><p>2）AdaBoost + 决策树 = 提升树</p><p>3）Gradient Boosting + 决策树 = GBDT</p><h3 id="随机森林（Random-Forests）"><a href="#随机森林（Random-Forests）" class="headerlink" title="随机森林（Random Forests）"></a>随机森林（Random Forests）</h3><p>随机森林是一种重要的基于Bagging的集成学习方法，可以用来做分类、回归等问题。</p><h3 id="随机森林有许多优点："><a href="#随机森林有许多优点：" class="headerlink" title="随机森林有许多优点："></a>随机森林有许多优点：</h3><ul><li>具有极高的准确率</li><li>随机性的引入，使得随机森林不容易过拟合</li><li>随机性的引入，使得随机森林有很好的抗噪声能力</li><li>能处理很高维度的数据，并且不用做特征选择</li><li>既能处理离散型数据，也能处理连续型数据，数据集无需规范化</li><li>训练速度快，可以得到变量重要性排序</li><li>容易实现并行化</li></ul><h3 id="随机森林的缺点："><a href="#随机森林的缺点：" class="headerlink" title="随机森林的缺点："></a>随机森林的缺点：</h3><ul><li>当随机森林中的决策树个数很多时，训练时需要的空间和时间会较大</li><li>随机森林模型还有许多不好解释的地方，有点算个黑盒模型</li></ul><h3 id="随机森林构建过程"><a href="#随机森林构建过程" class="headerlink" title="随机森林构建过程"></a>随机森林构建过程</h3><h4 id="与上面介绍的Bagging过程相似，随机森林的构建过程大致如下："><a href="#与上面介绍的Bagging过程相似，随机森林的构建过程大致如下：" class="headerlink" title="与上面介绍的Bagging过程相似，随机森林的构建过程大致如下："></a>与上面介绍的Bagging过程相似，随机森林的构建过程大致如下：</h4><ol><li>从原始训练集中使用Bootstraping方法随机有放回采样选出m个样本，共进行n_tree次采样，生成n_tree个训练集</li><li>对于n_tree个训练集，我们分别训练n_tree个决策树模型</li><li>对于单个决策树模型，假设训练样本特征的个数为n，那么每次分裂时根据信息增益/信息增益比/基尼指数选择最好的特征进行分裂</li><li>每棵树都一直这样分裂下去，直到该节点的所有训练样例都属于同一类。在决策树的分裂过程中不需要剪枝</li><li>将生成的多棵决策树组成随机森林。对于分类问题，按多棵树分类器投票决定最终分类结果；对于回归问题，由多棵树预测值的均值决定最终预测结果</li></ol>]]></content>
    
    
    <categories>
      
      <category>机器学习</category>
      
    </categories>
    
    
    <tags>
      
      <tag>机器学习</tag>
      
      <tag>决策树</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>机器学习-逻辑回归</title>
    <link href="/2018/07/ml-logisticRegression/"/>
    <url>/2018/07/ml-logisticRegression/</url>
    
    <content type="html"><![CDATA[<h2 id="逻辑回归"><a href="#逻辑回归" class="headerlink" title="逻辑回归"></a>逻辑回归</h2><h3 id="逻辑回归的概念"><a href="#逻辑回归的概念" class="headerlink" title="逻辑回归的概念"></a>逻辑回归的概念</h3><p><strong>Logistic Regression</strong> 在《机器学习》-周志华一书中又叫<strong>对数几率回归</strong>。逻辑回归和多重线性回归实际上有很多的相同之处，除了它们的因变量（函数）不同外，其他的基本差不多，所以逻辑回归和线性回归又统属于<strong>广义线性模型</strong>（generalizedlinear  model）。</p><p>广义线性模型的形式其实都差不多，不同的就是因变量（函数）的不同。</p><ul><li>如果是<strong>连续</strong>的，就是<strong>多重线性回归</strong></li><li>如果是<strong>二项分布</strong>，就是<strong>Logistic回归</strong></li><li>如果是<strong>Poisson分布</strong>，就是<strong>Poisson分布</strong></li><li>如果是<strong>负二项分布</strong>，就是<strong>负二项回归</strong></li></ul><p>Logistic回归的因变量可以是二分类的，也可以是多分类的，但是二分类的更为常用，也更加容易解释。所以实际中最常用的就是二分类的Logistic回归。 </p><h3 id="线性回归-Logistic回归"><a href="#线性回归-Logistic回归" class="headerlink" title="线性回归-Logistic回归"></a>线性回归-Logistic回归</h3><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20200530230108.png" srcset="/img/loading.gif" alt="image"></p><h3 id="Logistic回归的主要用途："><a href="#Logistic回归的主要用途：" class="headerlink" title="Logistic回归的主要用途："></a>Logistic回归的<strong>主要用途</strong>：</h3><ul><li>寻找危险因素：寻找某一疾病的危险因素等；</li><li>预测：根据模型，预测在不同的自变量情况下，发生某病或某种情况的概率有多大；</li><li>判别：实际上跟预测有些类似，也是根据模型，判断某人属于某病或属于某种情况的概率有多大，也就是看一下这个人有多大的可能性是属于某病。</li></ul><p>Logistic回归主要在<strong>流行病学</strong>中应用较多，比较常用的情形是探索某疾病的危险因素，根据危险因素预测某疾病发生的概率，等等。例如，想探讨胃癌发生的危险因素，可以选择两组人群，一组是胃癌组，一组是非胃癌组，两组人群肯定有不同的体征和生活方式等。这里的因变量就是是否胃癌，即“是”或“否”，自变量就可以包括很多了，例如年龄、性别、饮食习惯、幽门螺杆菌感染等。自变量既可以是连续的，也可以是分类的。 </p><h3 id="常规步骤"><a href="#常规步骤" class="headerlink" title="常规步骤"></a><strong>常规步骤</strong></h3><p>Regression问题的常规步骤为：</p><ol><li>寻找<strong>h</strong>函数（即hypothesis）；</li><li>构造<strong>J</strong>函数（损失函数）；</li><li>想办法使得J<strong>函数最小</strong>并求得回归参数（θ）</li></ol><h3 id="构造预测函数（hypothesis）"><a href="#构造预测函数（hypothesis）" class="headerlink" title="构造预测函数（hypothesis）"></a>构造预测函数（hypothesis）</h3><p>Logistic回归虽然名字里带“回归”，但是它实际上是一种分类方法，主要用于两分类问题（即输出只有两种，分别代表两个类别），所以利用了Logistic函数（或称为<strong>Sigmoid函数</strong>），函数形式为：</p><h2 id="g-z-frac-1-1-e-z"><a href="#g-z-frac-1-1-e-z" class="headerlink" title="$g(z)=\frac{1}{1+e^{-z}}$"></a>$g(z)=\frac{1}{1+e^{-z}}$</h2><p>为了方便后面使用我们求出$g(z)$的导数：</p><h2 id="g-‘-z-frac1-1-e-z-cdot-1-frac1-1-e-z-g-z-1-g-z"><a href="#g-‘-z-frac1-1-e-z-cdot-1-frac1-1-e-z-g-z-1-g-z" class="headerlink" title="$g^{‘}(z)=\frac1{1+e^{-z} } \cdot (1- \frac1{1+e^{-z} })=g(z)(1-g(z))$"></a>$g^{‘}(z)=\frac1{1+e^{-z} } \cdot (1- \frac1{1+e^{-z} })=g(z)(1-g(z))$</h2><p>Sigmoid 函数在有个很漂亮的“S”形，如下图所示 </p><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20200530230109.png" srcset="/img/loading.gif" alt="image"></p><h3 id="构建预测函数"><a href="#构建预测函数" class="headerlink" title="构建预测函数"></a>构建预测函数</h3><p><strong>决策边界</strong>又分为<strong>线性的决策边界</strong>和<strong>非线性的决策边界</strong></p><p>线性的决策边界：</p><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20200530230110.png" srcset="/img/loading.gif" alt="1"></p><p>非线性的决策边界：</p><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20200530230111.png" srcset="/img/loading.gif" alt="2"></p><p>对于线性边界的情况，边界形式如下：</p><h2 id="theta-0-theta-1x-1-cdot-cdot-cdot-theta-nx-n-sum-i-1-n-theta-ix-i-theta-Tx"><a href="#theta-0-theta-1x-1-cdot-cdot-cdot-theta-nx-n-sum-i-1-n-theta-ix-i-theta-Tx" class="headerlink" title="$\theta_0+\theta_1x_1+\cdot \cdot \cdot + \theta_nx_n=\sum_{i=1}^n \theta_ix_i = \theta^Tx$"></a>$\theta_0+\theta_1x_1+\cdot \cdot \cdot + \theta_nx_n=\sum_{i=1}^n \theta_ix_i = \theta^Tx$</h2><p>构建的预测函数为：</p><h2 id="h-theta-x-g-theta-Tx-frac1-1-e-theta-Tx"><a href="#h-theta-x-g-theta-Tx-frac1-1-e-theta-Tx" class="headerlink" title="$h_\theta(x)=g(\theta^Tx)=\frac1{1+e^{-\theta^Tx}}$"></a>$h_\theta(x)=g(\theta^Tx)=\frac1{1+e^{-\theta^Tx}}$</h2><h3 id="构建损失函数"><a href="#构建损失函数" class="headerlink" title="构建损失函数"></a>构建损失函数</h3><p>由于是二项分布，函数<strong>$h_ \theta(x)$</strong>的值就有特殊的含义，它所表示的是结果取1的概率，因此对于输入x的分类结果就判别为类别1和类别0的概率分别为：</p><h2 id="P-y-1-x-theta-h-theta-x-P-y-0-x-theta-1-h-theta-x"><a href="#P-y-1-x-theta-h-theta-x-P-y-0-x-theta-1-h-theta-x" class="headerlink" title="$P(y=1|x;\theta)=h_\theta(x) \\ P(y=0|x;\theta)=1-h_\theta(x)$"></a>$P(y=1|x;\theta)=h_\theta(x) \\ P(y=0|x;\theta)=1-h_\theta(x)$</h2><p>所以：</p><h2 id="P-y-x-theta-h-theta-x-y-1-h-theta-x-1-y"><a href="#P-y-x-theta-h-theta-x-y-1-h-theta-x-1-y" class="headerlink" title="$P(y|x;\theta) = {h_\theta(x) }^y{(1-h_\theta(x)) }^{1-y}$"></a>$P(y|x;\theta) = {h_\theta(x) }^y{(1-h_\theta(x)) }^{1-y}$</h2><h3 id="构建似然函数"><a href="#构建似然函数" class="headerlink" title="构建似然函数"></a>构建似然函数</h3><h2 id="L-theta-prod-i-1-n-P-y-i-x-i-theta-prod-i-1-n-h-theta-x-i-y-i-1-h-theta-x-i-1-y-i"><a href="#L-theta-prod-i-1-n-P-y-i-x-i-theta-prod-i-1-n-h-theta-x-i-y-i-1-h-theta-x-i-1-y-i" class="headerlink" title="$L(\theta)=\prod_{i=1}^n P(y_i|x_i;\theta) =\prod_{i=1}^n {h_\theta(x_i) }^{y_i}{(1-h_\theta(x_i)) }^{1-y_i}$"></a>$L(\theta)=\prod_{i=1}^n P(y_i|x_i;\theta) =\prod_{i=1}^n {h_\theta(x_i) }^{y_i}{(1-h_\theta(x_i)) }^{1-y_i}$</h2><h3 id="对数似然函数"><a href="#对数似然函数" class="headerlink" title="对数似然函数"></a>对数似然函数</h3><h2 id="l-theta-logL-theta-sum-i-1-n-y-ilogh-theta-x-i-1-y-i-log-1-h-theta-x-i"><a href="#l-theta-logL-theta-sum-i-1-n-y-ilogh-theta-x-i-1-y-i-log-1-h-theta-x-i" class="headerlink" title="$l(\theta)=logL(\theta)=\sum_{i=1}^n(y_ilogh_\theta(x_i)+(1-y_i)log(1-h_\theta(x_i)))$"></a>$l(\theta)=logL(\theta)=\sum_{i=1}^n(y_ilogh_\theta(x_i)+(1-y_i)log(1-h_\theta(x_i)))$</h2><p>最大似然估计就是求使<strong>$l(\theta)$</strong>取最大值时的θ，其实这里可以使用梯度上升法求解，求得的θ就是要求的最佳参数。</p><h3 id="梯度下降法求的最小值"><a href="#梯度下降法求的最小值" class="headerlink" title="梯度下降法求的最小值"></a><strong>梯度下降法求的最小值</strong></h3><h4 id="θ更新过程："><a href="#θ更新过程：" class="headerlink" title="θ更新过程："></a>θ更新过程：</h4><h2 id="theta-j-theta-j-a-frac-partial-l-theta-partial-theta-j"><a href="#theta-j-theta-j-a-frac-partial-l-theta-partial-theta-j" class="headerlink" title="$\theta_j :=\theta_j-a(\frac{\partial l(\theta)}{\partial \theta_j})$"></a><strong><em>$\theta_j :=\theta_j-a(\frac{\partial l(\theta)}{\partial \theta_j})$</em></strong></h2><h4 id="对-theta-求偏导"><a href="#对-theta-求偏导" class="headerlink" title="对$\theta$求偏导"></a>对$\theta$求偏导</h4><h2 id="frac-partial-l-theta-partial-theta-j-frac-partial-g-theta-Tx-partial-theta-j-frac-y-g-theta-Tx-frac-1-y-g-theta-Tx"><a href="#frac-partial-l-theta-partial-theta-j-frac-partial-g-theta-Tx-partial-theta-j-frac-y-g-theta-Tx-frac-1-y-g-theta-Tx" class="headerlink" title="$\frac{\partial l(\theta)}{\partial \theta_j}=\frac{\partial g(\theta^Tx)}{\partial \theta_j}(\frac{y}{g(\theta^Tx)}-\frac{1-y}{g(\theta^Tx)})$"></a>$\frac{\partial l(\theta)}{\partial \theta_j}=\frac{\partial g(\theta^Tx)}{\partial \theta_j}(\frac{y}{g(\theta^Tx)}-\frac{1-y}{g(\theta^Tx)})$</h2><h2 id="g-theta-Tx-1-g-theta-Tx-frac-partial-theta-Tx-partial-theta-j-frac-y-g-theta-Tx-frac-1-y-g-theta-Tx"><a href="#g-theta-Tx-1-g-theta-Tx-frac-partial-theta-Tx-partial-theta-j-frac-y-g-theta-Tx-frac-1-y-g-theta-Tx" class="headerlink" title="$=g(\theta^Tx)(1-g(\theta^Tx)) \frac{\partial\theta^Tx}{\partial \theta_j}(\frac{y}{g(\theta^Tx)}-\frac{1-y}{g(\theta^Tx)})$"></a>$=g(\theta^Tx)(1-g(\theta^Tx)) \frac{\partial\theta^Tx}{\partial \theta_j}(\frac{y}{g(\theta^Tx)}-\frac{1-y}{g(\theta^Tx)})$</h2><h2 id="y-1-g-theta-Tx-1-y-g-theta-Tx-x-j"><a href="#y-1-g-theta-Tx-1-y-g-theta-Tx-x-j" class="headerlink" title="$=(y(1-g(\theta^Tx))-(1-y)g(\theta^Tx))x_j$"></a>$=(y(1-g(\theta^Tx))-(1-y)g(\theta^Tx))x_j$</h2><h2 id="y-h-theta-x-x-j"><a href="#y-h-theta-x-x-j" class="headerlink" title="$=(y-h_\theta(x))x_j$"></a>$=(y-h_\theta(x))x_j$</h2><h4 id="θ更新过程就可以写为："><a href="#θ更新过程就可以写为：" class="headerlink" title="θ更新过程就可以写为："></a>θ更新过程就可以写为：</h4><h2 id="theta-j-theta-j-a-sum-i-1-n-y-i-h-theta-x-i-x-i-j"><a href="#theta-j-theta-j-a-sum-i-1-n-y-i-h-theta-x-i-x-i-j" class="headerlink" title="$\theta_j :=\theta_j-a\sum_{i=1}^n (y_i-h_\theta(x_i))x_i^j$"></a>$\theta_j :=\theta_j-a\sum_{i=1}^n (y_i-h_\theta(x_i))x_i^j$</h2><h4 id="但是在在Andrew-Ng的课程中将-J-theta-取为下式，即："><a href="#但是在在Andrew-Ng的课程中将-J-theta-取为下式，即：" class="headerlink" title="但是在在Andrew Ng的课程中将 $J(\theta)$取为下式，即："></a>但是在在Andrew Ng的课程中将 $J(\theta)$取为下式，即：</h4><h2 id="J-theta-frac-1-m-l-theta"><a href="#J-theta-frac-1-m-l-theta" class="headerlink" title="$J(\theta)=-\frac{1}{m}l(\theta)$"></a>$J(\theta)=-\frac{1}{m}l(\theta)$</h2><h4 id="因为乘了一个负的系数-1-m，所以取-J-theta-最小值时的θ为要求的最佳参数。"><a href="#因为乘了一个负的系数-1-m，所以取-J-theta-最小值时的θ为要求的最佳参数。" class="headerlink" title="因为乘了一个负的系数-1/m，所以取 $J(\theta)$最小值时的θ为要求的最佳参数。"></a>因为乘了一个负的系数-1/m，所以取 $J(\theta)$最小值时的θ为要求的最佳参数。</h4><h2 id="frac-partial-l-theta-partial-theta-j-frac-1-m-sum-i-1-n-h-theta-x-i-y-i-x-i-j"><a href="#frac-partial-l-theta-partial-theta-j-frac-1-m-sum-i-1-n-h-theta-x-i-y-i-x-i-j" class="headerlink" title="$\frac{\partial l(\theta)}{\partial \theta_j}=\frac 1 m \sum_{i=1}^n(h_\theta(x_i)-y_i)x_i^j$"></a>$\frac{\partial l(\theta)}{\partial \theta_j}=\frac 1 m \sum_{i=1}^n(h_\theta(x_i)-y_i)x_i^j$</h2><h4 id="相应的-theta"><a href="#相应的-theta" class="headerlink" title="相应的$\theta$:"></a>相应的$\theta$:</h4><h2 id="theta-j-theta-j-a-frac-1-m-sum-i-1-n-h-theta-x-i-y-i-x-i-j"><a href="#theta-j-theta-j-a-frac-1-m-sum-i-1-n-h-theta-x-i-y-i-x-i-j" class="headerlink" title="$\theta_j :=\theta_j-a \frac 1 m \sum_{i=1}^n (h_\theta(x_i)-y_i)x_i^j$"></a>$\theta_j :=\theta_j-a \frac 1 m \sum_{i=1}^n (h_\theta(x_i)-y_i)x_i^j$</h2><h3 id="向量化（Vectorization-）"><a href="#向量化（Vectorization-）" class="headerlink" title="向量化（Vectorization ）"></a>向量化（<strong>Vectorization</strong> ）</h3><p>Vectorization是使用矩阵计算来代替for循环，以简化计算过程，提高效率。</p><p>如上式，Σ(…)是一个求和的过程，显然需要一个for语句循环m次，所以根本没有完全的实现vectorization。</p><h4 id="下面介绍向量化的过程："><a href="#下面介绍向量化的过程：" class="headerlink" title="下面介绍向量化的过程："></a>下面介绍向量化的过程：</h4><p>约定训练数据的矩阵形式如下，x的每一行为一条训练样本，而每一列为不同的特称取值：</p><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20200530230112.png" srcset="/img/loading.gif" alt="image"></p><h5 id="g-A-的参数A为一列向量，所以实现g函数时要支持列向量作为参数，并返回列向量。由上式可知-h-theta-x-y-可以由-g-A-y-一次求得"><a href="#g-A-的参数A为一列向量，所以实现g函数时要支持列向量作为参数，并返回列向量。由上式可知-h-theta-x-y-可以由-g-A-y-一次求得" class="headerlink" title="g(A)的参数A为一列向量，所以实现g函数时要支持列向量作为参数，并返回列向量。由上式可知 $h_\theta(x)-y$可以由$g(A)-y$一次求得"></a>g(A)的参数A为一列向量，所以实现g函数时要支持列向量作为参数，并返回列向量。由上式可知 $h_\theta(x)-y$可以由$g(A)-y$一次求得</h5><h4 id="θ更新过程可以改为"><a href="#θ更新过程可以改为" class="headerlink" title="θ更新过程可以改为"></a>θ更新过程可以改为</h4><h2 id="theta-j-theta-j-a-frac-1-m-sum-i-1-n-h-theta-x-i-y-i-x-i-j-theta-j-a-frac-1-m-sum-i-1-n-e-ix-i-j-theta-j-a-frac1-m-x-TE"><a href="#theta-j-theta-j-a-frac-1-m-sum-i-1-n-h-theta-x-i-y-i-x-i-j-theta-j-a-frac-1-m-sum-i-1-n-e-ix-i-j-theta-j-a-frac1-m-x-TE" class="headerlink" title="$\theta_j :=\theta_j-a \frac 1 m \sum_{i=1}^n (h_\theta(x_i)-y_i)x_i^j=\theta_j-a \frac 1 m \sum_{i=1}^n e_ix_i^j=\theta_j-a \frac1 m x^TE$"></a>$\theta_j :=\theta_j-a \frac 1 m \sum_{i=1}^n (h_\theta(x_i)-y_i)x_i^j=\theta_j-a \frac 1 m \sum_{i=1}^n e_ix_i^j=\theta_j-a \frac1 m x^TE$</h2><h4 id="综上所述，Vectorization后θ更新的步骤如下："><a href="#综上所述，Vectorization后θ更新的步骤如下：" class="headerlink" title="综上所述，Vectorization后θ更新的步骤如下："></a>综上所述，Vectorization后θ更新的步骤如下：</h4><hr><ol><li><h4 id="求-A-x-cdot-theta"><a href="#求-A-x-cdot-theta" class="headerlink" title="求:$A=x \cdot \theta$"></a>求:$A=x \cdot \theta$</h4></li><li><h4 id="求-E-g-A-y"><a href="#求-E-g-A-y" class="headerlink" title="求$E=g(A)-y$"></a>求$E=g(A)-y$</h4></li><li><h4 id="求-theta-theta-ax-TE"><a href="#求-theta-theta-ax-TE" class="headerlink" title="求$\theta:=\theta-ax^TE$"></a>求$\theta:=\theta-ax^TE$</h4></li></ol><hr><h3 id="正则化Regularization"><a href="#正则化Regularization" class="headerlink" title="正则化Regularization"></a><strong>正则化Regularization</strong></h3><p>同样逻辑回归也有<strong>欠拟合、适合拟合、过拟合问题</strong></p><p>对于线性回归或逻辑回归的损失函数构成的模型，可能会有些权重很大，有些权重很小，导致过拟合（就是过分拟合了训练数据），使得模型的复杂度提高，泛化能力较差（对未知数据的预测能力）。</p><p>下面左图即为欠拟合，中图为合适的拟合，右图为过拟合。</p><p>​             <img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20200530230113.png" srcset="/img/loading.gif" alt="image"></p><p>过拟合问题往往源自过多的特征。</p><h4 id="解决方法"><a href="#解决方法" class="headerlink" title="解决方法"></a><strong>解决方法</strong></h4><p>1）减少特征数量（减少特征会失去一些信息，即使特征选的很好）</p><ul><li>可用人工选择要保留的特征；</li><li>模型选择算法；</li></ul><p>2）正则化（特征较多时比较有效）</p><ul><li>保留所有特征，但减少θ的大小</li></ul><h4 id="正则化方法"><a href="#正则化方法" class="headerlink" title="正则化方法"></a><strong>正则化方法</strong></h4><p>正则化是结构风险最小化策略的实现，是在经验风险上加一个正则化项或惩罚项。正则化项一般是模型复杂度的单调递增函数，模型越复杂，正则化项就越大。</p><p>在<a href="https://0520.tech/2018/07/21/2018-07-21-ml-linearRegressionL1L2/">线性回归算法的正则化问题</a>,正则项可以取不同的形式，在回归问题中取平方损失，就是参数的L2范数，也可以取L1范数。取平方损失时，模型的损失函数变为： </p><h2 id="J-theta-frac1-2m-sum-i-1-n-h-theta-x-i-y-i-2-lambda-sum-j-1-n-theta-j-2"><a href="#J-theta-frac1-2m-sum-i-1-n-h-theta-x-i-y-i-2-lambda-sum-j-1-n-theta-j-2" class="headerlink" title="$J(\theta)=\frac1{2m}\sum_{i=1}^{n}(h_\theta(x_i)-y_i)^2+\lambda\sum_{j=1}^n \theta_j^2$"></a>$J(\theta)=\frac1{2m}\sum_{i=1}^{n}(h_\theta(x_i)-y_i)^2+\lambda\sum_{j=1}^n \theta_j^2$</h2><h4 id="lambda是正则项系数："><a href="#lambda是正则项系数：" class="headerlink" title="lambda是正则项系数："></a>lambda是正则项系数：</h4><ul><li>如果它的值很大，说明对模型的复杂度惩罚大，对拟合数据的损失惩罚小，这样它就不会过分拟合数据，在训练数据上的偏差较大，在未知数据上的方差较小，但是可能出现欠拟合的现象；</li><li>如果它的值很小，说明比较注重对训练数据的拟合，在训练数据上的偏差会小，但是可能会导致过拟合。</li></ul><h4 id="正则化后的梯度下降算法θ的更新变为："><a href="#正则化后的梯度下降算法θ的更新变为：" class="headerlink" title="正则化后的梯度下降算法θ的更新变为："></a>正则化后的梯度下降算法θ的更新变为：</h4><h2 id="theta-j-theta-j-a-frac-1-m-sum-i-1-n-h-theta-x-i-y-i-x-i-j-frac-lambda-m-theta-j"><a href="#theta-j-theta-j-a-frac-1-m-sum-i-1-n-h-theta-x-i-y-i-x-i-j-frac-lambda-m-theta-j" class="headerlink" title="$\theta_j :=\theta_j-a \frac 1 m \sum_{i=1}^n (h_\theta(x_i)-y_i)x_i^j - \frac \lambda m \theta_j$"></a>$\theta_j :=\theta_j-a \frac 1 m \sum_{i=1}^n (h_\theta(x_i)-y_i)x_i^j - \frac \lambda m \theta_j$</h2><h3 id="其他优化算法"><a href="#其他优化算法" class="headerlink" title="其他优化算法"></a><strong>其他优化算法</strong></h3><ul><li>Conjugate gradient method(共轭梯度法)</li><li>Quasi-Newton method(拟牛顿法)</li><li>BFGS method(局部优化法)</li><li>L-BFGS(Limited-memory BFGS)（有限内存局部优化法）</li></ul><p>后二者由拟牛顿法引申出来，与梯度下降算法相比，这些算法的优点是：</p><ul><li>第一，不需要手动的选择步长；</li><li>第二，通常比梯度下降算法快；</li></ul><p>但是缺点是更复杂。</p><h3 id="多类分类问题"><a href="#多类分类问题" class="headerlink" title="多类分类问题"></a>多类分类问题</h3><p>多类分类问题中,我们的训练集中有多个类(&gt;2),我们无法仅仅用一个二元变量(0或1)来做判断依据。例如我们要预测天气情况分四种类型:晴天、多云、下雨或下雪。下面是一个多类分类问题可能的情况: </p><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20200530230114.png" srcset="/img/loading.gif" alt="image"></p><p>一种解决这类问题的途径是采用一对多(One-vs-All)方法（可以将其看做成二类分类问题：保留其中的一类，剩下的作为另一类 ）。在一对多方法中,我们将多类分类问题转化成二元分类问题。为了能实现这样的转变,我们将多个类中的一个类标记为正向类(y=1),然后将其他所有类都标记为负向类,这个模型记作：</p><h2 id="h-theta-1-x"><a href="#h-theta-1-x" class="headerlink" title="$h_\theta^{(1)}(x)$"></a>$h_\theta^{(1)}(x)$</h2><p>接着,类似地第我们选择另一个类标记为正向类(y=2),再将其它类都标记为负向类,将这个模型记作,</p><h2 id="h-theta-2-x"><a href="#h-theta-2-x" class="headerlink" title="$h_\theta^{(2)}(x)$"></a>$h_\theta^{(2)}(x)$</h2><p>依此类推。最后我们得到一系列的模型简记为: </p><h2 id="h-theta-i-x-p-y-i-x-theta"><a href="#h-theta-i-x-p-y-i-x-theta" class="headerlink" title="$h_\theta^{(i)}(x)=p(y=i|x;\theta)$"></a>$h_\theta^{(i)}(x)=p(y=i|x;\theta)$</h2><p>其中 i = 1,2,3,…,k步骤可以记作下图： </p><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20200530230115.png" srcset="/img/loading.gif" alt="image"></p><p>最后,在我们需要做预测时,我们将所有的分类机都运行一遍,然后对每一个输入变量,都选择最高可能性的输出变量。</p>]]></content>
    
    
    <categories>
      
      <category>机器学习</category>
      
    </categories>
    
    
    <tags>
      
      <tag>机器学习</tag>
      
      <tag>逻辑回归</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>机器学习之线性回归python实现</title>
    <link href="/2018/07/ml-linearRegression-python/"/>
    <url>/2018/07/ml-linearRegression-python/</url>
    
    <content type="html"><![CDATA[<h2 id="首先导入所需要的库文件"><a href="#首先导入所需要的库文件" class="headerlink" title="首先导入所需要的库文件"></a>首先导入所需要的库文件</h2><pre><code class="hljs python"><span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt  <span class="hljs-comment"># 导入可视化库</span><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np               <span class="hljs-comment"># 导入数据处理库</span><span class="hljs-keyword">from</span> sklearn <span class="hljs-keyword">import</span> datasets     <span class="hljs-comment"># 导入sklearn自带的数据集</span></code></pre><h2 id="把我们的参数值求解公式-theta-X-TX-1-X-TY-转换为代码"><a href="#把我们的参数值求解公式-theta-X-TX-1-X-TY-转换为代码" class="headerlink" title="把我们的参数值求解公式$\theta=(X^TX)^{-1}X^TY$转换为代码"></a>把我们的参数值求解公式$\theta=(X^TX)^{-1}X^TY$转换为代码</h2><pre><code class="hljs python"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">fit</span><span class="hljs-params">(self, X, y)</span>:</span>                    <span class="hljs-comment"># 训练集的拟合</span>        X = np.insert(X, <span class="hljs-number">0</span>, <span class="hljs-number">1</span>, axis=<span class="hljs-number">1</span>)  <span class="hljs-comment"># 增加一个维度</span>        <span class="hljs-keyword">print</span> (X.shape)                X_ = np.linalg.inv(X.T.dot(X))  <span class="hljs-comment"># 公式求解 -- X.T表示转置，X.dot(Y)表示矩阵相乘</span>        self.w = X_.dot(X.T).dot(y)     <span class="hljs-comment"># 返回theta的值</span></code></pre><h1 id="其中：-X-TX-1-表示为："><a href="#其中：-X-TX-1-表示为：" class="headerlink" title="其中：$(X^TX)^{-1} $表示为："></a>其中：$(X^TX)^{-1} $表示为：</h1><pre><code class="hljs python">X_ = np.linalg.inv(X.T.dot(X))  <span class="hljs-comment"># 公式求解 -- X.T表示转置，X.dot(Y)表示矩阵相乘</span> np.linalg.inv() 表示求逆矩阵</code></pre><h1 id="其中：-X-TY-表示为："><a href="#其中：-X-TY-表示为：" class="headerlink" title="其中：$X^TY $表示为："></a>其中：$X^TY $表示为：</h1><pre><code class="hljs python">X.T.dot(X)</code></pre><h1 id="所以完整公式-theta-X-TX-1-X-TY-表示为："><a href="#所以完整公式-theta-X-TX-1-X-TY-表示为：" class="headerlink" title="所以完整公式$\theta=(X^TX)^{-1}X^TY$表示为："></a>所以完整公式$\theta=(X^TX)^{-1}X^TY$表示为：</h1><pre><code class="hljs python">X_.dot(X.T).dot(y)</code></pre><h2 id="由于我们最终得到的线性回归函数是-y-theta-x-b-即预测函数："><a href="#由于我们最终得到的线性回归函数是-y-theta-x-b-即预测函数：" class="headerlink" title="由于我们最终得到的线性回归函数是$y=\theta x+b$即预测函数："></a>由于我们最终得到的线性回归函数是$y=\theta x+b$即预测函数：</h2><pre><code class="hljs python"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">predict</span><span class="hljs-params">(self, X)</span>:</span>               <span class="hljs-comment"># 测试集的测试反馈</span>                                    <span class="hljs-comment"># 为偏置权值插入常数项</span>    X = np.insert(X, <span class="hljs-number">0</span>, <span class="hljs-number">1</span>, axis=<span class="hljs-number">1</span>)  <span class="hljs-comment"># 增加一个维度</span>    y_pred = X.dot(self.w)          <span class="hljs-comment"># 测试集与拟合的训练集相乘</span>    <span class="hljs-keyword">return</span> y_pred                   <span class="hljs-comment"># 返回最终的预测值</span></code></pre><h2 id="其中得到的预测结果y-pred-X-text-cdot-theta"><a href="#其中得到的预测结果y-pred-X-text-cdot-theta" class="headerlink" title="其中得到的预测结果y_pred=X_text$\cdot\theta$"></a>其中得到的预测结果y_pred=X_text$\cdot\theta$</h2><pre><code class="hljs python">y_pred = X.dot(self.w)          <span class="hljs-comment"># 测试集与参数值相乘</span></code></pre><h2 id="最终得出线性回归代码："><a href="#最终得出线性回归代码：" class="headerlink" title="最终得出线性回归代码："></a>最终得出线性回归代码：</h2><pre><code class="hljs python"><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">LinearRegression</span><span class="hljs-params">()</span>:</span>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span><span class="hljs-params">(self)</span>:</span>          <span class="hljs-comment"># 新建变量</span>        self.w = <span class="hljs-literal">None</span>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">fit</span><span class="hljs-params">(self, X, y)</span>:</span>         <span class="hljs-comment"># 训练集的拟合</span>        X = np.insert(X, <span class="hljs-number">0</span>, <span class="hljs-number">1</span>, axis=<span class="hljs-number">1</span>)  <span class="hljs-comment"># 增加一个维度</span>        <span class="hljs-keyword">print</span> (X.shape)                X_ = np.linalg.inv(X.T.dot(X))  <span class="hljs-comment"># 公式求解 -- X.T表示转置，X.dot(Y)表示矩阵相乘</span>        self.w = X_.dot(X.T).dot(y)     <span class="hljs-comment"># 返回theta的值</span>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">predict</span><span class="hljs-params">(self, X)</span>:</span>               <span class="hljs-comment"># 测试集的测试反馈</span>                                        <span class="hljs-comment"># 为偏置权值插入常数项</span>        X = np.insert(X, <span class="hljs-number">0</span>, <span class="hljs-number">1</span>, axis=<span class="hljs-number">1</span>)  <span class="hljs-comment"># 增加一个维度</span>        y_pred = X.dot(self.w)          <span class="hljs-comment"># 测试集与拟合的训练集相乘</span>        <span class="hljs-keyword">return</span> y_pred                   <span class="hljs-comment"># 返回最终的预测值</span></code></pre><h2 id="同时我们需要得出预测值与真实值的一个平方平均值"><a href="#同时我们需要得出预测值与真实值的一个平方平均值" class="headerlink" title="同时我们需要得出预测值与真实值的一个平方平均值"></a>同时我们需要得出预测值与真实值的一个平方平均值</h2><pre><code class="hljs python"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">mean_squared_error</span><span class="hljs-params">(y_true, y_pred)</span>:</span>                                        <span class="hljs-comment">#真实数据与预测数据之间的差值（平方平均）</span>    mse = np.mean(np.power(y_true - y_pred, <span class="hljs-number">2</span>))    <span class="hljs-keyword">return</span> mse</code></pre><h2 id="最后就是进行我数据的加载，训练，测试过程以及可视化"><a href="#最后就是进行我数据的加载，训练，测试过程以及可视化" class="headerlink" title="最后就是进行我数据的加载，训练，测试过程以及可视化"></a>最后就是进行我数据的加载，训练，测试过程以及可视化</h2><pre><code class="hljs python"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">main</span><span class="hljs-params">()</span>:</span>    <span class="hljs-comment"># 第一步：导入数据</span>    <span class="hljs-comment"># 加载糖尿病数据集</span>        diabetes = datasets.load_diabetes()    <span class="hljs-comment"># 只使用其中一个特征值</span>    X = diabetes.data[:, np.newaxis, <span class="hljs-number">2</span>]    <span class="hljs-keyword">print</span> (X.shape)    <span class="hljs-comment">#第二步：将数据分为训练集以及测试集</span>    x_train, x_test = X[:<span class="hljs-number">-20</span>], X[<span class="hljs-number">-20</span>:]    y_train, y_test = diabetes.target[:<span class="hljs-number">-20</span>], diabetes.target[<span class="hljs-number">-20</span>:]    <span class="hljs-comment">#第三步：导入线性回归类（之前定义的）</span>    clf = LinearRegression()    clf.fit(x_train, y_train)    <span class="hljs-comment"># 训练</span>    y_pred = clf.predict(x_test) <span class="hljs-comment"># 测试</span>    <span class="hljs-comment">#第四步：测试误差计算（需要引入一个函数）</span>    <span class="hljs-comment"># 打印平均值平方误差</span>    <span class="hljs-keyword">print</span> (<span class="hljs-string">"Mean Squared Error:"</span>, mean_squared_error(y_test, y_pred))    <span class="hljs-comment">#matplotlib可视化输出</span>    <span class="hljs-comment"># Plot the results</span>    plt.scatter(x_test[:,<span class="hljs-number">0</span>], y_test,  color=<span class="hljs-string">'black'</span>)         <span class="hljs-comment"># 散点输出</span>    plt.plot(x_test[:,<span class="hljs-number">0</span>], y_pred, color=<span class="hljs-string">'blue'</span>, linewidth=<span class="hljs-number">3</span>) <span class="hljs-comment"># 预测输出</span>    plt.show()</code></pre><h2 id="可视化结果"><a href="#可视化结果" class="headerlink" title="可视化结果"></a>可视化结果</h2><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20200530225637.png" srcset="/img/loading.gif" alt="image"></p><h2 id="完整线性回归的代码"><a href="#完整线性回归的代码" class="headerlink" title="完整线性回归的代码"></a>完整线性回归的代码</h2><pre><code class="hljs python"><span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt  <span class="hljs-comment"># 导入可视化库</span><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np               <span class="hljs-comment"># 导入数据处理库</span><span class="hljs-keyword">from</span> sklearn <span class="hljs-keyword">import</span> datasets     <span class="hljs-comment"># 导入sklearn自带的数据集</span><span class="hljs-keyword">import</span> csv<span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">LinearRegression</span><span class="hljs-params">()</span>:</span>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span><span class="hljs-params">(self)</span>:</span>          <span class="hljs-comment"># 新建变量</span>        self.w = <span class="hljs-literal">None</span>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">fit</span><span class="hljs-params">(self, X, y)</span>:</span>         <span class="hljs-comment"># 训练集的拟合</span>        X = np.insert(X, <span class="hljs-number">0</span>, <span class="hljs-number">1</span>, axis=<span class="hljs-number">1</span>)  <span class="hljs-comment"># 增加一个维度</span>        <span class="hljs-keyword">print</span> (X.shape)                X_ = np.linalg.inv(X.T.dot(X))  <span class="hljs-comment"># 公式求解 -- X.T表示转置，X.dot(Y)表示矩阵相乘</span>        self.w = X_.dot(X.T).dot(y)     <span class="hljs-comment"># 返回theta的值</span>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">predict</span><span class="hljs-params">(self, X)</span>:</span>               <span class="hljs-comment"># 测试集的测试反馈</span>                                        <span class="hljs-comment"># 为偏置权值插入常数项</span>        X = np.insert(X, <span class="hljs-number">0</span>, <span class="hljs-number">1</span>, axis=<span class="hljs-number">1</span>)  <span class="hljs-comment"># 增加一个维度</span>        y_pred = X.dot(self.w)          <span class="hljs-comment"># 测试集与拟合的训练集相乘</span>        <span class="hljs-keyword">return</span> y_pred                   <span class="hljs-comment"># 返回最终的预测值</span><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">mean_squared_error</span><span class="hljs-params">(y_true, y_pred)</span>:</span>                                        <span class="hljs-comment">#真实数据与预测数据之间的差值（平方平均）</span>    mse = np.mean(np.power(y_true - y_pred, <span class="hljs-number">2</span>))    <span class="hljs-keyword">return</span> mse<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">main</span><span class="hljs-params">()</span>:</span>    <span class="hljs-comment"># 第一步：导入数据</span>    <span class="hljs-comment"># 加载糖尿病数据集</span>    diabetes = datasets.load_diabetes()    <span class="hljs-comment"># 只使用其中一个特征值(把一个422x10的矩阵提取其中一列变成422x1)</span>    X = diabetes.data[:, np.newaxis, <span class="hljs-number">2</span>]  <span class="hljs-comment"># np.newaxis的作用就是在原来的数组上增加一个维度。2表示提取第三列数据</span>    <span class="hljs-keyword">print</span> (X.shape)    <span class="hljs-comment"># 第二步：将数据分为训练集以及测试集</span>    x_train, x_test = X[:<span class="hljs-number">-20</span>], X[<span class="hljs-number">-20</span>:]    print(x_train.shape,x_test.shape)  <span class="hljs-comment"># (422, 1) (20, 1)</span>    <span class="hljs-comment"># 将目标分为训练/测试集合</span>    y_train, y_test = diabetes.target[:<span class="hljs-number">-20</span>], diabetes.target[<span class="hljs-number">-20</span>:]    print(y_train.shape,y_test.shape)  <span class="hljs-comment"># (422,) (20,)</span>    <span class="hljs-comment">#第三步：导入线性回归类（之前定义的）</span>    clf = LinearRegression()    clf.fit(x_train, y_train)    <span class="hljs-comment"># 训练</span>    y_pred = clf.predict(x_test) <span class="hljs-comment"># 测试</span>    <span class="hljs-comment">#第四步：测试误差计算（需要引入一个函数）</span>    <span class="hljs-comment"># 打印平均值平方误差</span>    <span class="hljs-keyword">print</span> (<span class="hljs-string">"Mean Squared Error:"</span>, mean_squared_error(y_test, y_pred))  <span class="hljs-comment"># Mean Squared Error: 2548.072398725972</span>    <span class="hljs-comment">#matplotlib可视化输出</span>    <span class="hljs-comment"># Plot the results</span>    plt.scatter(x_test[:,<span class="hljs-number">0</span>], y_test,  color=<span class="hljs-string">'black'</span>)         <span class="hljs-comment"># 散点输出</span>    plt.plot(x_test[:,<span class="hljs-number">0</span>], y_pred, color=<span class="hljs-string">'blue'</span>, linewidth=<span class="hljs-number">3</span>) <span class="hljs-comment"># 预测输出</span>    plt.show()<span class="hljs-keyword">if</span> __name__ == <span class="hljs-string">'__main__'</span>:    main()</code></pre>]]></content>
    
    
    <categories>
      
      <category>机器学习</category>
      
    </categories>
    
    
    <tags>
      
      <tag>python</tag>
      
      <tag>机器学习</tag>
      
      <tag>线性回归</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>机器学习-线性回归推导总结</title>
    <link href="/2018/07/ml-linearRegressionL1L2/"/>
    <url>/2018/07/ml-linearRegressionL1L2/</url>
    
    <content type="html"><![CDATA[<h2 id="线性回归中的最小二乘法和L1、L2推导"><a href="#线性回归中的最小二乘法和L1、L2推导" class="headerlink" title="线性回归中的最小二乘法和L1、L2推导"></a>线性回归中的最小二乘法和L1、L2推导</h2><p>上一节<a href="https://sevenold.github.io/2018/07/ml-linearRegression/" target="_blank" rel="noopener">线性回归-算法推导</a> , 我们已经大致的知道了，线性回归的算法推导过程，但是往往我们在使用线性回归算法的过程中模型会出现<strong>过拟合的现象</strong>， 我们现从例子来看看什么是<strong>过拟合</strong>。</p><p>还是以房价预测为例，来看几张张图片：</p><h3 id="1-欠拟合（Underfitting）"><a href="#1-欠拟合（Underfitting）" class="headerlink" title="1.欠拟合（Underfitting）"></a><strong>1.欠拟合（Underfitting）</strong></h3><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20200530231824.png" srcset="/img/loading.gif" alt="image"></p><p>上图中，我们用$h_\theta(x)=\theta_0+\theta_1x $来拟合训练集中的数据，但是我们可以很明显的从图中看出，房价是不会随着面积成比例的增长的，这种情况，我们就称之为<strong>欠拟合</strong>。</p><h3 id="2-过拟合（Overfitting）"><a href="#2-过拟合（Overfitting）" class="headerlink" title="2.过拟合（Overfitting）"></a><strong>2.过拟合（Overfitting）</strong></h3><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20200530230043.png" srcset="/img/loading.gif" alt="image"></p><p>如上图所示，我们用一条高次的曲线 $h_θ(x)=θ_0+θ_1x+θ_2x^2+θ_3x^3+θ_4x^4$ 来拟合训练集中的数据，因为参数过多，对训练集的匹配度太高、太准确，以至于在后面的预测过程中可能会导致预测值非常偏离合适的值，预测非常不准确，也就是说能力太强了，导致震荡的非常强烈。这就是<strong>过拟合</strong>。 </p><h3 id="3-合适的拟合（Properfitting）"><a href="#3-合适的拟合（Properfitting）" class="headerlink" title="3.合适的拟合（Properfitting）"></a><strong>3.合适的拟合（Properfitting）</strong></h3><p>​                                    <img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20200530230044.png" srcset="/img/loading.gif" alt="image"></p><p>如上图，如何参数选择的恰当，选用一个合适的曲线，比如说是$h_θ(x)=θ_0+θ_1x+θ_2x^2$来拟合上面的数据集就非常适合，这样这就是一个比较恰当的假设参数（hypothesis function）.</p><h3 id="简单总结一下"><a href="#简单总结一下" class="headerlink" title="简单总结一下"></a><strong>简单总结一下</strong></h3><p>一般在实际的应用中是不会遇到<strong>欠拟合</strong>的情况的，但是<strong>过拟合</strong>是经常出现的，一般情况下，<strong>过拟合</strong>（Overfitting）就是：如果我在训练一个数据集的时候，用了太多的特征（features）来训练一个假设函数，就会造成匹配度非常高（误差几乎就为0， 也就是我上一节得出的损失函数：$ J(θ)=∑_N^{i=1}(y_i−θ^Tx_i)^2$),但是不能推广到其他的未知数据上，也就是对于其他的训练集是没有任何用的，不能做出正确的预测。</p><p>所以为了避免这种<strong>过拟合现象</strong>的发生，我们也有对应得惩罚，让他的能力不要那么强，所以就有L1(LASSO)、岭回归L2(Ridge)。我们来直观的了解下这两种正则。</p><ol><li><p>最小均方函数导数不为0时，L2导数加上最小均方函数导数肯定不为0。但是L1的正则项是绝对值函数，导数为0只要在x从左边趋向于0和从右边趋向于0时导数异号就行，所以更容易得到稀疏解。</p></li><li><p>目标函数最小均方差解空间为同心圆，L2解空间也为同心圆，L1解空间为菱形，两个解空间相交处为最优值。如图所示。</p><p>​                                 <img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20200530230045.png" srcset="/img/loading.gif" alt="image"></p></li></ol><h3 id="数学推导"><a href="#数学推导" class="headerlink" title="数学推导"></a>数学推导</h3><p>总结上节的知识点，我们就有三种方式解出线性回归算法的表达式或解析解：</p><ol><li>最小二乘法的解析解可以用高斯分布（Gaussian）以及最大似然估计法求得</li><li>岭回归Ridge（L2正则）的解析解可以用高斯分布（Gaussian）以及最大后验概率解释</li><li>LASSO（L1正则）的解释解可以用拉普拉斯（Laplace）分布以及最大后验概率解释</li></ol><h4 id="在推导之前："><a href="#在推导之前：" class="headerlink" title="在推导之前："></a>在推导之前：</h4><ol><li>假设你已经懂得：高斯分布，拉普拉斯分布，最大似然估计，最大后验估计</li><li>机器学习的三要素：<strong>模型、策略、算法</strong>（李航《统计学习方法》）。就是说，一种模型可以有多种求解策略，每一种求解策略可能又有多种计算方法。所以先把模型策略搞懂，然后算法。</li></ol><h3 id="线性回归模型总结"><a href="#线性回归模型总结" class="headerlink" title="线性回归模型总结"></a>线性回归模型总结</h3><p>首先我们先假设线性回归模型：</p><h4 id="f-x-sum-i-1-nx-iw-i-b-w-TX-b"><a href="#f-x-sum-i-1-nx-iw-i-b-w-TX-b" class="headerlink" title="$f(x)= \sum_{i=1}^nx_iw_i+b=w^TX+b$"></a>$f(x)= \sum_{i=1}^nx_iw_i+b=w^TX+b$</h4><h4 id="其中-x-in-R-1-times-n-w-in-R-1-times-n-当前已知：X-x-1-cdot-cdot-cdot-x-m-in-R-m-times-n-y-in-R-n-times-1-b-in-R-求出-w"><a href="#其中-x-in-R-1-times-n-w-in-R-1-times-n-当前已知：X-x-1-cdot-cdot-cdot-x-m-in-R-m-times-n-y-in-R-n-times-1-b-in-R-求出-w" class="headerlink" title="其中$x \in R^{1\times n}, w\in R^{1\times n},当前已知：X=(x_1 \cdot \cdot \cdot x_m) \in R^{m\times n}, y \in R^{n\times 1}, b \in R$,求出$w$"></a>其中$x \in R^{1\times n}, w\in R^{1\times n},当前已知：X=(x_1 \cdot \cdot \cdot x_m) \in R^{m\times n}, y \in R^{n\times 1}, b \in R$,求出$w$</h4><h3 id="最小二乘法"><a href="#最小二乘法" class="headerlink" title="最小二乘法"></a>最小二乘法</h3><p>如果$b \sim N(u, \sigma^2)$, 其中$u=0$, 也就是说$y_i \sim N(w^TX,\sigma^2)$</p><h4 id="采用最大似然估计法："><a href="#采用最大似然估计法：" class="headerlink" title="采用最大似然估计法："></a>采用最大似然估计法：</h4><h4 id="L-w-prod-i-1-N-frac-1-sqrt-2-pi-sigma-e-frac-y-i-w-Tx-i-2-2-sigma-2"><a href="#L-w-prod-i-1-N-frac-1-sqrt-2-pi-sigma-e-frac-y-i-w-Tx-i-2-2-sigma-2" class="headerlink" title="$L(w)=\prod_{i=1}^{N}\frac{1}{\sqrt{2\pi}\sigma}e^{-(\frac{(y_i-w^Tx_i)^2}{2\sigma^2})}$"></a>$L(w)=\prod_{i=1}^{N}\frac{1}{\sqrt{2\pi}\sigma}e^{-(\frac{(y_i-w^Tx_i)^2}{2\sigma^2})}$</h4><h4 id="对数似然函数："><a href="#对数似然函数：" class="headerlink" title="对数似然函数："></a>对数似然函数：</h4><h4 id="l-w-nlog-sigma-sqrt-2-pi-frac-1-2-sigma-2-sum-i-1-n-y-i-w-Tx-i-2"><a href="#l-w-nlog-sigma-sqrt-2-pi-frac-1-2-sigma-2-sum-i-1-n-y-i-w-Tx-i-2" class="headerlink" title="$l(w)=-nlog\sigma\sqrt{2\pi}-\frac{1}{2\sigma^2}\sum_{i=1}^{n}(y_i-w^Tx_i)^2 $"></a>$l(w)=-nlog\sigma\sqrt{2\pi}-\frac{1}{2\sigma^2}\sum_{i=1}^{n}(y_i-w^Tx_i)^2 $</h4><p>因为我们要求的是似然函数的最大值：</p><h4 id="arg-max-w-L-w-prod-i-1-N-frac-1-sqrt-2-pi-sigma-e-frac-y-i-w-Tx-i-2-2-sigma-2"><a href="#arg-max-w-L-w-prod-i-1-N-frac-1-sqrt-2-pi-sigma-e-frac-y-i-w-Tx-i-2-2-sigma-2" class="headerlink" title="$arg max_w \ L(w)=\prod_{i=1}^{N}\frac{1}{\sqrt{2\pi}\sigma}e^{-(\frac{(y_i-w^Tx_i)^2}{2\sigma^2})}$"></a>$arg max_w \ L(w)=\prod_{i=1}^{N}\frac{1}{\sqrt{2\pi}\sigma}e^{-(\frac{(y_i-w^Tx_i)^2}{2\sigma^2})}$</h4><p>通过对数似然进行变换后，因为$-nlog\sigma\sqrt{2\pi}$是定值，所以最终解析解：</p><h4 id="arg-min-w-f-w-frac-1-2-sigma-2-sum-i-1-n-y-i-w-Tx-i-2-y-w-TX-2-2"><a href="#arg-min-w-f-w-frac-1-2-sigma-2-sum-i-1-n-y-i-w-Tx-i-2-y-w-TX-2-2" class="headerlink" title="$arg min_w f(w)=\frac{1}{2\sigma^2}\sum_{i=1}^{n}(y_i-w^Tx_i)^2 =||y-w^TX||_2^2$"></a>$arg min_w f(w)=\frac{1}{2\sigma^2}\sum_{i=1}^{n}(y_i-w^Tx_i)^2 =||y-w^TX||_2^2$</h4><h3 id="岭回归Redge-L2正则"><a href="#岭回归Redge-L2正则" class="headerlink" title="岭回归Redge(L2正则)"></a>岭回归Redge(L2正则)</h3><p>如果$b \sim N(u, \sigma^2), w_i \sim N(u, \tau^2)$,其中$u=0$;</p><p>所以使用最大后验估计推导：</p><p>构建似然函数：</p><h4 id="L-w-prod-i-1-n-frac-1-sqrt-2-pi-sigma-e-frac-y-i-w-Tx-i-2-2-sigma-2-cdot-prod-j-1-d-frac-1-sqrt-2-pi-tau-e-frac-w-j-2-2-tau-2"><a href="#L-w-prod-i-1-n-frac-1-sqrt-2-pi-sigma-e-frac-y-i-w-Tx-i-2-2-sigma-2-cdot-prod-j-1-d-frac-1-sqrt-2-pi-tau-e-frac-w-j-2-2-tau-2" class="headerlink" title="$L(w)=\prod_{i=1}^{n}\frac{1}{\sqrt{2\pi}\sigma}e^{-(\frac{(y_i-w^Tx_i)^2}{2\sigma^2})} \cdot \prod_{j=1}^{d}\frac{1}{\sqrt{2\pi}\tau}e^{-(\frac{(w_j)^2}{2\tau^2})}$"></a>$L(w)=\prod_{i=1}^{n}\frac{1}{\sqrt{2\pi}\sigma}e^{-(\frac{(y_i-w^Tx_i)^2}{2\sigma^2})} \cdot \prod_{j=1}^{d}\frac{1}{\sqrt{2\pi}\tau}e^{-(\frac{(w_j)^2}{2\tau^2})}$</h4><p>对数似然函数</p><h4 id="l-w-nln-sigma-sqrt-2-pi-frac-1-2-sigma-2-sum-i-1-n-y-i-w-Tx-i-2-dln-tau-sqrt-2-pi-frac-1-2-tau-2-sum-j-1-d-w-j-2"><a href="#l-w-nln-sigma-sqrt-2-pi-frac-1-2-sigma-2-sum-i-1-n-y-i-w-Tx-i-2-dln-tau-sqrt-2-pi-frac-1-2-tau-2-sum-j-1-d-w-j-2" class="headerlink" title="$l(w)=-nln\sigma\sqrt{2\pi}-\frac{1}{2\sigma^2}\sum_{i=1}^{n}(y_i-w^Tx_i)^2 -dln \tau \sqrt{2\pi}-\frac{1}{2\tau^2}\sum_{j=1}^{d}(w_j)^2 $"></a>$l(w)=-nln\sigma\sqrt{2\pi}-\frac{1}{2\sigma^2}\sum_{i=1}^{n}(y_i-w^Tx_i)^2 -dln \tau \sqrt{2\pi}-\frac{1}{2\tau^2}\sum_{j=1}^{d}(w_j)^2 $</h4><p>因为$-nln\sigma\sqrt{2\pi}-dln \tau \sqrt{2\pi}$是定值，最后的解析解：</p><h4 id="arg-max-w-L-w-arg-min-w-f-w-frac-1-2-sigma-2-sum-i-1-n-y-i-w-Tx-i-2-lambda-sum-j-1-d-w-j-2-y-w-TX-2-2-lambda-w-2-2"><a href="#arg-max-w-L-w-arg-min-w-f-w-frac-1-2-sigma-2-sum-i-1-n-y-i-w-Tx-i-2-lambda-sum-j-1-d-w-j-2-y-w-TX-2-2-lambda-w-2-2" class="headerlink" title="$arg max_w \ L(w) = arg min_w f(w)=\frac{1}{2\sigma^2}\sum_{i=1}^{n}(y_i-w^Tx_i)^2+ \lambda\sum_{j=1}^{d}(w_j)^2 \\  =||y-w^TX||_2^2+\lambda||w||_2^2 $"></a>$arg max_w \ L(w) = arg min_w f(w)=\frac{1}{2\sigma^2}\sum_{i=1}^{n}(y_i-w^Tx_i)^2+ \lambda\sum_{j=1}^{d}(w_j)^2 \\  =||y-w^TX||_2^2+\lambda||w||_2^2 $</h4><h3 id="LASSO-L1正则"><a href="#LASSO-L1正则" class="headerlink" title="LASSO(L1正则)"></a>LASSO(L1正则)</h3><p>如果$b \sim N(u, \sigma^2), w_i \sim Laplace(u,b)$,其中$u=0$;</p><p>所以使用最大后验估计推导：</p><p>构建似然函数：</p><h4 id="L-w-prod-i-1-n-frac-1-sqrt-2-pi-sigma-e-frac-y-i-w-Tx-i-2-2-sigma-2-cdot-prod-j-1-d-frac-1-2b-e-frac-w-i-b"><a href="#L-w-prod-i-1-n-frac-1-sqrt-2-pi-sigma-e-frac-y-i-w-Tx-i-2-2-sigma-2-cdot-prod-j-1-d-frac-1-2b-e-frac-w-i-b" class="headerlink" title="$L(w)=\prod_{i=1}^{n}\frac{1}{\sqrt{2\pi}\sigma}e^{-(\frac{(y_i-w^Tx_i)^2}{2\sigma^2})} \cdot \prod_{j=1}^{d}\frac{1}{2b}e^{-(\frac{|w_i|}{b})}$"></a>$L(w)=\prod_{i=1}^{n}\frac{1}{\sqrt{2\pi}\sigma}e^{-(\frac{(y_i-w^Tx_i)^2}{2\sigma^2})} \cdot \prod_{j=1}^{d}\frac{1}{2b}e^{-(\frac{|w_i|}{b})}$</h4><p>对数似然：</p><h4 id="l-w-nln-sigma-sqrt-2-pi-frac-1-2-sigma-2-sum-i-1-n-y-i-w-Tx-i-2-dln-2b-frac-1-b-sum-j-1-d-w-j"><a href="#l-w-nln-sigma-sqrt-2-pi-frac-1-2-sigma-2-sum-i-1-n-y-i-w-Tx-i-2-dln-2b-frac-1-b-sum-j-1-d-w-j" class="headerlink" title="$l(w)=-nln\sigma\sqrt{2\pi}-\frac{1}{2\sigma^2}\sum_{i=1}^{n}(y_i-w^Tx_i)^2 -dln 2b -\frac{1}{b}\sum_{j=1}^{d}|w_j| $"></a>$l(w)=-nln\sigma\sqrt{2\pi}-\frac{1}{2\sigma^2}\sum_{i=1}^{n}(y_i-w^Tx_i)^2 -dln 2b -\frac{1}{b}\sum_{j=1}^{d}|w_j| $</h4><p>因为$-nln\sigma\sqrt{2\pi}-dln2b$是定值，最后的解析解：</p><h4 id="arg-max-w-L-w-arg-min-w-f-w-frac-1-2-sigma-2-sum-i-1-n-y-i-w-Tx-i-2-lambda-sum-j-1-d-w-j-y-w-TX-2-2-lambda-w-1"><a href="#arg-max-w-L-w-arg-min-w-f-w-frac-1-2-sigma-2-sum-i-1-n-y-i-w-Tx-i-2-lambda-sum-j-1-d-w-j-y-w-TX-2-2-lambda-w-1" class="headerlink" title="$arg max_w \ L(w) = arg min_w f(w)=\frac{1}{2\sigma^2}\sum_{i=1}^{n}(y_i-w^Tx_i)^2+ \lambda\sum_{j=1}^{d}|w_j| \\  =||y-w^TX||_2^2+\lambda||w||_1$"></a>$arg max_w \ L(w) = arg min_w f(w)=\frac{1}{2\sigma^2}\sum_{i=1}^{n}(y_i-w^Tx_i)^2+ \lambda\sum_{j=1}^{d}|w_j| \\  =||y-w^TX||_2^2+\lambda||w||_1$</h4><h3 id="线性回归正则化总结"><a href="#线性回归正则化总结" class="headerlink" title="线性回归正则化总结"></a>线性回归正则化总结</h3><p>L1正则化和L2正则化可以看做是损失函数的惩罚项，所谓『惩罚』是指对损失函数中的某些参数做一些限制 ，都能防止过拟合，一般L2的效果更好一些，L1能够产生稀疏模型，能够帮助我们去除某些特征，因此可以用于特征选择。</p><hr><ul><li>L2正则化可以防止模型过拟合（overfitting）；一定程度上，L1也可以防止过拟合</li><li>L1正则化可以产生稀疏权值矩阵，即产生一个稀疏模型，可以用于特征选择</li></ul><hr>]]></content>
    
    
    <categories>
      
      <category>机器学习</category>
      
    </categories>
    
    
    <tags>
      
      <tag>python</tag>
      
      <tag>机器学习</tag>
      
      <tag>线性回归</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>机器学习之线性回归</title>
    <link href="/2018/07/ml-linearRegression/"/>
    <url>/2018/07/ml-linearRegression/</url>
    
    <content type="html"><![CDATA[<h1 id="线性回归–-y-wx-b"><a href="#线性回归–-y-wx-b" class="headerlink" title="线性回归–$y=wx+b$"></a><strong>线性回归</strong>–$y=wx+b$</h1><p><strong>回归</strong>，统计学术语，表示变量之间的某种数量依存关系，并由此引出回归方程，回归系数。</p><h3 id="线性回归（Linear-Regression）"><a href="#线性回归（Linear-Regression）" class="headerlink" title="线性回归（Linear Regression）"></a><strong>线性回归（Linear Regression）</strong></h3><p>数理统计中回归分析，用来确定两种或两种以上变量间相互依赖的定量关系的一种统计分析方法，其表达形式为$y = wx+e$，e为误差服从均值为0的正态分布，其中只有一个自变量的情况称为简单回归，多个自变量的情况叫多元回归。</p><p>注意，统计学中的回归并如线性回归非与严格直线函数完全能拟合，所以我们统计中称之为回归用以与其直线函数区别。 </p><p>我们先来看下这个图 </p><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20200530222940.png" srcset="/img/loading.gif" alt="image"></p><p> 这个是近期比较火的现金贷产品的贷款额度。这个表格表示的是<code>可贷款的金额</code>与 <code>工资</code>和 <code>房屋面积</code>之间的关系，其中 <strong>工资</strong> 和 <strong>房屋面积</strong> 为 特征，<strong>可贷款金额</strong>为目标函数值。  那么根据线性函数可得到以下公式。 </p><p>$h_\theta(x)=\theta_{1}x_{1}+\theta_{2}x_{2} $                                           </p><p>上面的这个式子是当一个模型只有两个特征$(x_1,x_2)$的时候的线性回归式子。  正常情况下，现金贷中可贷款的额度和用户的很多特征相关联，并不只是简单的这两个特征。所以我们需要把这个式子进行通用化。  假如有n个特征的话，那么式子就会变成下面的样子  </p><p>$h_\theta(x)=\theta_{1}x_{1}+\theta_{2}x_{2} + \cdot \cdot \cdot \cdot \cdot+\theta_{n}x_{n} = \sum_{i=1}^{n}\theta_{i}x_{i}$    </p><h3 id="利用矩阵的知识对线性公式进行整合。"><a href="#利用矩阵的知识对线性公式进行整合。" class="headerlink" title="利用矩阵的知识对线性公式进行整合。"></a>利用矩阵的知识对线性公式进行整合。</h3><p> 因为机器学习中基本上都是用矩阵的方式来表示参数的，也就是说我们需要把这个多项求和的式子用矩阵的方式表达出来，这样才方便后续的计算。  </p><p>$\theta_{i \times 1} = [\theta_1,\theta_2,\cdot\cdot\cdot\theta_i,]$     </p><p>$X_{i\times1}=[x_1,x_2,\cdot \cdot \cdot x_i]$</p><h3 id="把上述线性函数写成矩阵相乘的形式"><a href="#把上述线性函数写成矩阵相乘的形式" class="headerlink" title="把上述线性函数写成矩阵相乘的形式"></a>把上述线性函数写成矩阵相乘的形式</h3><p>$\theta^TX=\begin{bmatrix}  \theta_1 \\  \theta_2 \\ \cdot \\ \cdot \\ \cdot \\ \theta_i \end{bmatrix} \cdot [x_1,x_2,\cdot \cdot \cdot x_i] = \sum_{i=1}^{n}\theta_{i}x_{i} $</p><p>我们把权重参数和特征参数，都看成是1行n列的矩阵(或者是行向量)。那么就可以根据矩阵乘法的相关知识，把上述多项求和的式子，转换成矩阵的乘法的表达式。  由此我们就把多项求和化简称了 。</p><p>$h_\theta(x)=\theta^TX$</p><h3 id="误差项的分析"><a href="#误差项的分析" class="headerlink" title="误差项的分析"></a>误差项的分析</h3><p>原式:$$y =wx+b$$ 其中$b$就是我们所说的偏移量，或者叫误差项。</p><p>我们再看看下面这个图：</p><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20200530222941.png" srcset="/img/loading.gif" alt="image"></p><p>图中的横坐标$x_1$ 和 X$x)_2$分别代表着 两个特征(工资、房屋平米) 。纵坐标Y代表目标(可贷款的额度)。其中红点代表的就是实际的目标值(每个人可贷款的额度).而平面上和红点竖向相交的点代表着我们根据线性回归模型得到的点。也就是说实际得到的钱和预估的钱之间是有一定误差的，这个就是误差项。  因为误差项是真实值和误差值之间的一个差距。那么肯定我们希望误差项越小越好。  </p><p>然后我们对应整理成线性回归函数：</p><p>$h_\theta(x)=\theta^Tx+\varepsilon$</p><p>我们根据实际情况，假设认为这个误差项是满足以下几个条件的。 </p><ol><li>误差$\varepsilon_{(i)}$是独立。</li><li>具有相同的分布。</li><li>服从均值为0方差为$\theta^2$的高斯分布。</li></ol><p>然我们回到刚开始的现金贷产品的贷款额度问题上面</p><p>  1.独立：张三和李四一起使用这款产品，可贷款额互不影响  </p><p>  2.同分布：张三和李四是使用的是同一款产品  </p><p>  3.高斯分布：绝大多数的情况下，在一个的空间内浮动不大 </p><h3 id="似然函数的理解"><a href="#似然函数的理解" class="headerlink" title="似然函数的理解"></a>似然函数的理解</h3><p>由前面两步，我们已经把线性回归模型，推导成下面的这个式子了 </p><p>$y_{(i)}=\theta^Tx_i+\varepsilon_i$</p><p>我们已经知道误差项是符合高斯分布的，所以误差项的概率值：</p><p>$P(\varepsilon_i)=\frac{1}{\sqrt{2\pi}\sigma}e^{-(\frac{(\varepsilon_i)^2}{2\sigma^2})}$</p><p>然后把误差值带入式子中:</p><p>$P(y_i|x_i,\theta)=\frac{1}{\sqrt{2\pi}\sigma}e^{-(\frac{(y_i-\theta^Tx_i)^2}{2\sigma^2})}$</p><p>由于是误差值，所以是越小越好，所以我们接下来就是讨论什么样的特征值和特征组合能够让误差值最小，现在就要看似然函数的作用了，似然函数的作用就是要根据样本求什么样的参数和特征的组成能够接近真实值，所以越接近真实值则误差就越小。</p><p>引入似然函数(似然函数就是求能让真实值和预测值相等的那个参数的 )：</p><p>$L(\theta) = \prod_{i=1}^{N} P(y_i|x_i,\theta)=\prod_{i=1}^{N}\frac{1}{\sqrt{2\pi}\sigma}e^{-(\frac{(y_i-\theta^Tx_i)^2}{2\sigma^2})}$</p><p>$\prod$表示各元素相乘的结果</p><p>上面的式子是多个参数的乘积的形式，很难进行计算，所以我们又采用了对数的一个小技巧，把多个数相乘，转化成多个数相加的形式。 </p><p>因为对数的性质</p><p>$logA\cdot B = logA+logB$</p><p>根据上面的这种换算关系，我们就把似然函数的式子换算成下面的这个。  (因为似然函数是越大越好，似然函数的值和对数似然函数的值是成正比的，对值求对数，并不会影响到最后求极限的值。所以才敢进行对数处理。)  </p><p>$l(\theta) = logL(\theta) = log\prod_{i=1}^{N}\frac{1}{\sqrt{2\pi}\sigma}e^{-(\frac{(y_i-\theta^Tx_i)^2}{2\sigma^2})}$</p><p>对上式进行整理：</p><p>$l(\theta) = logL(\theta) = \sum_{i=1}^{N}log\frac{1}{\sqrt{2\pi}\sigma}e^{-(\frac{(y_i-\theta^Tx_i)^2}{2\sigma^2})}$</p><p>$= \sum_{i=1}^{N}(log\frac{1}{\sqrt{2\pi}\sigma}+loge^{-(\frac{(y_i-\theta^Tx_i)^2}{2\sigma^2})})$</p><p>$= Nlog\frac{1}{\sqrt{2\pi}\sigma}-\frac{1}{2\sigma^2}\sum_{i=1}^{N}(y_i-\theta^Tx_i)^2 $</p><p>因为：</p><p>$Nlog\frac{1}{\sqrt{2\pi}\sigma}$ 是一个定值</p><p>似然函数是要越大越好</p><p>所以：</p><p>$-\frac{1}{2\sigma^2}(y_i-\theta^Tx_i)^2$越大越好</p><p>再因为：</p><p>$-\frac{1}{2\sigma^2}$也为定值</p><p>最终：</p><p>$l(\theta) = \sum_{i=1}^{N}(y_i-\theta^Tx_i)^2$</p><p>$\sum_{i=1}^{N}(y_i-\theta^Tx_i)^2$越小越好——最小二乘法（损失函数）</p><h3 id="最小二乘法"><a href="#最小二乘法" class="headerlink" title="最小二乘法"></a><strong>最小二乘法</strong></h3><p>上述代价函数中使用的均方误差，其实对应了我们常用的欧几里得的距离（欧式距离，<strong>Euclidean Distance</strong>）, 基于均方误差最小化进行模型求解的方法称为“最小二乘法”（<strong>least square method</strong>），即通过最小化误差的平方和寻找数据的最佳函数匹配；</p><p>当函数子变量为一维时，最小二乘法就蜕变成寻找一条直线；</p><p> 然后我们把得到的损失函数推广到n维，转换成矩阵形式（参考前面利用矩阵的知识对线性公式进行整合）：</p><p>$J(\theta)=\sum_{i=1}^{N}(y_i-\theta^Tx_i)^2$       损失函数</p><h3 id="其对应的均方误差表示为如下矩阵"><a href="#其对应的均方误差表示为如下矩阵" class="headerlink" title="其对应的均方误差表示为如下矩阵"></a>其对应的均方误差表示为如下矩阵</h3><p>$J(\theta) = {(y-X\theta)^T(y-X\theta)}$   </p><p>其中X：</p><p>$X=\begin{bmatrix} 1 &amp;&amp; x_1^T  \\ 1 &amp;&amp; x_2^T  \\ \cdot \\ \cdot \\ \cdot  \\ 1 &amp;&amp; x_N^T \end{bmatrix} =\begin{bmatrix} 1 &amp;&amp;    x_{11} &amp;&amp; x_{12}  &amp;&amp; \cdot \cdot \cdot  x_{1n}  \\ 1 &amp;&amp; x_{21} &amp;&amp; x_{22} &amp;&amp; \cdot \cdot \cdot x_{2n}   \\ \cdot  \\  \cdot \\ \cdot \\ 1&amp;&amp; x_{m1} &amp;&amp; x_{m2}  &amp;&amp; \cdot \cdot \cdot x_{mn}  \end{bmatrix} $</p><p> 对$\theta$求导</p><p>$J(\theta) = {(y-X\theta)^T(y-X\theta)}=y^Ty-y^Tx\theta-\theta^Tx^Ty+\theta^Tx^Tx\theta$   </p><p>$\frac{\partial J(\theta)}{\partial(\theta)} = \frac{\partial y^Ty}{\partial(\theta)} - \frac{\partial y^Tx\theta}{\partial(\theta)} - \frac{\partial \theta^Tx^Ty}{\partial(\theta)} + \frac{\partial \theta^Tx^Tx\theta}{\partial(\theta)} $</p><p>$\frac{\partial J(\theta)}{\partial(\theta)} = 0-x^Ty-x^Ty+2x^Tx\theta$</p><p>$\frac{\partial J(\theta)}{\partial(\theta)} =2x^T(x\theta-y)$</p><p>根据导数的性质，该值在导数为0时为最小</p><p>所以：根据微积分定理，令上式等于零，可以得到 θ 最优的闭式解。当 </p><p>$2(x^Ty-x^Tx\theta)=0$时取得最小</p><p>矩阵求导的知识：</p><p><img src="https://eveseven.oss-cn-shanghai.aliyuncs.com/20200530222842.png" srcset="/img/loading.gif" alt="image"></p><p>最终：$\theta = (x^Tx)^{-1}x^Ty$</p><p>X和Y都是已知的，那么得到了最终的参数值。 </p><h3 id="那我们来看看数学原理"><a href="#那我们来看看数学原理" class="headerlink" title="那我们来看看数学原理"></a>那我们来看看<strong>数学原理</strong></h3><p><strong>微积分角度来讲</strong>，最小二乘法是采用非迭代法，针对代价函数求导数而得出全局极值，进而对所给定参数进行估算。</p><p><strong>计算数学角度来讲</strong>，最小二乘法的本质上是一个线性优化问题，试图找到一个最优解。</p><p><strong>线性代数角度来讲</strong>，最小二乘法是求解线性方程组，当方程个数大于未知量个数，其方程本身 无解，而最小二乘法则试图找到最优残差。</p><p><strong>几何角度来讲</strong>，最小二乘法中的几何意义是高维空间中的一个向量在低维子空间的投影。</p><p><strong>概率论角度来讲</strong>，如果数据的观测误差是/或者满足高斯分布，则最小二乘解就是使得观测数据出现概率最大的解，即<strong>最大似然估计-Maximum Likelihood Estimate，MLE</strong>（利用已知的样本结果，反推最有可能（最大概率）导致这样结果的参数值）。</p>]]></content>
    
    
    <categories>
      
      <category>机器学习</category>
      
    </categories>
    
    
    <tags>
      
      <tag>机器学习</tag>
      
      <tag>线性回归</tag>
      
    </tags>
    
  </entry>
  
  
  
  
</search>
