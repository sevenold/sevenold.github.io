---
layout: post
title: "递归神经网络之LSTM"
date: 2018-08-30
description: "卷积神经网络，长短时记忆，LSTM"
tag: 深度学习
---

### LSTM模型

由于RNN也有梯度消失的问题，因此很难处理长序列的数据，大牛们对RNN做了改进，得到了RNN的特例LSTM（Long Short-Term Memory），它可以避免常规RNN的梯度消失，因此在工业界得到了广泛的应用。



### 从RNN到LSTM

在RNN模型中，我们总结了RNN具有如下结构：

![images](/images/dl/97.png)

RNN的模型可以简化成如下图的形式，所有 RNN 都具有一种重复神经网络模块的链式的形式。在标准的 RNN 中，这个重复的模块只有一个非常简单的结构，例如一个 tanh 层：

![images](/images/dl/98.png)

由于RNN梯度消失的问题，大牛们对于序列索引位置t的隐藏结构做了改进，可以说通过一些技巧让隐藏结构复杂了起来，来避免梯度消失的问题，这样的特殊RNN就是我们的LSTM。由于LSTM有很多的变种，这里我们以最常见的LSTM为例讲述。LSTM的结构如下图：

![images](/images/dl/99.png)

![images](/images/dl/100.png)

### LSTM模型结构剖析

上面我们给出了LSTM的模型结构，下面我们就一点点的剖析LSTM模型在每个序列索引位置t时刻的内部结构。

### LSTM关键--：“细胞状态”

细胞状态类似于传送带。直接在整个链上运行，只有一些少量的线性交互。信息在上面流传会很容易保持不变。

![images](/images/dl/101.png)

#### **LSTM**控制“细胞状态”的方式：

- 通过“门”让信息选择性通过，来去除或者增加信息到细胞状态。
- 包含一个`SIGMOD`神经元层和一个`pointwise`乘法操作。
- `SIGMOD`层输出0到1之间的概率值，描述每个部分有多少量可以通过。0代表“不许任何量通过”，1就表示“允许任意量通过”。

![images](/images/dl/102.png)

除了细胞状态，LSTM图中还有了很多奇怪的结构，这些结构一般称之为门控结构(Gate)。

LSTM在在每个序列索引位置t的门一般包括**遗忘门**，**输入门**和**输出门**三种。



### LSTM之遗忘门

遗忘门（forget gate）顾名思义，是控制是否遗忘的，在LSTM中即以一定的概率控制是否遗忘上一层的隐藏细胞状态。遗忘门子结构如下图所示：

![images](/images/dl/103.png)

图中输入的有上一序列的隐藏状态$h(t-1)$和本序列数据$x_{(t)}$，通过一个激活函数，一般情况下是`SIGMOD`，得到遗忘门的输出$f(t)$。由于SIGMOD的输出$f(t)$在[0,1]之间，因此这里的输出$f(t)$代表了遗忘上一层隐藏细胞的概率。

**数学表达式**：

#### $f(t)=\sigma(W_fh(t-1)+U_fx(t)+b_f)$

其中：$W_f、U_f、b_f$为线性关系的权重项和偏置项，$\sigma$为SIGMOD激活函数。



### LSTM之输入门

输入门（input gate）负责处理当前序列位置的输入，它的子结构如下图：

![images](/images/dl/104.png)

从图中可以看到输入门由两部分组成，第一部分使用了sigmoid激活函数，输出为$i_{(t)}$,第二部分使用了tanh激活函数，输出为$\tilde c_{(t)}$, 两者的结果后面会相乘再去更新细胞状态。

- SIGMOD层决定什么值需要更新。
- Tanh层创建一个新的候选值向量$\tilde c_{(t)}$
- 第二步还是为状态更新做准备。

**数学表达式**：

#### $i{(t)} = \sigma(W_ih{(t-1)} + U_ix^{(t)} + b_i)$

#### $\tilde c{(t)} =tanh(W_ah{(t-1)} + U_ax^{(t)} + b_a)$

其中$W_i, U_i, b_i, W_a, U_a, b_a$，为线性关系的权重项和偏置项，$\sigma$为SIGMOD激活函数。



### LSTM之更新“细胞状态”

在研究LSTM输出门之前，我们要先看看LSTM之细胞状态。前面的遗忘门和输入门的结果都会作用于细胞状态 $C_{(t)}$，我们来看看细胞如何从$C_{(t-1)}$到$C_{(t)}$:

![images](/images/dl/105.png)

由图可知：细胞状态$C_{(t)}$由两部分组成；第一部分是$C_{(t-1)}$和遗忘门输出$f(t)$的乘积，第二部分是输入门的$i_{(t)}$和$\tilde c_{(t)}$的乘积，总结为如下三点：

- 更新$C_{(t-1)}$为$C_{(t)}$。
- 把就状态和$f(t)$相乘，丢弃掉我们确定需要丢弃的信息。
- 加上$i(t) * \tilde c_{(t)}$。最后得到新的候选值，根据我们决定更新每个状态的程度进行变化。

**数学表达式**：

#### $C_{(t)} = C_{(t-1)} \odot f{(t)} + i_{(t)} \odot \tilde c_{(t)}$

其中，$\bigodot$为Hadamard积.



### LSTM之输出门

有了新的隐藏细胞状态$C_{(t)}$，我们就可以来看输出门了，子结构如下：

![images](/images/dl/106.png)

从图中可以看出：隐藏状态$h(t)$的更新由两个部分组成：第一部分是$o_{(t)}$，它是由上一序列的隐藏状态$h_{(t-1)}$和本序列的$x_{(t)}$，以及激活函数SIGMOD得到的，第二部分是由隐藏状态$C_{(t)}$和$Tanh$激活函数组成，即：

- 最开始先运行一个SIGMOD层来确定细胞状态的那个部分将输出。
- 接着用tanh处理细胞状态（得到一个-1到1之间的值），再讲它和SIGMOD门的输出相乘。输出我们确定输出的那部分值。

**数学表达式**：

#### $o_t=\sigma(W_o[h_{t-1},x_t]+b_o)$

#### $h_t=o_t*tanh(C_t)$



### LSTM的变体

- 增加`peephole connection`
- 让门层也会接受细胞状态的输入。

![images](/images/dl/107.png)

**数学表达式**：

#### $f_t=\sigma(W_f \cdot[C_{t-1}, h_{t-1},x_t]+b_f)$

#### $i_t=\sigma(W_i \cdot[C_{t-1}, h_{t-1},x_t]+b_i)$

#### $o_t=\sigma(W_o \cdot[C_{t-1}, h_{t-1},x_t]+b_o)$



- 通过使用`coupled`忘记和输入门
- 之前是分开确定需要忘记和添加的信息，然后一同做出决定。

![images](/images/dl/108.png)

**数学表达式**：

#### $C_t=f_t * C_{t-1}+(1-f_t)  *  \tilde C_t$



### GRU

**Gatad Reacurrent Unit (GRU)**，2014年提出。

- 将忘记门和输入门合成了一个单一的**更新门**
- 混合了细胞状态和隐藏状态
- 比标准的LSTM简单

![images](/images/dl/109.png)

**数学表达式**：

#### $z_t=\sigma(W_z \cdot [h_{t-1},x_t])$

#### $r_t=\sigma(W_r \cdot [h_{t-1},x_t])$

#### $\tilde h_t= tanh(W \cdot [r_t*h_{t-1},x_t])$

#### $h_t=(1-z_t)  * h_{t-1} + z_t  *  \tilde h_t$





转载请注明：[Seven的博客](http://sevenold.github.io)