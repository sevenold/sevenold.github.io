---
layout: post
title: "机器学习系列（二）特征工程"
date: 2019-01-02
description: "机器学习 特征工程"
tag: 机器学习
---


### 数据质量分析

数据质量分析是数据挖掘中数据准备过程的重要一环，是数据预处理的前提，也是数据挖掘分析结论有效性和准确性的基础，没有可信的数据，数据挖掘构建的模型将是空中楼阁。

数据质量分析的主要任务是检查原始数据中是否存在脏数据，脏数据一般是指不符合要求，以及不能直接进行相应分析的数据。在常见的数据挖掘工作中，脏数据包括如下内容。

1. 缺失值。
2. 异常值
3. 不一致的值。
4. 重复数据及含有特殊符号（如#、¥、*)的数据。

#### 缺少值分析

数据的缺失主要包括记录的缺失和记录中某个字段信息的缺失，两者都会造成分析结果的不准确，以下从缺失值产生的原因及影响等方面展开分析。

(1)缺失值产生的原因

- 有些信息暂时无法获取，或者获取信息的代价太大。
- 有些信息是被遗漏的。可能是因为输人时认为不重要、忘记填写或对数据理解错误等一些人为因素而遗漏，也可能是由于数据采集设备的故障、存储介质的故障、传输媒体的故障等非人为原因而丢失。
- 属性值不存在。在某些情况下，缺失值并不意味着数据有错误。对一些对象来说某 些属性值是不存在的，如一个未婚者的配偶姓名、一个儿童的固定收人等。

(2 )缺失值的影响

- 数据挖掘建模将丢失大量的有用信息。
- 数据挖掘模型所表现出的不确定性更加显著，模型中蕴涵的规律更难把握。
- 包含空值的数据会使建模过程陷人混乱，导致不可靠的输出。

(3)缺失值的分析

- 使用简单的统计分析，可以得到含有缺失值的属性的个数，以及每个属性的未缺失数、 缺失数与缺失率等。
- 从总体上来说，缺失值的处理分为删除存在缺失值的记录、对可能值进行插补和不处理 3种情况

#### 异常值分析

- 异常值分析是检验数据是否有录人错误以及含有不合常理的数据。忽视异常值的存在是十分危险的，不加剔除地把异常值包括进数据的计算分析过程中，对结果会产生不良影响； 重视异常值的出现，分析其产生的原因，常常成为发现问题进而改进决策的契机。
- 异常值是指样本中的个别值，其数值明显偏离其余的观测值。异常值也称为离群点，异常值的分析也称为离群点分析。

(1)简单统计量分析

- 可以先对变量做一个描述性统计，进而查看哪些数据是不合理的。最常用的统计量是最大值和最小值，用来判断这个变量的取值是否超出了合理的范围。如客户年龄的最大值为 199岁，则该变量的取值存在异常。

(2) $3\sigma$原则

- 如果数据服从正态分布，在$3\sigma$原则下，异常值被定义为一组测定值中与平均值的偏差超过3倍标准差的值。在正态分布的假设下，距离平均值$3\sigma$之外的值出现的概率为$P(|x-\mu|>3\sigma)\le 0.003$,属于极个别的小概率事件。
- 如果数据不服从正态分布，也可以用远离平均值的多少倍标准差来描述。

(3)箱型图分析

- 箱型图提供了识别异常值的一个标准：异常值通常被定义为小于$Q_L-1.5IQR$或大于$Q_U+1.5IQR$的值。$Q_L$称为下四分位数，表示全部观察值中有四分之一的数据取值比它小;$Q_U$称为上四分位数，表示全部观察值中有四分之一的数据取值比它大；$IQR$称为四分位数间距，是上四分位数$Q_U$与下四分位数$Q_L$之差，其间包含了全部观察值的一半。
- 箱型图依据实际数据绘制，没有对数据作任何限制性要求 (如服从某种特定的分布形式)，它只是真实直观地表现数据分布的本来面貌；另一方面，箱型图判断异常值的标准以四分位数和四分位距为基础，四分位数具有一定的鲁棒性：多达25%的数据可以变得任意远而不会很大地扰动四分位数，所以异常值不能对这个标准施加影响。由此可见，箱型图识别异常值的结果比较客观，在识别异常值方面有一定的优越性

![箱线图](http://eveseven.oss-cn-shanghai.aliyuncs.com/19-1-2/1373573.jpg)

> Qi所在位置=i（n+1）/4，其中i=1，2，3。n表示序列中包含的项数。
>
> 四分位距IQR=Q3-Q1

#### 一致性分析

数据不一致性是指数据的矛盾性、不相容性。直接对不一致的数据进行挖掘，可能会产生与实际相违背的挖掘结果。在数据挖掘过程中，不一致数据的产生主要发生在数据集成的过程中，这可能是由于被挖掘数据是来自于从不同的数据源、对于重复存放的数据未能进行一致性更新造成的。例如，两张表中都存储了用户的电话号码，但在 4000 用户的电话号码发生改变时只更新了一张表 中的数据，那么这两张表中就有了不一致的数据。

### 数据特征分析

- 定量分析
- 定性分析
- 对比分析
- 统计量分析
- 周期性分析
- 贡献度分析
- 相关性分析

#### 相关性分析

分析连续变量之间线性相关程度的强弱，并用适当的统计指标表示出来的过程称为相关分析。

1、直接绘制散点图

- 判断两个变量是否具有线性相关关系的最直观的方法是直接绘制散点图

![相关关系](http://eveseven.oss-cn-shanghai.aliyuncs.com/19-1-2/57035403.jpg)

2、绘制散点图矩阵

- 需要同时考察多个变量间的相关关系时，一一绘制它们间的简单散点图是十分麻烦的。 此时可利用散点图矩阵同时绘制各变量间的散点图，从而快速发现多个变量间的主要相关性，这在进行多元线性回归时显得尤为重要。

![散点图矩阵](http://eveseven.oss-cn-shanghai.aliyuncs.com/19-1-2/11180617.jpg)

3、计算相关系数

- 为了更加准确地描述变量之间的线性相关程度，可以通过计算相关系数来进行相关分 析。在二元变量的相关分析过程中比较常用的有Pearson相关系数、Spearman秩相关系数和判定系数。

#### Pearson相关系数

一般用于分析两个连续性变量之间的关系，其计算公式如下。

#### $r= \frac{\sum^n_{i=1}(x_i-\bar{x})(y_i-\bar{y})}{\sqrt{\sum^n_{i=1}(x_i-\bar{x})^2\sum^n_{i=1}(y_i-\bar{y})^2}}$

相关系数$r$的取值范围：$-1\le r\le 1 $

- $r > 0 为正相关，r < 0 为负相关$
- $|r| = 0表示不存在线性关系$
- $|r| = 1表示完全线性相关$
- $0<|r|<1 表示存在不同程度线性相关： $

- $|r|\le 0.3 为不存在线性相关$
- $0.3 < |r|\le 0.5 为低度线性相关$
- $0.5<|r|\le0.8 为显著线性相关$
- $|r|>0.8为高度线性相关$



#### Spearman秩相关系数

Pearson线性相关系数要求连续变量的取值服从正态分布。不服从正态分布的变量、分类或等级变量之间的关联性可采用Spearman秩相关系数，也称等级相关系数来描述。
其公式如下：

#### $r_s=1- \frac{6\sum^n_{i=1}(R_i-Q_i)^2}{n(n^2-1)}$

对两个变量成对的取值分别按照从小到大（或者从大到大小）顺序编秩，$R_i$代表$x_i$的秩次，$Q_i$代表$y_i$的秩次，$R_i-Q_i$为$x_i$、$y_i$的秩次之差。

| $x_i$从小到大排序 | 从小到达排序时的位置 | 秩次$𝑅_𝑖$     |
| ----------------- | -------------------- | ------------- |
| 0.5               | 1                    | 1             |
| 0.8               | 2                    | 2             |
| 1.0               | 3                    | 3             |
| 1.2               | 4                    | (4+5)/2 = 4.5 |
| 1.2               | 5                    | (4+5)/2 = 4.5 |
| 2.3               | 6                    | 6             |

| 位置 | 原始X | 排序后 | 秩次 | 原始Y | 排序后 | 秩次 | 秩次差 |
| ---- | ----- | ------ | ---- | ----- | ------ | ---- | ------ |
| 1    | 12    | 546    | 5    | 1     | 78     | 6    | 1      |
| 2    | 546   | 45     | 1    | 78    | 46     | 1    | 0      |
| 3    | 13    | 32     | 4    | 2     | 45     | 5    | 1      |
| 4    | 45    | 13     | 2    | 46    | 6      | 2    | 0      |
| 5    | 32    | 12     | 3    | 6     | 2      | 4    | 1      |
| 6    | 2     | 2      | 6    | 45    | 1      | 3    | -3     |

对于上表数据，算出Spearman秩相关系数为： 

#### $1-\frac{6(1+1+1+9)}{6*(6^2-1)}=0.6571$

因为一个变量的相同的取值必须有相同的秩次，所以在计算中采用的秩次是排序后所在位置的平均值。

只要两个变量具有严格单调的函数关系，那么它们就是完全Spearman相关的，这与Pearson 相关不同，Pearson相关只有在变量具有线性关系时才是完全相关的。

在实际应用计算中，上述两种相关系数都要对其进行假设检验，使用t检验方法检验其显著性水平以确定其相关程度。研究表明，在正态分布假定下，Spearman秩相关系数与Pearson 相关系数在效率上是等价的，而对于连续测量数据，更适合用Pearson相关系数来进行分析。

(3)判定系数

判定系数是相关系数的平方，用$R^2$表示；用来衡量回归方程对y的解释程度。判定系数取值范围：$0\le r^2\le1$。$r^2$越接近于1,表明x与y之间的相关性越强；$r^2$越接近于0,表明 两个变量之间几乎没有直线相关关系。

#### 判定系数

判定系数是相关系数的平方，用$R^2$表示；用来衡量回归方程对y的解释程度。判定系数取值范围：$0\le r^2\le1$。$r^2$越接近于1,表明x与y之间的相关性越强；$r^2$越接近于0,表明 两个变量之间几乎没有直线相关关系。

### 基本统计特征函数

统计特征函数用于计算数据的均值、方差、标准差、分位数、相关系数和协方差等，这些统计特征能反映出数据的整体分布。

| 方法名     | 函数功能                                         | 所属库 |
| ---------- | ------------------------------------------------ | ------ |
| sum()      | 计算数据样本的总和（按列计算）                   | Pandas |
| mean()     | 计算数据样本的算术平均数                         | Pandas |
| var()      | 计算数据样本的方差                               | Pandas |
| st()       | 计算数据样本的标准差                             | Pandas |
| corr()     | 计算数据样本的Spearman (Pearson)相关系数矩阵     | Pandas |
| cov()      | 计算数据样本的协方差矩阵                         | Pandas |
| skew()     | 样本值的偏度（三阶矩）                           | Pandas |
| kurt()     | 样本值的峰度（四阶矩）                           | Pandas |
| describe() | 给出样本的基本描述（基本统计量如均值、标准差等） | Pandas |

#### `偏度（skewness）`

是统计数据分布偏斜方向和程度的度量，是统计数据分布非对称程度的数字特征。定义上偏度是样本的三阶标准化矩。

偏度定义中包括正态分布（偏度=0），右偏分布（也叫正偏分布，其偏度>0），左偏分布（也叫负偏分布，其偏度<0）

#### `峰度（peakedness；kurtosis）`

又称峰态系数。表征概率密度分布曲线在平均值处峰值高低的特征数。直观看来，峰度反映了峰部的尖度。随机变量的峰度计算方法为：随机变量的四阶中心矩与方差平方的比值。

峰度包括正态分布（峰度值=3），厚尾（峰度值>3），瘦尾（峰度值<3）



### 数据清洗

数据清洗主要是删除原始数据集中的无关数数据、重复数据，平滑噪声数据，筛选掉与挖掘主题无关的数据，处理缺失值、异常值等。



### 缺失值处理

处理缺失值的方法可分为3类：删除记录、数据插补和不处理。

| 插补方法             | 方法描述                                                     |
| -------------------- | ------------------------------------------------------------ |
| 均值/中位数/众数插补 | 根据属性值的类型，用该属性取值的平均数/中位数/众数进行插补   |
| 使用固定值           | 将缺失的属性值用一个常量替换。如广州一个工厂普通外来务工人员的“基本工资”属性的空缺值可以用2015年广州市普通外来务工人员工资标准1895元/月，该方法就是使用固定值 |
| 最近临插补           | 在记录中找到与缺失样本最接近的样本的该属性值插补             |
| 回归方法             | 对带有缺失值的变量，根据已有数据和与其有关的其他变量（因变量）的数据建立拟合模型来预测缺失的属性值 |
| 插值法               | 插值法是利用已知点建立合适的插值函数$f(x)$,未知值由对应点$x_i$,求出的函数值$f(x_i)$功近似代替 |

如果通过简单的删除小部分记录达到既定的目标，那么删除含有缺失值的记录的方法是最有效的。然而，这种方法却有很大的局限性。它是以减少历史数据来换取数据的完备，会造成资源的大量浪费，将丢弃了大量隐藏在这些记录中的信息。尤其在数据集本来就包含很少记录的情况下，删除少量记录可能会严重影响到分析结果的客观性和正确性。一些模型可 以将缺失值视作一种特殊的取值，允许直接在含有缺失值的数据上进行建模。

拉格朗日插值法和牛顿插值法。其他的插值方法还有Hermite插值、分段插值、样条插值法等。

### 异常值处理

在数据预处理时，异常值是否剔除，需视具体情况而定，因为有些异常值可能蕴含着有用的信息。

| 异常值处理方法       | 方法描述                                         |
| -------------------- | ------------------------------------------------ |
| 删除含有异常值的记录 | 直接将含有异常值的记录删除                       |
| 视为缺失值           | 将异常值视为缺失值，利用缺失值处理的方法进行处理 |
| 平均值修正           | 可用前后两个观测值的平均值修正该异常值           |
| 不处理               | 直接在具有异常值的数据集上进行挖掘建模           |

将含有异常值的记录直接删除的方法简单易行，但缺点也很明显，在观测值很少的情况下，这种删除会造成样本量不足，可能会改变变量的原有分布，从而造成分析结果的不准确。视为缺失值处理的好处是可以利用现有变量的信息，对异常值（缺失值）进行填补。

在很多情况下，要先分析异常值出现的可能原因，再判断异常值是否应该舍弃，如果是正确的数据，可以直接在具有异常值的数据集上进行挖掘建模。

### 数据集成

数据挖掘需要的数据往往分布在不同的数据源中，数据集成就是将多个数据源合并存放在一个一致的数据存储（如数据仓库）中的过程。

在数据集成时，来自多个数据源的现实世界实体的表达形式是不一样的，有可能不匹配，要考虑实体识别问题和属性冗余问题，从而将源数据在最低层上加以转换、提炼和集成。

### 实体识别

实体识别是指从不同数据源识别出现实世界的实体，它的任务是统一不同源数据的矛盾之处

(1 )同名异义

数据源A中的属性ID和数据源B中的属性ID分别描述的是菜品编号和订单编号，即描述的是不同的实体。

(2)异名同义

数据源A中的sales_dt和数据源B中的sales_date都是描述销售日期的，即A. sales_dt= B. sales_date〇

(3 )单位不统一

描述同一个实体分别用的是国际单位和中国传统的计量单位。

检测和解决这些冲突就是实体识别的任务。

### 冗杂属性识别

数据集成往往导致数据冗余，例如，

1)	同一属性多次出现；

2)	同一属性命名不一致导致重复。

仔细整合不同源数据能减少甚至避免数据冗余与不一致，从而提高数据挖掘的速度和质量。对于冗余属性要先分析，检测到后再将其删除。

有些冗余属性可以用相关分析检测。给定两个数值型的属性A和B,根据其属性值，用相关系数度量一个属性在多大程度上蕴含另一个属性。



### 数据变换

数据变换主要是对数据进行规范化处理，将数据转换成“适当的”形式，以适用于挖掘任务及算法的需要。

#### 简单函数变换

简单函数变换是对原始数据进行某些数学函数变换，常用的变换包括平方、开方、取对数、差分运算等

简单的函数变换常用来将不具有正态分布的数据变换成具有正态分布的数据。在时间序列分析中，有时简单的对数变换或者差分运算就可以将非平稳序列转换成平稳序列。在数据挖掘中，简单的函数变换可能更有必要，比如个人年收人的取值范围为10000元到10亿元，这是一个很大的区间，使用对数变换对其进行压缩是常用的一种变换处理方法。



#### 规范化

最小-最大规范化也称为离差标准化，是对原始数据的线性变换，将数值值映射到$[0,1]$之间.

#### $x=\frac{x-min}{max-min}$

> max为样本数据的最大值，min为样本数据的最小值。max-min为极差。离差标准化保留了原来数据中存在的关系，是消除量纲和数据取值范围影响的最简单方法。这种处理方法的缺点是若数值集中且某个数值很大，则规范化后各值会接近于0,并且将会相差不大。若将来遇到超过目前属性[min,max]取值范围的时候，会引起系统出错，需要重新确定min和max。

零-均值规范化也称标准差标准化，经过处理的数据的均值为0,标准差为1。

#### $x=\frac{x-\bar{x}}{\sigma}$

> 其中$\bar{x}$为原始数据的均值，$\sigma$为原始数据的标准差，是当前用得最多的数据标准化方法。

小数定标规范化

通过移动属性值的小数位数，将属性值映射到[-1,1]之间，移动的小数位数取决于属性值绝对值的最大值。

> data/10** np.ceil(np.log10(data.abs().max()))

#### 连续属性离散化

1. 离散化的过程

连续属性的离散化就是在数据的取值范围内设定若干个离散的划分点，将取值范围划分 为一些离散化的区间，最后用不同的符号或整数值代表落在每个子区间中的数据值。所以，离散化涉及两个子任务：确定分类数以及如何将连续属性值映射到这些分类值。

1. 常用的离散化方法

常用的离散化方法有等宽法、等频法和（一维）聚类。

(1)等宽法

将属性的值域分成具有相同宽度的区间，区间的个数由数据本身的特点决定，或者由用户指定，类似于制作频率分布表。

(2 )等频法

将相同数量的记录放进每个区间。

这两种方法简单，易于操作，但都需要人为地规定划分区间的个数。同时，等宽法的缺点在于它对离群点比较敏感，倾向于不均匀地把属性值分布到各个区间。有些区间包含许多数据，而另外一些区间的数据极少，这样会严重损坏建立的决策模型。等频法虽然避免了上述问题的产生，却可能将相同的数据值分到不同的区间以满足每个区间中固定的数据个数。

(3 )基于聚类分析的方法

一维聚类的方法包括两个步骤，首先将连续属性的值用聚类算法（如K-Means算法）进行聚类，然后再将聚类得到的簇进行处理，合并到一个簇的连续属性值并做同一标记。聚类分析的离散化方法也需要用户指定簇的个数，从而决定产生的区间数。

### 属性构造

利用已有的属性构造出新的属性，并加入到现有的属性集合中。




转载请注明：[Seven的博客](http://sevenold.github.io)

























