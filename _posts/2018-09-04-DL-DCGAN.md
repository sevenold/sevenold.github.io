---
layout: post
title: "神经网络之深度卷积生成对抗网络"
date: 2018-09-4
description: "深度卷积生成对抗网络、DCGAN"
tag: 深度学习
---

### 深度卷积生成对抗网络

深度卷积生成对抗网络（Deep Convolutional Generative Adversarial Networks, DCGANs）由Alec Radford等于2015年提出。在了解[GAN](https://sevenold.github.io/2018/09/DL-GAN/)的架构后，我们可以较为容易地了解DCGAN的原理。

### 稳定的DCGAN架构指南

1. 将**pooling layer**替换成**convolutions layer**
   1. 对于**生成模型**：允许网络学习自己的**空间下采样**。
   	. 对于**判别模型**：允许网络学习自己的**空间上采样**。	
2. 除了**生成器的输出层**和**判别器的输入层**之外，使用**批量标准化--batchnorm**。
   1. 解决初始化差的问题。
   2. 帮助梯度传播到每一层。
   3. 防止**生成器**把所有的样本都收敛到同一个点。
3. 在CNN中移除**全连接层**。
4. 在**生成器**的除了**输出层**外的所有层使用**ReLU**，**输出层**采用**tanh**。
5. 在**判别器**的所有层中使用**LeakyReLU**。



### DCGAN生成器

在DCGAN中，生成式模型$G(z)$使用一个比较特殊的深度卷积网络来实现，如下图所示：

![images](/images/dl/119.png)

### DCGAN判别器

而判别式模型$D(x)$则仍是一个传统的深度卷积网络，如下图所示:

![images](/images/dl/120.png)

### 反卷积

从前面两幅图中可以看出，DCGAN的生成式模型$G(z)$中出现了上采样（upsampling）。

卷积神经网络的下采样很好理解，加入polling层即可，然而这里的上采样要如何实现呢？

这里，DCGAN通过“微步幅卷积”（fractionally-strided convolution）进行上采样。

假设有一个3×3的输入，希望输出的尺寸比这要大，那么可以把这个3×3的输入通过在像素之间插入0的方式来进行扩展，如下图所示。当扩展到7×7的尺寸后，再进行卷积，就可以得到尺寸比原来大的输出。

![images](/images/dl/121.gif)



### DCGAN的特点：

- 判别模型：使用带步长的卷积（strided convolutions）取代了的空间池化（spatial pooling），容许网络学习自己的空间下采样（spatial downsampling）。
- 生成模型：使用微步幅卷积（fractional strided），容许它学习自己的空间上采样（spatial upsampling）。
- 激活函数： LeakyReLU
- Batch Normalization 批标准化：解决因糟糕的初始化引起的训练问题，使得梯度能传播更深层次。 Batch Normalization证明了生成模型初始化的重要性，避免生成模型崩溃：生成的所有样本都在一个点上（样本相同），这是训练GANs经常遇到的失败现象。



### DCGAN调优技巧

- 所有模型均采用小批量随机梯度下降（SGD）进行训练，最小批量为128。

- 所有权重均从零中心正态分布初始化，标准偏差为0.02。

- 在LeakyReLU中，所有model的leak斜率均设为0.2。

- 虽然之前的GAN工作已经使用了**momentum**来加速训练，但我们使用了**Adam**优化器和调整的超参数。

- 建议的学习率为0.001，过高，使用0.0002代替。

- 建议将**momentum**项$β_1$保持在0.9，会导致训练振荡和不稳定，所以将其降低到0.5有助于稳定训练。



