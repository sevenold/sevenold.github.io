---
layout: post
title: "神经网络之卷积神经网络"
date: 2018-08-21
description: "卷积神经网络基础知识点，卷积运算、池化、卷积神经网络"
tag: 深度学习
---



### 卷积神经网络（Convolutional Neural Network, CNN） 

`卷积神经网络`是近年来广泛应用于模式识别、图像处理等领域的一种高效识别算法，它具有结构简单、训练参数少和适应性强等特点。

`卷积神经网络`是一种前馈神经网络，它的人工神经元可以响应一部分覆盖范围内的周围单元，对于大型图像处理有出色表现。

`卷积神经网络`由一个或多个卷积计算层和顶端的全连接层（经典神经网络）组成，同时也包括关联权重和池化层。这一结构是的卷积神经网络能够利用输入数据的二维结构。与其他神经网络结构相比，卷积神经网络在图像和语音识别方面能够给出更优的结果。相比较其他深度、前馈神经网络，卷积神经网络需要估计的参数更少，从而使之成为一种颇具吸引力的深度学习框架。

我们前面所接触到的神经网络的结构是这样的：

![images](/images/dl/65.png)

卷积神经网络依旧是层级网络，只是层的功能和形式做了变化，也可以是传统神经网络的一个改进，不同的层次有不同运算与功能，如下图中就多了很多传统神经网络中所没有的层次。

![images](/images/dl/66.png)



### 卷积神经网络的核心思想

- `局部感受野`：普通的多层感知器中，隐层节点会全连接到一个图像的每个像素点上；而在卷积神经网络中，每个隐层的节点只连接到图像某个足够小的像素点上，从而大大减少需要训练的权重参数。
- `权值共享`：在卷积神经网中，同一个卷积核内，所有的神经元的权值是相同的，从而大大减少需要训练的参数。
-  `池化`：在卷积神经网络中，没有必要一定就要对原图像做处理，而是可以使用某种“压缩”方法，这就是池化，也就是每次将原图像卷积后，都通过一个下采样的过程，来减小图像的规模。 



### 卷积神经网络的层级结构

![images](/images/dl/71.png)

- #### **数据输入层（Input layer）**

- #### **卷积计算层（CONV layer）**

- #### **ReLU激活层（ReLU layer）**

- #### **池化层（POOling layer）**

- #### **全连接层（FC layer）**

  

### **数据输入层（Input layer）**

#### `任务`：主要是对原始图像进行预处理

- #### `去均值`：把输入数据的各个维度都中心化到0。

- #### `归一化`：把数据的幅度归一化到同样的范围。

- #### `PCA/白化`：用PCA降维、白化是对数据的每个特征轴上的幅度归一化。

#### 举个栗子：

#### `去均值与归一化`

去均值的目的在于把样本的中心拉回到坐标系原点上；归一化的目的就是减少各维度数据取值范围的差异而带来的干扰。比如，我们有两个维度的特征A和B，A范围是0到10，而B范围是0到10000，如果直接使用这两个特征是有问题的，好的做法就是归一化，即A和B的数据都变为0到1的范围。 

![images](/images/dl/67.png)

#### `去相关与白化`

![images](/images/dl/68.png)

### **卷积计算层（CONV layer）**

#### `任务`：对输入的图像进行特征提取

卷积计算层是卷积神经网络中最重要的一个层次，也是“卷积神经网络”的名字来源。

![images](/images/dl/75.gif)

#### 举个栗子：

假设一张图像有 5x5 个像素，1 代表白，0 代表黑，这幅图像被视为 5x5 的单色图像。现在用一个由随机地 0 和 1 组成的 3x3 矩阵去和图像中的子区域做Hadamard乘积，每次迭代移动一个像素，这样该乘法会得到一个新的 3x3 的矩阵。下面的动图展示了这个过程。 

![images](/images/dl/69.gif)

#### **直观上来理解**：

- 用一个小的权重矩阵去覆盖输入数据，对应位置元素加权相乘，其和作为结果的一个像素点。
- 这个权重在输入数据上滑动，形成一张新的矩阵
- 这个权重矩阵就被称为`卷积核`（convolution kernel）
- 其覆盖的位置称为`感受野`（receptive fileld ）
- 生成的新矩阵叫做`特征图`（feature map）

分解开来，就如下图所示：

![images](/images/dl/72.png)

#### 其中：

- 滑动的像素数量就叫做`步长`（stride），步长为1，表示跳过1个像素，步长为2，就表示跳过2个像素，以此类推

  ![images](/images/dl/76.gif)

- 以卷积核的边还是中心点作为开始/结束的依据，决定了卷积的`补齐`（padding）方式。前面我们所举的栗子是`valid`方式，而`same`方式则会在图像的边缘用0补齐，如将前面的`valid`改为`same`方式，如图所示：

![images](/images/dl/73.png)

其采样方式对应变换为：

![images](/images/dl/77.gif)



- 我们前面所提到的输入图像都是灰色的，只有一个通道，但是我们一般会遇到输入通道不只有一个，那么卷积核是三阶的，也就是说所有的通道的结果做累加。

![images](/images/dl/74.png)

![images](/images/dl/78.gif)

![images](/images/dl/79.gif)

当然，最后，这里有一个术语：“`偏置`（bias）”，每个输出滤波器都有一个偏置项，偏置被添加到输出通道产生最终输出通道。 

![images](/images/dl/80.gif)

#### 再举个栗子：

下面蓝色矩阵周围有一圈灰色的框，那些就是上面所说到的填充值 (padding=same)的方式。这里的蓝色矩阵就是输入的图像，粉色矩阵就是卷积层的神经元，这里表示了有两个神经元（w0,w1）。绿色矩阵就是经过卷积运算后的输出矩阵，这里的步长设置为2。 

![images](/images/dl/70.gif)



### **ReLU激活层（ReLU layer）**

#### `任务`：卷积后的结果压缩到某一个固定的范围做非线性映射，这样可以一直保持一层一层下去的数值范围是可控的。 

![images](/images/dl/81.png)

#### 激活函数：

- `Sigmoid`
- `Tanh`（双曲正切）
- `ReLU`
- `Leaky ReLU`
- `ELU`
- `Maxout`



卷积神经网络一般采用的激活函数是ReLU(The Rectified Linear Unit/修正线性单元)，它的特点是收敛快，求梯度简单，但较脆弱，图像如下：

![images](/images/dl/82.png)

#### 激励层的实践经验： 　　

- 不要用sigmoid！不要用sigmoid！不要用sigmoid！ 　　
- 首先试RELU，因为快，但要小心点 　　、
- 如果2失效，请用Leaky ReLU或者Maxout 　　
- 某些情况下tanh倒是有不错的结果，但是很少 



### **池化层（POOling layer）**

#### `任务`：对特征进行采样，即用一个数值替代一块区域，主要是为了降低网络训练参数及模型的过拟合程度。 

通过卷积层获得了图像的特征之后，理论上我们可以直接使用这些特征训练分类器（如softmax），但是这样做将面临巨大的计算量的挑战，而且容易产生过拟合的现象。为了进一步降低网络训练参数及模型的过拟合程度，对卷积层进行池化/采样(Pooling)处理。池化/采样的方式通常有以下两种：

1. 最大池化（Max Pooling: 选择Pooling窗口中的最大值作为采样值；
2. 均值池化（Mean Pooling）: 将Pooling窗口中的所有值相加取平均，以平均值作为采样值
3. 高斯池化：借鉴高斯模糊的方法。不常用。
4. 可训练池化：使用一个训练函数$y=f(x)$。不常用。



主要使用不同的函数为输入降维。通常，最大池化层（max-pooling layer）出现在卷积层之后。池化层使用 2*2 的矩阵，以卷积层相同的方式处理图像，不过它是给图像本身降维。下面分别是使用「最大池化」和「平均池化」的示例。 

![images](/images/dl/83.png)

图像经过池化后，得到的是一系列的特征图，而多层感知器接受的输入是一个向量。因此需要将这些特征图中的像素依次取出，排列成一个向量（这个过程被称为光栅化）。 



### **全连接层（FC layer）**

#### `任务`：全连接层的每一个结点都与上一层的所有结点相连，用来把前边提取到的特征综合起来。由于其全相连的特性，一般全连接层的参数也是最多的。主要是为了分类或回归，当然也可以没有。 

其原理和我们前面所推导的DNN是一样的。

![images](/images/dl/85.png)

#### 两层之间的所有神经元都有权重连接，通常全连接层在卷积神经网络的尾部。



### **Dropout层（Dropout layer）**

#### `任务`：在模型训练时随机让网络某些隐含层节点的权重不工作，不工作的那些节点可以暂时认为不是网络结构的一部分，但是它的权重得保留下来（只是暂时不更新而已），因为下次样本输入时它可能又得工作了。主要是为了防止过拟合。

![images](/images/dl/84.png)

上图a是标准的一个全连接的神经网络，b是对a应用了dropout的结果，它会以一定的概率(dropout probability)随机的丢弃掉一些神经元。 



### **卷积神经网络之优缺点**

####  优点 　　

- 共享卷积核，对高维数据处理无压力 　　
- 无需手动选取特征，训练好权重，即得特征分类效果好 

#### 缺点 　　

- 需要调参，需要大样本量，训练最好要GPU 　　
- 物理含义不明确（也就说，我们并不知道每个卷积层到底提取到的是什么特征，而且神经网络本身就是一种难以解释的“黑箱模型”） 



### **卷积神经网络之典型CNN** 

- `LeNet`，这是最早用于数字识别的CNN 　　
- `AlexNet`， 2012 ILSVRC比赛远超第2名的CNN，比 LeNet更深，用多层小卷积层叠加替换单大卷积层。 
- `ZF Net`， 2013 ILSVRC比赛冠军 　　
- `GoogLeNet`， 2014 ILSVRC比赛冠军 　　
- `VGGNet`， 2014 ILSVRC比赛中的模型，图像识别略差于GoogLeNet，但是在很多图像转化学习问题(比如object detection)上效果奇好 



### **卷积神经网络的常用框架** 

#### **Caffe**

- ####  源于Berkeley的主流CV工具包，支持C++,python,matlab 

- #### Model Zoo中有大量预训练好的模型供使用 

#### **Torch** 

- #### Facebook用的卷积神经网络工具包 

- #### 通过时域卷积的本地接口，使用非常直观 

- #### 定义新网络层简单 

#### **TensorFlow** 

- #### Google的深度学习框架 

- #### TensorBoard可视化很方便 

- #### 数据和模型并行化好，速度快 

转载请注明：[Seven的博客](http://sevenold.github.io)